{"meta":{"version":1,"warehouse":"5.0.1"},"models":{"Asset":[{"_id":"themes\\yilia\\source\\main.0cf68a.css","path":"main.0cf68a.css","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\main.0cf68a.js","path":"main.0cf68a.js","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\mobile.992cbe.js","path":"mobile.992cbe.js","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\slider.e37972.js","path":"slider.e37972.js","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\fonts\\default-skin.b257fa.svg","path":"fonts/default-skin.b257fa.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\fonts\\iconfont.16acc2.ttf","path":"fonts/iconfont.16acc2.ttf","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\fonts\\iconfont.45d7ee.svg","path":"fonts/iconfont.45d7ee.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\fonts\\iconfont.8c627f.woff","path":"fonts/iconfont.8c627f.woff","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\fonts\\iconfont.b322fa.eot","path":"fonts/iconfont.b322fa.eot","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\fonts\\tooltip.4004ff.svg","path":"fonts/tooltip.4004ff.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\0.png","path":"img/0.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\default-skin.png","path":"img/default-skin.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\head.png","path":"img/head.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\preloader.gif","path":"img/preloader.gif","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\scrollbar_arrow.png","path":"img/scrollbar_arrow.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\hexo\\1.png","path":"img/hexo/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\hexo\\2.png","path":"img/hexo/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\hexo\\3.png","path":"img/hexo/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\hexo\\6.png","path":"img/hexo/6.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\hexo\\5.png","path":"img/hexo/5.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\hexo\\4.png","path":"img/hexo/4.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\hexo\\7.png","path":"img/hexo/7.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\hexo\\8.png","path":"img/hexo/8.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\2_linearlist\\1.png","path":"img/datastruct/2_linearlist/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\2_linearlist\\2.png","path":"img/datastruct/2_linearlist/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\2_linearlist\\3.png","path":"img/datastruct/2_linearlist/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\2_linearlist\\4.png","path":"img/datastruct/2_linearlist/4.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\2_linearlist\\5.png","path":"img/datastruct/2_linearlist/5.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\1_extract\\1.png","path":"img/datastruct/1_extract/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\1_extract\\2.png","path":"img/datastruct/1_extract/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\1_extract\\4.png","path":"img/datastruct/1_extract/4.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\1_extract\\3.png","path":"img/datastruct/1_extract/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\1_extract\\5.png","path":"img/datastruct/1_extract/5.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\1_extract\\6.png","path":"img/datastruct/1_extract/6.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\1_extract\\7.png","path":"img/datastruct/1_extract/7.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\1_extract\\9.png","path":"img/datastruct/1_extract/9.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\1_extract\\8.png","path":"img/datastruct/1_extract/8.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\4_string\\1.png","path":"img/datastruct/4_string/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\4_string\\11.png","path":"img/datastruct/4_string/11.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\4_string\\2.png","path":"img/datastruct/4_string/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\4_string\\22.png","path":"img/datastruct/4_string/22.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\4_string\\33.png","path":"img/datastruct/4_string/33.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\4_string\\3.png","path":"img/datastruct/4_string/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\4_string\\4.png","path":"img/datastruct/4_string/4.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\4_string\\6.png","path":"img/datastruct/4_string/6.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\4_string\\7.png","path":"img/datastruct/4_string/7.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\4_string\\5.png","path":"img/datastruct/4_string/5.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\4_string\\8.png","path":"img/datastruct/4_string/8.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\4_string\\9.png","path":"img/datastruct/4_string/9.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\keyboard\\1.png","path":"img/experience/keyboard/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\keyboard\\2.png","path":"img/experience/keyboard/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\keyboard\\4.png","path":"img/experience/keyboard/4.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\keyboard\\3.png","path":"img/experience/keyboard/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\keyboard\\6.png","path":"img/experience/keyboard/6.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\keyboard\\5.png","path":"img/experience/keyboard/5.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\keyboard\\7.png","path":"img/experience/keyboard/7.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\keyboard\\9.png","path":"img/experience/keyboard/9.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\keyboard\\8.png","path":"img/experience/keyboard/8.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\website\\1.png","path":"img/experience/website/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\website\\2.png","path":"img/experience/website/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\website\\3.png","path":"img/experience/website/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\linux\\install\\1.png","path":"img/linux/install/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\linux\\install\\2.png","path":"img/linux/install/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\linux\\install\\3.png","path":"img/linux/install/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\linux\\commend\\4.png","path":"img/linux/commend/4.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\hexo\\config\\2.png","path":"img/hexo/config/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\hexo\\config\\1.png","path":"img/hexo/config/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\linux\\commend\\5.png","path":"img/linux/commend/5.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\linux\\usage\\1.png","path":"img/linux/usage/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\linux\\commend\\7.png","path":"img/linux/commend/7.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\linux\\commend\\6.png","path":"img/linux/commend/6.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\linux\\usage\\10.png","path":"img/linux/usage/10.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\linux\\usage\\4.png","path":"img/linux/usage/4.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\linux\\usage\\3.png","path":"img/linux/usage/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\linux\\usage\\2.png","path":"img/linux/usage/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\linux\\usage\\5.png","path":"img/linux/usage/5.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\linux\\usage\\7.png","path":"img/linux/usage/7.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\linux\\usage\\6.png","path":"img/linux/usage/6.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\linux\\user\\8.png","path":"img/linux/user/8.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\linux\\usage\\9.png","path":"img/linux/usage/9.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\linux\\usage\\8.png","path":"img/linux/usage/8.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\1.png","path":"img/java/produce_practice/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\10.png","path":"img/java/produce_practice/10.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\11.png","path":"img/java/produce_practice/11.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\13.png","path":"img/java/produce_practice/13.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\12.png","path":"img/java/produce_practice/12.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\14.png","path":"img/java/produce_practice/14.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\16.png","path":"img/java/produce_practice/16.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\15.png","path":"img/java/produce_practice/15.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\17.png","path":"img/java/produce_practice/17.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\18.png","path":"img/java/produce_practice/18.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\19.png","path":"img/java/produce_practice/19.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\20.png","path":"img/java/produce_practice/20.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\21.png","path":"img/java/produce_practice/21.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\23.png","path":"img/java/produce_practice/23.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\3.png","path":"img/java/produce_practice/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\35.png","path":"img/java/produce_practice/35.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\4.png","path":"img/java/produce_practice/4.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\5.png","path":"img/java/produce_practice/5.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\7.png","path":"img/java/produce_practice/7.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\6.png","path":"img/java/produce_practice/6.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\8.png","path":"img/java/produce_practice/8.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\9.png","path":"img/java/produce_practice/9.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\2_linearlist\\doublelinklist\\1.png","path":"img/datastruct/2_linearlist/doublelinklist/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\2_linearlist\\doublelinklist\\2.png","path":"img/datastruct/2_linearlist/doublelinklist/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\2_linearlist\\doublelinklist\\3.png","path":"img/datastruct/2_linearlist/doublelinklist/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\2_linearlist\\doublelinklist\\4.png","path":"img/datastruct/2_linearlist/doublelinklist/4.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\2_linearlist\\doublelinklist\\5.png","path":"img/datastruct/2_linearlist/doublelinklist/5.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\2_linearlist\\linklist\\1.png","path":"img/datastruct/2_linearlist/linklist/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\2_linearlist\\linklist\\2.png","path":"img/datastruct/2_linearlist/linklist/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\2_linearlist\\linklist\\3.png","path":"img/datastruct/2_linearlist/linklist/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\2_linearlist\\linklist\\4.png","path":"img/datastruct/2_linearlist/linklist/4.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\2_linearlist\\linklist\\5.png","path":"img/datastruct/2_linearlist/linklist/5.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\2_linearlist\\linklist\\6.png","path":"img/datastruct/2_linearlist/linklist/6.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\exam\\1.png","path":"img/datastruct/3_stack/exam/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\exam\\2.png","path":"img/datastruct/3_stack/exam/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\exam\\3.png","path":"img/datastruct/3_stack/exam/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\exam\\4.png","path":"img/datastruct/3_stack/exam/4.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\exam\\5.png","path":"img/datastruct/3_stack/exam/5.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\exam\\6.png","path":"img/datastruct/3_stack/exam/6.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\queue\\1.png","path":"img/datastruct/3_stack/queue/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\queue\\2.png","path":"img/datastruct/3_stack/queue/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\queue\\3.png","path":"img/datastruct/3_stack/queue/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\queue\\4.png","path":"img/datastruct/3_stack/queue/4.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\queue\\5.png","path":"img/datastruct/3_stack/queue/5.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\matrix\\1.png","path":"img/datastruct/3_stack/matrix/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\matrix\\2.png","path":"img/datastruct/3_stack/matrix/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\matrix\\3.png","path":"img/datastruct/3_stack/matrix/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\matrix\\4.png","path":"img/datastruct/3_stack/matrix/4.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\matrix\\5.png","path":"img/datastruct/3_stack/matrix/5.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\matrix\\6.png","path":"img/datastruct/3_stack/matrix/6.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\matrix\\7.png","path":"img/datastruct/3_stack/matrix/7.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\matrix\\8.png","path":"img/datastruct/3_stack/matrix/8.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\matrix\\9.png","path":"img/datastruct/3_stack/matrix/9.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\stack\\1.png","path":"img/datastruct/3_stack/stack/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\stack\\2.png","path":"img/datastruct/3_stack/stack/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\stack\\3.png","path":"img/datastruct/3_stack/stack/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\stack\\4.png","path":"img/datastruct/3_stack/stack/4.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\1.png","path":"img/datastruct/5_tree/bintree/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\10.png","path":"img/datastruct/5_tree/bintree/10.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\11.png","path":"img/datastruct/5_tree/bintree/11.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\12.png","path":"img/datastruct/5_tree/bintree/12.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\13.png","path":"img/datastruct/5_tree/bintree/13.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\14.png","path":"img/datastruct/5_tree/bintree/14.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\15.png","path":"img/datastruct/5_tree/bintree/15.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\16.png","path":"img/datastruct/5_tree/bintree/16.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\17.png","path":"img/datastruct/5_tree/bintree/17.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\2.png","path":"img/datastruct/5_tree/bintree/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\18.png","path":"img/datastruct/5_tree/bintree/18.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\19.png","path":"img/datastruct/5_tree/bintree/19.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\20.png","path":"img/datastruct/5_tree/bintree/20.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\21.png","path":"img/datastruct/5_tree/bintree/21.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\22.png","path":"img/datastruct/5_tree/bintree/22.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\3.png","path":"img/datastruct/5_tree/bintree/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\4.png","path":"img/datastruct/5_tree/bintree/4.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\5.png","path":"img/datastruct/5_tree/bintree/5.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\6.png","path":"img/datastruct/5_tree/bintree/6.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\7.png","path":"img/datastruct/5_tree/bintree/7.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\9.png","path":"img/datastruct/5_tree/bintree/9.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\8.png","path":"img/datastruct/5_tree/bintree/8.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\hafmantree\\1.png","path":"img/datastruct/5_tree/hafmantree/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\set\\1.png","path":"img/datastruct/5_tree/set/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\hafmantree\\2.png","path":"img/datastruct/5_tree/hafmantree/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\hafmantree\\3.png","path":"img/datastruct/5_tree/hafmantree/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\hafmantree\\4.png","path":"img/datastruct/5_tree/hafmantree/4.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\set\\2.png","path":"img/datastruct/5_tree/set/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\set\\4.png","path":"img/datastruct/5_tree/set/4.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\set\\3.png","path":"img/datastruct/5_tree/set/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\tree\\1.png","path":"img/datastruct/5_tree/tree/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\set\\5.png","path":"img/datastruct/5_tree/set/5.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\tree\\2.png","path":"img/datastruct/5_tree/tree/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\tree\\3.png","path":"img/datastruct/5_tree/tree/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\treesave\\1.png","path":"img/datastruct/5_tree/treesave/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\treesave\\3.png","path":"img/datastruct/5_tree/treesave/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\treesave\\2.png","path":"img/datastruct/5_tree/treesave/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\treesave\\4.png","path":"img/datastruct/5_tree/treesave/4.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\treesave\\5.png","path":"img/datastruct/5_tree/treesave/5.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\AVL\\1.png","path":"img/datastruct/7_search/AVL/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\treesave\\6.png","path":"img/datastruct/5_tree/treesave/6.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\treesave\\8.png","path":"img/datastruct/5_tree/treesave/8.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\treesave\\7.png","path":"img/datastruct/5_tree/treesave/7.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\AVL\\2.png","path":"img/datastruct/7_search/AVL/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\AVL\\3.png","path":"img/datastruct/7_search/AVL/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\AVL\\4.png","path":"img/datastruct/7_search/AVL/4.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\AVL\\6.png","path":"img/datastruct/7_search/AVL/6.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\AVL\\5.png","path":"img/datastruct/7_search/AVL/5.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\BST\\1.png","path":"img/datastruct/7_search/BST/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\AVL\\7.png","path":"img/datastruct/7_search/AVL/7.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\B\\1.png","path":"img/datastruct/7_search/B/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\BST\\2.png","path":"img/datastruct/7_search/BST/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\BST\\3.png","path":"img/datastruct/7_search/BST/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\search\\1.png","path":"img/datastruct/7_search/search/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\BST\\身份证1.jpg","path":"img/datastruct/7_search/BST/身份证1.jpg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\BST\\4.png","path":"img/datastruct/7_search/BST/4.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\BST\\身份证2.jpg","path":"img/datastruct/7_search/BST/身份证2.jpg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\search\\2.png","path":"img/datastruct/7_search/search/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\search\\3.png","path":"img/datastruct/7_search/search/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\search\\4.png","path":"img/datastruct/7_search/search/4.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\search\\5.png","path":"img/datastruct/7_search/search/5.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\search\\6.png","path":"img/datastruct/7_search/search/6.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\search\\7.png","path":"img/datastruct/7_search/search/7.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\search\\8.png","path":"img/datastruct/7_search/search/8.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\red\\1.png","path":"img/datastruct/7_search/red/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\red\\3.png","path":"img/datastruct/7_search/red/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\red\\2.png","path":"img/datastruct/7_search/red/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\red\\4.png","path":"img/datastruct/7_search/red/4.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\generatetree\\1.png","path":"img/datastruct/6_graph/generatetree/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\red\\5.png","path":"img/datastruct/7_search/red/5.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\generatetree\\10.png","path":"img/datastruct/6_graph/generatetree/10.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\generatetree\\3.png","path":"img/datastruct/6_graph/generatetree/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\generatetree\\2.png","path":"img/datastruct/6_graph/generatetree/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\generatetree\\5.png","path":"img/datastruct/6_graph/generatetree/5.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\generatetree\\4.png","path":"img/datastruct/6_graph/generatetree/4.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\generatetree\\6.png","path":"img/datastruct/6_graph/generatetree/6.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\DAG\\1.png","path":"img/datastruct/6_graph/DAG/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\generatetree\\7.png","path":"img/datastruct/6_graph/generatetree/7.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\generatetree\\8.png","path":"img/datastruct/6_graph/generatetree/8.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\generatetree\\9.png","path":"img/datastruct/6_graph/generatetree/9.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\DAG\\10.png","path":"img/datastruct/6_graph/DAG/10.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\DAG\\12.png","path":"img/datastruct/6_graph/DAG/12.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\DAG\\11.png","path":"img/datastruct/6_graph/DAG/11.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\DAG\\13.png","path":"img/datastruct/6_graph/DAG/13.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\DAG\\2.png","path":"img/datastruct/6_graph/DAG/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\DAG\\3.png","path":"img/datastruct/6_graph/DAG/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\DAG\\5.png","path":"img/datastruct/6_graph/DAG/5.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\DAG\\4.png","path":"img/datastruct/6_graph/DAG/4.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\DAG\\6.png","path":"img/datastruct/6_graph/DAG/6.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\DAG\\7.png","path":"img/datastruct/6_graph/DAG/7.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\DAG\\8.png","path":"img/datastruct/6_graph/DAG/8.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\DAG\\9.png","path":"img/datastruct/6_graph/DAG/9.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\10.png","path":"img/datastruct/6_graph/graph/10.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\1.png","path":"img/datastruct/6_graph/graph/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\11.png","path":"img/datastruct/6_graph/graph/11.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\14.png","path":"img/datastruct/6_graph/graph/14.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\13.png","path":"img/datastruct/6_graph/graph/13.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\12.png","path":"img/datastruct/6_graph/graph/12.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\15.png","path":"img/datastruct/6_graph/graph/15.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\16.png","path":"img/datastruct/6_graph/graph/16.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\17.png","path":"img/datastruct/6_graph/graph/17.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\18.png","path":"img/datastruct/6_graph/graph/18.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\2.png","path":"img/datastruct/6_graph/graph/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\19.png","path":"img/datastruct/6_graph/graph/19.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\20.png","path":"img/datastruct/6_graph/graph/20.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\21.png","path":"img/datastruct/6_graph/graph/21.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\22.png","path":"img/datastruct/6_graph/graph/22.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\23.png","path":"img/datastruct/6_graph/graph/23.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\26.png","path":"img/datastruct/6_graph/graph/26.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\24.png","path":"img/datastruct/6_graph/graph/24.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\25.png","path":"img/datastruct/6_graph/graph/25.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\3.png","path":"img/datastruct/6_graph/graph/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\4.png","path":"img/datastruct/6_graph/graph/4.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\5.png","path":"img/datastruct/6_graph/graph/5.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\6.png","path":"img/datastruct/6_graph/graph/6.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\8.png","path":"img/datastruct/6_graph/graph/8.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\7.png","path":"img/datastruct/6_graph/graph/7.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\9.png","path":"img/datastruct/6_graph/graph/9.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\shortestpath\\10.png","path":"img/datastruct/6_graph/shortestpath/10.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\shortestpath\\1.png","path":"img/datastruct/6_graph/shortestpath/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\shortestpath\\12.png","path":"img/datastruct/6_graph/shortestpath/12.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\shortestpath\\2.png","path":"img/datastruct/6_graph/shortestpath/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\shortestpath\\13.png","path":"img/datastruct/6_graph/shortestpath/13.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\shortestpath\\11.png","path":"img/datastruct/6_graph/shortestpath/11.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\shortestpath\\14.png","path":"img/datastruct/6_graph/shortestpath/14.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\shortestpath\\3.png","path":"img/datastruct/6_graph/shortestpath/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\shortestpath\\4.png","path":"img/datastruct/6_graph/shortestpath/4.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\shortestpath\\5.png","path":"img/datastruct/6_graph/shortestpath/5.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\shortestpath\\6.png","path":"img/datastruct/6_graph/shortestpath/6.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\shortestpath\\7.png","path":"img/datastruct/6_graph/shortestpath/7.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\shortestpath\\8.png","path":"img/datastruct/6_graph/shortestpath/8.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\shortestpath\\9.png","path":"img/datastruct/6_graph/shortestpath/9.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\app\\lrc_to_smi\\3.png","path":"img/experience/app/lrc_to_smi/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\app\\lrc_to_smi\\1.png","path":"img/experience/app/lrc_to_smi/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\app\\lrc_to_smi\\2.png","path":"img/experience/app/lrc_to_smi/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\app\\mouse\\2.png","path":"img/experience/app/mouse/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\app\\mouse\\1.png","path":"img/experience/app/mouse/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\app\\lrc_to_smi\\5.png","path":"img/experience/app/lrc_to_smi/5.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\app\\lrc_to_smi\\4.png","path":"img/experience/app/lrc_to_smi/4.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\app\\mouse\\3.png","path":"img/experience/app/mouse/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\app\\word\\1.png","path":"img/experience/app/word/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\pycharm\\link_server\\1.png","path":"img/experience/pycharm/link_server/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\pycharm\\link_server\\10.png","path":"img/experience/pycharm/link_server/10.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\pycharm\\link_server\\11.png","path":"img/experience/pycharm/link_server/11.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\pycharm\\link_server\\2.png","path":"img/experience/pycharm/link_server/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\pycharm\\link_server\\3.png","path":"img/experience/pycharm/link_server/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\pycharm\\link_server\\4.png","path":"img/experience/pycharm/link_server/4.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\pycharm\\link_server\\5.png","path":"img/experience/pycharm/link_server/5.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\pycharm\\link_server\\8.png","path":"img/experience/pycharm/link_server/8.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\pycharm\\link_server\\9.png","path":"img/experience/pycharm/link_server/9.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\pycharm\\link_server\\7.png","path":"img/experience/pycharm/link_server/7.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\pycharm\\link_server\\6.png","path":"img/experience/pycharm/link_server/6.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\vscode\\relative_path\\1.png","path":"img/experience/vscode/relative_path/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\vscode\\relative_path\\2.png","path":"img/experience/vscode/relative_path/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\windows\\ssh\\1.png","path":"img/experience/windows/ssh/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\windows\\powershell\\1.png","path":"img/experience/windows/powershell/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\windows\\ssh\\2.png","path":"img/experience/windows/ssh/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\windows\\powershell\\2.png","path":"img/experience/windows/powershell/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\windows\\ssh\\3.png","path":"img/experience/windows/ssh/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\0\\10.png","path":"img/java/produce_practice/0/10.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\0\\12.png","path":"img/java/produce_practice/0/12.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\0\\1.png","path":"img/java/produce_practice/0/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\0\\11.png","path":"img/java/produce_practice/0/11.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\0\\2.png","path":"img/java/produce_practice/0/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\0\\4.png","path":"img/java/produce_practice/0/4.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\0\\7.png","path":"img/java/produce_practice/0/7.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\0\\6.png","path":"img/java/produce_practice/0/6.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\0\\8.png","path":"img/java/produce_practice/0/8.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\0\\5.png","path":"img/java/produce_practice/0/5.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\0\\9.png","path":"img/java/produce_practice/0/9.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\2\\10.png","path":"img/java/produce_practice/2/10.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\2\\3.png","path":"img/java/produce_practice/2/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\2\\2.png","path":"img/java/produce_practice/2/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\2\\1.png","path":"img/java/produce_practice/2/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\2\\4.png","path":"img/java/produce_practice/2/4.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\2\\5.png","path":"img/java/produce_practice/2/5.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\2\\7.png","path":"img/java/produce_practice/2/7.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\2\\8.png","path":"img/java/produce_practice/2/8.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\2\\6.png","path":"img/java/produce_practice/2/6.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\2\\9.png","path":"img/java/produce_practice/2/9.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\4\\1.png","path":"img/java/produce_practice/4/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\3\\2.png","path":"img/java/produce_practice/3/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\3\\1.png","path":"img/java/produce_practice/3/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\3\\3.png","path":"img/java/produce_practice/3/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\4\\2.png","path":"img/java/produce_practice/4/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\4\\3.png","path":"img/java/produce_practice/4/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\4\\4.png","path":"img/java/produce_practice/4/4.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\4\\6.png","path":"img/java/produce_practice/4/6.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\4\\5.png","path":"img/java/produce_practice/4/5.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\4\\7.png","path":"img/java/produce_practice/4/7.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\4\\8.png","path":"img/java/produce_practice/4/8.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\5\\1.png","path":"img/java/produce_practice/5/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\5\\3.png","path":"img/java/produce_practice/5/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\5\\2.png","path":"img/java/produce_practice/5/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\6\\1.png","path":"img/java/produce_practice/6/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\6\\0.png","path":"img/java/produce_practice/6/0.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\6\\2.png","path":"img/java/produce_practice/6/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\6\\3.png","path":"img/java/produce_practice/6/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\app\\word\\page\\0.png","path":"img/experience/app/word/page/0.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\app\\word\\page\\1.png","path":"img/experience/app/word/page/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\app\\word\\page\\2.png","path":"img/experience/app/word/page/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\app\\word\\page\\3.png","path":"img/experience/app/word/page/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\app\\word\\page\\5.png","path":"img/experience/app/word/page/5.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\app\\word\\page\\4.png","path":"img/experience/app/word/page/4.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\app\\word\\page\\6.png","path":"img/experience/app/word/page/6.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\CDM\\1.png","path":"img/deeplearning/paper/SR/CDM/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\CDM\\2.png","path":"img/deeplearning/paper/SR/CDM/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\CDM\\3.png","path":"img/deeplearning/paper/SR/CDM/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\app\\word\\to_pdf\\1.png","path":"img/experience/app/word/to_pdf/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\experience\\app\\word\\to_pdf\\2.png","path":"img/experience/app/word/to_pdf/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SRDiff\\1.png","path":"img/deeplearning/paper/SR/SRDiff/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SwinIR\\1.png","path":"img/deeplearning/paper/SR/SwinIR/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SwinIR\\10.png","path":"img/deeplearning/paper/SR/SwinIR/10.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SRDiff\\2.png","path":"img/deeplearning/paper/SR/SRDiff/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SRDiff\\3.png","path":"img/deeplearning/paper/SR/SRDiff/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SwinIR\\11.png","path":"img/deeplearning/paper/SR/SwinIR/11.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SwinIR\\12.png","path":"img/deeplearning/paper/SR/SwinIR/12.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SwinIR\\2.png","path":"img/deeplearning/paper/SR/SwinIR/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SwinIR\\13.png","path":"img/deeplearning/paper/SR/SwinIR/13.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SwinIR\\3.png","path":"img/deeplearning/paper/SR/SwinIR/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SwinIR\\4.png","path":"img/deeplearning/paper/SR/SwinIR/4.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SwinIR\\5.png","path":"img/deeplearning/paper/SR/SwinIR/5.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SwinIR\\7.png","path":"img/deeplearning/paper/SR/SwinIR/7.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SwinIR\\8.png","path":"img/deeplearning/paper/SR/SwinIR/8.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SwinIR\\6.png","path":"img/deeplearning/paper/SR/SwinIR/6.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SwinIR\\9.png","path":"img/deeplearning/paper/SR/SwinIR/9.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\11.png","path":"img/deeplearning/paper/SR/HAT/11.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\1.png","path":"img/deeplearning/paper/SR/HAT/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\3.png","path":"img/deeplearning/paper/SR/HAT/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\4.png","path":"img/deeplearning/paper/SR/HAT/4.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\2.png","path":"img/deeplearning/paper/SR/HAT/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\5.png","path":"img/deeplearning/paper/SR/HAT/5.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\7.png","path":"img/deeplearning/paper/SR/HAT/7.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\9.png","path":"img/deeplearning/paper/SR/HAT/9.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\6.png","path":"img/deeplearning/paper/SR/HAT/6.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\8.png","path":"img/deeplearning/paper/SR/HAT/8.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\2_linear_neural_network\\2_linear_regression_realize_files\\2_linear_regression_realize_3_2.svg","path":"img/deeplearning/code/pytorch/2_linear_neural_network/2_linear_regression_realize_files/2_linear_regression_realize_3_2.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\2_linear_neural_network\\4img\\1.png","path":"img/deeplearning/code/pytorch/2_linear_neural_network/4img/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\2_linear_neural_network\\4img\\2.png","path":"img/deeplearning/code/pytorch/2_linear_neural_network/4img/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\2_linear_neural_network\\4img\\3.png","path":"img/deeplearning/code/pytorch/2_linear_neural_network/4img/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\2_linear_neural_network\\6_softmax_realize_files\\6_softmax_realize_13_0.svg","path":"img/deeplearning/code/pytorch/2_linear_neural_network/6_softmax_realize_files/6_softmax_realize_13_0.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\2_linear_neural_network\\6_softmax_realize_files\\6_softmax_realize_15_0.svg","path":"img/deeplearning/code/pytorch/2_linear_neural_network/6_softmax_realize_files/6_softmax_realize_15_0.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\2_linear_neural_network\\1_linear_regression_files\\1.png","path":"img/deeplearning/code/pytorch/2_linear_neural_network/1_linear_regression_files/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\2_linear_neural_network\\1_linear_regression_files\\1_linear_regression_4_0.svg","path":"img/deeplearning/code/pytorch/2_linear_neural_network/1_linear_regression_files/1_linear_regression_4_0.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\2_linear_neural_network\\5_image_classification_dataset_files\\5_image_classification_dataset_3_1.svg","path":"img/deeplearning/code/pytorch/2_linear_neural_network/5_image_classification_dataset_files/5_image_classification_dataset_3_1.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\2_linear_neural_network\\7_softmax_realize_simple_files\\7_softmax_realize_simple_5_0.svg","path":"img/deeplearning/code/pytorch/2_linear_neural_network/7_softmax_realize_simple_files/7_softmax_realize_simple_5_0.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\1_prepare\\6_probability_files\\6_probability_1_2.svg","path":"img/deeplearning/code/pytorch/1_prepare/6_probability_files/6_probability_1_2.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\1_prepare\\4_calculus_files\\4_calculus_1_1.svg","path":"img/deeplearning/code/pytorch/1_prepare/4_calculus_files/4_calculus_1_1.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\1_prepare\\4_calculus_files\\4_calculus_3_1.svg","path":"img/deeplearning/code/pytorch/1_prepare/4_calculus_files/4_calculus_3_1.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\5_convolution\\3_padding\\1.png","path":"img/deeplearning/code/pytorch/5_convolution/3_padding/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\5_convolution\\3_padding\\2.png","path":"img/deeplearning/code/pytorch/5_convolution/3_padding/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\5_convolution\\2_picture_cov\\1.png","path":"img/deeplearning/code/pytorch/5_convolution/2_picture_cov/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\5_convolution\\6_LeNet_files\\6_LeNet_3_1.svg","path":"img/deeplearning/code/pytorch/5_convolution/6_LeNet_files/6_LeNet_3_1.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\5_convolution\\4_mult_channel\\1.png","path":"img/deeplearning/code/pytorch/5_convolution/4_mult_channel/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\5_convolution\\5_polling\\1.png","path":"img/deeplearning/code/pytorch/5_convolution/5_polling/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\5_convolution\\4_mult_channel\\2.png","path":"img/deeplearning/code/pytorch/5_convolution/4_mult_channel/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\5_convolution\\6_LeNet\\1.png","path":"img/deeplearning/code/pytorch/5_convolution/6_LeNet/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\10_kaggle_predict_price_files\\10_kaggle_predict_price_15_1.svg","path":"img/deeplearning/code/pytorch/3_mlp/10_kaggle_predict_price_files/10_kaggle_predict_price_15_1.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\1img\\1.png","path":"img/deeplearning/code/pytorch/3_mlp/1img/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\10_kaggle_predict_price_files\\10_kaggle_predict_price_17_1.svg","path":"img/deeplearning/code/pytorch/3_mlp/10_kaggle_predict_price_files/10_kaggle_predict_price_17_1.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\1_mlp_files\\1_mlp_3_0.svg","path":"img/deeplearning/code/pytorch/3_mlp/1_mlp_files/1_mlp_3_0.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\1_mlp_files\\1_mlp_5_0.svg","path":"img/deeplearning/code/pytorch/3_mlp/1_mlp_files/1_mlp_5_0.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\1_mlp_files\\1_mlp_1_0.svg","path":"img/deeplearning/code/pytorch/3_mlp/1_mlp_files/1_mlp_1_0.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\3_mlp_realize_simple_files\\3_mlp_realize_simple_1_0.svg","path":"img/deeplearning/code/pytorch/3_mlp/3_mlp_realize_simple_files/3_mlp_realize_simple_1_0.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\2_mlp_realize_files\\2_mlp_realize_1_0.svg","path":"img/deeplearning/code/pytorch/3_mlp/2_mlp_realize_files/2_mlp_realize_1_0.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\2_mlp_realize_files\\2_mlp_realize_1_1.svg","path":"img/deeplearning/code/pytorch/3_mlp/2_mlp_realize_files/2_mlp_realize_1_1.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\4_overfitting_files\\4_overfitting_11_1.svg","path":"img/deeplearning/code/pytorch/3_mlp/4_overfitting_files/4_overfitting_11_1.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\4_overfitting_files\\4_overfitting_9_1.svg","path":"img/deeplearning/code/pytorch/3_mlp/4_overfitting_files/4_overfitting_9_1.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\4_overfitting_files\\4_overfitting_7_1.svg","path":"img/deeplearning/code/pytorch/3_mlp/4_overfitting_files/4_overfitting_7_1.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\5_weight_decay_files\\5_weight_decay_1_1.svg","path":"img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_1_1.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\4img\\1.png","path":"img/deeplearning/code/pytorch/3_mlp/4img/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\5_weight_decay_files\\5_weight_decay_2_1.svg","path":"img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_2_1.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\5_weight_decay_files\\5_weight_decay_4_1.svg","path":"img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_4_1.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\5_weight_decay_files\\5_weight_decay_5_1.svg","path":"img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_5_1.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\6_deopout_files\\6_deopout_3_0.svg","path":"img/deeplearning/code/pytorch/3_mlp/6_deopout_files/6_deopout_3_0.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\6_deopout_files\\6_deopout_5_0.svg","path":"img/deeplearning/code/pytorch/3_mlp/6_deopout_files/6_deopout_5_0.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\7\\1.png","path":"img/deeplearning/code/pytorch/3_mlp/7/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\6img\\1.png","path":"img/deeplearning/code/pytorch/3_mlp/6img/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\8_init_files\\8_init_2_0.svg","path":"img/deeplearning/code/pytorch/3_mlp/8_init_files/8_init_2_0.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\7_rnn\\1_seqmodel\\2.png","path":"img/deeplearning/code/pytorch/7_rnn/1_seqmodel/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\7_rnn\\1_seqmodel\\1.png","path":"img/deeplearning/code/pytorch/7_rnn/1_seqmodel/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\7_rnn\\1_seqmodel_files\\1_seqmodel_11_0.svg","path":"img/deeplearning/code/pytorch/7_rnn/1_seqmodel_files/1_seqmodel_11_0.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\7_rnn\\1_seqmodel_files\\1_seqmodel_13_0.svg","path":"img/deeplearning/code/pytorch/7_rnn/1_seqmodel_files/1_seqmodel_13_0.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\7_rnn\\1_seqmodel_files\\1_seqmodel_1_0.svg","path":"img/deeplearning/code/pytorch/7_rnn/1_seqmodel_files/1_seqmodel_1_0.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\7_rnn\\1_seqmodel_files\\1_seqmodel_9_0.svg","path":"img/deeplearning/code/pytorch/7_rnn/1_seqmodel_files/1_seqmodel_9_0.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\7_rnn\\4_rnn\\1.png","path":"img/deeplearning/code/pytorch/7_rnn/4_rnn/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\7_rnn\\4_rnn\\2.png","path":"img/deeplearning/code/pytorch/7_rnn/4_rnn/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\7_rnn\\3_lmdataset_files\\3_lmdataset_3_0.svg","path":"img/deeplearning/code/pytorch/7_rnn/3_lmdataset_files/3_lmdataset_3_0.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\7_rnn\\3_lmdataset_files\\3_lmdataset_9_0.svg","path":"img/deeplearning/code/pytorch/7_rnn/3_lmdataset_files/3_lmdataset_9_0.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\7_rnn\\3_lmdataset\\1.png","path":"img/deeplearning/code/pytorch/7_rnn/3_lmdataset/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\7_rnn\\7_backward\\1.png","path":"img/deeplearning/code/pytorch/7_rnn/7_backward/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\7_rnn\\7_backward\\2.png","path":"img/deeplearning/code/pytorch/7_rnn/7_backward/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\7_rnn\\6_rnn_sample_realize_files\\6_rnn_sample_realize_13_1.svg","path":"img/deeplearning/code/pytorch/7_rnn/6_rnn_sample_realize_files/6_rnn_sample_realize_13_1.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\7_rnn\\5_rnn_realize_files\\5_rnn_realize_27_1.svg","path":"img/deeplearning/code/pytorch/7_rnn/5_rnn_realize_files/5_rnn_realize_27_1.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\7_rnn\\5_rnn_realize_files\\5_rnn_realize_29_1.svg","path":"img/deeplearning/code/pytorch/7_rnn/5_rnn_realize_files/5_rnn_realize_29_1.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\1_AlexNet\\1.png","path":"img/deeplearning/code/pytorch/6_moden_conv/1_AlexNet/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\2_vgg\\1.png","path":"img/deeplearning/code/pytorch/6_moden_conv/2_vgg/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\1_AlexNet_files\\1_AlexNet_0_1.svg","path":"img/deeplearning/code/pytorch/6_moden_conv/1_AlexNet_files/1_AlexNet_0_1.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\2_vgg_files\\2_vgg_7_0.svg","path":"img/deeplearning/code/pytorch/6_moden_conv/2_vgg_files/2_vgg_7_0.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\3_NiN\\1.png","path":"img/deeplearning/code/pytorch/6_moden_conv/3_NiN/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\3_NiN_files\\3_NiN_7_0.svg","path":"img/deeplearning/code/pytorch/6_moden_conv/3_NiN_files/3_NiN_7_0.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\4_googleNet_files\\4_googleNet_7_1.svg","path":"img/deeplearning/code/pytorch/6_moden_conv/4_googleNet_files/4_googleNet_7_1.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\4_googlenet\\1.png","path":"img/deeplearning/code/pytorch/6_moden_conv/4_googlenet/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\4_googlenet\\2.png","path":"img/deeplearning/code/pytorch/6_moden_conv/4_googlenet/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\5_batchnormal_files\\5_batchnormal_5_1.svg","path":"img/deeplearning/code/pytorch/6_moden_conv/5_batchnormal_files/5_batchnormal_5_1.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\5_batchnormal_files\\5_batchnormal_7_1.svg","path":"img/deeplearning/code/pytorch/6_moden_conv/5_batchnormal_files/5_batchnormal_7_1.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\6_resnet\\1.png","path":"img/deeplearning/code/pytorch/6_moden_conv/6_resnet/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\6_resnet\\2.png","path":"img/deeplearning/code/pytorch/6_moden_conv/6_resnet/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\6_resnet\\3.png","path":"img/deeplearning/code/pytorch/6_moden_conv/6_resnet/3.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\6_resnet_files\\6_resnet_13_1.svg","path":"img/deeplearning/code/pytorch/6_moden_conv/6_resnet_files/6_resnet_13_1.svg","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\6_resnet\\4.png","path":"img/deeplearning/code/pytorch/6_moden_conv/6_resnet/4.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\7_densenet\\1.png","path":"img/deeplearning/code/pytorch/6_moden_conv/7_densenet/1.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\7_densenet\\2.png","path":"img/deeplearning/code/pytorch/6_moden_conv/7_densenet/2.png","modified":0,"renderable":1},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\7_DenseNet_files\\7_DenseNet_17_1.svg","path":"img/deeplearning/code/pytorch/6_moden_conv/7_DenseNet_files/7_DenseNet_17_1.svg","modified":0,"renderable":1}],"Cache":[{"_id":"source\\_drafts\\1_DDPM.md","hash":"7671adbb402c865080617b04082d61c62320c72b","modified":1698118711023},{"_id":"source\\_drafts\\progress1.md","hash":"c51366bc76ec2d0499a9c5ef02c18724cb53460a","modified":1677147418035},{"_id":"source\\_drafts\\yilia配置.md","hash":"652c0eaa09d219c5ba9cd082b86c3cf5eb632811","modified":1708574709517},{"_id":"source\\_posts\\hello-world.md","hash":"7d98d6592de80fdcd2949bd7401cec12afd98cdf","modified":1714977193320},{"_id":"source\\categories\\index.md","hash":"736c605a6585b78360deb7eaea3451f3e4b4eeac","modified":1690344261211},{"_id":"source\\tags\\index.md","hash":"a9b6dfbb10c311720b9d8e66804f219f91f2bd19","modified":1698055450779},{"_id":"source\\_posts\\hexo\\hello-world.md","hash":"795449ef0e6e4d87eb446e60d1e6857383499e1f","modified":1698054930261},{"_id":"source\\_posts\\linux\\Linux_0安装.md","hash":"4a96d12c9614f243d93dfdbef57e51e471fc2a10","modified":1714974729458},{"_id":"source\\_posts\\hexo\\hexo搭建.md","hash":"6dc7bd61ada157d87a8047f250085448fd922a5c","modified":1714978206927},{"_id":"source\\_posts\\linux\\Linux_1.基础命令.md","hash":"4ed5491744fb1d95b0ec50b6351957bd27cb81b2","modified":1714974729458},{"_id":"source\\_posts\\wechartapp\\weixin.md","hash":"157f8b711b8fce6b443086f5a9784d5783100afc","modified":1705552596694},{"_id":"source\\_posts\\linux\\Linux_2.用户和权限.md","hash":"81f11973eb778dc69676b61bbcfe752690f81b39","modified":1714974729459},{"_id":"source\\_posts\\datastruct\\2_linearlist\\1_linearlist.md","hash":"d7bd03e466cf991023eb7845dfe26b0e4b0500db","modified":1698062554209},{"_id":"source\\_posts\\datastruct\\2_linearlist\\2_linklist.md","hash":"2cd1a0ab4f4e81f63cc7cf9a3ae20aa1f8b5d32e","modified":1698062568746},{"_id":"source\\_posts\\datastruct\\2_linearlist\\3_doublelinklist.md","hash":"eb1f564dcacdc514d76434c6b6841babd720c319","modified":1698062620512},{"_id":"source\\_posts\\datastruct\\1_extract\\1_基础.md","hash":"01d0ac070c51a281402c20ae1d8c3454260985fa","modified":1698062355750},{"_id":"source\\_posts\\datastruct\\4_string\\1_string.md","hash":"192e8c0c9fb2abf12f6129446e3f5d9872bd7623","modified":1698063339435},{"_id":"source\\_posts\\datastruct\\3_stack\\1_stack.md","hash":"c5bac3f9b0df15b30b34a5eaa912c1f6066c2317","modified":1698067215453},{"_id":"source\\_posts\\datastruct\\3_stack\\2_queue.md","hash":"c5e8e2879013a17a3c36d453b8e50b97d3fba922","modified":1698062900396},{"_id":"source\\_posts\\datastruct\\7_search\\1_search.md","hash":"a217f11a8e1b8e16c5941a6b58ecdefc86346bd9","modified":1698065418625},{"_id":"source\\_posts\\datastruct\\3_stack\\4_matrix.md","hash":"801bbb8d81c49ffbe6cfe9e56c1cd636fb687fd6","modified":1698062953828},{"_id":"source\\_posts\\datastruct\\3_stack\\3_exam.md","hash":"09d1f7e08b9e0736d823003453848e9ebd61362e","modified":1698062918400},{"_id":"source\\_posts\\datastruct\\7_search\\2_BST.md","hash":"05269080838cf628f5ca643c6746aaf57f1161fc","modified":1698065629690},{"_id":"source\\_posts\\datastruct\\7_search\\4_redblacktree.md","hash":"ee66caf5d73fff95bfafd6e3505e001090edc9e7","modified":1714978001822},{"_id":"source\\_posts\\datastruct\\7_search\\5_Btree.md","hash":"0e5f235316ec62a4b9c93208aa1b059c1cd74f40","modified":1714977993873},{"_id":"source\\_posts\\datastruct\\7_search\\3_AVL.md","hash":"11a699c50548f43467f0fd08e0259ec9cec2dd45","modified":1714978013793},{"_id":"source\\_posts\\datastruct\\5_tree\\2_bintree.md","hash":"f080a883557453884928b4fed627e43a581276a7","modified":1698063380694},{"_id":"source\\_posts\\datastruct\\5_tree\\1_tree.md","hash":"09e75c758b997fa656e0bec274410e5f9345e4ff","modified":1698063355713},{"_id":"source\\_posts\\datastruct\\5_tree\\4_Hafmantree.md","hash":"56c2c29ee3d30867eb2534a0f2176fd9a1d8904d","modified":1698063679991},{"_id":"source\\_posts\\datastruct\\5_tree\\3_treesave.md","hash":"b3c8097e12215a278d82a08f9bbddfdb225b4bc2","modified":1698063608035},{"_id":"source\\_posts\\datastruct\\5_tree\\5_set.md","hash":"96744e1e200311a3c4d5e0e21682916aeb9d3982","modified":1698063800125},{"_id":"source\\_posts\\datastruct\\6_graph\\2_generatetree.md","hash":"0145a02d79dabab66ccf45bd6e7bb7af5af70d20","modified":1698064235357},{"_id":"source\\_posts\\datastruct\\6_graph\\1_graph.md","hash":"8d4c50d24df9889a112b4a2bfd938c49c76b4b6f","modified":1698064007885},{"_id":"source\\_posts\\datastruct\\6_graph\\4_DAG.md","hash":"555bfaac97a76d0724658f8abc7f1b65ff0fd8cd","modified":1698064476586},{"_id":"source\\_posts\\datastruct\\6_graph\\3_shortestpath.md","hash":"ead39c8afafeaf71678b4bb5bd408f50d7e639ad","modified":1698064227797},{"_id":"source\\_posts\\experience\\app\\lrc_to_smi.md","hash":"0a4d033a946fcde5bb54ac9dc0eb8eb6108d4790","modified":1709810352644},{"_id":"source\\_posts\\algorithm\\1_base\\1_base.md","hash":"ef8baf80a47af49838cf57dd0fb57b1cfe327d26","modified":1714974729448},{"_id":"source\\_posts\\deeplearning\\paper\\SR\\CDM.md","hash":"3cd0fd968738243ef330a7670480b29c88cbfeec","modified":1706628964579},{"_id":"source\\_posts\\experience\\vscode\\vscode.md","hash":"0a6f55881a037b487675a371be5f10a496d8d756","modified":1713848215865},{"_id":"source\\_posts\\experience\\pycharm\\link_server.md","hash":"c175d38a524b054a0cb6bf2f950332099c9744e2","modified":1709810324296},{"_id":"source\\_posts\\deeplearning\\paper\\SR\\Resshift.md","hash":"619af24a5769247cb5379c11e920a0031d071464","modified":1706628970587},{"_id":"source\\_posts\\deeplearning\\paper\\SR\\HAT.md","hash":"5475e08292bd4659feca64e0d8a5fe9e684d7cf2","modified":1706267723399},{"_id":"source\\_posts\\deeplearning\\paper\\SR\\SRDiff.md","hash":"ea222f8466f4cccd9278252f314f227de07baee7","modified":1706628975672},{"_id":"source\\_posts\\deeplearning\\paper\\SR\\SwinIR.md","hash":"303b0c8ae5c9a728afb0e2c81290a381f848f416","modified":1706625438986},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\1_prepare\\1_dataoperation.ipynb","hash":"5681374ba622e52c661610d4ceb4348aee47bccc","modified":1714640140796},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\1_prepare\\1_dataoperation.md","hash":"e1df5dae2d775a1204637af4c760460d8498f22a","modified":1714640144030},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\1_prepare\\2_preprocessing.ipynb","hash":"8b1dcb91ffc8c714e2b53700394a24f322afe7ee","modified":1714640154095},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\1_prepare\\2_preprocessing.md","hash":"963c77bb000ff6a29a188d76c80315e71514000d","modified":1714640161084},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\1_prepare\\3_linearalgebra.ipynb","hash":"00491e3bfc6ca94605b4c1bd35223a3153e50e64","modified":1714640169148},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\data\\house_tiny.csv","hash":"b32168bc42e12b5594eb2b0a8cae6f77c459a72e","modified":1703739275650},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\1_prepare\\.ipynb_checkpoints\\2_preprocessing-checkpoint.ipynb","hash":"9bc3b4afb0145f5795387830e5376960d315e439","modified":1703740534185},{"_id":"themes\\yilia\\layout\\_partial\\toc.ejs","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1690258400287},{"_id":"themes\\yilia\\source-src\\css\\article.scss","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1690363683164},{"_id":"themes\\yilia\\.babelrc","hash":"b1b76475ac17dc9e2fa50af96c9e31eea2d0f2b4","modified":1714977524056},{"_id":"themes\\yilia\\.editorconfig","hash":"da6d022b8f4d9c961e2f8f80677e92af8de0db4d","modified":1714977524056},{"_id":"themes\\yilia\\.eslintignore","hash":"df0a50b13cc00acb749226fee3cee6e0351fb1d9","modified":1714977524057},{"_id":"themes\\yilia\\.eslintrc.js","hash":"5696ae049de010ed3786768b0c359f14c05b5ec6","modified":1714977524057},{"_id":"themes\\yilia\\.gitattributes","hash":"e0f24dceeb1e6878a1dd9b01a2b9df1bc037a867","modified":1714977524058},{"_id":"themes\\yilia\\.gitignore","hash":"9c4b7d27a1e3e5efa0c8ed143a032a85d586b03b","modified":1714977524058},{"_id":"themes\\yilia\\_config.yml","hash":"ba7d701c3b0ae07b725d5a7070e40a517f8b1814","modified":1708846442559},{"_id":"themes\\yilia\\README.md","hash":"1bf755806af9d8874bd22e1abbdaaa24328ef4dc","modified":1714977524058},{"_id":"themes\\yilia\\webpack.config.js","hash":"05ba46a4ae744272f5312e684928910dccad3755","modified":1714977524103},{"_id":"themes\\yilia\\package.json","hash":"367cb9579d35968a942c243ab248a5f5ebfaf462","modified":1714977524075},{"_id":"themes\\yilia\\layout\\archive.ejs","hash":"2703b07cc8ac64ae46d1d263f4653013c7e1666b","modified":1690258400287},{"_id":"themes\\yilia\\layout\\index.ejs","hash":"ec498c6c0606acde997ce195dad97b267418d980","modified":1690258400287},{"_id":"themes\\yilia\\layout\\categories.ejs","hash":"9041a223a9965188cd5c1a893b3e9e2f1cf67223","modified":1697888836478},{"_id":"themes\\yilia\\layout\\layout.ejs","hash":"b471ab706d48e0be3f783eab1c94bf5878ef5a94","modified":1690258400287},{"_id":"themes\\yilia\\layout\\category.ejs","hash":"765426a9c8236828dc34759e604cc2c52292835a","modified":1690258400287},{"_id":"themes\\yilia\\languages\\fr.yml","hash":"b4be1c1592a72012e48df2b3ec41cc9685573e50","modified":1690258400271},{"_id":"themes\\yilia\\languages\\default.yml","hash":"f26a34a7983d4bc17c65c7f0f14da598e62ce66d","modified":1690258400271},{"_id":"themes\\yilia\\layout\\post.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1690258400287},{"_id":"themes\\yilia\\layout\\page.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1690258400287},{"_id":"themes\\yilia\\layout\\tag.ejs","hash":"eaa7b4ccb2ca7befb90142e4e68995fb1ea68b2e","modified":1690258400287},{"_id":"themes\\yilia\\languages\\nl.yml","hash":"3d82ec703d0b3287739d7cb4750a715ae83bfcb3","modified":1690258400271},{"_id":"themes\\yilia\\languages\\no.yml","hash":"ddf2035e920a5ecb9076138c184257d9f51896a7","modified":1690258400271},{"_id":"themes\\yilia\\languages\\zh-tw.yml","hash":"f5f0ca88185da7a8457760d84bf221781473bd7c","modified":1690258400271},{"_id":"themes\\yilia\\languages\\zh-CN.yml","hash":"b057f389c6713010f97d461e48ec959b0b6f3b44","modified":1690258400271},{"_id":"themes\\yilia\\languages\\ru.yml","hash":"2a476b4c6e04900914c81378941640ac5d58a1f0","modified":1690258400271},{"_id":"themes\\yilia\\source-src\\script.ejs","hash":"28abac2426761d7e715b38aadd86ce6549c8ae77","modified":1690258400318},{"_id":"themes\\yilia\\source\\slider.e37972.js","hash":"ce5eac88301fe4f2fce0fb6203adfd58eb8313ac","modified":1690258400333},{"_id":"themes\\yilia\\source-src\\css.ejs","hash":"cf7eab48d626433120d1ef9697f719a359817018","modified":1690258400287},{"_id":"themes\\yilia\\layout\\_partial\\archive-post.ejs","hash":"edc0154b30a4127acda10297bec6aacf754b4ac4","modified":1690258400271},{"_id":"themes\\yilia\\layout\\_partial\\after-footer.ejs","hash":"c70f367f54064a441e574c913f5e0ea121d0f899","modified":1690258400271},{"_id":"themes\\yilia\\layout\\_partial\\archive.ejs","hash":"a4eacc2bc1278095a0ef99f904b0634c78f980eb","modified":1690258400271},{"_id":"themes\\yilia\\layout\\_partial\\article.ejs","hash":"8dea8f5f93a60185439b330b0f1d1649a6ab4bd0","modified":1690258400271},{"_id":"themes\\yilia\\layout\\_partial\\baidu-analytics.ejs","hash":"155327c23607f69989b58845f24d842a54e504b8","modified":1690258400271},{"_id":"themes\\yilia\\layout\\_partial\\css.ejs","hash":"236f8a377b2e4e35754319c3029bcd4a4115431d","modified":1690258400271},{"_id":"themes\\yilia\\layout\\_partial\\aside.ejs","hash":"751e5deab5365348be5243688b419c82d337ab9a","modified":1690258400271},{"_id":"themes\\yilia\\layout\\_partial\\footer.ejs","hash":"871f81cacd5d41cb2eb001cd56254217a857dc2f","modified":1690258400271},{"_id":"themes\\yilia\\layout\\_partial\\google-analytics.ejs","hash":"1ccc627d7697e68fddc367c73ac09920457e5b35","modified":1690258400271},{"_id":"themes\\yilia\\layout\\_partial\\head.ejs","hash":"12ca7d8dba56bc767b9309dda9526dcbaffc1614","modified":1690258400271},{"_id":"themes\\yilia\\layout\\_partial\\left-col.ejs","hash":"fb1b8457b9eb15b55da1bf7b133e12c375dd26f8","modified":1690258400271},{"_id":"themes\\yilia\\layout\\_partial\\mathjax.ejs","hash":"11550a418921d330e6553be0569a94ab5a217967","modified":1690258400271},{"_id":"themes\\yilia\\layout\\_partial\\header.ejs","hash":"b69855e07b65117769adc515cb64b803932068c9","modified":1690258400271},{"_id":"themes\\yilia\\layout\\_partial\\mobile-nav.ejs","hash":"ccec1fc70f021cb50ac85b524e7949878ab93a18","modified":1690258400271},{"_id":"themes\\yilia\\layout\\_partial\\tools.ejs","hash":"0ffcb251b79e8a920c9b4cb6bb7a96a808816165","modified":1690258400287},{"_id":"themes\\yilia\\source\\fonts\\default-skin.b257fa.svg","hash":"2ac727c9e092331d35cce95af209ccfac6d4c7c7","modified":1690258400318},{"_id":"themes\\yilia\\layout\\_partial\\viewer.ejs","hash":"cc1c39903aed0a0601d104238d2bbd13ad2a36f3","modified":1690258400287},{"_id":"themes\\yilia\\source\\fonts\\iconfont.16acc2.ttf","hash":"f342ac8bf4d937f42a7d6a0032ad267ab47eb7f2","modified":1690258400318},{"_id":"themes\\yilia\\source\\fonts\\iconfont.45d7ee.svg","hash":"75767c904d483d9b93469afb6b92bb6bdface639","modified":1690258400333},{"_id":"themes\\yilia\\source\\fonts\\iconfont.8c627f.woff","hash":"aa9672cb097f7fd73ae5a03bcd3d9d726935bc0a","modified":1690258400333},{"_id":"themes\\yilia\\source\\fonts\\iconfont.b322fa.eot","hash":"bc8c5e88f4994a852041b4d83f126d9c4d419b4a","modified":1690258400333},{"_id":"themes\\yilia\\source\\fonts\\tooltip.4004ff.svg","hash":"397fe4b1093bf9b62457dac48aa15dac06b54a3c","modified":1690258400333},{"_id":"themes\\yilia\\source\\img\\default-skin.png","hash":"ed95a8e40a2c3478c5915376acb8e5f33677f24d","modified":1690258400333},{"_id":"themes\\yilia\\source\\img\\preloader.gif","hash":"6342367c93c82da1b9c620e97c84a389cc43d96d","modified":1690258400333},{"_id":"themes\\yilia\\source\\img\\scrollbar_arrow.png","hash":"d64a33c4ddfbdb89deeb6f4e3d36eb84dc4777c0","modified":1690258400333},{"_id":"themes\\yilia\\source-src\\js\\Q.js","hash":"e56d9710afa79b31ca6b9fbd845f6d1895f5214b","modified":1690258400318},{"_id":"themes\\yilia\\source-src\\js\\anm.js","hash":"d18f6276a352b871390a4112d479b9e58b8cdbbe","modified":1690258400318},{"_id":"themes\\yilia\\source-src\\js\\aside.js","hash":"5e4c3c3d61f1e1ce2f09688d3aff25fadc851fff","modified":1690258400318},{"_id":"themes\\yilia\\source-src\\js\\browser.js","hash":"4dc04845cf27f350922b63f1813a9c82e6e33b05","modified":1690258400318},{"_id":"themes\\yilia\\source-src\\js\\fix.js","hash":"67b8819abb886c9d066fb3b0624ca15e06f63fe0","modified":1690258400318},{"_id":"themes\\yilia\\source-src\\js\\main.js","hash":"fe98bf90ce61658fe16ae057f8b6a512a845af3b","modified":1690258400318},{"_id":"themes\\yilia\\source-src\\js\\mobile.js","hash":"461c08ffcbc724d74ec7e0ff38e171eefe0f89fd","modified":1690258400318},{"_id":"themes\\yilia\\source-src\\js\\report.js","hash":"57680f9a23bd0a1eaafd64ae08cc33e20627ab15","modified":1690258400318},{"_id":"themes\\yilia\\source-src\\js\\slider.js","hash":"0beaa112657ad57c723d9e773d5b79de60c1dd74","modified":1690258400318},{"_id":"themes\\yilia\\source-src\\js\\util.js","hash":"3bcdeb95072b85600874424e6929e3e22cfddaa0","modified":1690258400318},{"_id":"themes\\yilia\\source-src\\js\\share.js","hash":"d4ccff8266c37363b3904226f5d035b7db882c61","modified":1690258400318},{"_id":"themes\\yilia\\source-src\\css\\_core.scss","hash":"29ba600e98ed55f7af4ade8038272c84cba21188","modified":1690258400287},{"_id":"themes\\yilia\\source-src\\js\\viewer.js","hash":"c699cf3c89409ec8f044258e0715a470861b5d5d","modified":1690258400318},{"_id":"themes\\yilia\\source-src\\css\\_function.scss","hash":"ce227b6f5a9af194fd5d455200630f32c05e151f","modified":1690258400287},{"_id":"themes\\yilia\\source-src\\css\\archive.scss","hash":"d6a7dd88404b383b5b94e4c7ec675a410c41f3cc","modified":1690258400287},{"_id":"themes\\yilia\\source-src\\css\\article-inner.scss","hash":"f7388f5c11370ef462f7cb913d8f72edf24ecaf9","modified":1690258400287},{"_id":"themes\\yilia\\source-src\\css\\article-nav.scss","hash":"8f82fe898ba1c1bd00c24a7d8270feddc7eba3bc","modified":1690258400302},{"_id":"themes\\yilia\\source-src\\css\\article-main.scss","hash":"1577a2336b3ad122f49f60dff2bc1a97d4e7b18b","modified":1690258400287},{"_id":"themes\\yilia\\source-src\\css\\aside.scss","hash":"07244c188f58ecfb90bb7c047b8cde977f1dc4b4","modified":1690258400302},{"_id":"themes\\yilia\\source-src\\css\\comment.scss","hash":"b85f344f2c66d43d7094746e0a9ccb21d0534201","modified":1690258400302},{"_id":"themes\\yilia\\source-src\\css\\category.scss","hash":"5accf5b4cb4de54317f5a45b28ae84a4cc21d3ef","modified":1690362971612},{"_id":"themes\\yilia\\source-src\\css\\fonts.scss","hash":"96d7eb1d42c06fdcccb8ef969f6ecd30c3194903","modified":1690258400302},{"_id":"themes\\yilia\\source-src\\css\\footer.scss","hash":"7ca837a4cc34db1c35f01baec85eb10ccc64ea86","modified":1690258400302},{"_id":"themes\\yilia\\source-src\\css\\grid.scss","hash":"f53ea8270752b5919ec5d79224d22af91f2eda12","modified":1690258400302},{"_id":"themes\\yilia\\source-src\\css\\global.scss","hash":"db5e94a4f3e2768d85da58f5fcb4b211928a0de0","modified":1690355445904},{"_id":"themes\\yilia\\source-src\\css\\highlight.scss","hash":"40e5aa5056dc0b3b9f51c5b387370b612e265d4e","modified":1690258400302},{"_id":"themes\\yilia\\source-src\\css\\left.scss","hash":"80dac621e43581a254d0152d5df901e4d0b01c09","modified":1690258400302},{"_id":"themes\\yilia\\source-src\\css\\mobile.scss","hash":"d995dcd483a250fe61b426158afb61bf8923a927","modified":1690258400302},{"_id":"themes\\yilia\\source-src\\css\\mobile-slider.scss","hash":"19f10fd2f0c3377aa4b165b3c2291ecf86dd9351","modified":1690258400302},{"_id":"themes\\yilia\\source-src\\css\\main.scss","hash":"dfb309c9801595a3c3978b5edb821eff85d9649c","modified":1690363018427},{"_id":"themes\\yilia\\source-src\\css\\page.scss","hash":"244c4d75c375978ff9edb74acc68825e63c6b235","modified":1690258400318},{"_id":"themes\\yilia\\source-src\\css\\reward.scss","hash":"a557a9ed244c82b8b71e9da9de3339d92783499f","modified":1690258400318},{"_id":"themes\\yilia\\source-src\\css\\share.scss","hash":"9d6f6884f40c191882e56a1e1e1192400944a515","modified":1690258400318},{"_id":"themes\\yilia\\source-src\\css\\scroll.scss","hash":"2495f7e4e3b055735c531f944b5f40a118a351ec","modified":1690258400318},{"_id":"themes\\yilia\\source-src\\css\\tags-cloud.scss","hash":"399744e98e7c67939ed9b23c2670d8baad044eda","modified":1690258400318},{"_id":"themes\\yilia\\source-src\\css\\social.scss","hash":"a10a038a1dac8953cb4ffc7e04272eff9fac54e4","modified":1690258400318},{"_id":"themes\\yilia\\source-src\\css\\tags.scss","hash":"915c93edd67c5326695cc7dc84b14c5f154dbcc8","modified":1690258400318},{"_id":"themes\\yilia\\source-src\\css\\tools.scss","hash":"2924fb6f77c4a9973cd928c2c7db0acb848ed483","modified":1690258400318},{"_id":"themes\\yilia\\source-src\\css\\tooltip.scss","hash":"b81cedbe31accca82e597801186911a7b5e6841c","modified":1690258400318},{"_id":"themes\\yilia\\layout\\_partial\\post\\changyan.ejs","hash":"086c8a88fd3bcae7ec13258df58e25d6354af2fa","modified":1690258400287},{"_id":"themes\\yilia\\layout\\_partial\\post\\category.ejs","hash":"e777cbf959b11c4dfda649c562799899b90ab4a3","modified":1690258400287},{"_id":"themes\\yilia\\layout\\_partial\\post\\date.ejs","hash":"aae96de18d48cd3b9b7bf6fed0100e15b53cca97","modified":1690258400287},{"_id":"themes\\yilia\\layout\\_partial\\post\\duoshuo.ejs","hash":"f6b4c4eaafb5ac386273354b5f64a26139b7a3b0","modified":1690258400287},{"_id":"themes\\yilia\\layout\\_partial\\post\\nav.ejs","hash":"b6a97043f9ec37e571aacacfedcda1d4d75e3c7c","modified":1690258400287},{"_id":"themes\\yilia\\layout\\_partial\\post\\gitment.ejs","hash":"25655016773aa5d0774c56115ae1736a9fc9ea1f","modified":1690258400287},{"_id":"themes\\yilia\\layout\\_partial\\post\\share.ejs","hash":"345b262e3c3b75c0cd9a93d9ecabcf06e33e54ff","modified":1690258400287},{"_id":"themes\\yilia\\layout\\_partial\\post\\title.ejs","hash":"d4a460a35e2112d0c7414fd5e19b3a16093f1caf","modified":1690258400287},{"_id":"themes\\yilia\\layout\\_partial\\post\\tag.ejs","hash":"2c4e4ca36c9bb4318506c38aca7127f1f44d827f","modified":1690258400287},{"_id":"themes\\yilia\\layout\\_partial\\post\\wangyiyun.ejs","hash":"fb022502c741b4a26bad6b2ad37245c10ede3f1a","modified":1690258400287},{"_id":"themes\\yilia\\source\\img\\hexo\\1.png","hash":"eb5d590c866b115b711b6b18e688d0fb750b0fbc","modified":1661746009070},{"_id":"themes\\yilia\\source\\img\\hexo\\2.png","hash":"1a2488cab647b2d9ee0645f8b1404f20d8f6391b","modified":1661746807535},{"_id":"themes\\yilia\\source\\img\\hexo\\3.png","hash":"1c144b98e4d21f31d06466142a48efaf2a94d5e2","modified":1661780828596},{"_id":"themes\\yilia\\source\\img\\hexo\\6.png","hash":"e48c5ecbd482f3d30befe212e0bdbb7edfc8e1ce","modified":1661821525083},{"_id":"themes\\yilia\\source\\img\\hexo\\4.png","hash":"1386eb7152ca993db6bb1b73ec224640ed73ace1","modified":1661781049168},{"_id":"themes\\yilia\\source\\img\\hexo\\7.png","hash":"e6b833a8a600ccde88c4204782cc3e8004e0edb2","modified":1661821616108},{"_id":"themes\\yilia\\source\\img\\hexo\\8.png","hash":"2caad5ef038d9170ea58f0a0026bc8306280ec37","modified":1661821801772},{"_id":"themes\\yilia\\source-src\\css\\core\\_media-queries.scss","hash":"262ffcd88775080b7f511db37f58d2bcb1b2bfc7","modified":1690258400302},{"_id":"themes\\yilia\\source-src\\css\\core\\_animation.scss","hash":"1834c3ed8560716e63bb3a50be94cac87fbbeaf3","modified":1690258400302},{"_id":"themes\\yilia\\source-src\\css\\core\\_mixin.scss","hash":"91db061c9c17628291a005e5bd4936cf9d35a6c4","modified":1690258400302},{"_id":"themes\\yilia\\source-src\\css\\core\\_reset.scss","hash":"398a49913b4a47d928103562b1ce94520be4026a","modified":1690258400302},{"_id":"themes\\yilia\\source-src\\css\\fonts\\iconfont.eot","hash":"bc8c5e88f4994a852041b4d83f126d9c4d419b4a","modified":1690258400302},{"_id":"themes\\yilia\\source-src\\css\\core\\_variables.scss","hash":"6e75bdaa46de83094ba0873099c6e7d656a22453","modified":1690258400302},{"_id":"themes\\yilia\\source-src\\css\\fonts\\iconfont.svg","hash":"75767c904d483d9b93469afb6b92bb6bdface639","modified":1690258400302},{"_id":"themes\\yilia\\source-src\\css\\fonts\\iconfont.ttf","hash":"f342ac8bf4d937f42a7d6a0032ad267ab47eb7f2","modified":1690258400302},{"_id":"themes\\yilia\\source-src\\css\\fonts\\iconfont.woff","hash":"aa9672cb097f7fd73ae5a03bcd3d9d726935bc0a","modified":1690258400302},{"_id":"themes\\yilia\\source-src\\css\\img\\scrollbar_arrow.png","hash":"d64a33c4ddfbdb89deeb6f4e3d36eb84dc4777c0","modified":1690258400302},{"_id":"themes\\yilia\\source-src\\css\\img\\checkered-pattern.png","hash":"049262fa0886989d750637b264bed34ab51c23c8","modified":1690258400302},{"_id":"themes\\yilia\\source-src\\css\\img\\tooltip.svg","hash":"397fe4b1093bf9b62457dac48aa15dac06b54a3c","modified":1690258400302},{"_id":"themes\\yilia\\source\\img\\datastruct\\1_extract\\2.png","hash":"6d1421cc038218e94daff7ef39c7d3ac9d2e881c","modified":1690680047457},{"_id":"themes\\yilia\\source\\img\\datastruct\\1_extract\\4.png","hash":"743e8d48e403ebd444289b1668ec6512099c9bb5","modified":1690681296164},{"_id":"themes\\yilia\\source\\img\\datastruct\\1_extract\\3.png","hash":"571cded10b5c4269a889e56dab07bc689818e102","modified":1690680342728},{"_id":"themes\\yilia\\source\\img\\experience\\website\\3.png","hash":"aa0520b3b2ccc24d5cb1bdd6f0470f39156e50d6","modified":1708773092431},{"_id":"themes\\yilia\\source\\img\\linux\\install\\2.png","hash":"23d49dab591d627c6673fb2a75fd5d2eb9d2fed3","modified":1672553372958},{"_id":"themes\\yilia\\source\\img\\linux\\install\\1.png","hash":"f5e18416748c8cc86938da9ae1f1ff42e91e08bd","modified":1672553166188},{"_id":"themes\\yilia\\source\\img\\hexo\\config\\2.png","hash":"368052a20c17d974130d81cad86bf9eefbe696e6","modified":1706626651446},{"_id":"themes\\yilia\\source\\img\\linux\\usage\\1.png","hash":"672cdb2ff4bef0cb1db9fd9dc7af2c64ee49a689","modified":1709371943275},{"_id":"themes\\yilia\\source\\img\\linux\\usage\\2.png","hash":"ecb6b7300ecb1e57a5ec396896b31f5076e30a29","modified":1709371960021},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\1.png","hash":"30554ce2998b7e51ad6c95ba0a8dd64d7184e7d5","modified":1709254499920},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\10.png","hash":"e24850fd076e7deb6510d6eab70e1b983a8143de","modified":1709275557514},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\11.png","hash":"a1dca93a880850cf39a90afe2ab00b2d4af3d05d","modified":1709275580280},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\13.png","hash":"5a8cba5a88ea57b8fbc0b3d03e170bd78d63c9d6","modified":1709275909052},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\12.png","hash":"a6083435996c6b697b5b5b9c64cdf1bed6a119f8","modified":1709275784997},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\14.png","hash":"d454f6af516b04b392c643aff34c133c27c89566","modified":1709277999305},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\16.png","hash":"05fe99b97b36da5852d48717d487cd7bcdaa62ea","modified":1709278477364},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\18.png","hash":"662e19f9ede67fbf97ba64db2b1dd570493f42bc","modified":1709278709707},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\19.png","hash":"5b6c4d4ca1fafa15fc502684b8ef690dd783b1f0","modified":1709278987299},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\23.png","hash":"5bfb5c0dfd8a20cb41b6c8fa1c0f066de5b58ce0","modified":1709282806971},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\4.png","hash":"4ba2f5dd63299422bc74f7debefac14fc5c2abbf","modified":1709272041206},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\35.png","hash":"b598e1f3995b79f9ca23beecec9f286ff5ee7637","modified":1709268114873},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\6.png","hash":"68d24416da9cf00e2c1f3389d69780fe7b0aefcc","modified":1709272713646},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\8.png","hash":"2d231f50629980b55a959bbbb7ce4b44627428da","modified":1709272836338},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\9.png","hash":"2c930e47793ff23442b7e0047e667d26b2ce652f","modified":1709274595282},{"_id":"themes\\yilia\\source\\img\\datastruct\\2_linearlist\\linklist\\3.png","hash":"8f35efb483252f734fcce32a59701f95ae08ca08","modified":1691217646864},{"_id":"themes\\yilia\\source\\img\\experience\\app\\lrc_to_smi\\1.png","hash":"3e6bf1d0664a78945a62cb8efe7291c559bdd95f","modified":1702357564761},{"_id":"themes\\yilia\\source\\img\\experience\\app\\lrc_to_smi\\2.png","hash":"29999d449f335d959a928b47ee573094d6407e6f","modified":1702358201820},{"_id":"themes\\yilia\\source\\img\\experience\\app\\mouse\\1.png","hash":"45d7532c50960f7a6770563e0c462b831dd478f0","modified":1710751013278},{"_id":"themes\\yilia\\source\\img\\experience\\app\\lrc_to_smi\\4.png","hash":"2739ef20646d80910f98405f596227832d73dca7","modified":1702358320554},{"_id":"themes\\yilia\\source\\img\\experience\\app\\mouse\\3.png","hash":"c0202f1aaf6cd8c29563e94172ba1a5664ca80eb","modified":1710751340931},{"_id":"themes\\yilia\\source\\img\\experience\\app\\word\\1.png","hash":"ade633e21e81c9671297135c43eb0a4c465b391f","modified":1709468931510},{"_id":"themes\\yilia\\source\\img\\experience\\pycharm\\link_server\\10.png","hash":"39b9fd6684a084dab653c885553bdfb48e616d96","modified":1702802732650},{"_id":"themes\\yilia\\source\\img\\experience\\pycharm\\link_server\\11.png","hash":"b6ebbcce0e705315d2b9d49cfe47a5b0c7d738ef","modified":1702803286275},{"_id":"themes\\yilia\\source\\img\\experience\\pycharm\\link_server\\2.png","hash":"155e96cd73c2a354ba31581c3ae7990c35f1e77f","modified":1702801057457},{"_id":"themes\\yilia\\source\\img\\experience\\pycharm\\link_server\\3.png","hash":"d5c06088084abdd3d3f791266baf3e490353fa8e","modified":1702801133568},{"_id":"themes\\yilia\\source\\img\\experience\\pycharm\\link_server\\4.png","hash":"c2347bb419ce0b59e6a10bf13e67aabb6f1d6000","modified":1702801183526},{"_id":"themes\\yilia\\source\\img\\experience\\pycharm\\link_server\\5.png","hash":"528b88149b3eab3ef15c7cd04f18cf14463a9f8e","modified":1702801274883},{"_id":"themes\\yilia\\source\\img\\experience\\pycharm\\link_server\\8.png","hash":"faedba9e3148fa893311cd9ddf820ae269dcc09a","modified":1702802406352},{"_id":"themes\\yilia\\source\\img\\experience\\pycharm\\link_server\\9.png","hash":"d38b41763eab121538504eba96a933f860d3fca0","modified":1702802438755},{"_id":"themes\\yilia\\source\\img\\experience\\pycharm\\link_server\\7.png","hash":"8998704ce34bfa8d04b681a1b979c2ec712511df","modified":1702802255878},{"_id":"themes\\yilia\\source\\img\\experience\\vscode\\relative_path\\1.png","hash":"5bf68e898eaedce375a846d2766c9d3c5f2c21c6","modified":1661864089789},{"_id":"themes\\yilia\\source\\img\\experience\\windows\\ssh\\1.png","hash":"83b6719725d0d73dba3041940c7d2f71777087f0","modified":1710147462362},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\0\\10.png","hash":"4535da2390045add4db4e2657210d660907fe0aa","modified":1709726180752},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\0\\6.png","hash":"0ed212c966e155fd3a6d64e67b54d3fb0639e090","modified":1709725715235},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\2\\1.png","hash":"bfc399b235e2f301d714db59b129576937bd9206","modified":1709519502904},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\2\\5.png","hash":"0252bf34bb632f23d60ceff347041279a3f88f74","modified":1709556171122},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\2\\8.png","hash":"2643438edc15e378358aca36a1d02428a208f50f","modified":1709558547046},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\4\\8.png","hash":"6590259d237e2ff2b1c2d6a2053180a455f1a2ab","modified":1709625524733},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\5\\2.png","hash":"42a50479f9b6e7b69169ac6cda432a1c3ad3bf54","modified":1709643397549},{"_id":"themes\\yilia\\source\\img\\experience\\app\\word\\page\\0.png","hash":"172b9a8e838a56f3034cf412ad5374822235bc49","modified":1713105721334},{"_id":"themes\\yilia\\source\\img\\experience\\app\\word\\page\\2.png","hash":"d6a0322ad9d5a6ab3b811be1262f93a739c67c01","modified":1713106272841},{"_id":"themes\\yilia\\source\\img\\experience\\app\\word\\page\\5.png","hash":"723a629f7ce633859822d10bf34e9346542869b1","modified":1713106928201},{"_id":"themes\\yilia\\source\\img\\experience\\app\\word\\page\\6.png","hash":"fa9c6834203e9e4fcf6a5ea0840c2268691c5df9","modified":1713107380308},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\CDM\\2.png","hash":"7e63a4151dc6c4e94cf281abe73dcdd544fe67e1","modified":1706269099314},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\3.png","hash":"8557ef65ec097d18405fe25075ce508a569cdf3c","modified":1706094402890},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\4.png","hash":"bd2061a683d43542f298d3b6c369a7a73e940802","modified":1706094657110},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\6.png","hash":"f626d8e52052327ea89c2ef3f164331a2c5813e6","modified":1706094865295},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\2_linear_neural_network\\4img\\2.png","hash":"866042909a57b337001a6ccdb384fca0de6b0aef","modified":1713446232300},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\2_linear_neural_network\\4img\\3.png","hash":"be26c997c7cfb8f358d42d65b6defe79a7d98bf9","modified":1713446388209},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\2_linear_neural_network\\6_softmax_realize_files\\6_softmax_realize_13_0.svg","hash":"286de42a50f439d72b9a4431c26633c68c9e5244","modified":1707049500016},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\2_linear_neural_network\\6_softmax_realize_files\\6_softmax_realize_15_0.svg","hash":"0b967ebe55c0efd5a10943f13da50b17012b8d9b","modified":1707049500017},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\2_linear_neural_network\\1_linear_regression_files\\1_linear_regression_4_0.svg","hash":"117d33ae7e24490dd351cb3dda9f01697604a260","modified":1706947544080},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\2_linear_neural_network\\7_softmax_realize_simple_files\\7_softmax_realize_simple_5_0.svg","hash":"e7326b636020ee834dedefa365a0bc50df191fcf","modified":1707061270220},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\1_prepare\\4_calculus_files\\4_calculus_1_1.svg","hash":"2268e578363aa8e3ea0d9fc6f9048b5476965606","modified":1706791900006},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\1_prepare\\4_calculus_files\\4_calculus_3_1.svg","hash":"4388b61bde02e3677bad2d58f1cc537f2ce40879","modified":1706791900029},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\5_convolution\\2_picture_cov\\1.png","hash":"adc2c914ac0d265928985e26d2b327fd17e74823","modified":1713619518386},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\5_convolution\\6_LeNet_files\\6_LeNet_3_1.svg","hash":"aa3f445749740d26383175f7fe4e5257684d9aa0","modified":1713759571751},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\5_convolution\\5_polling\\1.png","hash":"7a7e066d35969acce72f9dee184a3df899d53fb9","modified":1713693735580},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\5_convolution\\4_mult_channel\\2.png","hash":"d88a9c4e83a75875220f7168f823bcea364c330e","modified":1713690413078},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\10_kaggle_predict_price_files\\10_kaggle_predict_price_15_1.svg","hash":"f44ae9c476fc897885aa995ec11c55c79cc47821","modified":1710932210219},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\10_kaggle_predict_price_files\\10_kaggle_predict_price_17_1.svg","hash":"52dcc486e79b2bcb5727ecad64f1672ee4f36279","modified":1710932210220},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\1_mlp_files\\1_mlp_3_0.svg","hash":"b40bd446671c91e8423328496f591d47f9b855fe","modified":1707126900118},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\1_mlp_files\\1_mlp_5_0.svg","hash":"0aeb69df6b54fa10fc0bba6dd6b8f74bc906c713","modified":1707126900119},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\1_mlp_files\\1_mlp_1_0.svg","hash":"916e9f81fd64d864e8928b2d075bbf51b853735f","modified":1707126900117},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\3_mlp_realize_simple_files\\3_mlp_realize_simple_1_0.svg","hash":"893bf7bd6b1facaafeaa59f0a74e1ec9efbcb4d9","modified":1707129399808},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\2_mlp_realize_files\\2_mlp_realize_1_0.svg","hash":"2cdad356448f53f436b46f6a1dfb3301d9edeca1","modified":1707127769954},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\2_mlp_realize_files\\2_mlp_realize_1_1.svg","hash":"15de8b4f4bdc9f439bd6567de0e4a4a511710dd1","modified":1707127769955},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\4_overfitting_files\\4_overfitting_11_1.svg","hash":"ef90ef3ede804f998cc308e825b05d226570b0ca","modified":1707214733202},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\4_overfitting_files\\4_overfitting_9_1.svg","hash":"aeac8a3e0b2759086aaaa1c870717c441ead05c6","modified":1707214733201},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\4_overfitting_files\\4_overfitting_7_1.svg","hash":"c4a09daa3725c074825db20b1ae97145f33cbb24","modified":1707214733200},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\5_weight_decay_files\\5_weight_decay_1_1.svg","hash":"430b01e4581ecbbdfa0b2bf4990c15efdcab7603","modified":1707296564136},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\4img\\1.png","hash":"acef488b314a0eafa2b537f365d5fc373e271623","modified":1707139509508},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\5_weight_decay_files\\5_weight_decay_2_1.svg","hash":"28bfe489d1ba3253bd9a1369f8ecf0deb413a211","modified":1707296564137},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\5_weight_decay_files\\5_weight_decay_5_1.svg","hash":"53c269365154a5776b6af1b449e163d41f59fa77","modified":1707296564139},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\5_weight_decay_files\\5_weight_decay_4_1.svg","hash":"0cb1e259525ec525d6356f7f8c112a6e9f91977e","modified":1707296564139},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\6_deopout_files\\6_deopout_3_0.svg","hash":"06c12519eeda924c77fc54eb45521bc677fb69ca","modified":1707314614177},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\7\\1.png","hash":"67738f990d722b9ab6d296d108ebfb303b9c1246","modified":1710561944677},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\6_deopout_files\\6_deopout_5_0.svg","hash":"90fa91d6679990bbb69a51ae6fba74606e031747","modified":1707314614179},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\8_init_files\\8_init_2_0.svg","hash":"8bdf2739f73260fef223d873c4de25d11de3b380","modified":1710659696509},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\7_rnn\\1_seqmodel\\2.png","hash":"e12c7a2ea34d4fb9da12fa2a8df17c4860dc8fdd","modified":1714466291534},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\7_rnn\\1_seqmodel_files\\1_seqmodel_1_0.svg","hash":"46dff896e8b549a6614da348ae3f7f08e392ce96","modified":1714639203858},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\7_rnn\\4_rnn\\1.png","hash":"411a99ced4a5fd4bf2fd4ad8130fb4f4509734bc","modified":1714725586270},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\7_rnn\\3_lmdataset_files\\3_lmdataset_3_0.svg","hash":"a97892ac1dff0ae2ca8740d2750e72ef9f51c17e","modified":1714723969829},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\7_rnn\\4_rnn\\2.png","hash":"340ce180b73a6e0c96dc2eb1f5f46db6a0b8828f","modified":1714729576803},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\7_rnn\\3_lmdataset_files\\3_lmdataset_9_0.svg","hash":"a03e6da250fbde8a2f1985dff035e5fd83aeceb8","modified":1714723969830},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\7_rnn\\7_backward\\1.png","hash":"39a7b7de47ea88fbfbca3f737847da502075d8ec","modified":1714901876124},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\7_rnn\\6_rnn_sample_realize_files\\6_rnn_sample_realize_13_1.svg","hash":"5e14adcf331d6e58bcdb08f681ec0814c858bfef","modified":1714918786663},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\7_rnn\\6_rnn_sample_realize_files\\1.png","hash":"19b9af39430aaa7b41b69160a66502631a76d16f","modified":1714920679275},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\7_rnn\\5_rnn_realize_files\\5_rnn_realize_27_1.svg","hash":"ce56a41bd07065d7882dd31a9792e38bcd60d4b7","modified":1714815317048},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\7_rnn\\5_rnn_realize_files\\5_rnn_realize_29_1.svg","hash":"45f2aa928530a7d50e44ce41717511d15bbd2fc2","modified":1714815317049},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\1_AlexNet_files\\1_AlexNet_0_1.svg","hash":"dbe778e552f1d54c23c4cd9714cdbc502840af37","modified":1714652920458},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\2_vgg_files\\2_vgg_7_0.svg","hash":"05303fdd497222c62710848d6fa65fd4426ca07f","modified":1714652983721},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\3_NiN_files\\3_NiN_7_0.svg","hash":"d7b21aac1e71119d9e1d7a3ef8fef5c124b2e279","modified":1714460754057},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\4_googleNet_files\\4_googleNet_7_1.svg","hash":"e7fad7fdf658dff125323a8f6e14b04a00767364","modified":1714550997515},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\4_googlenet\\2.png","hash":"276d3cc9e737f976d64d7b9d7261acd06f0331a6","modified":1713797192435},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\5_batchnormal_files\\5_batchnormal_5_1.svg","hash":"22f052f17b6cd789860b726e81aee437f4f02d87","modified":1714551009402},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\5_batchnormal_files\\5_batchnormal_7_1.svg","hash":"dcbf9a308cc84db9017cc79a5062c368028caf97","modified":1714551009402},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\6_resnet\\2.png","hash":"722720f90a4bf449e8f880507a9f7cfc7f5bb16b","modified":1714225933852},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\6_resnet_files\\6_resnet_13_1.svg","hash":"51f4f00b0beee6c74bca6a4d4a3722bc99e72569","modified":1714551018647},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\7_densenet\\1.png","hash":"680996307eeaa52d0a4cd002f7cd6862d71115d8","modified":1714290253963},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\7_densenet\\2.png","hash":"64bbd286cfd666244ee1973aaf0339071c3b3aa2","modified":1714290765136},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\7_DenseNet_files\\7_DenseNet_17_1.svg","hash":"e2c6b0977781d4618ec76bfda2265290cfbba0e7","modified":1714551028484},{"_id":"themes\\yilia\\source\\main.0cf68a.css","hash":"061e9c742c59a6cd017b9437f1a4879aa36cc101","modified":1708574416275},{"_id":"themes\\yilia\\source\\mobile.992cbe.js","hash":"1801ef448909ea23c0a48e9d63b80d0cfd5534ce","modified":1690258400333},{"_id":"themes\\yilia\\source\\main.0cf68a.js","hash":"283ae27ea37ac3e0e45b2e05c2482a4c594b9c25","modified":1690258400333},{"_id":"themes\\yilia\\layout\\_partial\\script.ejs","hash":"e98ec0b3b56f14d1d79af99ceb42727719a584f3","modified":1690258400287},{"_id":"themes\\yilia\\source\\img\\0.png","hash":"18cd3f368afc326152fddaf38353d0a3a5b88cce","modified":1661821028108},{"_id":"themes\\yilia\\source\\img\\hexo\\5.png","hash":"795caa1da3dd3b5194ca066cc4a3dc684ae5c4a7","modified":1661783564220},{"_id":"themes\\yilia\\source\\img\\experience\\keyboard\\1.png","hash":"b0bcd7c8a2b79784fe255d793deba3680dfa8393","modified":1709810565136},{"_id":"themes\\yilia\\source\\img\\linux\\commend\\4.png","hash":"312407c7570bf5d0226a98c51f1e400f9e426418","modified":1672894004665},{"_id":"themes\\yilia\\source\\img\\linux\\usage\\4.png","hash":"30492036c274902af6117aceadc01bb6f7ad421b","modified":1709371979026},{"_id":"themes\\yilia\\source\\img\\linux\\usage\\3.png","hash":"53692b5667fdd9f5018856c35d6fc71be66fdd10","modified":1709371973803},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\15.png","hash":"a5b0974e97c1c0c5f5f2dcfde1bd5aeefd7d09e8","modified":1709278254941},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\20.png","hash":"82240a09e18c6fcb0f38ff232c5329f0ca2baf23","modified":1709280982522},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\21.png","hash":"1656f0f4f2fc7ce8069b11669560fb868f96ffd4","modified":1709281324094},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\3.png","hash":"c2852be662928c0650e58095caa3ad963ef79b32","modified":1709271574203},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\5.png","hash":"cafffe809df3332e48b3f9fb6d2c2519e4d08be7","modified":1709272306923},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\7.png","hash":"981d763df9f521ac01471891dffc8c42c1bcf2d5","modified":1709272743602},{"_id":"themes\\yilia\\source\\img\\datastruct\\2_linearlist\\linklist\\4.png","hash":"cd2d22e47a4ccf7d698aa886967d484994fe68c7","modified":1691217715248},{"_id":"themes\\yilia\\source\\img\\experience\\app\\lrc_to_smi\\3.png","hash":"a1fce07f832cae4b3b251dde1c87376f4386ad2c","modified":1702358281362},{"_id":"themes\\yilia\\source\\img\\experience\\app\\lrc_to_smi\\5.png","hash":"fe826175ce8041af3cd87afada1a66204a489e4e","modified":1702358837390},{"_id":"themes\\yilia\\source\\img\\experience\\pycharm\\link_server\\1.png","hash":"18f61385a1aa2a42b57ed9fd4eb058988038bc79","modified":1702800985856},{"_id":"themes\\yilia\\source\\img\\experience\\pycharm\\link_server\\6.png","hash":"8e2eb0e6a096b258aa6d87f122a6901cadc81c22","modified":1702801338326},{"_id":"themes\\yilia\\source\\img\\experience\\vscode\\relative_path\\2.png","hash":"6845b7d833c89764ce7e87a437079bc76022ad43","modified":1661864909823},{"_id":"themes\\yilia\\source\\img\\experience\\windows\\powershell\\2.png","hash":"06b19fe5ed694fd898dd53e01267d41885e75544","modified":1710208484344},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\0\\1.png","hash":"bfbc6e4bc6036266324cc25c8e0b9be9872062d3","modified":1709721631734},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\0\\11.png","hash":"77fdc5313f871e514dda20d85bf727faef13d880","modified":1709726368560},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\0\\2.png","hash":"f8686690ba00c0c1c374c0d5c04ef51699a04df2","modified":1709721909644},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\0\\4.png","hash":"2be74d8a0d882cb4176c475ae0f35562886b6cf3","modified":1709723397584},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\0\\7.png","hash":"6afbf6573f8ffa9d2d5708e8c59573b556e81748","modified":1709725799816},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\0\\8.png","hash":"d5c39a047fad558afc3c38242590c366caf693c1","modified":1709725817464},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\0\\9.png","hash":"44c3143a2eabc2dfce309be0e275767a54abde6e","modified":1709725994533},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\2\\10.png","hash":"41aa473866a58bc198e011888355e70b6ea2334b","modified":1709559046368},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\2\\3.png","hash":"3927e0ec015178ba68fece35c1c15af61cdf92f8","modified":1709519815611},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\2\\2.png","hash":"cd5fc603a9c1d337e47c5c8352248bb0043246fe","modified":1709519667039},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\2\\4.png","hash":"8d1f48fe6aa2854cba53710b6942543408d68fae","modified":1709522243114},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\2\\9.png","hash":"281e0c904285464473694e159791cdcc61f2810b","modified":1709619712584},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\3\\3.png","hash":"e5deb84f3ed4c4cf16923a0445dbed94fd3ed1e6","modified":1709566464870},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\4\\3.png","hash":"ede885e2c91398a34f7cb88932988e37f6ab1832","modified":1709623178020},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\4\\4.png","hash":"61b4c8cc0c2cb7965d620a69e04039a7f97b19da","modified":1709623509738},{"_id":"themes\\yilia\\source\\img\\experience\\app\\word\\page\\1.png","hash":"dbd95e4b6418abac3b4a5f9619f61c7e8b330c7a","modified":1713105939961},{"_id":"themes\\yilia\\source\\img\\experience\\app\\word\\page\\3.png","hash":"4c46804b2a6a460cbb32ad520fb5199bf32a7e7f","modified":1713106763531},{"_id":"themes\\yilia\\source\\img\\experience\\app\\word\\page\\4.png","hash":"7b389400f80ea6131ed37c3499d00ca678c62580","modified":1713106996647},{"_id":"themes\\yilia\\source\\img\\experience\\app\\word\\to_pdf\\1.png","hash":"de6a40d1ed287d516dac9cc7cbcd09a88f046169","modified":1713101911231},{"_id":"themes\\yilia\\source\\img\\experience\\app\\word\\to_pdf\\2.png","hash":"aed291bef545d63c6f694c87cf59d8392f8f17c4","modified":1713103651138},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SRDiff\\3.png","hash":"12aefac57158ded661913fee37c1348baeb1fb5f","modified":1706339037074},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SwinIR\\2.png","hash":"c62663719646ff08f57958e388c64cadf00bec40","modified":1705998220007},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\2_linear_neural_network\\4img\\1.png","hash":"a65d5f49e5bb5f2cdc688e75966e4f8d23530336","modified":1706962050655},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\2_linear_neural_network\\2_linear_regression_realize_files\\2_linear_regression_realize_3_2.svg","hash":"7745038901e6c501386a5bc1d7afda51874a0a9f","modified":1706952737930},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\2_linear_neural_network\\1_linear_regression_files\\1.png","hash":"29edd99abc43a64b80c27ae3404d354bbaf2e1aa","modified":1706947212470},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\2_linear_neural_network\\5_image_classification_dataset_files\\5_image_classification_dataset_3_1.svg","hash":"a84269a32f13a9403e16b3b83416bbe856798469","modified":1707022695907},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\1_prepare\\6_probability_files\\6_probability_1_2.svg","hash":"4fb11c7574654e616736bb655a7365c62e42810c","modified":1706930246524},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\5_convolution\\3_padding\\2.png","hash":"a9bb3af80eebe3ab3765fcd76d2abd2aa95a6cc8","modified":1713680727879},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\5_convolution\\3_padding\\1.png","hash":"f5ff3f979568b627d255973a670973d3944066e5","modified":1713678333846},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\5_convolution\\4_mult_channel\\1.png","hash":"d38802cfc41d4ee8da2556acc182522b03188bcb","modified":1713683013171},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\5_convolution\\6_LeNet\\1.png","hash":"fc931e8000daae1e2709cb7d8cd3ba3ca4dfae98","modified":1713704286188},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\7_rnn\\1_seqmodel\\1.png","hash":"0dfb92eaddb3cdd94b49b471b6a63c01383fd6c1","modified":1714462407784},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\7_rnn\\1_seqmodel_files\\1_seqmodel_11_0.svg","hash":"d46f3ba1a8a8aec6430cda48c1ef56df387c1dbd","modified":1714639203861},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\7_rnn\\1_seqmodel_files\\1_seqmodel_13_0.svg","hash":"2a6dea1dc158df8efa310ab7492e2fa7792e6e73","modified":1714639203863},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\7_rnn\\1_seqmodel_files\\1_seqmodel_9_0.svg","hash":"37da951b540aa0b1931b32570a847bd1874a94e2","modified":1714639203859},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\7_rnn\\7_backward\\2.png","hash":"530e1826bac164be050fd96dbb6a2bd4b5f1b713","modified":1714902604352},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\1_AlexNet\\1.png","hash":"273cf87da7ba0cc03609497ff079811e8c689939","modified":1713769945521},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\2_vgg\\1.png","hash":"3616c743a2317766c63f429fbbedee5452347e27","modified":1713777702852},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\3_NiN\\1.png","hash":"923f7fc4aa2e369052c09bebca01361f2d7dd30d","modified":1713786265710},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\4_googlenet\\1.png","hash":"f4550013e59746198c7bf925bbeac66e96a5e76e","modified":1713794139619},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\6_resnet\\3.png","hash":"53e61da5477dd528abb9ea522b8fd30c9453ac59","modified":1714227599609},{"_id":"themes\\yilia\\source\\img\\experience\\website\\1.png","hash":"e3f3761e0a8d13765d07c62356c8cb531085832f","modified":1708746521632},{"_id":"themes\\yilia\\source\\img\\linux\\install\\3.png","hash":"aff317a4f01469d43aea727220d5f35dda5ad82f","modified":1672554935824},{"_id":"themes\\yilia\\source\\img\\hexo\\config\\1.png","hash":"57c220f0666493e9aa46e655dd3e4c8d8769526f","modified":1706626629440},{"_id":"themes\\yilia\\source\\img\\linux\\usage\\7.png","hash":"81ea48b124c5945df2b299db4f45d459449c7b49","modified":1709379300345},{"_id":"themes\\yilia\\source\\img\\datastruct\\2_linearlist\\doublelinklist\\3.png","hash":"1be61897417af386bdaab09257f60b624fa4b7a7","modified":1691384095099},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\0\\12.png","hash":"47a42a001aebf4be7c404c0c7fa033197239c850","modified":1709726618615},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\0\\5.png","hash":"ec4f869a042f2f354585e3767eca5d69d788a4c9","modified":1709725325226},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\2\\7.png","hash":"f3cc060659d39dbff8cb3438dd057fb21aa27ffa","modified":1709558065095},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\4\\1.png","hash":"bc477c350f3cdad28016bda6c01802a952e04be4","modified":1709599092830},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\3\\2.png","hash":"dccb5e6caaad6984015fb8dfbb63b95cedb1a785","modified":1709563332056},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\4\\6.png","hash":"fe08d6dc598eac909afdf2849420e3816ac9bf7c","modified":1709624008241},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\4\\7.png","hash":"957c4beb4e68c29a672dd19e7f81010a225c1f61","modified":1709624743709},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\5\\3.png","hash":"e07018c0b52ce32b9f326c49a7544b22750aec19","modified":1709644058944},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\5\\1.png","hash":"67e90e725f052ff9e17461f0c081cf0881ea2b92","modified":1709636805028},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\6\\1.png","hash":"9a4b59e5330ed14ecca4ff8976b9882602877694","modified":1709860334359},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\6\\3.png","hash":"1f6b9d58a0f8d67c607bd6de71c8988b246b6d77","modified":1709860405279},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SRDiff\\1.png","hash":"8859dd1b37ba832fcf39bea466172166a4872ca1","modified":1706331071359},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SwinIR\\3.png","hash":"ad4657eb56551df2f18233832075681a371be953","modified":1705998561333},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SwinIR\\5.png","hash":"05264fcc3adc2b358081b88c0f8cbb3a3ac7bce9","modified":1705999852098},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\1img\\1.png","hash":"32187b3f3d27abf58d84102ec722a3bdfc1aaa95","modified":1707062625616},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\6_resnet\\1.png","hash":"3fe1ac08b72a62b408b1102e92f246e2ad3e4ee6","modified":1714225576696},{"_id":"themes\\yilia\\source\\img\\head.png","hash":"c695336ba203c92400d8801be883217f516ccfa9","modified":1689933128876},{"_id":"themes\\yilia\\source\\img\\datastruct\\4_string\\22.png","hash":"69fc98bbd8b19f77b749ff2aac2fc1d84da041fc","modified":1691550343135},{"_id":"themes\\yilia\\source\\img\\datastruct\\4_string\\6.png","hash":"02c58cbb86448ab130e8de56a8ed30190a270441","modified":1691502055706},{"_id":"themes\\yilia\\source\\img\\datastruct\\4_string\\7.png","hash":"5045a59070a4dc3abf0b75adc7ea361e417a9ac6","modified":1691503180357},{"_id":"themes\\yilia\\source\\img\\experience\\keyboard\\2.png","hash":"bb2c5f88eb5f2530ec237baa4b3bb634252d7138","modified":1709810821276},{"_id":"themes\\yilia\\source\\img\\experience\\website\\2.png","hash":"386f3140232edd163a74963d3f1fb71b631c7ccc","modified":1708748005485},{"_id":"themes\\yilia\\source\\img\\linux\\commend\\7.png","hash":"b5334dff05fddfb776b67cfa62e1bde08ad5a7d1","modified":1672894983674},{"_id":"themes\\yilia\\source\\img\\linux\\usage\\8.png","hash":"bd109f6a36e82cfed8d9b84d2a151852037c9d74","modified":1709379372098},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\17.png","hash":"694d762b0dd7a91ef0118f3cbb744cfa92f1d6c7","modified":1709278609673},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\treesave\\5.png","hash":"53bae744eb1d75032b76b2a34393b483c507d45d","modified":1691590471263},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\BST\\身份证1.jpg","hash":"f4ad46fab9a1874986ccd963cb4f4d8a8602554c","modified":1691897167185},{"_id":"themes\\yilia\\source\\img\\experience\\windows\\powershell\\1.png","hash":"0dbea432a36d62b157691a5eec9ca69a7f994d9d","modified":1710208476780},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\3\\1.png","hash":"b61d9c32cd346e8d35c477d176c55900a84a2ecb","modified":1709561407668},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\6\\0.png","hash":"831b58c87c051ac4f9aa08c9b1f8059e94b4910c","modified":1709882932954},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\6\\2.png","hash":"788b890ce1947fbbf3d21a303346b245f91fa2e0","modified":1709860355659},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\CDM\\1.png","hash":"d0284a0aa9cfc0bb236b9b2d21da006c12aeba82","modified":1706268247587},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\CDM\\3.png","hash":"f1489a76081146c43436f33662b8fb24dd93705b","modified":1706330569143},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SwinIR\\7.png","hash":"e6e1d6e4486b4a67ccf1de20a0c3fcd7e12fc168","modified":1706008471089},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SwinIR\\6.png","hash":"13827355692c474261b3db2cd451e6704ac78953","modified":1706000581563},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\11.png","hash":"d1c328c62f3a3894356be5017e148cf82228ccb3","modified":1706096480418},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\2.png","hash":"4949684eeb63cf7260e919617454a2d551b3f64d","modified":1706094334407},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\6img\\1.png","hash":"9e19fa51e5bb17e6afa8a8fc5ff26081074e890d","modified":1707310830587},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\7_rnn\\3_lmdataset\\1.png","hash":"1922e1609f01d2ac8f91a3de180e7462c2e77c2e","modified":1714656159709},{"_id":"themes\\yilia\\source\\img\\datastruct\\4_string\\11.png","hash":"16f3b25aabe8fabac62e4dee790ebd76f9f5cd7b","modified":1691505320863},{"_id":"themes\\yilia\\source\\img\\datastruct\\4_string\\33.png","hash":"51078f091563b3fc4126d3a1ff3e46adb6dbf1dd","modified":1691551707442},{"_id":"themes\\yilia\\source\\img\\datastruct\\4_string\\9.png","hash":"89fb11a7191f94bd33c84aad1f97b63e15def4e5","modified":1691505233957},{"_id":"themes\\yilia\\source\\img\\experience\\keyboard\\4.png","hash":"cd17b09ae23e58c49be9116984329ffb5799a8e8","modified":1709811806387},{"_id":"themes\\yilia\\source\\img\\experience\\keyboard\\3.png","hash":"dcd35a10bf55442eba698de5c53edead0edac20b","modified":1709811366344},{"_id":"themes\\yilia\\source\\img\\experience\\keyboard\\6.png","hash":"a61ddd048d5a3e6db9da2d0e1ddc9117122c0cc0","modified":1709812315849},{"_id":"themes\\yilia\\source\\img\\experience\\keyboard\\7.png","hash":"0d09d0ed585febea50d08c80671eb9f67a86fc2f","modified":1709812365173},{"_id":"themes\\yilia\\source\\img\\linux\\commend\\5.png","hash":"9877ec4411499460626515956172ac17c0461194","modified":1672894495658},{"_id":"themes\\yilia\\source\\img\\linux\\usage\\9.png","hash":"00fd2fdb4fa8668c63949e5d6b78a8ea1a046ec2","modified":1709379835031},{"_id":"themes\\yilia\\source\\img\\datastruct\\2_linearlist\\linklist\\5.png","hash":"c3b59117bc61e06960711487d2c9faae1d8490af","modified":1691228570021},{"_id":"themes\\yilia\\source\\img\\datastruct\\2_linearlist\\linklist\\6.png","hash":"37e2c7e31f6b34a0c678c3241da2028ea694b61a","modified":1691304383828},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\stack\\4.png","hash":"ccf102d7ac6a969e1bdcd81001538a5e14bc0685","modified":1691420162004},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\2.png","hash":"cfd647157788822a1ee87152474331d6b72983c1","modified":1691556786300},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\3.png","hash":"bb65dc7f7d243290cdfc39e6ad91d29a44a5a978","modified":1691556831286},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\BST\\身份证2.jpg","hash":"c7248cff105487f06b45d89d6b51e972489f667a","modified":1691897146994},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\DAG\\1.png","hash":"f518bf1aa4309d3129066c1f7a4de9d0126bfc64","modified":1691759834419},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\4\\2.png","hash":"6645a10d38500e53719a0c399ed9809d7a51a914","modified":1709607717051},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SwinIR\\4.png","hash":"d5be991310455fd4f492c541ef0b327914c7c15b","modified":1705999226659},{"_id":"themes\\yilia\\source\\img\\datastruct\\2_linearlist\\4.png","hash":"b0f7022db8b8bdf2dfd25b3be51b483c35c86632","modified":1690708324052},{"_id":"themes\\yilia\\source\\img\\datastruct\\1_extract\\1.png","hash":"f62dbc992f719d4edde6745400129010f2df805f","modified":1690602999185},{"_id":"themes\\yilia\\source\\img\\datastruct\\1_extract\\5.png","hash":"f8bc031e79eea55114892711cede3ef873029495","modified":1690684253517},{"_id":"themes\\yilia\\source\\img\\datastruct\\1_extract\\6.png","hash":"58eecabd401a7bb5dab4706e4a65b0ad3ab55d3e","modified":1690688739273},{"_id":"themes\\yilia\\source\\img\\experience\\keyboard\\5.png","hash":"25b5d4f99e72c3c14b56c0eedc0a5f27c31b64b2","modified":1709812265655},{"_id":"themes\\yilia\\source\\img\\experience\\keyboard\\9.png","hash":"4b16a65451bb3a20785c60d2a02fb3d1a1bd0a0b","modified":1709812391646},{"_id":"themes\\yilia\\source\\img\\experience\\keyboard\\8.png","hash":"4c1c98aa0f39491c23244e6a1031c976c2eb464a","modified":1709812383021},{"_id":"themes\\yilia\\source\\img\\linux\\usage\\10.png","hash":"ea8380762314a5543160492a0c0dc4b74bd82244","modified":1709380166394},{"_id":"themes\\yilia\\source\\img\\linux\\usage\\5.png","hash":"dd649fda55ce139c7604954518cd6ebdb5d535f1","modified":1709374508794},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\queue\\1.png","hash":"c188269b5cbcbb8fbc67391b7be94cfea7ca8ecb","modified":1691420396781},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\matrix\\7.png","hash":"303a2e13032a6ab2b2e45c50c1e3666232dc39ef","modified":1691498549869},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\6.png","hash":"dc6fafa3d3c1dac6a0df6a46537738fb955991bc","modified":1691564823431},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\7.png","hash":"a4960ce0bb113834995b83f1e72a5f1d59b5b580","modified":1691564859427},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\set\\1.png","hash":"f1791e355115d078e7858dcb207f8e87bbd736d6","modified":1691591913410},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\set\\2.png","hash":"33126a20ebac6b4dd33fda2b1337f0132e432f28","modified":1691592072637},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\tree\\2.png","hash":"c36f61b2f331b4794bb7affb5653a3c70b6bf7eb","modified":1691556321740},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\AVL\\4.png","hash":"42244dfc16a8545eb1d6f29cbefa1d7b10b8908f","modified":1693034802616},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\generatetree\\3.png","hash":"273b817773cfca2cefedf37e00beffce3aac979d","modified":1691746400702},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\generatetree\\2.png","hash":"7e507b8326c93a6720e559066f42886e7a8255e9","modified":1691746322625},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\DAG\\2.png","hash":"3a0427868ede3bc4fbd980e1eb44d1ddcf8ba662","modified":1691759872116},{"_id":"themes\\yilia\\source\\img\\experience\\windows\\ssh\\2.png","hash":"3c96724ab9a9441e1e483a9f178cc1a5d093bd27","modified":1710148477056},{"_id":"themes\\yilia\\source\\img\\experience\\windows\\ssh\\3.png","hash":"afca0a1a9ac95645e8929fb982258508361b9824","modified":1710148528755},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SwinIR\\1.png","hash":"103d09498654fe7acde2c2bd20371fe75cba9456","modified":1705995313277},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SwinIR\\11.png","hash":"5f5dd2e2cdc0161d68f514f5f6fa92a17097cc15","modified":1706008853684},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\8.png","hash":"090cb62b2d7077ec1bddd563b06245be0a144a7e","modified":1706096218818},{"_id":"themes\\yilia\\source\\img\\datastruct\\2_linearlist\\2.png","hash":"8c9d5f586b67d8973d280205ad376279861d9fa3","modified":1690705661628},{"_id":"themes\\yilia\\source\\img\\datastruct\\1_extract\\8.png","hash":"f46809238bf064e700e57d6e341b568072b9c1ef","modified":1690688916784},{"_id":"themes\\yilia\\source\\img\\datastruct\\4_string\\8.png","hash":"c6fff07a1454c9886d48afe21a3b2dcf4190090e","modified":1691503712680},{"_id":"themes\\yilia\\source\\img\\datastruct\\2_linearlist\\doublelinklist\\1.png","hash":"400a7006ed7e701ecf338868d027fa9d4fc09aa4","modified":1691311294470},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\exam\\1.png","hash":"0355d11dc2d67894016a00819bb2f9fa048e3456","modified":1691467561263},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\queue\\3.png","hash":"7389121d9a2981d2f7a913f27ceb495c9b3e3f85","modified":1691465700780},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\queue\\5.png","hash":"1d3549ac63517ed5a5fdb36c60ec68570530c6ee","modified":1691466497758},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\matrix\\9.png","hash":"483683c7cd2491fcaab026f2501fb097a99e5af2","modified":1691498712107},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\stack\\1.png","hash":"320bb1490c7237abc6cfa31f75d80a2ff667d10e","modified":1691412904112},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\stack\\3.png","hash":"bbfeff2baa2a09d1cfde6efd8ced65950969d451","modified":1691419743434},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\1.png","hash":"f2ab94acf03f8427536230e7e4492f6286a8863b","modified":1691556489183},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\AVL\\5.png","hash":"2ac3fdffca3ece0b482b346dc80124c9b1a9b293","modified":1693034837983},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\search\\1.png","hash":"abcd55aa8269afb2ec7d383f5bc03379182b18e3","modified":1691764394502},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\search\\8.png","hash":"b86ec6b6a477e655a484cb62d2a8372e71bbf398","modified":1691764991881},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\red\\1.png","hash":"24ed2aaf9c398cf0bd548949cc530a82f26bbe3b","modified":1693036044722},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\red\\5.png","hash":"91fa356e862977aedcd8380883ebba68ce55ff51","modified":1693039095844},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\generatetree\\4.png","hash":"4bd74ef02a7ce777be6d6be132b809eadd333795","modified":1691746532552},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\DAG\\5.png","hash":"0ef37d1a33ac8f7adfc64f2df93ddc46db02a255","modified":1691760417873},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\10.png","hash":"f8f49b35a8464be6d441c4d1c9d89eacd46c99d0","modified":1691743303424},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\17.png","hash":"96a3732857d13b320d44a5d2bba58b24e63eb2c3","modified":1691743889302},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\21.png","hash":"96cd917e4e00ec460d56bf1497a6ecf875ac2412","modified":1691744568147},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\shortestpath\\4.png","hash":"f198322bd8b57e7f7730ad78a1502b72d34af28b","modified":1691756652403},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\4\\5.png","hash":"e73081332379c3fd8f6f2ae1f620359d0a757fa5","modified":1709623988958},{"_id":"themes\\yilia\\source\\img\\datastruct\\2_linearlist\\1.png","hash":"f3127e2c823a2115d3eee9c9186e66775f7c8c56","modified":1690689329458},{"_id":"themes\\yilia\\source\\img\\datastruct\\1_extract\\7.png","hash":"719ea9a0ff7bdeb47bb6068bacaf8799080544bd","modified":1690688753401},{"_id":"themes\\yilia\\source\\img\\datastruct\\1_extract\\9.png","hash":"352ebd7320c0daf59164cc606c54748d4461364c","modified":1690688988499},{"_id":"themes\\yilia\\source\\img\\datastruct\\4_string\\3.png","hash":"0263d9a7b8552db5293da17683a78fc6b18add52","modified":1691499407217},{"_id":"themes\\yilia\\source\\img\\datastruct\\4_string\\5.png","hash":"48c8a60a2eade60271df902361219dc60182b147","modified":1691499684778},{"_id":"themes\\yilia\\source\\img\\linux\\commend\\6.png","hash":"1ad966f50410b760b5c4da236b00fa3c87d6e794","modified":1672894794061},{"_id":"themes\\yilia\\source\\img\\datastruct\\2_linearlist\\linklist\\1.png","hash":"93d909b592f79bb93cb05e461f1cfe5b19975588","modified":1691217017748},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\exam\\3.png","hash":"d4697e7aa4faf3faaec31795d056369aee9a0747","modified":1691477782460},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\matrix\\8.png","hash":"a14b20161150d5ec27c26a8769d2fbc63882cc02","modified":1691498617510},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\11.png","hash":"d6da827299c4424e820f13b641b8cf1d7dcc8647","modified":1691573880053},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\14.png","hash":"e09f44ccfd173d7e22a664ba1da0496578fe71fa","modified":1691573669468},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\16.png","hash":"ea7d466a5c4df3dab2b7d5fd1db34983ad20bcc9","modified":1691573739880},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\21.png","hash":"01cce10b1c97bfe85564457670c02523f9e22bbf","modified":1691586981364},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\4.png","hash":"a1a7f2f52dea3bb78b7e610b99cd46c8eb638324","modified":1691561708831},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\9.png","hash":"617b5e71fbb4b6cbde53988e84b7db14227521aa","modified":1691565503866},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\hafmantree\\2.png","hash":"4b489a3880d5c72496e6bb3e9a0fb1867c72a61e","modified":1691591243003},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\tree\\3.png","hash":"5c23b6d576d9ab930ff7bed0ef93f7286fece156","modified":1691556359280},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\treesave\\8.png","hash":"fa5bb9c206f9d820b4829b82a815acfe9535f698","modified":1691590739780},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\BST\\2.png","hash":"11152e9108147bf5ddaf92da0a6ae3b40310f0f3","modified":1691765426369},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\BST\\4.png","hash":"4ce88adb5b069d4d0f3e019c8baaff3daf93577d","modified":1691765563689},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\search\\2.png","hash":"ee0dcdd98d36ebf968203e086842f82983833393","modified":1691764532822},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\generatetree\\10.png","hash":"8f1df1e8b36321baa8454a6e408fc3132f3a0b49","modified":1691746859035},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\generatetree\\5.png","hash":"c31e028791baa33832977eba8885d7c2fa495f5a","modified":1691746557384},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\generatetree\\8.png","hash":"ad9e53a5806f86ec4f2e076279d81ba1faf82b80","modified":1691746765810},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\generatetree\\9.png","hash":"9af35488623060bb152e66796eed3d11de500b69","modified":1691746775997},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\DAG\\10.png","hash":"367b5db4a5df4056bc852b4d15d90569f16d47ad","modified":1691762989848},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\DAG\\12.png","hash":"2c57ffd92b97d2c5cb9cd6e86d45ba7a7c3ef152","modified":1691763259051},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\DAG\\11.png","hash":"2c57ffd92b97d2c5cb9cd6e86d45ba7a7c3ef152","modified":1691763100420},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\DAG\\13.png","hash":"f82c00b83d4707743244ced4344eb173088f6537","modified":1691763299482},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\DAG\\7.png","hash":"1640ed8edab3a2235f8d1890dd911df427202d5b","modified":1691760901602},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\DAG\\9.png","hash":"1c087e833e9d95f77fdfc1c5393872f67b161620","modified":1691762393639},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\11.png","hash":"3b8f079b47feb4ead637c8e8799668081da3b3c7","modified":1691743725482},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\14.png","hash":"ad20949a8476a1eab20b23a627f5d1c2c5245b72","modified":1691743791966},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\13.png","hash":"1a0b6b8999306d37bb4e5e021dcd2860fb91003a","modified":1691743782462},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\12.png","hash":"f5f8a7f60d80bdea3901f24d757d71f354b21076","modified":1691743739055},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\16.png","hash":"8accfd8baed48f5129fd22bc026b3baeea3f53dc","modified":1691743869442},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\19.png","hash":"3b6cc333c977fabc8759d19010d40eadfec4e126","modified":1691744359775},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\shortestpath\\1.png","hash":"4f739f585dcd7d2fadc90c2ec487d810e50b12b5","modified":1691747129640},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\shortestpath\\12.png","hash":"8af073f178b71de473f6779005b693b6fe931e35","modified":1691757741065},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\shortestpath\\5.png","hash":"ccabac77b9f87025d39cb178c736ab08be966a53","modified":1691757014920},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\shortestpath\\9.png","hash":"6cdb024dea1b8f80280a985b970deb30f92229dc","modified":1691757054076},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\1.png","hash":"9645f93ecfba9f5b9206fde1c0aa73917896f582","modified":1706093510193},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\5.png","hash":"a743c446dfcf9b937205f58dc028f41d070e48b5","modified":1706094671973},{"_id":"themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\6_moden_conv\\6_resnet\\4.png","hash":"96001a0cfeffb33021afbea123aa58b02a544191","modified":1714286122578},{"_id":"themes\\yilia\\source\\img\\datastruct\\2_linearlist\\3.png","hash":"5a8a80187065f83608b3d27cf134ca2b1345ea50","modified":1690708244606},{"_id":"themes\\yilia\\source\\img\\datastruct\\4_string\\2.png","hash":"fc16f97df3cd583499ebfdc08a17a59d584f5dbe","modified":1691499178548},{"_id":"themes\\yilia\\source\\img\\linux\\user\\8.png","hash":"0b8417b51075ac859bcf37582f6ca9cdc6d1ed9e","modified":1672921178317},{"_id":"themes\\yilia\\source\\img\\datastruct\\2_linearlist\\doublelinklist\\5.png","hash":"2fd148d973e964c5a66a6677e08c8d40bb4086a7","modified":1691412524458},{"_id":"themes\\yilia\\source\\img\\datastruct\\2_linearlist\\linklist\\2.png","hash":"e9347e1f225c5cbb9c549e8dfb9f5bec9b0aeb0c","modified":1691222122560},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\exam\\4.png","hash":"a3e580c7a52b96615056d54e0901e927210ec6bd","modified":1691482513563},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\queue\\2.png","hash":"372e33824e44870739b0b2dd31473891d8be39d5","modified":1691421821662},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\queue\\4.png","hash":"ccc7388fe9d234b85dfaf0f13df2011fdd448cc9","modified":1691466312459},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\matrix\\1.png","hash":"4232903681e5acd2874ed02adbb613f650cbd3a4","modified":1691485812683},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\matrix\\2.png","hash":"e5f12710a7ee824a071f4b667c21c6adcacf57f8","modified":1691487476795},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\matrix\\4.png","hash":"52cfb649fee62c2d0248b8a19634016a6e7dfeb0","modified":1691487901540},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\12.png","hash":"7b9ec3a8aeea46a650d5f324f6bcd450d7c6f1ce","modified":1691573450339},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\13.png","hash":"5c51b87683faf0b42ff105b4b509847ce28f033c","modified":1691573506131},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\15.png","hash":"642ac58ff76d07fdb5f34aa690c5d9198947b50b","modified":1691573704026},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\19.png","hash":"4850932f1aa82436d5d483bf8c65762911e63d5d","modified":1691575204498},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\20.png","hash":"e3ccd07772d3937fe059f7030cb5637d07272705","modified":1691575240008},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\5.png","hash":"5606a7bf100e9f72e25622c5364701c62e995057","modified":1691561772170},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\hafmantree\\1.png","hash":"c93b19d02e8eecbcd748e4e7d0b212fdb258e3d2","modified":1691591181645},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\set\\4.png","hash":"60b7cd6266dd054a37d619083c49e76d936471e7","modified":1691592496783},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\set\\3.png","hash":"2656e0c353bab48a03f657aff960e69d4ccf488e","modified":1691592192803},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\treesave\\1.png","hash":"5f3d151b0ef530ec331a88871f4c790ad1e7de76","modified":1691589073455},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\treesave\\3.png","hash":"554d3cbfb2d7a8276695dfab55b6fa00d6b41e29","modified":1691589958934},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\treesave\\4.png","hash":"0ed7fed4738e93c716e7d8947eb29ec60bbcb3b3","modified":1691590339553},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\treesave\\7.png","hash":"b0407aed45fdf2778c37aeb86e16f0d76246bd1b","modified":1691590661218},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\AVL\\3.png","hash":"8736726d7645894dc84807c3e9ce0e54fa76c717","modified":1693034747447},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\BST\\1.png","hash":"55a425118fb566256fc2dcb7267670c2e0c8b6be","modified":1691765227262},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\search\\3.png","hash":"b167c48347e3f40f71169249aa9e57149f456030","modified":1691764704613},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\search\\6.png","hash":"ec0afe78ba75251e35923d39189ae047b28b0898","modified":1691764882087},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\red\\3.png","hash":"e07c568186d1ff720b927af647502bf84095c922","modified":1693036644495},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\generatetree\\6.png","hash":"f70727738f1d45d51a0b5ace85e38f9ef4fbefbc","modified":1691746574409},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\generatetree\\7.png","hash":"430730c045a26ff7a2671b3bf42b81d36e288ec3","modified":1691746756084},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\18.png","hash":"23ab682c9b0e7ed5a5372d575801e89b1735a9c8","modified":1691743941833},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\2.png","hash":"30c586539ef983e91f127a9d117be4334832c7b4","modified":1691734075098},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\22.png","hash":"6db8e47e30d615832eebb062a162791b7c310fd4","modified":1691744785301},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\26.png","hash":"186dae57f64593ba80a9484c2b1746c1f8d29999","modified":1691745783999},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\25.png","hash":"04097ccd7e31337fc15b0b4e6590f0e46ed5e92c","modified":1691745493311},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\4.png","hash":"7a3bb1906f0ad5afccbed87ff851e3819cbafc36","modified":1691734224044},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\6.png","hash":"5617cd959a09a2687fe2cb9a32abfdf070d389fa","modified":1691734588767},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\8.png","hash":"177c49dc3942bb19e887c89c2e795bc944d2994c","modified":1691735103874},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\shortestpath\\2.png","hash":"bfe530abb9fac2ea1b2eb295a832de6b1f725218","modified":1691756495914},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\shortestpath\\11.png","hash":"ddf8ab48eb171c8ef1e1f0114616ad849ddfb034","modified":1691757712933},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\shortestpath\\13.png","hash":"f09b3c157a8364de40821317b2677a175b3f4dc6","modified":1691757797077},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\shortestpath\\6.png","hash":"707dbf184c801ed3e8a6f0c333ab4797dda2b238","modified":1691757023476},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\shortestpath\\7.png","hash":"daf3d4a4a195019ed2629e892f3fd8e2aee5978c","modified":1691757032146},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\shortestpath\\8.png","hash":"4320444060e2fa35d0b0e4dc8f33a03cec5bea7d","modified":1691757043016},{"_id":"themes\\yilia\\source\\img\\experience\\app\\mouse\\2.png","hash":"2f146858a80f46078881f11bc45721aa9bff1646","modified":1710751222696},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SRDiff\\2.png","hash":"bc499602dd3514cfa3e48a170a0254f98de118a9","modified":1706338981638},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SwinIR\\12.png","hash":"35982c423def1509d333ab9842840239de01b7d2","modified":1706008939838},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SwinIR\\9.png","hash":"2d2e66a07ec289a15511a707862c44c0fa9b9c6e","modified":1706008781907},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\7.png","hash":"b0c033daaece2d1342f4017cd64850e19e1973ea","modified":1706094875609},{"_id":"themes\\yilia\\source\\img\\datastruct\\4_string\\1.png","hash":"dd8f568d408f4066857dcaf5f837b76ff72681ea","modified":1691498978232},{"_id":"themes\\yilia\\source\\img\\datastruct\\2_linearlist\\doublelinklist\\2.png","hash":"ca655c8903c294c2561adf00e03115d85daac0d3","modified":1691383605503},{"_id":"themes\\yilia\\source\\img\\datastruct\\2_linearlist\\doublelinklist\\4.png","hash":"7f6f39075bd9d99fc80bed11a9572929861608fe","modified":1691384213543},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\matrix\\3.png","hash":"de128228b37c7f67472db3f69319469c15577bb1","modified":1691487846644},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\matrix\\5.png","hash":"ddc34b510edf878733390cf6280c17e24430ca44","modified":1691498193472},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\matrix\\6.png","hash":"5796e5f5deedd912d609d786b9bcdf81e5952265","modified":1691498424660},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\10.png","hash":"809602cc17c15b735981b02272c065543b829903","modified":1691573912441},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\18.png","hash":"0c53deb30a095262bace6fe55d49b9528d79932b","modified":1691575069273},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\8.png","hash":"da3f2d7eb809212e929a5124c89161424edb4b19","modified":1691565191904},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\hafmantree\\4.png","hash":"6e0aa2e3cb77d483683e3d0d11cf91ed42a1b86f","modified":1691591459593},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\tree\\1.png","hash":"adf29d834bac15c3cbb5a59ffe8ad8beb0e84a37","modified":1691555053411},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\set\\5.png","hash":"28d6733723d49340995dec2fde9e88dd9f0adb05","modified":1691593008586},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\treesave\\2.png","hash":"a3b2cc5ec1d3f6641c562ec91fd67158681d54fd","modified":1691589184598},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\treesave\\6.png","hash":"29ebb70f952f15f45953541376e244d40361dce0","modified":1691590617234},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\AVL\\7.png","hash":"8cdb063bca970837e89246b7c242aa8df4ef69ab","modified":1693035869074},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\search\\4.png","hash":"52e1734955fdec29f5967e9ee9474d2cddc98d86","modified":1691764731437},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\search\\7.png","hash":"66712ddccabed87d61c763cac232635c37dfba4c","modified":1691764955669},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\red\\4.png","hash":"1565be0433a56f3b59c04e9ac45580ed38c6e49e","modified":1693039041843},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\DAG\\3.png","hash":"ab89866de5a90543cb94ba68f91772cbea6885c0","modified":1691759917325},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\20.png","hash":"e1ea39884791587ef05f620d70bea10cad4517ca","modified":1691744388175},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\7.png","hash":"17fe699b83593d7b5a5a63cf6f8e96850e5a0c78","modified":1691734656378},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\shortestpath\\10.png","hash":"904d546f835be088ef8c6431fcba3099f29a89b6","modified":1691757619745},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\shortestpath\\14.png","hash":"e5fe49cda495ad811654a44c5c4df75c58a26b15","modified":1691757816077},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\shortestpath\\3.png","hash":"4a56bcbe87473960ae402584e0da20c8c16cfee1","modified":1691756527686},{"_id":"themes\\yilia\\source\\img\\datastruct\\4_string\\4.png","hash":"7838d4d94535fa7a23ea70e174f18224109af7a3","modified":1691499621674},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\stack\\2.png","hash":"a908d6604057c26915ac3f6755cbb618fc315bac","modified":1691419128920},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\22.png","hash":"b42865762e3f79e13238c99d3fea6e9a34b48b7a","modified":1691588893459},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\AVL\\2.png","hash":"62329d059d1524ccb694e74674724d123b1d1ccb","modified":1693034737641},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\AVL\\6.png","hash":"81c02a6561ed5dfb1a5d2523c54f6f3d033c1828","modified":1693035044769},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\red\\2.png","hash":"561a4f0a6d89e9dc5059fe6ad41fcf3ccb064c71","modified":1693036214634},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\generatetree\\1.png","hash":"a38f3f3818fb59ffa2e0c2f149baae5ad3e08906","modified":1691746022745},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\15.png","hash":"ddaa0073e7a050c6da2e15bb92edef4b263e71f8","modified":1691743828007},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\23.png","hash":"507a10492249735e4eff71110dccc08e32d75723","modified":1691745075647},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\5.png","hash":"e526a1db7825500f3915b81cff0b2a0649501224","modified":1691734323307},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\9.png","hash":"af0f3d264f6346953c00662e13262590c718288d","modified":1691742551640},{"_id":"themes\\yilia\\source\\img\\datastruct\\2_linearlist\\5.png","hash":"d4eecc47637ac57e721533941890b5db93c6cd96","modified":1690710397035},{"_id":"themes\\yilia\\source\\img\\linux\\usage\\6.png","hash":"3596c1c5283d231a1b453e01963033a8d698611b","modified":1709378392775},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\bintree\\17.png","hash":"7e9e417dd7e92cf4518e7478df7a61dc792b5c79","modified":1691574473511},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\AVL\\1.png","hash":"1cba4a7760b4e3198c1d948e381a13e76b5522a5","modified":1693034311826},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\BST\\3.png","hash":"9d522c4b3ed9fc8d4cad85e1d574895e0edf5ffd","modified":1691765512206},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\DAG\\6.png","hash":"160fc9cbaf10916bf63315abe80c023af57cb663","modified":1691760482169},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\DAG\\8.png","hash":"6d7b99c9cfe359e941a1a0a9f733fd37ca979430","modified":1691761005967},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\24.png","hash":"24195d899250ca557aa68f3c5b50872ebf724b25","modified":1691745314935},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SwinIR\\8.png","hash":"a057c763efb0ce8ad6ed81c7fcc687b8cfd61d64","modified":1706008706238},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\exam\\5.png","hash":"70eb4243c88e9cb6f9d64dc4b1ce0123f93d7b70","modified":1691483230497},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\search\\5.png","hash":"1be79056d4fb1d9c03850ce2161c2f2c22373751","modified":1691764808300},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\1.png","hash":"3c95625309a2e2f7fddf78c14bc972a018c82916","modified":1691725714454},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\graph\\3.png","hash":"1c63775db55f4624289bc1ef592ed88d4c2258ab","modified":1691734140060},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\exam\\2.png","hash":"23650f4e8a06111ed5639cc0891fd7490c8eb7b7","modified":1691476660769},{"_id":"themes\\yilia\\source\\img\\datastruct\\5_tree\\hafmantree\\3.png","hash":"20d3db5968e55586130cdf0fc98d5dcf77d79408","modified":1691591377972},{"_id":"themes\\yilia\\source\\img\\datastruct\\7_search\\B\\1.png","hash":"bba1aaa2fb53ede0dff30e60d86ac9c9e7ad3cc8","modified":1693039964839},{"_id":"themes\\yilia\\source\\img\\datastruct\\6_graph\\DAG\\4.png","hash":"1aee5cdce7b83cbb17cc5c7c14ac9d0a02a787f8","modified":1691760000494},{"_id":"themes\\yilia\\source\\img\\datastruct\\3_stack\\exam\\6.png","hash":"721fc6eedb785c67120633ed94159a0d4ea49270","modified":1691484394221},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SwinIR\\13.png","hash":"75213541cc70afc1d1918c479ac78f8d56850e52","modified":1706009262701},{"_id":"themes\\yilia\\source\\img\\java\\produce_practice\\2\\6.png","hash":"d36ac357d5632da635fc28f525bda84457954a28","modified":1709557855900},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SwinIR\\10.png","hash":"88289ae9893a394b64ec2faf0759100abbf224a1","modified":1706009183949},{"_id":"themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\9.png","hash":"a43c143ea90005eb68c6b17343f20d5526e99d2e","modified":1706096356459},{"_id":"public/categories/index.html","hash":"fafb28042a168c049000b79e15ed782733c95633","modified":1714977814509},{"_id":"public/tags/index.html","hash":"30735289a009d3d08563415f60af15a389edcb9a","modified":1714977814509},{"_id":"public/2024/05/06/hexo/hexo搭建/index.html","hash":"85faea79ea01fba905530aa93656ed63c06a5b46","modified":1714977814509},{"_id":"public/2024/05/06/datastruct/7_search/5_Btree/index.html","hash":"8acd92c969ed0b08b25e7fa36fc2b1acdbc65c90","modified":1714977814509},{"_id":"public/2024/05/06/datastruct/7_search/4_redblacktree/index.html","hash":"77e5788e30544298044cea81c6b337e908a3d2c2","modified":1714977814509},{"_id":"public/2024/05/06/datastruct/7_search/3_AVL/index.html","hash":"eb777f3952b809c2eeb8a31dcf50a6510916a04a","modified":1714977814509},{"_id":"public/2024/05/06/hello-world/index.html","hash":"86d79c794c2d0dc3e0309bac7672bab1edde6339","modified":1714977814509},{"_id":"public/2024/01/27/deeplearning/paper/SR/Resshift/index.html","hash":"9ef2ddc7085c23f2a57d57003de546526863f131","modified":1714977814509},{"_id":"public/2024/01/27/deeplearning/paper/SR/SRDiff/index.html","hash":"93930a2a9815812157cb9a4441ecc8328f3aab2e","modified":1714977814509},{"_id":"public/2024/01/26/deeplearning/paper/SR/CDM/index.html","hash":"ff7d5021cef96824430c9340ea71e83a9fd2288c","modified":1714977814509},{"_id":"public/2024/01/25/deeplearning/paper/SR/HAT/index.html","hash":"37be3231bf5fa9082cb5c6cd6dcc1eeeee708ad5","modified":1714977814509},{"_id":"public/2024/01/25/deeplearning/paper/SR/SwinIR/index.html","hash":"7861ae1c05f316c5248e4e7c15b1a19fc5870662","modified":1714977814509},{"_id":"public/2023/12/17/experience/pycharm/link_server/index.html","hash":"e3fc76d4cb8edb1de900c01022b439d55e0ae7c3","modified":1714978435246},{"_id":"public/2023/12/12/experience/app/lrc_to_smi/index.html","hash":"6d3300206453db0b7356743ad8e73fdc562ee1bd","modified":1714978435246},{"_id":"public/2023/11/27/deeplearning/code/pytorch/1_prepare/2_preprocessing/index.html","hash":"103d194c532059c860326d10b936d070f91e036d","modified":1714977814509},{"_id":"public/2023/11/11/deeplearning/code/pytorch/1_prepare/1_dataoperation/index.html","hash":"6b73eb7d0019d923cddd62bc4033358c10909928","modified":1714977814509},{"_id":"public/2023/10/02/algorithm/1_base/1_base/index.html","hash":"e2570098f9663c8eefc8e82451e7580cb4d5e28a","modified":1714977814509},{"_id":"public/2023/08/07/datastruct/4_string/1_string/index.html","hash":"7ab8bc05b278418194886c3d65790a600c5a9c6d","modified":1714977814509},{"_id":"public/2023/08/07/datastruct/3_stack/1_stack/index.html","hash":"951386198b85b2f103266f73363a2bdcaa45635b","modified":1714977814509},{"_id":"public/2023/08/07/datastruct/3_stack/2_queue/index.html","hash":"c11449159d292ae5f51dff64549546e77706b2aa","modified":1714977814509},{"_id":"public/2023/08/07/datastruct/3_stack/3_exam/index.html","hash":"2196a0d9eb3acac38d480258a4f0192aa198d831","modified":1714977814509},{"_id":"public/2023/08/07/datastruct/7_search/1_search/index.html","hash":"7ff8d33f07d09b34f4f2ef40f782719e1039174d","modified":1714977814509},{"_id":"public/2023/08/07/datastruct/7_search/2_BST/index.html","hash":"074e2b5545881a45d7cfdbc02ef22ba8e95ccecd","modified":1714977814509},{"_id":"public/2023/08/07/datastruct/3_stack/4_matrix/index.html","hash":"b9cbad72caa59a506ad77519e86dd599d4033e43","modified":1714977814509},{"_id":"public/2023/08/07/datastruct/5_tree/1_tree/index.html","hash":"47886c794484c88c732248645bcd007865d3f4f7","modified":1714977814509},{"_id":"public/2023/08/07/datastruct/5_tree/3_treesave/index.html","hash":"df11522b55742027f1d3666e562b7fb77b136f48","modified":1714977814509},{"_id":"public/2023/08/07/datastruct/5_tree/4_Hafmantree/index.html","hash":"f535a87325f689f68be3b12d4e103845a56f7e05","modified":1714977814509},{"_id":"public/2023/08/07/datastruct/5_tree/5_set/index.html","hash":"5b20e8c64789c0b89e302aded503502e24e75ef3","modified":1714977814509},{"_id":"public/2023/08/07/datastruct/5_tree/2_bintree/index.html","hash":"da01587dea86f20c8af72d0410b28f581cf71c3c","modified":1714977814509},{"_id":"public/2023/08/07/datastruct/6_graph/2_generatetree/index.html","hash":"14b3738117e685d6b6881defcac48777c98652bb","modified":1714977814509},{"_id":"public/2023/08/07/datastruct/6_graph/3_shortestpath/index.html","hash":"726af7097bf3e8c14751df9978617649eea2e3e4","modified":1714977814509},{"_id":"public/2023/08/07/datastruct/6_graph/1_graph/index.html","hash":"eba8189945d0ccee1dc765fd1d2ff89206f7e40f","modified":1714977814509},{"_id":"public/2023/08/07/datastruct/6_graph/4_DAG/index.html","hash":"8db49f7498228a624e2e8c5f22b5c619fac5b3e8","modified":1714977814509},{"_id":"public/2023/08/05/datastruct/2_linearlist/3_doublelinklist/index.html","hash":"d5abd4f09d42102433ff3ea5f5a610c576093d03","modified":1714977814509},{"_id":"public/2023/08/05/datastruct/2_linearlist/2_linklist/index.html","hash":"98b486ca4aaa58a6a70a58b998950bcb87d1f2fa","modified":1714977814509},{"_id":"public/2023/02/01/datastruct/2_linearlist/1_linearlist/index.html","hash":"b35f2763d26973d377d0ef84dbe21a608f2f98dd","modified":1714977814509},{"_id":"public/2023/02/01/datastruct/1_extract/1_基础/index.html","hash":"fd02d5c4f3c21eb2ad602eb1f948471a7e815d5c","modified":1714978435246},{"_id":"public/2021/03/01/linux/Linux_0安装/index.html","hash":"4daf48470ad1c2b8b9fdb8512e0b3de4c3d5853d","modified":1714977814509},{"_id":"public/2021/03/01/wechartapp/weixin/index.html","hash":"2ab8a294c291fa8e15d7b33052d5a8ab59235975","modified":1714977814509},{"_id":"public/2021/03/01/linux/Linux_1.基础命令/index.html","hash":"8ab03f4a93ff38ec16a70ede149dcc0eb8545949","modified":1714977814509},{"_id":"public/2021/03/01/linux/Linux_2.用户和权限/index.html","hash":"284222a5b0db0e287715f7a8e50105cc22c11b0a","modified":1714977814509},{"_id":"public/2021/03/01/experience/vscode/vscode/index.html","hash":"1de4ddbc7b2e84e28ee5715496878e5ca215bb8c","modified":1714977814509},{"_id":"public/2014/12/24/hexo/hello-world/index.html","hash":"c04085a69cf6c2db3bcceb8849d532c1ac3af6fd","modified":1714977814509},{"_id":"public/archives/index.html","hash":"e76234ff12c334a69e78aecf5541402d641a7fee","modified":1714978435246},{"_id":"public/archives/page/2/index.html","hash":"ad0f50a5ddff7a476bb35a2d0fccd1596b316f9e","modified":1714978435246},{"_id":"public/archives/page/3/index.html","hash":"39f05aaaf149743ee1f458b74151f5e7b880d42b","modified":1714978435246},{"_id":"public/archives/page/4/index.html","hash":"cfdd91210284e0229895cd472e7642aa7c8e4014","modified":1714978435246},{"_id":"public/archives/page/5/index.html","hash":"8e004366887432d916e15bd1dc0a98cde1a930a0","modified":1714978435246},{"_id":"public/archives/page/6/index.html","hash":"ba6087a3c2a839ce0ee71ec8654c59c0a634b221","modified":1714978435246},{"_id":"public/archives/page/7/index.html","hash":"df154e230d28c2b2cb50f6a16f2032fffd304002","modified":1714978435246},{"_id":"public/archives/page/8/index.html","hash":"69af3fe7255e9a1c842d614aeec8467301b57f64","modified":1714978435246},{"_id":"public/archives/page/9/index.html","hash":"4ba7568bacdc5b9352949497fe7a124e9a401afe","modified":1714978435246},{"_id":"public/archives/2014/index.html","hash":"d307e5d35fb900f2b983fe3904d2b374086e9dbf","modified":1714977814509},{"_id":"public/archives/2014/12/index.html","hash":"9927b7d1ad037de0dba2aabdf4586e7efaceae03","modified":1714977814509},{"_id":"public/archives/2021/index.html","hash":"05297d5e3c71617f3270a2526a275980e92fc96f","modified":1714977814509},{"_id":"public/archives/2021/03/index.html","hash":"c7e919f15abc8c8bf5796a94fa5a43406d6e3bce","modified":1714977814509},{"_id":"public/archives/2023/index.html","hash":"906bbcdb20105773405dd0397f85049418a32599","modified":1714978435246},{"_id":"public/archives/2023/page/2/index.html","hash":"1cb74214a11e5a6df31a5e46b228c3540dab52b8","modified":1714978435246},{"_id":"public/archives/2023/page/3/index.html","hash":"9a0bc2996999b8b81f469871964f8fe674f5e809","modified":1714978435246},{"_id":"public/archives/2023/page/4/index.html","hash":"c5a24ba01d599475ba5d6a6f9f59f79553e6c8b4","modified":1714978435246},{"_id":"public/archives/2023/page/5/index.html","hash":"f6d4dbe4e5c2ea2e35301104ddf44b84e8dc87ac","modified":1714978435246},{"_id":"public/archives/2023/02/index.html","hash":"afd4c5b770195a5653a26e36fc81d158b40f1539","modified":1714977814509},{"_id":"public/archives/2023/08/index.html","hash":"757fe85a92c135d7467ddc4f9579681491d8c527","modified":1714977814509},{"_id":"public/archives/2023/08/page/2/index.html","hash":"72983297e55ec58bd886c04e3386a530ef2247b9","modified":1714977814509},{"_id":"public/archives/2023/08/page/3/index.html","hash":"4c672f1c208f4a1eea29d95f973a00fb2b5dab56","modified":1714977814509},{"_id":"public/archives/2023/08/page/4/index.html","hash":"17c9f06e26894c48611cc71c6d214d3da74a122c","modified":1714977814509},{"_id":"public/archives/2023/10/index.html","hash":"7dd317318735c8c407d68eea4cb8fb055d6aeb5a","modified":1714977814509},{"_id":"public/archives/2023/11/index.html","hash":"18c9679b1facfcdc40912063e289af4491acc98a","modified":1714977814509},{"_id":"public/archives/2023/12/index.html","hash":"52e82ff91e760ad28b94b8e6131043d08ded54bf","modified":1714978435246},{"_id":"public/archives/2024/index.html","hash":"59b9c70cfbf009376616d15f9569637c7e7d7969","modified":1714978435246},{"_id":"public/archives/2024/page/2/index.html","hash":"511d188aee3fd64ff816f1e6f924ff28078ca733","modified":1714978435246},{"_id":"public/archives/2024/01/index.html","hash":"b83395351362a3455ee84d0409e4fda457e8ee45","modified":1714977814509},{"_id":"public/archives/2024/05/index.html","hash":"555b714789b58cf05627aeeb7e9d7f6fd8277263","modified":1714978435246},{"_id":"public/categories/博客/index.html","hash":"07208c23e0db7f15210814d2d5c5454d2c31535d","modified":1714978435246},{"_id":"public/categories/Linux/index.html","hash":"7ba7eacfe0fad71412ef200ac62f5ec2367edccf","modified":1714977814509},{"_id":"public/categories/微信小程序/index.html","hash":"b8d1bf79392a3682bdd05a6fd269db8ed6ade76a","modified":1714977814509},{"_id":"public/categories/数据结构/index.html","hash":"fd321a927b7b2674d2058112107d44f32d5d7cea","modified":1714978435246},{"_id":"public/categories/数据结构/page/2/index.html","hash":"731172b5c3cd0c8d868d6bf75846890ab96caf96","modified":1714977814509},{"_id":"public/categories/数据结构/page/3/index.html","hash":"c9656ee83a194bc20268b2edd929d0bebf45ceeb","modified":1714977814509},{"_id":"public/categories/数据结构/page/4/index.html","hash":"9c1f44c858d4c8c89b166f1b05fc2d961cd505ed","modified":1714977814509},{"_id":"public/categories/数据结构/page/5/index.html","hash":"74dee2141b72669a34232f0b3650d164580e97a8","modified":1714977814509},{"_id":"public/categories/算法/index.html","hash":"5c118b22a55253d5ad8658828e218878c068fd8d","modified":1714977814509},{"_id":"public/categories/vscode/index.html","hash":"b0f5385e4c8647054aec46d31ac5d11db476767e","modified":1714977718125},{"_id":"public/categories/其他/index.html","hash":"6e9be9d94ca7f22f9412b9266db10905a15e41f8","modified":1714977718125},{"_id":"public/categories/深度学习/index.html","hash":"06092bdad087b9a86ed39930cd59f39d7db1217d","modified":1714977814509},{"_id":"public/categories/深度学习/page/2/index.html","hash":"3124ab724244d0e55895a072bc2a4f4bd95fc3f0","modified":1714977718125},{"_id":"public/index.html","hash":"b29c5aa23daf7d5f0ab13866f2c67605158cca6c","modified":1714978435246},{"_id":"public/page/2/index.html","hash":"d4b4205ef76aefe2f6af368fafa57eb18e365a8a","modified":1714978435246},{"_id":"public/page/3/index.html","hash":"3f556cf16931be8be414194d5d2ce2f2b954d918","modified":1714978435246},{"_id":"public/page/4/index.html","hash":"60ae107825771de620c231910b347ca2afe40825","modified":1714978435246},{"_id":"public/page/5/index.html","hash":"e22dc37d73f338ea5b0697a10aac20b7c79be0d5","modified":1714978435246},{"_id":"public/page/6/index.html","hash":"c60c5c6d016c28d61e237f0da2f093e6f385836a","modified":1714978435246},{"_id":"public/page/7/index.html","hash":"3e80a4fdbd2f5cf845363e912865975659b2b9f9","modified":1714978435246},{"_id":"public/page/8/index.html","hash":"05bada4f971629a5cb4e7aeb687273b11e379c54","modified":1714978435246},{"_id":"public/page/9/index.html","hash":"20775c3c580b688226d7376ffe6eee12ee1d4d94","modified":1714978435246},{"_id":"public/tags/博客/index.html","hash":"5f29f2aa045f8d99493ce359a10601586351c729","modified":1714978435246},{"_id":"public/tags/hexo/index.html","hash":"9c7052c4cf6aff34f85ecfcc42c004acebfa7414","modified":1714978435246},{"_id":"public/tags/Linux/index.html","hash":"7af53774f6921a67059c8997946a0f1ee1f353ea","modified":1714977814509},{"_id":"public/tags/微信小程序/index.html","hash":"e30e34904a7727cb61c78b8c73e3e786ef7e43e2","modified":1714977814509},{"_id":"public/tags/数据结构/index.html","hash":"7e14508c1bc071e0120118950687870b323e6f77","modified":1714978435246},{"_id":"public/tags/数据结构/page/2/index.html","hash":"f7a3df322e9e9c6fcd866c15a87c7d31f2ff4cee","modified":1714977814509},{"_id":"public/tags/数据结构/page/3/index.html","hash":"704efd2bfbc59818b0b222e44e921e858d82945c","modified":1714977814509},{"_id":"public/tags/数据结构/page/4/index.html","hash":"e30dfa92ba6339bc663a8cda29e45c2c6ced0931","modified":1714977814509},{"_id":"public/tags/数据结构/page/5/index.html","hash":"ea475bca5dcbfe7fe173e1d2ebe2cecd841f6c50","modified":1714977814509},{"_id":"public/tags/线性表/index.html","hash":"aba3ce9bcd27ba588504640066f0c56b03c42e6a","modified":1714977814509},{"_id":"public/tags/双链表/index.html","hash":"9ab62bedb57e0a784f6ea45ade391f908d1fc789","modified":1714977814509},{"_id":"public/tags/单链表/index.html","hash":"a9f2a0b68625102f41d9e05150ce8124d4275a3a","modified":1714977814509},{"_id":"public/tags/串/index.html","hash":"d83030214b7a619a43350198e9af02deb7ac2a9c","modified":1714977814509},{"_id":"public/tags/栈/index.html","hash":"b08fab26c946e251e5e275d12c013b73a9bed07d","modified":1714977814509},{"_id":"public/tags/队列/index.html","hash":"fc15ff501a999c9617e6baf3434b59b125dc5e75","modified":1714977814509},{"_id":"public/tags/查找/index.html","hash":"ee0d9eb4b92483e64f677f716c99ec417f62d799","modified":1714978435246},{"_id":"public/tags/二叉排序树/index.html","hash":"76e7a7fa6135adb2032581da2ef26fedf163c1fe","modified":1714977814509},{"_id":"public/tags/平衡二叉树/index.html","hash":"36a97363a4e54d39edb6d7b6af7c4a026aa836b6","modified":1714978435246},{"_id":"public/tags/B树/index.html","hash":"676087ad2136c9b9a2e4ef1b435cd79e18a159d6","modified":1714978435246},{"_id":"public/tags/树/index.html","hash":"118ac741a63f1b80aa8fcd5c2e16de6ec3ede893","modified":1714977814509},{"_id":"public/tags/哈夫曼树/index.html","hash":"548d556b81890d2332f8cd0179dd42c0629771ed","modified":1714977814509},{"_id":"public/tags/并查集/index.html","hash":"18beba25e7a032b1a2f117ae2ba58090470274fa","modified":1714977814509},{"_id":"public/tags/二叉树/index.html","hash":"9331f993091d684726457f57f849b880eba68471","modified":1714977814509},{"_id":"public/tags/图/index.html","hash":"30a63a51305747d1a15fb2f35b3ee21dc17c3eb8","modified":1714977814509},{"_id":"public/tags/最小生成树/index.html","hash":"f315ba61173d6a4f737db593461d018a4c97c714","modified":1714977814509},{"_id":"public/tags/最短路径/index.html","hash":"34633db969dc0a57818b272bc5826c7c3a0f89dd","modified":1714977814509},{"_id":"public/tags/算法，排序/index.html","hash":"33fdd137a79cf3cf20548e3e65ae497eb5eba5c2","modified":1714977814509},{"_id":"public/tags/vscode/index.html","hash":"fe10253cb26eb261f9f4d80b0955e86be4a58a7a","modified":1714977814509},{"_id":"public/tags/其他/index.html","hash":"46d3da705a59aa723e649575ee6aad511b20417e","modified":1714977718125},{"_id":"public/tags/pycharm/index.html","hash":"d49ec4a4954545d54823b14048650ae1e8c37a68","modified":1714977814509},{"_id":"public/tags/深度学习/index.html","hash":"d09d38cf0253adcdd157a7b1f95f043b73bce14c","modified":1714977814509},{"_id":"public/tags/深度学习/page/2/index.html","hash":"af1de1704461edc8b2c40772fe08c39c6fb235e3","modified":1714977718125},{"_id":"public/tags/论文/index.html","hash":"ed0626d460ef13e1085ad230408e93d23027c991","modified":1714977814509},{"_id":"public/tags/超分/index.html","hash":"44833a777989036b775a1640929eb8d3ac4b3f9f","modified":1714977814509},{"_id":"public/tags/机器学习/index.html","hash":"22917d7baf23eff5f89d2dbc8a9b34c1aea1e6ff","modified":1714977814509},{"_id":"public/tags/pytorch/index.html","hash":"909ce28cc0d8ea8206de37aa8ca9deb62152fa9d","modified":1714977814509},{"_id":"public/tags/数据预处理/index.html","hash":"bcf84608da5644fe629b4d67bc95fa988b235877","modified":1714977718125},{"_id":"public/fonts/default-skin.b257fa.svg","hash":"2ac727c9e092331d35cce95af209ccfac6d4c7c7","modified":1714977718125},{"_id":"public/fonts/iconfont.16acc2.ttf","hash":"f342ac8bf4d937f42a7d6a0032ad267ab47eb7f2","modified":1714977718125},{"_id":"public/fonts/iconfont.8c627f.woff","hash":"aa9672cb097f7fd73ae5a03bcd3d9d726935bc0a","modified":1714977718125},{"_id":"public/fonts/iconfont.45d7ee.svg","hash":"75767c904d483d9b93469afb6b92bb6bdface639","modified":1714977718125},{"_id":"public/fonts/tooltip.4004ff.svg","hash":"397fe4b1093bf9b62457dac48aa15dac06b54a3c","modified":1714977718125},{"_id":"public/fonts/iconfont.b322fa.eot","hash":"bc8c5e88f4994a852041b4d83f126d9c4d419b4a","modified":1714977718125},{"_id":"public/img/default-skin.png","hash":"ed95a8e40a2c3478c5915376acb8e5f33677f24d","modified":1714977718125},{"_id":"public/img/preloader.gif","hash":"6342367c93c82da1b9c620e97c84a389cc43d96d","modified":1714977718125},{"_id":"public/img/scrollbar_arrow.png","hash":"d64a33c4ddfbdb89deeb6f4e3d36eb84dc4777c0","modified":1714977718125},{"_id":"public/img/hexo/1.png","hash":"eb5d590c866b115b711b6b18e688d0fb750b0fbc","modified":1714977718125},{"_id":"public/img/hexo/2.png","hash":"1a2488cab647b2d9ee0645f8b1404f20d8f6391b","modified":1714977718125},{"_id":"public/img/hexo/3.png","hash":"1c144b98e4d21f31d06466142a48efaf2a94d5e2","modified":1714977718125},{"_id":"public/img/hexo/6.png","hash":"e48c5ecbd482f3d30befe212e0bdbb7edfc8e1ce","modified":1714977718125},{"_id":"public/img/hexo/4.png","hash":"1386eb7152ca993db6bb1b73ec224640ed73ace1","modified":1714977718125},{"_id":"public/img/hexo/8.png","hash":"2caad5ef038d9170ea58f0a0026bc8306280ec37","modified":1714977718125},{"_id":"public/img/hexo/7.png","hash":"e6b833a8a600ccde88c4204782cc3e8004e0edb2","modified":1714977718125},{"_id":"public/img/datastruct/1_extract/2.png","hash":"6d1421cc038218e94daff7ef39c7d3ac9d2e881c","modified":1714977718125},{"_id":"public/img/datastruct/1_extract/3.png","hash":"571cded10b5c4269a889e56dab07bc689818e102","modified":1714977718125},{"_id":"public/img/datastruct/1_extract/4.png","hash":"743e8d48e403ebd444289b1668ec6512099c9bb5","modified":1714977718125},{"_id":"public/img/experience/website/3.png","hash":"aa0520b3b2ccc24d5cb1bdd6f0470f39156e50d6","modified":1714977718125},{"_id":"public/img/linux/install/1.png","hash":"f5e18416748c8cc86938da9ae1f1ff42e91e08bd","modified":1714977718125},{"_id":"public/img/linux/install/2.png","hash":"23d49dab591d627c6673fb2a75fd5d2eb9d2fed3","modified":1714977718125},{"_id":"public/img/hexo/config/2.png","hash":"368052a20c17d974130d81cad86bf9eefbe696e6","modified":1714977718125},{"_id":"public/img/linux/usage/1.png","hash":"672cdb2ff4bef0cb1db9fd9dc7af2c64ee49a689","modified":1714977718125},{"_id":"public/img/linux/usage/2.png","hash":"ecb6b7300ecb1e57a5ec396896b31f5076e30a29","modified":1714977718125},{"_id":"public/img/java/produce_practice/1.png","hash":"30554ce2998b7e51ad6c95ba0a8dd64d7184e7d5","modified":1714977718125},{"_id":"public/img/java/produce_practice/10.png","hash":"e24850fd076e7deb6510d6eab70e1b983a8143de","modified":1714977718125},{"_id":"public/img/java/produce_practice/13.png","hash":"5a8cba5a88ea57b8fbc0b3d03e170bd78d63c9d6","modified":1714977718125},{"_id":"public/img/java/produce_practice/14.png","hash":"d454f6af516b04b392c643aff34c133c27c89566","modified":1714977718125},{"_id":"public/img/java/produce_practice/11.png","hash":"a1dca93a880850cf39a90afe2ab00b2d4af3d05d","modified":1714977718125},{"_id":"public/img/java/produce_practice/12.png","hash":"a6083435996c6b697b5b5b9c64cdf1bed6a119f8","modified":1714977718125},{"_id":"public/img/java/produce_practice/16.png","hash":"05fe99b97b36da5852d48717d487cd7bcdaa62ea","modified":1714977718125},{"_id":"public/img/java/produce_practice/19.png","hash":"5b6c4d4ca1fafa15fc502684b8ef690dd783b1f0","modified":1714977718125},{"_id":"public/img/java/produce_practice/18.png","hash":"662e19f9ede67fbf97ba64db2b1dd570493f42bc","modified":1714977718125},{"_id":"public/img/java/produce_practice/35.png","hash":"b598e1f3995b79f9ca23beecec9f286ff5ee7637","modified":1714977718125},{"_id":"public/img/java/produce_practice/23.png","hash":"5bfb5c0dfd8a20cb41b6c8fa1c0f066de5b58ce0","modified":1714977718125},{"_id":"public/img/java/produce_practice/6.png","hash":"68d24416da9cf00e2c1f3389d69780fe7b0aefcc","modified":1714977718125},{"_id":"public/img/java/produce_practice/4.png","hash":"4ba2f5dd63299422bc74f7debefac14fc5c2abbf","modified":1714977718125},{"_id":"public/img/java/produce_practice/8.png","hash":"2d231f50629980b55a959bbbb7ce4b44627428da","modified":1714977718125},{"_id":"public/img/java/produce_practice/9.png","hash":"2c930e47793ff23442b7e0047e667d26b2ce652f","modified":1714977718125},{"_id":"public/img/datastruct/2_linearlist/linklist/3.png","hash":"8f35efb483252f734fcce32a59701f95ae08ca08","modified":1714977718125},{"_id":"public/img/experience/app/lrc_to_smi/1.png","hash":"3e6bf1d0664a78945a62cb8efe7291c559bdd95f","modified":1714977718125},{"_id":"public/img/experience/app/lrc_to_smi/2.png","hash":"29999d449f335d959a928b47ee573094d6407e6f","modified":1714977718125},{"_id":"public/img/experience/app/mouse/1.png","hash":"45d7532c50960f7a6770563e0c462b831dd478f0","modified":1714977718125},{"_id":"public/img/experience/app/lrc_to_smi/4.png","hash":"2739ef20646d80910f98405f596227832d73dca7","modified":1714977718125},{"_id":"public/img/experience/app/mouse/3.png","hash":"c0202f1aaf6cd8c29563e94172ba1a5664ca80eb","modified":1714977718125},{"_id":"public/img/experience/app/word/1.png","hash":"ade633e21e81c9671297135c43eb0a4c465b391f","modified":1714977718125},{"_id":"public/img/experience/pycharm/link_server/10.png","hash":"39b9fd6684a084dab653c885553bdfb48e616d96","modified":1714977718125},{"_id":"public/img/experience/pycharm/link_server/11.png","hash":"b6ebbcce0e705315d2b9d49cfe47a5b0c7d738ef","modified":1714977718125},{"_id":"public/img/experience/pycharm/link_server/3.png","hash":"d5c06088084abdd3d3f791266baf3e490353fa8e","modified":1714977718125},{"_id":"public/img/experience/pycharm/link_server/2.png","hash":"155e96cd73c2a354ba31581c3ae7990c35f1e77f","modified":1714977718125},{"_id":"public/img/experience/pycharm/link_server/4.png","hash":"c2347bb419ce0b59e6a10bf13e67aabb6f1d6000","modified":1714977718125},{"_id":"public/img/experience/pycharm/link_server/5.png","hash":"528b88149b3eab3ef15c7cd04f18cf14463a9f8e","modified":1714977718125},{"_id":"public/img/experience/pycharm/link_server/8.png","hash":"faedba9e3148fa893311cd9ddf820ae269dcc09a","modified":1714977718125},{"_id":"public/img/experience/pycharm/link_server/9.png","hash":"d38b41763eab121538504eba96a933f860d3fca0","modified":1714977718125},{"_id":"public/img/experience/pycharm/link_server/7.png","hash":"8998704ce34bfa8d04b681a1b979c2ec712511df","modified":1714977718125},{"_id":"public/img/experience/vscode/relative_path/1.png","hash":"5bf68e898eaedce375a846d2766c9d3c5f2c21c6","modified":1714977718125},{"_id":"public/img/experience/windows/ssh/1.png","hash":"83b6719725d0d73dba3041940c7d2f71777087f0","modified":1714977718125},{"_id":"public/img/java/produce_practice/0/10.png","hash":"4535da2390045add4db4e2657210d660907fe0aa","modified":1714977718125},{"_id":"public/img/java/produce_practice/0/6.png","hash":"0ed212c966e155fd3a6d64e67b54d3fb0639e090","modified":1714977718125},{"_id":"public/img/java/produce_practice/2/1.png","hash":"bfc399b235e2f301d714db59b129576937bd9206","modified":1714977718125},{"_id":"public/img/java/produce_practice/2/5.png","hash":"0252bf34bb632f23d60ceff347041279a3f88f74","modified":1714977718125},{"_id":"public/img/java/produce_practice/2/8.png","hash":"2643438edc15e378358aca36a1d02428a208f50f","modified":1714977718125},{"_id":"public/img/java/produce_practice/4/8.png","hash":"6590259d237e2ff2b1c2d6a2053180a455f1a2ab","modified":1714977718125},{"_id":"public/img/java/produce_practice/5/2.png","hash":"42a50479f9b6e7b69169ac6cda432a1c3ad3bf54","modified":1714977718125},{"_id":"public/img/experience/app/word/page/0.png","hash":"172b9a8e838a56f3034cf412ad5374822235bc49","modified":1714977718125},{"_id":"public/img/experience/app/word/page/2.png","hash":"d6a0322ad9d5a6ab3b811be1262f93a739c67c01","modified":1714977718125},{"_id":"public/img/experience/app/word/page/5.png","hash":"723a629f7ce633859822d10bf34e9346542869b1","modified":1714977718125},{"_id":"public/img/experience/app/word/page/6.png","hash":"fa9c6834203e9e4fcf6a5ea0840c2268691c5df9","modified":1714977718125},{"_id":"public/img/deeplearning/paper/SR/CDM/2.png","hash":"7e63a4151dc6c4e94cf281abe73dcdd544fe67e1","modified":1714977718125},{"_id":"public/img/deeplearning/paper/SR/HAT/3.png","hash":"8557ef65ec097d18405fe25075ce508a569cdf3c","modified":1714977718125},{"_id":"public/img/deeplearning/paper/SR/HAT/4.png","hash":"bd2061a683d43542f298d3b6c369a7a73e940802","modified":1714977718125},{"_id":"public/img/deeplearning/paper/SR/HAT/6.png","hash":"f626d8e52052327ea89c2ef3f164331a2c5813e6","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/2_linear_neural_network/4img/2.png","hash":"866042909a57b337001a6ccdb384fca0de6b0aef","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/2_linear_neural_network/4img/3.png","hash":"be26c997c7cfb8f358d42d65b6defe79a7d98bf9","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/2_linear_neural_network/6_softmax_realize_files/6_softmax_realize_13_0.svg","hash":"286de42a50f439d72b9a4431c26633c68c9e5244","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/2_linear_neural_network/6_softmax_realize_files/6_softmax_realize_15_0.svg","hash":"0b967ebe55c0efd5a10943f13da50b17012b8d9b","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/2_linear_neural_network/1_linear_regression_files/1_linear_regression_4_0.svg","hash":"117d33ae7e24490dd351cb3dda9f01697604a260","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/2_linear_neural_network/7_softmax_realize_simple_files/7_softmax_realize_simple_5_0.svg","hash":"e7326b636020ee834dedefa365a0bc50df191fcf","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/1_prepare/4_calculus_files/4_calculus_1_1.svg","hash":"2268e578363aa8e3ea0d9fc6f9048b5476965606","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/1_prepare/4_calculus_files/4_calculus_3_1.svg","hash":"4388b61bde02e3677bad2d58f1cc537f2ce40879","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/5_convolution/6_LeNet_files/6_LeNet_3_1.svg","hash":"aa3f445749740d26383175f7fe4e5257684d9aa0","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/5_convolution/2_picture_cov/1.png","hash":"adc2c914ac0d265928985e26d2b327fd17e74823","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/5_convolution/5_polling/1.png","hash":"7a7e066d35969acce72f9dee184a3df899d53fb9","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/5_convolution/4_mult_channel/2.png","hash":"d88a9c4e83a75875220f7168f823bcea364c330e","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/3_mlp/10_kaggle_predict_price_files/10_kaggle_predict_price_15_1.svg","hash":"f44ae9c476fc897885aa995ec11c55c79cc47821","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/3_mlp/10_kaggle_predict_price_files/10_kaggle_predict_price_17_1.svg","hash":"52dcc486e79b2bcb5727ecad64f1672ee4f36279","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/3_mlp/1_mlp_files/1_mlp_3_0.svg","hash":"b40bd446671c91e8423328496f591d47f9b855fe","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/3_mlp/3_mlp_realize_simple_files/3_mlp_realize_simple_1_0.svg","hash":"893bf7bd6b1facaafeaa59f0a74e1ec9efbcb4d9","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/3_mlp/1_mlp_files/1_mlp_5_0.svg","hash":"0aeb69df6b54fa10fc0bba6dd6b8f74bc906c713","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/3_mlp/2_mlp_realize_files/2_mlp_realize_1_0.svg","hash":"2cdad356448f53f436b46f6a1dfb3301d9edeca1","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/3_mlp/1_mlp_files/1_mlp_1_0.svg","hash":"916e9f81fd64d864e8928b2d075bbf51b853735f","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/3_mlp/2_mlp_realize_files/2_mlp_realize_1_1.svg","hash":"15de8b4f4bdc9f439bd6567de0e4a4a511710dd1","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/3_mlp/4_overfitting_files/4_overfitting_11_1.svg","hash":"ef90ef3ede804f998cc308e825b05d226570b0ca","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/3_mlp/4_overfitting_files/4_overfitting_9_1.svg","hash":"aeac8a3e0b2759086aaaa1c870717c441ead05c6","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/3_mlp/4_overfitting_files/4_overfitting_7_1.svg","hash":"c4a09daa3725c074825db20b1ae97145f33cbb24","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/3_mlp/4img/1.png","hash":"acef488b314a0eafa2b537f365d5fc373e271623","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_1_1.svg","hash":"430b01e4581ecbbdfa0b2bf4990c15efdcab7603","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_2_1.svg","hash":"28bfe489d1ba3253bd9a1369f8ecf0deb413a211","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_4_1.svg","hash":"0cb1e259525ec525d6356f7f8c112a6e9f91977e","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_5_1.svg","hash":"53c269365154a5776b6af1b449e163d41f59fa77","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/3_mlp/6_deopout_files/6_deopout_3_0.svg","hash":"06c12519eeda924c77fc54eb45521bc677fb69ca","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/3_mlp/7/1.png","hash":"67738f990d722b9ab6d296d108ebfb303b9c1246","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/3_mlp/6_deopout_files/6_deopout_5_0.svg","hash":"90fa91d6679990bbb69a51ae6fba74606e031747","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/3_mlp/8_init_files/8_init_2_0.svg","hash":"8bdf2739f73260fef223d873c4de25d11de3b380","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/7_rnn/1_seqmodel/2.png","hash":"e12c7a2ea34d4fb9da12fa2a8df17c4860dc8fdd","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/7_rnn/1_seqmodel_files/1_seqmodel_1_0.svg","hash":"46dff896e8b549a6614da348ae3f7f08e392ce96","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/7_rnn/4_rnn/1.png","hash":"411a99ced4a5fd4bf2fd4ad8130fb4f4509734bc","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/7_rnn/4_rnn/2.png","hash":"340ce180b73a6e0c96dc2eb1f5f46db6a0b8828f","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/7_rnn/7_backward/1.png","hash":"39a7b7de47ea88fbfbca3f737847da502075d8ec","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/7_rnn/3_lmdataset_files/3_lmdataset_9_0.svg","hash":"a03e6da250fbde8a2f1985dff035e5fd83aeceb8","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/7_rnn/3_lmdataset_files/3_lmdataset_3_0.svg","hash":"a97892ac1dff0ae2ca8740d2750e72ef9f51c17e","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/7_rnn/6_rnn_sample_realize_files/1.png","hash":"19b9af39430aaa7b41b69160a66502631a76d16f","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/7_rnn/6_rnn_sample_realize_files/6_rnn_sample_realize_13_1.svg","hash":"5e14adcf331d6e58bcdb08f681ec0814c858bfef","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/7_rnn/5_rnn_realize_files/5_rnn_realize_27_1.svg","hash":"ce56a41bd07065d7882dd31a9792e38bcd60d4b7","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/6_moden_conv/2_vgg_files/2_vgg_7_0.svg","hash":"05303fdd497222c62710848d6fa65fd4426ca07f","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/7_rnn/5_rnn_realize_files/5_rnn_realize_29_1.svg","hash":"45f2aa928530a7d50e44ce41717511d15bbd2fc2","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/6_moden_conv/1_AlexNet_files/1_AlexNet_0_1.svg","hash":"dbe778e552f1d54c23c4cd9714cdbc502840af37","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/6_moden_conv/3_NiN_files/3_NiN_7_0.svg","hash":"d7b21aac1e71119d9e1d7a3ef8fef5c124b2e279","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/6_moden_conv/4_googleNet_files/4_googleNet_7_1.svg","hash":"e7fad7fdf658dff125323a8f6e14b04a00767364","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/6_moden_conv/4_googlenet/2.png","hash":"276d3cc9e737f976d64d7b9d7261acd06f0331a6","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/6_moden_conv/5_batchnormal_files/5_batchnormal_7_1.svg","hash":"dcbf9a308cc84db9017cc79a5062c368028caf97","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/6_moden_conv/5_batchnormal_files/5_batchnormal_5_1.svg","hash":"22f052f17b6cd789860b726e81aee437f4f02d87","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/6_moden_conv/6_resnet/2.png","hash":"722720f90a4bf449e8f880507a9f7cfc7f5bb16b","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/6_moden_conv/6_resnet_files/6_resnet_13_1.svg","hash":"51f4f00b0beee6c74bca6a4d4a3722bc99e72569","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/6_moden_conv/7_densenet/2.png","hash":"64bbd286cfd666244ee1973aaf0339071c3b3aa2","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/6_moden_conv/7_densenet/1.png","hash":"680996307eeaa52d0a4cd002f7cd6862d71115d8","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/6_moden_conv/7_DenseNet_files/7_DenseNet_17_1.svg","hash":"e2c6b0977781d4618ec76bfda2265290cfbba0e7","modified":1714977718125},{"_id":"public/img/0.png","hash":"18cd3f368afc326152fddaf38353d0a3a5b88cce","modified":1714977718125},{"_id":"public/img/hexo/5.png","hash":"795caa1da3dd3b5194ca066cc4a3dc684ae5c4a7","modified":1714977718125},{"_id":"public/img/experience/keyboard/1.png","hash":"b0bcd7c8a2b79784fe255d793deba3680dfa8393","modified":1714977718125},{"_id":"public/img/linux/commend/4.png","hash":"312407c7570bf5d0226a98c51f1e400f9e426418","modified":1714977718125},{"_id":"public/img/linux/usage/4.png","hash":"30492036c274902af6117aceadc01bb6f7ad421b","modified":1714977718125},{"_id":"public/img/linux/usage/3.png","hash":"53692b5667fdd9f5018856c35d6fc71be66fdd10","modified":1714977718125},{"_id":"public/img/java/produce_practice/15.png","hash":"a5b0974e97c1c0c5f5f2dcfde1bd5aeefd7d09e8","modified":1714977718125},{"_id":"public/img/java/produce_practice/20.png","hash":"82240a09e18c6fcb0f38ff232c5329f0ca2baf23","modified":1714977718125},{"_id":"public/img/java/produce_practice/21.png","hash":"1656f0f4f2fc7ce8069b11669560fb868f96ffd4","modified":1714977718125},{"_id":"public/img/java/produce_practice/3.png","hash":"c2852be662928c0650e58095caa3ad963ef79b32","modified":1714977718125},{"_id":"public/img/java/produce_practice/5.png","hash":"cafffe809df3332e48b3f9fb6d2c2519e4d08be7","modified":1714977718125},{"_id":"public/img/java/produce_practice/7.png","hash":"981d763df9f521ac01471891dffc8c42c1bcf2d5","modified":1714977718125},{"_id":"public/slider.e37972.js","hash":"6dec4e220c89049037eebc44404abd8455d22ad7","modified":1714977718125},{"_id":"public/main.0cf68a.css","hash":"c35f1df998abd0f1d2f6a733f74e35723875635e","modified":1714977718125},{"_id":"public/main.0cf68a.js","hash":"993fadeb5f6d296e9d997a49ee20dc97333ceab7","modified":1714977718125},{"_id":"public/mobile.992cbe.js","hash":"01b35e71e37aa2849664eb5daf26daede2278398","modified":1714977718125},{"_id":"public/img/datastruct/2_linearlist/linklist/4.png","hash":"cd2d22e47a4ccf7d698aa886967d484994fe68c7","modified":1714977718125},{"_id":"public/img/experience/app/lrc_to_smi/3.png","hash":"a1fce07f832cae4b3b251dde1c87376f4386ad2c","modified":1714977718125},{"_id":"public/img/experience/app/lrc_to_smi/5.png","hash":"fe826175ce8041af3cd87afada1a66204a489e4e","modified":1714977718125},{"_id":"public/img/experience/pycharm/link_server/1.png","hash":"18f61385a1aa2a42b57ed9fd4eb058988038bc79","modified":1714977718125},{"_id":"public/img/experience/pycharm/link_server/6.png","hash":"8e2eb0e6a096b258aa6d87f122a6901cadc81c22","modified":1714977718125},{"_id":"public/img/experience/vscode/relative_path/2.png","hash":"6845b7d833c89764ce7e87a437079bc76022ad43","modified":1714977718125},{"_id":"public/img/experience/windows/powershell/2.png","hash":"06b19fe5ed694fd898dd53e01267d41885e75544","modified":1714977718125},{"_id":"public/img/java/produce_practice/0/1.png","hash":"bfbc6e4bc6036266324cc25c8e0b9be9872062d3","modified":1714977718125},{"_id":"public/img/java/produce_practice/0/11.png","hash":"77fdc5313f871e514dda20d85bf727faef13d880","modified":1714977718125},{"_id":"public/img/java/produce_practice/0/4.png","hash":"2be74d8a0d882cb4176c475ae0f35562886b6cf3","modified":1714977718125},{"_id":"public/img/java/produce_practice/0/2.png","hash":"f8686690ba00c0c1c374c0d5c04ef51699a04df2","modified":1714977718125},{"_id":"public/img/java/produce_practice/0/7.png","hash":"6afbf6573f8ffa9d2d5708e8c59573b556e81748","modified":1714977718125},{"_id":"public/img/java/produce_practice/0/8.png","hash":"d5c39a047fad558afc3c38242590c366caf693c1","modified":1714977718125},{"_id":"public/img/java/produce_practice/0/9.png","hash":"44c3143a2eabc2dfce309be0e275767a54abde6e","modified":1714977718125},{"_id":"public/img/java/produce_practice/2/10.png","hash":"41aa473866a58bc198e011888355e70b6ea2334b","modified":1714977718125},{"_id":"public/img/java/produce_practice/2/3.png","hash":"3927e0ec015178ba68fece35c1c15af61cdf92f8","modified":1714977718125},{"_id":"public/img/java/produce_practice/2/2.png","hash":"cd5fc603a9c1d337e47c5c8352248bb0043246fe","modified":1714977718125},{"_id":"public/img/java/produce_practice/2/4.png","hash":"8d1f48fe6aa2854cba53710b6942543408d68fae","modified":1714977718125},{"_id":"public/img/java/produce_practice/2/9.png","hash":"281e0c904285464473694e159791cdcc61f2810b","modified":1714977718125},{"_id":"public/img/java/produce_practice/3/3.png","hash":"e5deb84f3ed4c4cf16923a0445dbed94fd3ed1e6","modified":1714977718125},{"_id":"public/img/java/produce_practice/4/3.png","hash":"ede885e2c91398a34f7cb88932988e37f6ab1832","modified":1714977718125},{"_id":"public/img/java/produce_practice/4/4.png","hash":"61b4c8cc0c2cb7965d620a69e04039a7f97b19da","modified":1714977718125},{"_id":"public/img/experience/app/word/page/1.png","hash":"dbd95e4b6418abac3b4a5f9619f61c7e8b330c7a","modified":1714977718125},{"_id":"public/img/experience/app/word/page/3.png","hash":"4c46804b2a6a460cbb32ad520fb5199bf32a7e7f","modified":1714977718125},{"_id":"public/img/experience/app/word/page/4.png","hash":"7b389400f80ea6131ed37c3499d00ca678c62580","modified":1714977718125},{"_id":"public/img/experience/app/word/to_pdf/1.png","hash":"de6a40d1ed287d516dac9cc7cbcd09a88f046169","modified":1714977718125},{"_id":"public/img/experience/app/word/to_pdf/2.png","hash":"aed291bef545d63c6f694c87cf59d8392f8f17c4","modified":1714977718125},{"_id":"public/img/deeplearning/paper/SR/SRDiff/3.png","hash":"12aefac57158ded661913fee37c1348baeb1fb5f","modified":1714977718125},{"_id":"public/img/deeplearning/paper/SR/SwinIR/2.png","hash":"c62663719646ff08f57958e388c64cadf00bec40","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/2_linear_neural_network/2_linear_regression_realize_files/2_linear_regression_realize_3_2.svg","hash":"7745038901e6c501386a5bc1d7afda51874a0a9f","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/2_linear_neural_network/4img/1.png","hash":"a65d5f49e5bb5f2cdc688e75966e4f8d23530336","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/2_linear_neural_network/1_linear_regression_files/1.png","hash":"29edd99abc43a64b80c27ae3404d354bbaf2e1aa","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/2_linear_neural_network/5_image_classification_dataset_files/5_image_classification_dataset_3_1.svg","hash":"a84269a32f13a9403e16b3b83416bbe856798469","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/1_prepare/6_probability_files/6_probability_1_2.svg","hash":"4fb11c7574654e616736bb655a7365c62e42810c","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/5_convolution/3_padding/1.png","hash":"f5ff3f979568b627d255973a670973d3944066e5","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/5_convolution/3_padding/2.png","hash":"a9bb3af80eebe3ab3765fcd76d2abd2aa95a6cc8","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/5_convolution/4_mult_channel/1.png","hash":"d38802cfc41d4ee8da2556acc182522b03188bcb","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/5_convolution/6_LeNet/1.png","hash":"fc931e8000daae1e2709cb7d8cd3ba3ca4dfae98","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/7_rnn/1_seqmodel_files/1_seqmodel_11_0.svg","hash":"d46f3ba1a8a8aec6430cda48c1ef56df387c1dbd","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/7_rnn/1_seqmodel/1.png","hash":"0dfb92eaddb3cdd94b49b471b6a63c01383fd6c1","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/7_rnn/1_seqmodel_files/1_seqmodel_13_0.svg","hash":"2a6dea1dc158df8efa310ab7492e2fa7792e6e73","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/7_rnn/1_seqmodel_files/1_seqmodel_9_0.svg","hash":"37da951b540aa0b1931b32570a847bd1874a94e2","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/7_rnn/7_backward/2.png","hash":"530e1826bac164be050fd96dbb6a2bd4b5f1b713","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/6_moden_conv/1_AlexNet/1.png","hash":"273cf87da7ba0cc03609497ff079811e8c689939","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/6_moden_conv/2_vgg/1.png","hash":"3616c743a2317766c63f429fbbedee5452347e27","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/6_moden_conv/3_NiN/1.png","hash":"923f7fc4aa2e369052c09bebca01361f2d7dd30d","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/6_moden_conv/4_googlenet/1.png","hash":"f4550013e59746198c7bf925bbeac66e96a5e76e","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/6_moden_conv/6_resnet/3.png","hash":"53e61da5477dd528abb9ea522b8fd30c9453ac59","modified":1714977718125},{"_id":"public/img/experience/website/1.png","hash":"e3f3761e0a8d13765d07c62356c8cb531085832f","modified":1714977718125},{"_id":"public/img/linux/install/3.png","hash":"aff317a4f01469d43aea727220d5f35dda5ad82f","modified":1714977718125},{"_id":"public/img/hexo/config/1.png","hash":"57c220f0666493e9aa46e655dd3e4c8d8769526f","modified":1714977718125},{"_id":"public/img/linux/usage/7.png","hash":"81ea48b124c5945df2b299db4f45d459449c7b49","modified":1714977718125},{"_id":"public/img/datastruct/2_linearlist/doublelinklist/3.png","hash":"1be61897417af386bdaab09257f60b624fa4b7a7","modified":1714977718125},{"_id":"public/img/java/produce_practice/0/12.png","hash":"47a42a001aebf4be7c404c0c7fa033197239c850","modified":1714977718125},{"_id":"public/img/java/produce_practice/0/5.png","hash":"ec4f869a042f2f354585e3767eca5d69d788a4c9","modified":1714977718125},{"_id":"public/img/java/produce_practice/2/7.png","hash":"f3cc060659d39dbff8cb3438dd057fb21aa27ffa","modified":1714977718125},{"_id":"public/img/java/produce_practice/4/1.png","hash":"bc477c350f3cdad28016bda6c01802a952e04be4","modified":1714977718125},{"_id":"public/img/java/produce_practice/3/2.png","hash":"dccb5e6caaad6984015fb8dfbb63b95cedb1a785","modified":1714977718125},{"_id":"public/img/java/produce_practice/4/6.png","hash":"fe08d6dc598eac909afdf2849420e3816ac9bf7c","modified":1714977718125},{"_id":"public/img/java/produce_practice/5/1.png","hash":"67e90e725f052ff9e17461f0c081cf0881ea2b92","modified":1714977718125},{"_id":"public/img/java/produce_practice/4/7.png","hash":"957c4beb4e68c29a672dd19e7f81010a225c1f61","modified":1714977718125},{"_id":"public/img/java/produce_practice/5/3.png","hash":"e07018c0b52ce32b9f326c49a7544b22750aec19","modified":1714977718125},{"_id":"public/img/java/produce_practice/6/1.png","hash":"9a4b59e5330ed14ecca4ff8976b9882602877694","modified":1714977718125},{"_id":"public/img/java/produce_practice/6/3.png","hash":"1f6b9d58a0f8d67c607bd6de71c8988b246b6d77","modified":1714977718125},{"_id":"public/img/deeplearning/paper/SR/SRDiff/1.png","hash":"8859dd1b37ba832fcf39bea466172166a4872ca1","modified":1714977718125},{"_id":"public/img/deeplearning/paper/SR/SwinIR/5.png","hash":"05264fcc3adc2b358081b88c0f8cbb3a3ac7bce9","modified":1714977718125},{"_id":"public/img/deeplearning/paper/SR/SwinIR/3.png","hash":"ad4657eb56551df2f18233832075681a371be953","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/3_mlp/1img/1.png","hash":"32187b3f3d27abf58d84102ec722a3bdfc1aaa95","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/6_moden_conv/6_resnet/1.png","hash":"3fe1ac08b72a62b408b1102e92f246e2ad3e4ee6","modified":1714977718125},{"_id":"public/img/head.png","hash":"c695336ba203c92400d8801be883217f516ccfa9","modified":1714977718125},{"_id":"public/img/datastruct/4_string/22.png","hash":"69fc98bbd8b19f77b749ff2aac2fc1d84da041fc","modified":1714977718125},{"_id":"public/img/datastruct/4_string/6.png","hash":"02c58cbb86448ab130e8de56a8ed30190a270441","modified":1714977718125},{"_id":"public/img/datastruct/4_string/7.png","hash":"5045a59070a4dc3abf0b75adc7ea361e417a9ac6","modified":1714977718125},{"_id":"public/img/experience/keyboard/2.png","hash":"bb2c5f88eb5f2530ec237baa4b3bb634252d7138","modified":1714977718125},{"_id":"public/img/experience/website/2.png","hash":"386f3140232edd163a74963d3f1fb71b631c7ccc","modified":1714977718125},{"_id":"public/img/linux/commend/7.png","hash":"b5334dff05fddfb776b67cfa62e1bde08ad5a7d1","modified":1714977718125},{"_id":"public/img/linux/usage/8.png","hash":"bd109f6a36e82cfed8d9b84d2a151852037c9d74","modified":1714977718125},{"_id":"public/img/java/produce_practice/17.png","hash":"694d762b0dd7a91ef0118f3cbb744cfa92f1d6c7","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/treesave/5.png","hash":"53bae744eb1d75032b76b2a34393b483c507d45d","modified":1714977718125},{"_id":"public/img/datastruct/7_search/BST/身份证1.jpg","hash":"f4ad46fab9a1874986ccd963cb4f4d8a8602554c","modified":1714977718125},{"_id":"public/img/experience/windows/powershell/1.png","hash":"0dbea432a36d62b157691a5eec9ca69a7f994d9d","modified":1714977718125},{"_id":"public/img/java/produce_practice/3/1.png","hash":"b61d9c32cd346e8d35c477d176c55900a84a2ecb","modified":1714977718125},{"_id":"public/img/java/produce_practice/6/0.png","hash":"831b58c87c051ac4f9aa08c9b1f8059e94b4910c","modified":1714977718125},{"_id":"public/img/java/produce_practice/6/2.png","hash":"788b890ce1947fbbf3d21a303346b245f91fa2e0","modified":1714977718125},{"_id":"public/img/deeplearning/paper/SR/CDM/1.png","hash":"d0284a0aa9cfc0bb236b9b2d21da006c12aeba82","modified":1714977718125},{"_id":"public/img/deeplearning/paper/SR/CDM/3.png","hash":"f1489a76081146c43436f33662b8fb24dd93705b","modified":1714977718125},{"_id":"public/img/deeplearning/paper/SR/SwinIR/7.png","hash":"e6e1d6e4486b4a67ccf1de20a0c3fcd7e12fc168","modified":1714977718125},{"_id":"public/img/deeplearning/paper/SR/SwinIR/6.png","hash":"13827355692c474261b3db2cd451e6704ac78953","modified":1714977718125},{"_id":"public/img/deeplearning/paper/SR/HAT/11.png","hash":"d1c328c62f3a3894356be5017e148cf82228ccb3","modified":1714977718125},{"_id":"public/img/deeplearning/paper/SR/HAT/2.png","hash":"4949684eeb63cf7260e919617454a2d551b3f64d","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/3_mlp/6img/1.png","hash":"9e19fa51e5bb17e6afa8a8fc5ff26081074e890d","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/7_rnn/3_lmdataset/1.png","hash":"1922e1609f01d2ac8f91a3de180e7462c2e77c2e","modified":1714977718125},{"_id":"public/img/datastruct/4_string/11.png","hash":"16f3b25aabe8fabac62e4dee790ebd76f9f5cd7b","modified":1714977718125},{"_id":"public/img/datastruct/4_string/33.png","hash":"51078f091563b3fc4126d3a1ff3e46adb6dbf1dd","modified":1714977718125},{"_id":"public/img/datastruct/4_string/9.png","hash":"89fb11a7191f94bd33c84aad1f97b63e15def4e5","modified":1714977718125},{"_id":"public/img/experience/keyboard/3.png","hash":"dcd35a10bf55442eba698de5c53edead0edac20b","modified":1714977718125},{"_id":"public/img/experience/keyboard/6.png","hash":"a61ddd048d5a3e6db9da2d0e1ddc9117122c0cc0","modified":1714977718125},{"_id":"public/img/experience/keyboard/4.png","hash":"cd17b09ae23e58c49be9116984329ffb5799a8e8","modified":1714977718125},{"_id":"public/img/experience/keyboard/7.png","hash":"0d09d0ed585febea50d08c80671eb9f67a86fc2f","modified":1714977718125},{"_id":"public/img/linux/commend/5.png","hash":"9877ec4411499460626515956172ac17c0461194","modified":1714977718125},{"_id":"public/img/linux/usage/9.png","hash":"00fd2fdb4fa8668c63949e5d6b78a8ea1a046ec2","modified":1714977718125},{"_id":"public/img/datastruct/2_linearlist/linklist/5.png","hash":"c3b59117bc61e06960711487d2c9faae1d8490af","modified":1714977718125},{"_id":"public/img/datastruct/2_linearlist/linklist/6.png","hash":"37e2c7e31f6b34a0c678c3241da2028ea694b61a","modified":1714977718125},{"_id":"public/img/datastruct/3_stack/stack/4.png","hash":"ccf102d7ac6a969e1bdcd81001538a5e14bc0685","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/bintree/2.png","hash":"cfd647157788822a1ee87152474331d6b72983c1","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/bintree/3.png","hash":"bb65dc7f7d243290cdfc39e6ad91d29a44a5a978","modified":1714977718125},{"_id":"public/img/datastruct/7_search/BST/身份证2.jpg","hash":"c7248cff105487f06b45d89d6b51e972489f667a","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/DAG/1.png","hash":"f518bf1aa4309d3129066c1f7a4de9d0126bfc64","modified":1714977718125},{"_id":"public/img/java/produce_practice/4/2.png","hash":"6645a10d38500e53719a0c399ed9809d7a51a914","modified":1714977718125},{"_id":"public/img/deeplearning/paper/SR/SwinIR/4.png","hash":"d5be991310455fd4f492c541ef0b327914c7c15b","modified":1714977718125},{"_id":"public/img/datastruct/2_linearlist/4.png","hash":"b0f7022db8b8bdf2dfd25b3be51b483c35c86632","modified":1714977718125},{"_id":"public/img/datastruct/1_extract/1.png","hash":"f62dbc992f719d4edde6745400129010f2df805f","modified":1714977718125},{"_id":"public/img/datastruct/1_extract/6.png","hash":"58eecabd401a7bb5dab4706e4a65b0ad3ab55d3e","modified":1714977718125},{"_id":"public/img/datastruct/1_extract/5.png","hash":"f8bc031e79eea55114892711cede3ef873029495","modified":1714977718125},{"_id":"public/img/experience/keyboard/5.png","hash":"25b5d4f99e72c3c14b56c0eedc0a5f27c31b64b2","modified":1714977718125},{"_id":"public/img/experience/keyboard/8.png","hash":"4c1c98aa0f39491c23244e6a1031c976c2eb464a","modified":1714977718125},{"_id":"public/img/experience/keyboard/9.png","hash":"4b16a65451bb3a20785c60d2a02fb3d1a1bd0a0b","modified":1714977718125},{"_id":"public/img/linux/usage/10.png","hash":"ea8380762314a5543160492a0c0dc4b74bd82244","modified":1714977718125},{"_id":"public/img/linux/usage/5.png","hash":"dd649fda55ce139c7604954518cd6ebdb5d535f1","modified":1714977718125},{"_id":"public/img/datastruct/3_stack/queue/1.png","hash":"c188269b5cbcbb8fbc67391b7be94cfea7ca8ecb","modified":1714977718125},{"_id":"public/img/datastruct/3_stack/matrix/7.png","hash":"303a2e13032a6ab2b2e45c50c1e3666232dc39ef","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/bintree/7.png","hash":"a4960ce0bb113834995b83f1e72a5f1d59b5b580","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/bintree/6.png","hash":"dc6fafa3d3c1dac6a0df6a46537738fb955991bc","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/set/1.png","hash":"f1791e355115d078e7858dcb207f8e87bbd736d6","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/set/2.png","hash":"33126a20ebac6b4dd33fda2b1337f0132e432f28","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/tree/2.png","hash":"c36f61b2f331b4794bb7affb5653a3c70b6bf7eb","modified":1714977718125},{"_id":"public/img/datastruct/7_search/AVL/4.png","hash":"42244dfc16a8545eb1d6f29cbefa1d7b10b8908f","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/generatetree/3.png","hash":"273b817773cfca2cefedf37e00beffce3aac979d","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/generatetree/2.png","hash":"7e507b8326c93a6720e559066f42886e7a8255e9","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/DAG/2.png","hash":"3a0427868ede3bc4fbd980e1eb44d1ddcf8ba662","modified":1714977718125},{"_id":"public/img/experience/windows/ssh/2.png","hash":"3c96724ab9a9441e1e483a9f178cc1a5d093bd27","modified":1714977718125},{"_id":"public/img/experience/windows/ssh/3.png","hash":"afca0a1a9ac95645e8929fb982258508361b9824","modified":1714977718125},{"_id":"public/img/deeplearning/paper/SR/SwinIR/1.png","hash":"103d09498654fe7acde2c2bd20371fe75cba9456","modified":1714977718125},{"_id":"public/img/deeplearning/paper/SR/SwinIR/11.png","hash":"5f5dd2e2cdc0161d68f514f5f6fa92a17097cc15","modified":1714977718125},{"_id":"public/img/deeplearning/paper/SR/HAT/8.png","hash":"090cb62b2d7077ec1bddd563b06245be0a144a7e","modified":1714977718125},{"_id":"public/img/datastruct/2_linearlist/2.png","hash":"8c9d5f586b67d8973d280205ad376279861d9fa3","modified":1714977718125},{"_id":"public/img/datastruct/1_extract/8.png","hash":"f46809238bf064e700e57d6e341b568072b9c1ef","modified":1714977718125},{"_id":"public/img/datastruct/4_string/8.png","hash":"c6fff07a1454c9886d48afe21a3b2dcf4190090e","modified":1714977718125},{"_id":"public/img/datastruct/2_linearlist/doublelinklist/1.png","hash":"400a7006ed7e701ecf338868d027fa9d4fc09aa4","modified":1714977718125},{"_id":"public/img/datastruct/3_stack/exam/1.png","hash":"0355d11dc2d67894016a00819bb2f9fa048e3456","modified":1714977718125},{"_id":"public/img/datastruct/3_stack/queue/3.png","hash":"7389121d9a2981d2f7a913f27ceb495c9b3e3f85","modified":1714977718125},{"_id":"public/img/datastruct/3_stack/queue/5.png","hash":"1d3549ac63517ed5a5fdb36c60ec68570530c6ee","modified":1714977718125},{"_id":"public/img/datastruct/3_stack/stack/1.png","hash":"320bb1490c7237abc6cfa31f75d80a2ff667d10e","modified":1714977718125},{"_id":"public/img/datastruct/3_stack/matrix/9.png","hash":"483683c7cd2491fcaab026f2501fb097a99e5af2","modified":1714977718125},{"_id":"public/img/datastruct/3_stack/stack/3.png","hash":"bbfeff2baa2a09d1cfde6efd8ced65950969d451","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/bintree/1.png","hash":"f2ab94acf03f8427536230e7e4492f6286a8863b","modified":1714977718125},{"_id":"public/img/datastruct/7_search/AVL/5.png","hash":"2ac3fdffca3ece0b482b346dc80124c9b1a9b293","modified":1714977718125},{"_id":"public/img/datastruct/7_search/search/1.png","hash":"abcd55aa8269afb2ec7d383f5bc03379182b18e3","modified":1714977718125},{"_id":"public/img/datastruct/7_search/search/8.png","hash":"b86ec6b6a477e655a484cb62d2a8372e71bbf398","modified":1714977718125},{"_id":"public/img/datastruct/7_search/red/1.png","hash":"24ed2aaf9c398cf0bd548949cc530a82f26bbe3b","modified":1714977718125},{"_id":"public/img/datastruct/7_search/red/5.png","hash":"91fa356e862977aedcd8380883ebba68ce55ff51","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/generatetree/4.png","hash":"4bd74ef02a7ce777be6d6be132b809eadd333795","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/DAG/5.png","hash":"0ef37d1a33ac8f7adfc64f2df93ddc46db02a255","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/graph/10.png","hash":"f8f49b35a8464be6d441c4d1c9d89eacd46c99d0","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/graph/17.png","hash":"96a3732857d13b320d44a5d2bba58b24e63eb2c3","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/graph/21.png","hash":"96cd917e4e00ec460d56bf1497a6ecf875ac2412","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/shortestpath/4.png","hash":"f198322bd8b57e7f7730ad78a1502b72d34af28b","modified":1714977718125},{"_id":"public/img/java/produce_practice/4/5.png","hash":"e73081332379c3fd8f6f2ae1f620359d0a757fa5","modified":1714977718125},{"_id":"public/img/datastruct/2_linearlist/1.png","hash":"f3127e2c823a2115d3eee9c9186e66775f7c8c56","modified":1714977718125},{"_id":"public/img/datastruct/1_extract/7.png","hash":"719ea9a0ff7bdeb47bb6068bacaf8799080544bd","modified":1714977718125},{"_id":"public/img/datastruct/1_extract/9.png","hash":"352ebd7320c0daf59164cc606c54748d4461364c","modified":1714977718125},{"_id":"public/img/datastruct/4_string/3.png","hash":"0263d9a7b8552db5293da17683a78fc6b18add52","modified":1714977718125},{"_id":"public/img/datastruct/4_string/5.png","hash":"48c8a60a2eade60271df902361219dc60182b147","modified":1714977718125},{"_id":"public/img/linux/commend/6.png","hash":"1ad966f50410b760b5c4da236b00fa3c87d6e794","modified":1714977718125},{"_id":"public/img/datastruct/2_linearlist/linklist/1.png","hash":"93d909b592f79bb93cb05e461f1cfe5b19975588","modified":1714977718125},{"_id":"public/img/datastruct/3_stack/exam/3.png","hash":"d4697e7aa4faf3faaec31795d056369aee9a0747","modified":1714977718125},{"_id":"public/img/datastruct/3_stack/matrix/8.png","hash":"a14b20161150d5ec27c26a8769d2fbc63882cc02","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/bintree/11.png","hash":"d6da827299c4424e820f13b641b8cf1d7dcc8647","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/bintree/14.png","hash":"e09f44ccfd173d7e22a664ba1da0496578fe71fa","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/bintree/16.png","hash":"ea7d466a5c4df3dab2b7d5fd1db34983ad20bcc9","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/bintree/21.png","hash":"01cce10b1c97bfe85564457670c02523f9e22bbf","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/bintree/4.png","hash":"a1a7f2f52dea3bb78b7e610b99cd46c8eb638324","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/bintree/9.png","hash":"617b5e71fbb4b6cbde53988e84b7db14227521aa","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/hafmantree/2.png","hash":"4b489a3880d5c72496e6bb3e9a0fb1867c72a61e","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/tree/3.png","hash":"5c23b6d576d9ab930ff7bed0ef93f7286fece156","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/treesave/8.png","hash":"fa5bb9c206f9d820b4829b82a815acfe9535f698","modified":1714977718125},{"_id":"public/img/datastruct/7_search/BST/2.png","hash":"11152e9108147bf5ddaf92da0a6ae3b40310f0f3","modified":1714977718125},{"_id":"public/img/datastruct/7_search/BST/4.png","hash":"4ce88adb5b069d4d0f3e019c8baaff3daf93577d","modified":1714977718125},{"_id":"public/img/datastruct/7_search/search/2.png","hash":"ee0dcdd98d36ebf968203e086842f82983833393","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/generatetree/10.png","hash":"8f1df1e8b36321baa8454a6e408fc3132f3a0b49","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/generatetree/5.png","hash":"c31e028791baa33832977eba8885d7c2fa495f5a","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/generatetree/8.png","hash":"ad9e53a5806f86ec4f2e076279d81ba1faf82b80","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/generatetree/9.png","hash":"9af35488623060bb152e66796eed3d11de500b69","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/DAG/10.png","hash":"367b5db4a5df4056bc852b4d15d90569f16d47ad","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/DAG/11.png","hash":"2c57ffd92b97d2c5cb9cd6e86d45ba7a7c3ef152","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/DAG/12.png","hash":"2c57ffd92b97d2c5cb9cd6e86d45ba7a7c3ef152","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/DAG/13.png","hash":"f82c00b83d4707743244ced4344eb173088f6537","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/DAG/7.png","hash":"1640ed8edab3a2235f8d1890dd911df427202d5b","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/DAG/9.png","hash":"1c087e833e9d95f77fdfc1c5393872f67b161620","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/graph/11.png","hash":"3b8f079b47feb4ead637c8e8799668081da3b3c7","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/graph/14.png","hash":"ad20949a8476a1eab20b23a627f5d1c2c5245b72","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/graph/13.png","hash":"1a0b6b8999306d37bb4e5e021dcd2860fb91003a","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/graph/12.png","hash":"f5f8a7f60d80bdea3901f24d757d71f354b21076","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/graph/16.png","hash":"8accfd8baed48f5129fd22bc026b3baeea3f53dc","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/graph/19.png","hash":"3b6cc333c977fabc8759d19010d40eadfec4e126","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/shortestpath/1.png","hash":"4f739f585dcd7d2fadc90c2ec487d810e50b12b5","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/shortestpath/12.png","hash":"8af073f178b71de473f6779005b693b6fe931e35","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/shortestpath/5.png","hash":"ccabac77b9f87025d39cb178c736ab08be966a53","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/shortestpath/9.png","hash":"6cdb024dea1b8f80280a985b970deb30f92229dc","modified":1714977718125},{"_id":"public/img/deeplearning/paper/SR/HAT/1.png","hash":"9645f93ecfba9f5b9206fde1c0aa73917896f582","modified":1714977718125},{"_id":"public/img/deeplearning/paper/SR/HAT/5.png","hash":"a743c446dfcf9b937205f58dc028f41d070e48b5","modified":1714977718125},{"_id":"public/img/deeplearning/code/pytorch/6_moden_conv/6_resnet/4.png","hash":"96001a0cfeffb33021afbea123aa58b02a544191","modified":1714977718125},{"_id":"public/img/datastruct/2_linearlist/3.png","hash":"5a8a80187065f83608b3d27cf134ca2b1345ea50","modified":1714977718125},{"_id":"public/img/datastruct/4_string/2.png","hash":"fc16f97df3cd583499ebfdc08a17a59d584f5dbe","modified":1714977718125},{"_id":"public/img/linux/user/8.png","hash":"0b8417b51075ac859bcf37582f6ca9cdc6d1ed9e","modified":1714977718125},{"_id":"public/img/datastruct/2_linearlist/linklist/2.png","hash":"e9347e1f225c5cbb9c549e8dfb9f5bec9b0aeb0c","modified":1714977718125},{"_id":"public/img/datastruct/2_linearlist/doublelinklist/5.png","hash":"2fd148d973e964c5a66a6677e08c8d40bb4086a7","modified":1714977718125},{"_id":"public/img/datastruct/3_stack/exam/4.png","hash":"a3e580c7a52b96615056d54e0901e927210ec6bd","modified":1714977718125},{"_id":"public/img/datastruct/3_stack/queue/2.png","hash":"372e33824e44870739b0b2dd31473891d8be39d5","modified":1714977718125},{"_id":"public/img/datastruct/3_stack/matrix/1.png","hash":"4232903681e5acd2874ed02adbb613f650cbd3a4","modified":1714977718125},{"_id":"public/img/datastruct/3_stack/matrix/2.png","hash":"e5f12710a7ee824a071f4b667c21c6adcacf57f8","modified":1714977718125},{"_id":"public/img/datastruct/3_stack/queue/4.png","hash":"ccc7388fe9d234b85dfaf0f13df2011fdd448cc9","modified":1714977718125},{"_id":"public/img/datastruct/3_stack/matrix/4.png","hash":"52cfb649fee62c2d0248b8a19634016a6e7dfeb0","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/bintree/12.png","hash":"7b9ec3a8aeea46a650d5f324f6bcd450d7c6f1ce","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/bintree/13.png","hash":"5c51b87683faf0b42ff105b4b509847ce28f033c","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/bintree/15.png","hash":"642ac58ff76d07fdb5f34aa690c5d9198947b50b","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/bintree/20.png","hash":"e3ccd07772d3937fe059f7030cb5637d07272705","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/bintree/19.png","hash":"4850932f1aa82436d5d483bf8c65762911e63d5d","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/bintree/5.png","hash":"5606a7bf100e9f72e25622c5364701c62e995057","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/hafmantree/1.png","hash":"c93b19d02e8eecbcd748e4e7d0b212fdb258e3d2","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/set/4.png","hash":"60b7cd6266dd054a37d619083c49e76d936471e7","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/set/3.png","hash":"2656e0c353bab48a03f657aff960e69d4ccf488e","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/treesave/1.png","hash":"5f3d151b0ef530ec331a88871f4c790ad1e7de76","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/treesave/3.png","hash":"554d3cbfb2d7a8276695dfab55b6fa00d6b41e29","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/treesave/4.png","hash":"0ed7fed4738e93c716e7d8947eb29ec60bbcb3b3","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/treesave/7.png","hash":"b0407aed45fdf2778c37aeb86e16f0d76246bd1b","modified":1714977718125},{"_id":"public/img/datastruct/7_search/AVL/3.png","hash":"8736726d7645894dc84807c3e9ce0e54fa76c717","modified":1714977718125},{"_id":"public/img/datastruct/7_search/BST/1.png","hash":"55a425118fb566256fc2dcb7267670c2e0c8b6be","modified":1714977718125},{"_id":"public/img/datastruct/7_search/search/6.png","hash":"ec0afe78ba75251e35923d39189ae047b28b0898","modified":1714977718125},{"_id":"public/img/datastruct/7_search/search/3.png","hash":"b167c48347e3f40f71169249aa9e57149f456030","modified":1714977718125},{"_id":"public/img/datastruct/7_search/red/3.png","hash":"e07c568186d1ff720b927af647502bf84095c922","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/generatetree/6.png","hash":"f70727738f1d45d51a0b5ace85e38f9ef4fbefbc","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/generatetree/7.png","hash":"430730c045a26ff7a2671b3bf42b81d36e288ec3","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/graph/18.png","hash":"23ab682c9b0e7ed5a5372d575801e89b1735a9c8","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/graph/22.png","hash":"6db8e47e30d615832eebb062a162791b7c310fd4","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/graph/2.png","hash":"30c586539ef983e91f127a9d117be4334832c7b4","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/graph/26.png","hash":"186dae57f64593ba80a9484c2b1746c1f8d29999","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/graph/25.png","hash":"04097ccd7e31337fc15b0b4e6590f0e46ed5e92c","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/graph/4.png","hash":"7a3bb1906f0ad5afccbed87ff851e3819cbafc36","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/graph/6.png","hash":"5617cd959a09a2687fe2cb9a32abfdf070d389fa","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/graph/8.png","hash":"177c49dc3942bb19e887c89c2e795bc944d2994c","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/shortestpath/2.png","hash":"bfe530abb9fac2ea1b2eb295a832de6b1f725218","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/shortestpath/13.png","hash":"f09b3c157a8364de40821317b2677a175b3f4dc6","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/shortestpath/11.png","hash":"ddf8ab48eb171c8ef1e1f0114616ad849ddfb034","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/shortestpath/6.png","hash":"707dbf184c801ed3e8a6f0c333ab4797dda2b238","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/shortestpath/7.png","hash":"daf3d4a4a195019ed2629e892f3fd8e2aee5978c","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/shortestpath/8.png","hash":"4320444060e2fa35d0b0e4dc8f33a03cec5bea7d","modified":1714977718125},{"_id":"public/img/experience/app/mouse/2.png","hash":"2f146858a80f46078881f11bc45721aa9bff1646","modified":1714977718125},{"_id":"public/img/deeplearning/paper/SR/SRDiff/2.png","hash":"bc499602dd3514cfa3e48a170a0254f98de118a9","modified":1714977718125},{"_id":"public/img/deeplearning/paper/SR/SwinIR/12.png","hash":"35982c423def1509d333ab9842840239de01b7d2","modified":1714977718125},{"_id":"public/img/deeplearning/paper/SR/SwinIR/9.png","hash":"2d2e66a07ec289a15511a707862c44c0fa9b9c6e","modified":1714977718125},{"_id":"public/img/deeplearning/paper/SR/HAT/7.png","hash":"b0c033daaece2d1342f4017cd64850e19e1973ea","modified":1714977718125},{"_id":"public/img/datastruct/4_string/1.png","hash":"dd8f568d408f4066857dcaf5f837b76ff72681ea","modified":1714977718125},{"_id":"public/img/datastruct/2_linearlist/doublelinklist/2.png","hash":"ca655c8903c294c2561adf00e03115d85daac0d3","modified":1714977718125},{"_id":"public/img/datastruct/2_linearlist/doublelinklist/4.png","hash":"7f6f39075bd9d99fc80bed11a9572929861608fe","modified":1714977718125},{"_id":"public/img/datastruct/3_stack/matrix/3.png","hash":"de128228b37c7f67472db3f69319469c15577bb1","modified":1714977718125},{"_id":"public/img/datastruct/3_stack/matrix/6.png","hash":"5796e5f5deedd912d609d786b9bcdf81e5952265","modified":1714977718125},{"_id":"public/img/datastruct/3_stack/matrix/5.png","hash":"ddc34b510edf878733390cf6280c17e24430ca44","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/bintree/10.png","hash":"809602cc17c15b735981b02272c065543b829903","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/bintree/18.png","hash":"0c53deb30a095262bace6fe55d49b9528d79932b","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/bintree/8.png","hash":"da3f2d7eb809212e929a5124c89161424edb4b19","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/hafmantree/4.png","hash":"6e0aa2e3cb77d483683e3d0d11cf91ed42a1b86f","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/tree/1.png","hash":"adf29d834bac15c3cbb5a59ffe8ad8beb0e84a37","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/set/5.png","hash":"28d6733723d49340995dec2fde9e88dd9f0adb05","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/treesave/2.png","hash":"a3b2cc5ec1d3f6641c562ec91fd67158681d54fd","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/treesave/6.png","hash":"29ebb70f952f15f45953541376e244d40361dce0","modified":1714977718125},{"_id":"public/img/datastruct/7_search/AVL/7.png","hash":"8cdb063bca970837e89246b7c242aa8df4ef69ab","modified":1714977718125},{"_id":"public/img/datastruct/7_search/search/4.png","hash":"52e1734955fdec29f5967e9ee9474d2cddc98d86","modified":1714977718125},{"_id":"public/img/datastruct/7_search/search/7.png","hash":"66712ddccabed87d61c763cac232635c37dfba4c","modified":1714977718125},{"_id":"public/img/datastruct/7_search/red/4.png","hash":"1565be0433a56f3b59c04e9ac45580ed38c6e49e","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/DAG/3.png","hash":"ab89866de5a90543cb94ba68f91772cbea6885c0","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/graph/20.png","hash":"e1ea39884791587ef05f620d70bea10cad4517ca","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/graph/7.png","hash":"17fe699b83593d7b5a5a63cf6f8e96850e5a0c78","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/shortestpath/10.png","hash":"904d546f835be088ef8c6431fcba3099f29a89b6","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/shortestpath/14.png","hash":"e5fe49cda495ad811654a44c5c4df75c58a26b15","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/shortestpath/3.png","hash":"4a56bcbe87473960ae402584e0da20c8c16cfee1","modified":1714977718125},{"_id":"public/img/datastruct/4_string/4.png","hash":"7838d4d94535fa7a23ea70e174f18224109af7a3","modified":1714977718125},{"_id":"public/img/datastruct/3_stack/stack/2.png","hash":"a908d6604057c26915ac3f6755cbb618fc315bac","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/bintree/22.png","hash":"b42865762e3f79e13238c99d3fea6e9a34b48b7a","modified":1714977718125},{"_id":"public/img/datastruct/7_search/AVL/2.png","hash":"62329d059d1524ccb694e74674724d123b1d1ccb","modified":1714977718125},{"_id":"public/img/datastruct/7_search/AVL/6.png","hash":"81c02a6561ed5dfb1a5d2523c54f6f3d033c1828","modified":1714977718125},{"_id":"public/img/datastruct/7_search/red/2.png","hash":"561a4f0a6d89e9dc5059fe6ad41fcf3ccb064c71","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/generatetree/1.png","hash":"a38f3f3818fb59ffa2e0c2f149baae5ad3e08906","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/graph/15.png","hash":"ddaa0073e7a050c6da2e15bb92edef4b263e71f8","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/graph/23.png","hash":"507a10492249735e4eff71110dccc08e32d75723","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/graph/5.png","hash":"e526a1db7825500f3915b81cff0b2a0649501224","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/graph/9.png","hash":"af0f3d264f6346953c00662e13262590c718288d","modified":1714977718125},{"_id":"public/img/datastruct/2_linearlist/5.png","hash":"d4eecc47637ac57e721533941890b5db93c6cd96","modified":1714977718125},{"_id":"public/img/linux/usage/6.png","hash":"3596c1c5283d231a1b453e01963033a8d698611b","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/bintree/17.png","hash":"7e9e417dd7e92cf4518e7478df7a61dc792b5c79","modified":1714977718125},{"_id":"public/img/datastruct/7_search/AVL/1.png","hash":"1cba4a7760b4e3198c1d948e381a13e76b5522a5","modified":1714977718125},{"_id":"public/img/datastruct/7_search/BST/3.png","hash":"9d522c4b3ed9fc8d4cad85e1d574895e0edf5ffd","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/DAG/6.png","hash":"160fc9cbaf10916bf63315abe80c023af57cb663","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/DAG/8.png","hash":"6d7b99c9cfe359e941a1a0a9f733fd37ca979430","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/graph/24.png","hash":"24195d899250ca557aa68f3c5b50872ebf724b25","modified":1714977718125},{"_id":"public/img/deeplearning/paper/SR/SwinIR/8.png","hash":"a057c763efb0ce8ad6ed81c7fcc687b8cfd61d64","modified":1714977718125},{"_id":"public/img/datastruct/3_stack/exam/5.png","hash":"70eb4243c88e9cb6f9d64dc4b1ce0123f93d7b70","modified":1714977718125},{"_id":"public/img/datastruct/7_search/search/5.png","hash":"1be79056d4fb1d9c03850ce2161c2f2c22373751","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/graph/1.png","hash":"3c95625309a2e2f7fddf78c14bc972a018c82916","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/graph/3.png","hash":"1c63775db55f4624289bc1ef592ed88d4c2258ab","modified":1714977718125},{"_id":"public/img/datastruct/3_stack/exam/2.png","hash":"23650f4e8a06111ed5639cc0891fd7490c8eb7b7","modified":1714977718125},{"_id":"public/img/datastruct/5_tree/hafmantree/3.png","hash":"20d3db5968e55586130cdf0fc98d5dcf77d79408","modified":1714977718125},{"_id":"public/img/datastruct/6_graph/DAG/4.png","hash":"1aee5cdce7b83cbb17cc5c7c14ac9d0a02a787f8","modified":1714977718125},{"_id":"public/img/datastruct/7_search/B/1.png","hash":"bba1aaa2fb53ede0dff30e60d86ac9c9e7ad3cc8","modified":1714977718125},{"_id":"public/img/datastruct/3_stack/exam/6.png","hash":"721fc6eedb785c67120633ed94159a0d4ea49270","modified":1714977718125},{"_id":"public/img/deeplearning/paper/SR/SwinIR/13.png","hash":"75213541cc70afc1d1918c479ac78f8d56850e52","modified":1714977718125},{"_id":"public/img/java/produce_practice/2/6.png","hash":"d36ac357d5632da635fc28f525bda84457954a28","modified":1714977718125},{"_id":"public/img/deeplearning/paper/SR/SwinIR/10.png","hash":"88289ae9893a394b64ec2faf0759100abbf224a1","modified":1714977718125},{"_id":"public/img/deeplearning/paper/SR/HAT/9.png","hash":"a43c143ea90005eb68c6b17343f20d5526e99d2e","modified":1714977718125},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\1_prepare\\4_calculus_files\\4_calculus_3_1.svg","hash":"4388b61bde02e3677bad2d58f1cc537f2ce40879","modified":1706791900029},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\1_prepare\\4_calculus_files\\4_calculus_1_1.svg","hash":"2268e578363aa8e3ea0d9fc6f9048b5476965606","modified":1706791900006},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\2_linear_neural_network\\1_linear_regression.ipynb","hash":"68363485f2ca021ed22a150f011c9db3e67dcfc2","modified":1714640071608},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\2_linear_neural_network\\1_linear_regression.md","hash":"56f78b37cbdb1674c6587badb9d8da229c66dcf3","modified":1714640073883},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\2_linear_neural_network\\2_linear_regression_realize.md","hash":"f03a110492738169a6852377188b5fc6b96d837b","modified":1714640082138},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\2_linear_neural_network\\3_linear_regression_simplerealize.ipynb","hash":"edbce8f917aedc6de036e310dc05a1a42939b7e4","modified":1714640087730},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\2_linear_neural_network\\3_linear_regression_simplerealize.md","hash":"ed6900af7f3d98cca68c0f578b14fd9929e8d30f","modified":1714640090462},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\2_linear_neural_network\\4_softmax_regression.ipynb","hash":"fe785c2bcc3b9d38303445779e0c34fb1ebd12cf","modified":1714640097017},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\2_linear_neural_network\\5_image_classification_dataset.md","hash":"ea3795ba809c19c8c2790245376c7d94aacee345","modified":1714640108942},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\2_linear_neural_network\\4_softmax_regression.md","hash":"815cdd583d4b0d67f86bc64751dba8d21a96e90a","modified":1714640099596},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\1_prepare\\6_probability_files\\6_probability_1_2.svg","hash":"4fb11c7574654e616736bb655a7365c62e42810c","modified":1706930246524},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\2_linear_neural_network\\5_image_classification_dataset.ipynb","hash":"ae8e182a54be99cff7c82d2262771cbcbe438164","modified":1714640106500},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\2_linear_neural_network\\6_softmax_realize.ipynb","hash":"8b95c4215b666e6c5e90304664414e0b31e614fb","modified":1714640116079},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\2_linear_neural_network\\2_linear_regression_realize.ipynb","hash":"52e47b5dd3f11cd6bf8cb80eda7f84479ca9073d","modified":1714640079838},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\2_linear_neural_network\\1_linear_regression_files\\1_linear_regression_4_0.svg","hash":"117d33ae7e24490dd351cb3dda9f01697604a260","modified":1706947544080},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\2_linear_neural_network\\1_linear_regression_files\\1.png","hash":"29edd99abc43a64b80c27ae3404d354bbaf2e1aa","modified":1706947212470},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\2_linear_neural_network\\4img\\1.png","hash":"a65d5f49e5bb5f2cdc688e75966e4f8d23530336","modified":1706962050655},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\2_linear_neural_network\\2_linear_regression_realize_files\\2_linear_regression_realize_3_2.svg","hash":"7745038901e6c501386a5bc1d7afda51874a0a9f","modified":1706952737930},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\2_linear_neural_network\\6_softmax_realize_files\\6_softmax_realize_15_0.svg","hash":"0b967ebe55c0efd5a10943f13da50b17012b8d9b","modified":1707049500017},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\2_linear_neural_network\\6_softmax_realize_files\\6_softmax_realize_13_0.svg","hash":"286de42a50f439d72b9a4431c26633c68c9e5244","modified":1707049500016},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\2_linear_neural_network\\7_softmax_realize_simple_files\\7_softmax_realize_simple_5_0.svg","hash":"e7326b636020ee834dedefa365a0bc50df191fcf","modified":1707061270220},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\2_linear_neural_network\\5_image_classification_dataset_files\\5_image_classification_dataset_3_1.svg","hash":"a84269a32f13a9403e16b3b83416bbe856798469","modified":1707022695907},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\4_calculate\\1_block.ipynb","hash":"08c8627f5bc8f8673803e803cc27581f8dba887e","modified":1714639907781},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\4_calculate\\1_block.md","hash":"50c79559aed0c3ee3035daabccac0317185bf82d","modified":1714639910742},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\4_calculate\\3_laterinit.ipynb","hash":"d1c9205c857abb5f06b4d98abad5f10da9ab0f10","modified":1714639924936},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\4_calculate\\3_laterinit.md","hash":"ea89a2d4d64b02d16f42222dbb0da542587e40cd","modified":1714639927122},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\4_calculate\\2_paramter.ipynb","hash":"0cbf6854b17657996b3ddbc63f3c70036ffd50d9","modified":1714639916640},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\4_calculate\\4_selflayer.ipynb","hash":"878e915d5d9f66fcb9f4bef02b2c2e1a82354801","modified":1714639932405},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\4_calculate\\2_paramter.md","hash":"1f2d31cc82127414bf1411c1b63b8b369c1bea7c","modified":1714639919011},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\4_calculate\\5_file.ipynb","hash":"bc8256bce687086e3a65ff38f3be931e629baf40","modified":1714639940884},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\4_calculate\\5_file.md","hash":"48be37a36a92945374f13408a7fe3c994fd4ce62","modified":1714639943379},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\4_calculate\\6_GPU.md","hash":"b3c11ac92fefee514b61f57c997501e2c69322c9","modified":1714639952314},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\4_calculate\\4_selflayer.md","hash":"c8c4fd42ae40f41c7a715e064707b350f8655d60","modified":1714639934896},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\4_calculate\\mydict","hash":"9ea6e59a2b176b644d5c7bf6d19d7fe06c9c7839","modified":1713597457589},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\4_calculate\\6_GPU.ipynb","hash":"b96a7ad75f1fee16421b17a62cc31cc3118ec426","modified":1714639949700},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\4_calculate\\x-file","hash":"15322de4be7b5bdeb24debd6a371f01d86100343","modified":1713597457584},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\4_calculate\\mlp.params","hash":"568da2461d73fa5b7aa1f30122b1828a149823dd","modified":1713598154416},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\4_calculate\\x-files","hash":"7003c8306518ae12ef38e26f688cff2d20074326","modified":1713597457586},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\3_mlp\\10_kaggle_predict_price.md","hash":"0932ea5906a5fb5eeed89a4e94d520c0b8ae2417","modified":1714920584401},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\3_mlp\\2_mlp_realize.md","hash":"869e38e574226066c743e53a07d0e7a52af8d479","modified":1714639973843},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\3_mlp\\3_mlp_realize_simple.ipynb","hash":"59b24796a870815944146b37c643999f180d5cb2","modified":1714639980500},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\3_mlp\\1_mlp.md","hash":"5be013694308b6045f28664d8438c3afb16e3279","modified":1714639965621},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\3_mlp\\3_mlp_realize_simple.md","hash":"17cd76f073a99ec40fc3f106438377cb7edc4917","modified":1714639984058},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\3_mlp\\5_weight_decay.md","hash":"b8328b8de2cb3eada6e56b0c6b55005126609f1d","modified":1714640005241},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\3_mlp\\4_overfitting.md","hash":"b18ca2aaaf82c723a9b69eb7b2ac3e80cf32401e","modified":1714639993928},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\3_mlp\\6_deopout.md","hash":"b887b67eee6a973f56a58a2267ff4140095c7b0d","modified":1714640014214},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\3_mlp\\7_backward.md","hash":"fff8127ce9a2f33c3d685953e211fcb7f1382815","modified":1714920389994},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\3_mlp\\7_backward.ipynb","hash":"0b7107f63c5bf85ee7a4e14798bbb271ed5cb620","modified":1714640020622},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\3_mlp\\8_init.md","hash":"0d5940928b65807cd0c6724b66caff05a8424296","modified":1714640039316},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\3_mlp\\submission.csv","hash":"47282f5a962c44b477d3fc486b9e2569a50472a3","modified":1710931839699},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\3_mlp\\9_distribution_shift.ipynb","hash":"bf528a62218c5e603102e231b067a22274871fb2","modified":1714640045337},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\3_mlp\\8_init.ipynb","hash":"8b024da11068b1835e803e5256615c68c6a35814","modified":1714640036216},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\3_mlp\\9_distribution_shift.md","hash":"dea9242646af7b8b03bc1699a2fb9066e6af4343","modified":1714640047845},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\3_mlp\\10_kaggle_predict_price.ipynb","hash":"7e1c63afeaf4170801bc4cf5ab26ba8a2541d80d","modified":1714640054617},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\3_mlp\\1_mlp.ipynb","hash":"1525e4c447b32650c25db69a9f7899a89b9424e5","modified":1714639963417},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\3_mlp\\2_mlp_realize.ipynb","hash":"e354f7d59589a8b594cf85c88324464d0431cecd","modified":1714639971282},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\3_mlp\\6_deopout.ipynb","hash":"fe7db6ed2e545970a1efca790c9e183794f1cc31","modified":1714640011916},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\3_mlp\\4_overfitting.ipynb","hash":"64869be3d49dc6ceed6a5d93a2891464322e9096","modified":1714639990195},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\3_mlp\\5_weight_decay.ipynb","hash":"1e76247b1bbaee36fd8b97d9539118609b927836","modified":1714640000090},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\5_convolution\\1_cov.ipynb","hash":"388d4e3242be2fa57e0845d5add9c2790a5d4d96","modified":1714639828367},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\5_convolution\\2_picture_cov.md","hash":"ddd9c4f65534aa22cc4bcd12e0d1da95e3c2b12d","modified":1714639854544},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\5_convolution\\1_cov.md","hash":"b40ee0db6f6efb1ebd61996a6931c3dfe7b6589a","modified":1714639833054},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\5_convolution\\3_padding_stride.ipynb","hash":"2d60629048636e6935669b22f38dc0c5145a85e3","modified":1714639862283},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\5_convolution\\2_picture_cov.ipynb","hash":"e89eb9718565c2532bee13e6f2b73ec3abf6949f","modified":1714639850249},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\5_convolution\\3_padding_stride.md","hash":"cfca978c30202393955130962ebd9fdec8fdb516","modified":1714639864513},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\5_convolution\\4_mult_channel.md","hash":"f4611271250f7e5e7c721addb86414a171984216","modified":1714639875479},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\5_convolution\\4_mult_channel.ipynb","hash":"02c3e7c6f5c2b79b3b5c68d28e435b488d4be2bc","modified":1714639872394},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\5_convolution\\5_pooling.ipynb","hash":"afbca6173e70bc770aa8c918bf91a0650b649666","modified":1714639881899},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\5_convolution\\5_pooling.md","hash":"e70f3192d52fb2989a7c01c45e260d94c64c13da","modified":1714639886657},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\5_convolution\\6_LeNet.ipynb","hash":"3267df08d9ced5249b247ce4e4a76530e07df139","modified":1714917376192},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\5_convolution\\6_LeNet_files\\6_LeNet_3_1.svg","hash":"aa3f445749740d26383175f7fe4e5257684d9aa0","modified":1713759571751},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\5_convolution\\6_LeNet.md","hash":"51dca1915e0f1b8d3abfaac86a86f653c865f592","modified":1714917446706},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\6_moden_convolution\\1_AlexNet.ipynb","hash":"def2ba9aae990e79ef6dc3fc97a732093d46ee1e","modified":1714917720490},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\6_moden_convolution\\2_vgg.ipynb","hash":"1eeb175412b265df97bc5923f2f715d9ff772c47","modified":1714917845094},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\6_moden_convolution\\1_AlexNet.md","hash":"04cdfe9bf4edceaa01518262d608ad876feb97f1","modified":1714917712516},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\6_moden_convolution\\3_NiN.ipynb","hash":"c7ff2f83c2b143ba0e4694da712f39ca04697437","modified":1714917901446},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\6_moden_convolution\\2_vgg.md","hash":"571c350a37d657442f3aa309ecdff18a2b551896","modified":1714917830745},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\6_moden_convolution\\3_NiN.md","hash":"2c00bb82d24695a3a1d6733b3b44bbaca66005dd","modified":1714917889120},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\6_moden_convolution\\4_googleNet.ipynb","hash":"094f36158590d51f64e1056dbbe0e26503b29ac0","modified":1714917962272},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\6_moden_convolution\\4_googleNet.md","hash":"aea8e9169ccd63b885f8cea08755b6d19dd0bd00","modified":1714917947805},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\6_moden_convolution\\5_batchnormal.md","hash":"059342d8ab19a01aa537c1bf4ac7632621056273","modified":1714918026469},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\6_moden_convolution\\6_resnet.md","hash":"93a408df19b2553a9e07f14ab99768f5e0b6e42a","modified":1714918094371},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\6_moden_convolution\\6_resnet.ipynb","hash":"ae62625fcae0fe6b2700cf371058208c96f872f8","modified":1714918120659},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\6_moden_convolution\\7_DenseNet.ipynb","hash":"40e4e8912ccebce45a388e0fbd822c3069e413ab","modified":1714918158710},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\6_moden_convolution\\7_DenseNet.md","hash":"86bdfd587b3a9dbc65881d163f36fbb5b3567c2f","modified":1714918141508},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\6_moden_convolution\\2_vgg_files\\2_vgg_7_0.svg","hash":"05303fdd497222c62710848d6fa65fd4426ca07f","modified":1714652983721},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\6_moden_convolution\\1_AlexNet_files\\1_AlexNet_0_1.svg","hash":"dbe778e552f1d54c23c4cd9714cdbc502840af37","modified":1714652920458},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\6_moden_convolution\\3_NiN_files\\3_NiN_7_0.svg","hash":"d7b21aac1e71119d9e1d7a3ef8fef5c124b2e279","modified":1714460754057},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\6_moden_convolution\\4_googleNet_files\\4_googleNet_7_1.svg","hash":"e7fad7fdf658dff125323a8f6e14b04a00767364","modified":1714550997515},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\6_moden_convolution\\5_batchnormal_files\\5_batchnormal_7_1.svg","hash":"dcbf9a308cc84db9017cc79a5062c368028caf97","modified":1714551009402},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\6_moden_convolution\\5_batchnormal_files\\5_batchnormal_5_1.svg","hash":"22f052f17b6cd789860b726e81aee437f4f02d87","modified":1714551009402},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\6_moden_convolution\\6_resnet_files\\6_resnet_13_1.svg","hash":"51f4f00b0beee6c74bca6a4d4a3722bc99e72569","modified":1714551018647},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\3_mlp\\data\\kaggle_house_pred_test.csv","hash":"fa19780a7b011d9b009e8bff8e99922a8ee2eb90","modified":1710930250729},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\6_moden_convolution\\7_DenseNet_files\\7_DenseNet_17_1.svg","hash":"e2c6b0977781d4618ec76bfda2265290cfbba0e7","modified":1714551028484},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\6_moden_convolution\\5_batchnormal.ipynb","hash":"4d3787b3dc5a70bcbc684da71027ece3694068d5","modified":1714918066195},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\3_mlp\\data\\kaggle_house_pred_train.csv","hash":"585e9cc93e70b39160e7921475f9bcd7d31219ce","modified":1710930249885},{"_id":"source\\_posts\\experience\\website\\kaggle.md","hash":"61491e9c8d158eebfe176f57aa8a8409af01c5d9","modified":1709810308689},{"_id":"source\\_posts\\experience\\keyboard\\keymap.md","hash":"35ad4542c9d48157170b409365462a2e0f35b390","modified":1709812706971},{"_id":"source\\_posts\\experience\\install\\miniconda.md","hash":"db9d6d2e441e0767a0e1dea30d5e4d6e3f3df495","modified":1713864660724},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\7_rnn\\1_seqmodel.md","hash":"27f83084e1552ffcfd6d2c2b94eaae2c4d4062f6","modified":1714918419848},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\7_rnn\\2_textprecess.md","hash":"96f54def3ed97c3f706b339b7f74b6de204288b0","modified":1714639603550},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\7_rnn\\2_textprecess.ipynb","hash":"d943d614cde5907810cdd5ba900492b176f7c6d3","modified":1714639624911},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\7_rnn\\3_lmdataset.md","hash":"b3fb26a864da38e49896fd921b9b3c4ce0a21655","modified":1714918592833},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\7_rnn\\4_rnn.ipynb","hash":"7853f782e7615a2e0855cb73b5d71bb486d61d41","modified":1714746394279},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\7_rnn\\4_rnn.md","hash":"90d4e7885c4c259d014f24b77961e98433709078","modified":1714748446969},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\7_rnn\\6_rnn_sample_realize.ipynb","hash":"131151576d0505074a13f5b691cdf722f7fbe983","modified":1714918887105},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\7_rnn\\6_rnn_sample_realize.md","hash":"35bfb829b27de5f78c93fc8c41d2e48c0443b693","modified":1714978317343},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\7_rnn\\5_rnn_realize.md","hash":"29161349a5c7e7895223ae189bae6683fca27fb3","modified":1714920115315},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\7_rnn\\7_backward.md","hash":"17035fdc2eec680da158dc8910a1a06e7ba1629a","modified":1714919176652},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\7_rnn\\7_backward.ipynb","hash":"ea988620fbcc926a9e6d8405491109d15e660e95","modified":1714916884164},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\7_rnn\\1_seqmodel_files\\1_seqmodel_1_0.svg","hash":"46dff896e8b549a6614da348ae3f7f08e392ce96","modified":1714639203858},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\7_rnn\\3_lmdataset_files\\3_lmdataset_3_0.svg","hash":"a97892ac1dff0ae2ca8740d2750e72ef9f51c17e","modified":1714723969829},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\7_rnn\\3_lmdataset_files\\3_lmdataset_9_0.svg","hash":"a03e6da250fbde8a2f1985dff035e5fd83aeceb8","modified":1714723969830},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\7_rnn\\5_rnn_realize_files\\5_rnn_realize_27_1.svg","hash":"ce56a41bd07065d7882dd31a9792e38bcd60d4b7","modified":1714815317048},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\7_rnn\\5_rnn_realize_files\\5_rnn_realize_29_1.svg","hash":"45f2aa928530a7d50e44ce41717511d15bbd2fc2","modified":1714815317049},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\7_rnn\\6_rnn_sample_realize_files\\6_rnn_sample_realize_13_1.svg","hash":"5e14adcf331d6e58bcdb08f681ec0814c858bfef","modified":1714918786663},{"_id":"source\\_posts\\python\\PIL.md","hash":"e9ad969487faf737112fc87f111ef507c438a4cf","modified":1714657229797},{"_id":"source\\_posts\\python\\os.md","hash":"cded98f23cd32ba838b0f1e789a368a73d29c609","modified":1714657245642},{"_id":"source\\_posts\\python\\ramdom.md","hash":"fe378f55c64094f28dfb541e1f6a1ddbf9f6195d","modified":1714657375552},{"_id":"source\\_posts\\python\\re.md","hash":"5ea56aca914e0855b62ddd97b2edad7ea06255e0","modified":1714657252768},{"_id":"source\\_posts\\python\\tips.md","hash":"329b855776481b11c8474fc028dc90a5487ece82","modified":1714657254696},{"_id":"source\\_posts\\experience\\windows\\powershell.md","hash":"012c21870b3fd290aa4f1f3636361a2381b5d1b6","modified":1710208645668},{"_id":"source\\_posts\\experience\\windows\\ssh.md","hash":"a7620a7b0fed3b7e19174eb81b18ee3ecf2c5bff","modified":1710206666668},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\7_rnn\\5_rnn_realize.ipynb","hash":"814180a04fe8081f8acbb6023922789f87ee99d7","modified":1714918692105},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\7_rnn\\1_seqmodel_files\\1_seqmodel_11_0.svg","hash":"d46f3ba1a8a8aec6430cda48c1ef56df387c1dbd","modified":1714639203861},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\7_rnn\\1_seqmodel_files\\1_seqmodel_13_0.svg","hash":"2a6dea1dc158df8efa310ab7492e2fa7792e6e73","modified":1714639203863},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\7_rnn\\1_seqmodel_files\\1_seqmodel_9_0.svg","hash":"37da951b540aa0b1931b32570a847bd1874a94e2","modified":1714639203859},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\7_rnn\\3_lmdataset.ipynb","hash":"0e9029ac1ad1ab3ce76de3fe6480436c9dd5b101","modified":1714918588533},{"_id":"source\\_posts\\java\\produce_practice\\0_config.md","hash":"73755c9eb4f88205e8c55580d87a3d54d0b4bace","modified":1709726756798},{"_id":"source\\_posts\\java\\produce_practice\\3_userbase.md","hash":"4bbd91111372f42c3aa6c69d64b7675465f503ce","modified":1709876915083},{"_id":"source\\_posts\\java\\produce_practice\\2_static.md","hash":"8666c408a2bf1d9924d30a1a807bcd6ce7e1f436","modified":1709864004383},{"_id":"source\\_posts\\java\\produce_practice\\4_login.md","hash":"b7794b3846b3d0b6fd6dd81056a9d4f37dccb97f","modified":1709638762574},{"_id":"source\\_posts\\java\\produce_practice\\1_setup.md","hash":"b16bb9ff383e9670a868d00412d30c29f9432fd2","modified":1709721354176},{"_id":"source\\_posts\\java\\produce_practice\\5_usermanage.md","hash":"129f242824d72f1ae419264118514673588f60f8","modified":1709640972394},{"_id":"source\\_posts\\java\\produce_practice\\7_project.md","hash":"be1a4ef9e138b0b40271d61efb0fc4489aef07d4","modified":1709888945970},{"_id":"source\\_posts\\java\\produce_practice\\6_testlog.md","hash":"74d874b3df741e45b19eb7aca9c7458c8b46cec7","modified":1709649669469},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz","hash":"9caad14e1aff9adac77d3744963212d36af15bee","modified":1707047905345},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte","hash":"958122e8b4b97fd4e71eca2d08ef754ed77c3ad9","modified":1707047905356},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\data\\FashionMNIST\\raw\\train-labels-idx1-ubyte","hash":"4bccf37222e01638381eaa326bf730337a597267","modified":1707047896468},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz","hash":"09814cfef5a041118ceace42f8dae995319d331a","modified":1707047896466},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\7_rnn\\1_seqmodel.ipynb","hash":"a41206b3e5ad047ea27c0461b82ee8d9e5152a8c","modified":1714918484210},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz","hash":"5edda96c6d8c36ff915115a0e8136d370a021576","modified":1707047904123},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte","hash":"880056926db704f9f713097915f99da346f5b1d1","modified":1707047904219},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz","hash":"95978b76b6897f6ca69a25145d01716efb615989","modified":1707047894428},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\data\\FashionMNIST\\raw\\train-images-idx3-ubyte","hash":"b9bd999c0106c9b8b4b0cefce58c9a6060f27095","modified":1707047894898},{"_id":"source\\_drafts\\MarkDown.md","hash":"2b3be08ff18b21a8859e4a57a7655c34794d253a","modified":1710752425526},{"_id":"source\\_posts\\hexo\\sourcecode.md","hash":"7c3d319ff606827abf965f0c7fb00f74170f1590","modified":1713798484000},{"_id":"source\\_posts\\hexo\\latex.md","hash":"9650fa63bdfa24c30ab8438e8df985a2a9c3f82f","modified":1713965953226},{"_id":"source\\_posts\\linux\\Linux_0install.md","hash":"87c6ad8b50edf4efeaaf5d0916c205539a8a27ee","modified":1709347458204},{"_id":"source\\_posts\\hexo\\test.md","hash":"b217c144d6bf24a6f138e7eb073314737e71d4fc","modified":1714922702902},{"_id":"source\\_posts\\linux\\Linux_1.command.md","hash":"7eb3949d938d93e4ae2a987fb2bf8211b3948930","modified":1709358161789},{"_id":"source\\_posts\\linux\\Linux_2.user.md","hash":"014ea6c643408e34faa2f6cd822a2e509e7e9f5f","modified":1709360144848},{"_id":"source\\_posts\\linux\\Linux_4.usage2.md","hash":"db28e33512e8e71c4c875491329924be3736c7d9","modified":1709383646719},{"_id":"source\\_posts\\linux\\Linux_3.usage.md","hash":"de60d1164062cd60c0fd9d7413c1d08014dc57fc","modified":1709380499742},{"_id":"source\\_posts\\linux\\Linux_5.usage3.md","hash":"7e708889ffefc4e4bc8f49bf4fae3fe91897de90","modified":1709383659959},{"_id":"source\\_posts\\linux\\Linux_nopassword.md","hash":"3f1a0fad54b7007399c9c1806edd948bdd1ecd63","modified":1710160275983},{"_id":"source\\_posts\\linux\\tmux.md","hash":"dc155431eccc43b7890dbc8d4c5443aa74101b6c","modified":1710166748051},{"_id":"source\\_posts\\algorithm\\1_base\\1_qsort.md","hash":"8cd58c2e72c9bf2d46e499ad054106860af2f1b1","modified":1713537616578},{"_id":"source\\_posts\\algorithm\\1_base\\2_mergesort.md","hash":"22c8da3220874b6272c8f8da9fb61ca8a7e260c4","modified":1713418419799},{"_id":"source\\_posts\\algorithm\\1_base\\1_sqsort.cpp","hash":"85a4e2aad7365560216e7247ad65e2a08cc17459","modified":1710671706971},{"_id":"source\\_posts\\algorithm\\1_base\\3_dichotomy.md","hash":"16b28d4d43a606bc775882ba880934a59407d5f2","modified":1713439224711},{"_id":"source\\_posts\\experience\\app\\word.md","hash":"113e8dea70c662f1b2d7a95f3b8f4df3ae17fc17","modified":1713108162247},{"_id":"source\\_posts\\experience\\app\\mouse.md","hash":"6aa6265babc3f0c96a5e97adc0f64922c7b6270a","modified":1710752128271},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\1_prepare\\3_linearalgebra.md","hash":"30018b4b76669368958e8342f6b099d7dd31dba3","modified":1714640175385},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\1_prepare\\4_calculus.ipynb","hash":"57a1425247b235039a917122bc36ea7ae591e48a","modified":1714640183688},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\1_prepare\\4_calculus.md","hash":"a856bb73bb755208fc618e4eb6d67a743c87cdb8","modified":1714640188462},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\1_prepare\\5_autodifferential.ipynb","hash":"a18063b6c95a1d336ca8f4c4ec9fd3639ac99477","modified":1714640195721},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\1_prepare\\5_autodifferential.md","hash":"293e192d9c6429eab4dff6df71a6cb937af26d3c","modified":1714640198169},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\1_prepare\\6_probability.md","hash":"05c8fe8dc8b82b4adfc54f8af11cad7f0254372a","modified":1714640208243},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\1_prepare\\7_api.ipynb","hash":"90c802ec5d03e98b1da54fae44c8e3669bfdd5fd","modified":1714640215229},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\1_prepare\\7_api.md","hash":"e85291d565069bc2850c0dd91095de8adeacc499","modified":1714640219931},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\2_linear_neural_network\\6_softmax_realize.md","hash":"5ee516e3913d1d8e0f6281b27dd9c79115f0fbb0","modified":1714640118917},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\2_linear_neural_network\\7_softmax_realize_simple.ipynb","hash":"7c92f2dc5ff26acf6f83fc95dcfcf585384ff9a2","modified":1714640125703},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\2_linear_neural_network\\7_softmax_realize_simple.md","hash":"d1b5200e6c634fc572fceb5a9eeeaf2e5af12ae0","modified":1714640128336},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\1_prepare\\.ipynb_checkpoints\\4_calculus-checkpoint.ipynb","hash":"96341d722267f34e7425aaf6e549607172121990","modified":1706775885763},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\1_prepare\\.ipynb_checkpoints\\7_api-checkpoint.ipynb","hash":"3b7ddc391949f132f2d6f1bbdd9b413ee3ff9ca8","modified":1706931418606},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\1_prepare\\6_probability.ipynb","hash":"5c9c53b3c65f45f7b2a21adb0a52c6ae4b1e512a","modified":1714640205924},{"_id":"source\\_posts\\deeplearning\\code\\pytorch\\data\\timemachine.txt","hash":"090b5e7e70c295757f55df93cb0a180b9691891a","modified":1714552701896},{"_id":"source\\_posts\\algorithm\\1_base\\a.exe","hash":"9b9960863d836de96027d3211221a722abfde0a8","modified":1710671148656},{"_id":"public/2024/05/06/hexo/test/index.html","hash":"8d7296c46f2b71a84e848c169299c555f4837e8f","modified":1714977814509},{"_id":"public/2024/05/05/deeplearning/code/pytorch/7_rnn/7_backward/index.html","hash":"e32146c5d9dec18187e810ec43d5c7e3030ff838","modified":1714978435246},{"_id":"public/2024/05/04/deeplearning/code/pytorch/7_rnn/6_rnn_sample_realize/index.html","hash":"cee58eaff705d3a56f81a7ce1a11a324cf93330b","modified":1714978435246},{"_id":"public/2024/05/03/deeplearning/code/pytorch/7_rnn/5_rnn_realize/index.html","hash":"327bf4331536e5560ad25775345a69bb7d8c5651","modified":1714977814509},{"_id":"public/2024/05/03/deeplearning/code/pytorch/7_rnn/4_rnn/index.html","hash":"5d255a16e999ad3f62c025d08981a5383307b78b","modified":1714977814509},{"_id":"public/2024/05/02/deeplearning/code/pytorch/7_rnn/3_lmdataset/index.html","hash":"e4885577290875e1c89d3c3e622860e466aaa813","modified":1714977814509},{"_id":"public/2024/05/02/python/ramdom/index.html","hash":"749eb8bf39fdf1640f8c8ba87dd88f7b9cda9521","modified":1714977814509},{"_id":"public/2024/05/01/deeplearning/code/pytorch/7_rnn/2_textprecess/index.html","hash":"4c2b4bd0c3c96e0d27d4e30f4b7d30988e79b7c8","modified":1714977814509},{"_id":"public/2024/04/30/deeplearning/code/pytorch/7_rnn/1_seqmodel/index.html","hash":"720228849e4af7fa6f98b7c146bd5893d00b154c","modified":1714977814509},{"_id":"public/2024/04/28/deeplearning/code/pytorch/6_moden_convolution/7_DenseNet/index.html","hash":"13d0367fa4c57a831d8ab906f5898c3f8cee0cdf","modified":1714977814509},{"_id":"public/2024/04/27/deeplearning/code/pytorch/6_moden_convolution/6_resnet/index.html","hash":"946070dc3e057980ac6449d136235cd676b5c2e9","modified":1714977814509},{"_id":"public/2024/04/23/experience/install/miniconda/index.html","hash":"9595a5edecf7cd3165cbec8036a933888e70cc05","modified":1714977814509},{"_id":"public/2024/04/22/deeplearning/code/pytorch/6_moden_convolution/5_batchnormal/index.html","hash":"cf1000977be3a502dbaba86aef9ad7a27e9bd2f3","modified":1714977814509},{"_id":"public/2024/04/22/deeplearning/code/pytorch/6_moden_convolution/4_googleNet/index.html","hash":"11597c077dcffb0aec6051b5b5e2e88dbf79a4ca","modified":1714977814509},{"_id":"public/2024/04/22/deeplearning/code/pytorch/6_moden_convolution/3_NiN/index.html","hash":"70b5cbd87c3131d2cbb8fbe57096d8b20c6e20e5","modified":1714977814509},{"_id":"public/2024/04/22/deeplearning/code/pytorch/6_moden_convolution/2_vgg/index.html","hash":"7c69ccaa25bcad723ddba6c430dcb9b189b606dd","modified":1714977814509},{"_id":"public/2024/04/22/deeplearning/code/pytorch/6_moden_convolution/1_AlexNet/index.html","hash":"4c0ec77e1c48d7335c98ecac70dab9c5e793f671","modified":1714977814509},{"_id":"public/2024/04/21/deeplearning/code/pytorch/5_convolution/6_LeNet/index.html","hash":"0ebbd1408962d8e8b601a49cf3a3d0f40ab7f3af","modified":1714977814509},{"_id":"public/2024/04/21/deeplearning/code/pytorch/5_convolution/5_pooling/index.html","hash":"c0f10b79b6b560ae86ff852638fb5901d4efbbcd","modified":1714977814509},{"_id":"public/2024/04/21/deeplearning/code/pytorch/5_convolution/4_mult_channel/index.html","hash":"e8a5914f9775bc81f3af6b01e445478c1c90ba3e","modified":1714977814509},{"_id":"public/2024/04/21/deeplearning/code/pytorch/5_convolution/3_padding_stride/index.html","hash":"d5e2daf09f79526e98c4c078c142e39ca116d730","modified":1714977814509},{"_id":"public/2024/04/20/deeplearning/code/pytorch/5_convolution/2_picture_cov/index.html","hash":"9339d5254ac5e7acadeae822adf7107a286193f1","modified":1714977814509},{"_id":"public/2024/04/20/deeplearning/code/pytorch/5_convolution/1_cov/index.html","hash":"d0a8950766905c5ab6009f89d8b065581a904636","modified":1714977814509},{"_id":"public/2024/04/20/deeplearning/code/pytorch/4_calculate/6_GPU/index.html","hash":"396d10c79948967f03d0609849f340b0d05596e5","modified":1714977814509},{"_id":"public/2024/04/20/deeplearning/code/pytorch/4_calculate/5_file/index.html","hash":"50710da1d05fb54ce97fb47f762a0143a106e737","modified":1714977814509},{"_id":"public/2024/04/20/deeplearning/code/pytorch/4_calculate/3_laterinit/index.html","hash":"a5407bc3331912186c7ea0f093152a5cac933a9e","modified":1714977814509},{"_id":"public/2024/04/20/deeplearning/code/pytorch/4_calculate/4_selflayer/index.html","hash":"df26b323174aaba951901fab1002bd761fd5cabc","modified":1714977814509},{"_id":"public/2024/04/19/deeplearning/code/pytorch/4_calculate/2_paramter/index.html","hash":"59125d038f59fae34c4dc3ab7c6fbd2ea842b81d","modified":1714977814509},{"_id":"public/2024/04/18/deeplearning/code/pytorch/4_calculate/1_block/index.html","hash":"78b9350b8a98974b1ba1835105024d9185e291d1","modified":1714977814509},{"_id":"public/2024/04/18/algorithm/1_base/3_dichotomy/index.html","hash":"0ec9de520289f3196135b6970815f59d4335b4f7","modified":1714977814509},{"_id":"public/2024/04/14/experience/app/word/index.html","hash":"69d7611c7e9b092b8910215fa6842bc0cdf64e38","modified":1714977814509},{"_id":"public/2024/04/01/python/PIL/index.html","hash":"c9ea6bd99ffd696179ac4004b020ee8ad38dd76e","modified":1714977814509},{"_id":"public/2024/04/01/python/os/index.html","hash":"c6b646c7a903b2f7ed48bdedab1da07e6de2a12c","modified":1714977814509},{"_id":"public/2024/04/01/python/re/index.html","hash":"445e3c9fddb00cdb0e2d11aafb52d00d70a72bef","modified":1714977814509},{"_id":"public/2024/03/19/deeplearning/code/pytorch/3_mlp/10_kaggle_predict_price/index.html","hash":"5b0c73f1a999fa84747e82a9c6614818d06ee32f","modified":1714977814509},{"_id":"public/2024/03/18/experience/app/mouse/index.html","hash":"cb091f480942aff87b6eb66705f0dce360aa4828","modified":1714977814509},{"_id":"public/2024/03/17/deeplearning/code/pytorch/3_mlp/9_distribution_shift/index.html","hash":"44026869e85bb36541682c50b58bfa7fa35f71eb","modified":1714977814509},{"_id":"public/2024/03/16/deeplearning/code/pytorch/3_mlp/8_init/index.html","hash":"339f2808b04a49a1f5f1260c2b67ce4cfa4e45e6","modified":1714977814509},{"_id":"public/2024/03/12/experience/windows/powershell/index.html","hash":"8af8ef5bc27735c6260d4e610cbaac66924740d1","modified":1714977814509},{"_id":"public/2024/03/11/linux/tmux/index.html","hash":"6cd841249c92479e61a2115888f5a970037f7d00","modified":1714977814509},{"_id":"public/2024/03/11/experience/windows/ssh/index.html","hash":"354abb72c2669974c7439eade355da50b04cac62","modified":1714977814509},{"_id":"public/2024/03/10/python/tips/index.html","hash":"a82da1f1fac7d07ce7003491e9ec9a2a08e85ba7","modified":1714977814509},{"_id":"public/2024/03/09/linux/Linux_nopassword/index.html","hash":"068f94578b3d8fb28d1c020d2c9de5f577dd5126","modified":1714977814509},{"_id":"public/2024/03/08/java/produce_practice/7_project/index.html","hash":"3d9e4bea7b44af8bb68630f2c8a35ef8f2ecd6bd","modified":1714977814509},{"_id":"public/2024/03/07/experience/keyboard/keymap/index.html","hash":"de51baaedcefbbbd657bdd82f15ee2969e5e062b","modified":1714977814509},{"_id":"public/2024/03/05/java/produce_practice/6_testlog/index.html","hash":"787a28d2153e9afbd76fad766f65000d4d57fb84","modified":1714977814509},{"_id":"public/2024/03/05/java/produce_practice/5_usermanage/index.html","hash":"c70b99d5b6463ecc082632b541efa078c5d8b416","modified":1714977814509},{"_id":"public/2024/03/05/java/produce_practice/4_login/index.html","hash":"b233980f0b98f1765e3126bc8956a90470559c0b","modified":1714977814509},{"_id":"public/2024/03/04/java/produce_practice/3_userbase/index.html","hash":"721db3f44e7f54b6dea152353ed7984ad27a5fac","modified":1714977814509},{"_id":"public/2024/03/04/java/produce_practice/2_static/index.html","hash":"e02e29cdd87b2c8f340eacc0b0b511d7f397f0e5","modified":1714977814509},{"_id":"public/2024/03/02/linux/Linux_5.usage3/index.html","hash":"bbdc957f5336b5f14eeb7deebf391e8522e04473","modified":1714977814509},{"_id":"public/2024/03/02/linux/Linux_4.usage2/index.html","hash":"a7a1a39713826e7a02034e131fc88517005d36f9","modified":1714977814509},{"_id":"public/2024/03/02/linux/Linux_3.usage/index.html","hash":"19b8d14fbf1994f95226f71e70569ec3bd3de09c","modified":1714977814509},{"_id":"public/2024/03/02/java/produce_practice/1_setup/index.html","hash":"6bd6a23d10c66bc8e8f41936318db64d7260594b","modified":1714977814509},{"_id":"public/2024/03/01/java/produce_practice/0_config/index.html","hash":"29d3c4ffd557529845cbcd999864e41b8c771f68","modified":1714977814509},{"_id":"public/2024/02/24/experience/website/kaggle/index.html","hash":"03fe8043794c2279c65dd8cfa2e3466e364e9b34","modified":1714977814509},{"_id":"public/2024/02/07/deeplearning/code/pytorch/3_mlp/6_deopout/index.html","hash":"2010b768ffb93b5c2dbd83f4a88093fbe955d0b5","modified":1714977814509},{"_id":"public/2024/02/07/deeplearning/code/pytorch/3_mlp/7_backward/index.html","hash":"afbd85daa867e2dfffb8e86d140717013fef259c","modified":1714977814509},{"_id":"public/2024/02/06/deeplearning/code/pytorch/3_mlp/5_weight_decay/index.html","hash":"7150b63088ce341cf12d01c886bc8a77d053d15c","modified":1714977814509},{"_id":"public/2024/02/05/deeplearning/code/pytorch/3_mlp/2_mlp_realize/index.html","hash":"de5ab9a016be3dc10a2cf6cab3dab5e1068a13bf","modified":1714977814509},{"_id":"public/2024/02/05/deeplearning/code/pytorch/3_mlp/3_mlp_realize_simple/index.html","hash":"80716b09e36f7a50ba32695d471af0a4258132e6","modified":1714977814509},{"_id":"public/2024/02/05/deeplearning/code/pytorch/3_mlp/4_overfitting/index.html","hash":"905ef65afa2602bc54f7e7dfee18f1b7e05c0e1d","modified":1714977814509},{"_id":"public/2024/02/04/deeplearning/code/pytorch/2_linear_neural_network/5_image_classification_dataset/index.html","hash":"2f4b683ec48d4393726593c9d2e78e1c14b39809","modified":1714977814509},{"_id":"public/2024/02/04/deeplearning/code/pytorch/2_linear_neural_network/6_softmax_realize/index.html","hash":"746b7690d577951199b08410717fa19aa587d410","modified":1714977814509},{"_id":"public/2024/02/04/deeplearning/code/pytorch/2_linear_neural_network/7_softmax_realize_simple/index.html","hash":"4848e7aa36f2737810d77d6176d450e19046c7ac","modified":1714977814509},{"_id":"public/2024/02/04/deeplearning/code/pytorch/3_mlp/1_mlp/index.html","hash":"c4c5caea7cee4d8460e1bb6505c0330bc7965c3a","modified":1714977814509},{"_id":"public/2024/02/03/deeplearning/code/pytorch/2_linear_neural_network/2_linear_regression_realize/index.html","hash":"ca95d0c42b166bef40cee00597b4bc259f8476f3","modified":1714977814509},{"_id":"public/2024/02/03/deeplearning/code/pytorch/2_linear_neural_network/1_linear_regression/index.html","hash":"85cbd2e8fb976cdcd9e13e9611e258b54eaa82e9","modified":1714977814509},{"_id":"public/2024/02/03/deeplearning/code/pytorch/2_linear_neural_network/3_linear_regression_simplerealize/index.html","hash":"9b2595b084ca69b67fa43c901de3bc30e8279907","modified":1714977814509},{"_id":"public/2024/02/03/deeplearning/code/pytorch/2_linear_neural_network/4_softmax_regression/index.html","hash":"834d567cbeb5db8cb0ecf649342f36c68c4a763b","modified":1714977814509},{"_id":"public/2024/02/02/deeplearning/code/pytorch/1_prepare/6_probability/index.html","hash":"1e371e41e7352e7478c5fe72d084551878ef2146","modified":1714977814509},{"_id":"public/2024/02/02/deeplearning/code/pytorch/1_prepare/7_api/index.html","hash":"54991d8b97c4908e9c4d014bd9ae9333f2ad9906","modified":1714977814509},{"_id":"public/2024/02/01/deeplearning/code/pytorch/1_prepare/3_linearalgebra/index.html","hash":"73a2ddb8290ca95c45851ef394d01eabe140c39b","modified":1714977814509},{"_id":"public/2024/02/01/deeplearning/code/pytorch/1_prepare/4_calculus/index.html","hash":"22ff08bd333ee4185a6d7308bac568dcaeed7849","modified":1714977814509},{"_id":"public/2024/02/01/deeplearning/code/pytorch/1_prepare/5_autodifferential/index.html","hash":"c8177b4e13b47e2792926b2175c6a93b488178b7","modified":1714977814509},{"_id":"public/2024/01/30/hexo/sourcecode/index.html","hash":"ba34dc3e61c0965f801c326b8f4f31f6f1545494","modified":1714977814509},{"_id":"public/2024/01/30/hexo/latex/index.html","hash":"6f3c72258fdcbe2ac7abf28ef252735bda2ff61e","modified":1714977814509},{"_id":"public/2023/10/02/algorithm/1_base/1_qsort/index.html","hash":"b97daee5017058429bdfa7bfd98bd72c0043d5f7","modified":1714977814509},{"_id":"public/2023/10/02/algorithm/1_base/2_mergesort/index.html","hash":"936a2a015b89ab0dbd997d2756d6cc545e5124da","modified":1714977814509},{"_id":"public/2021/03/03/linux/Linux_2.user/index.html","hash":"bdc9505198542ab9ebeff7aae9489dfd1cf05829","modified":1714978435246},{"_id":"public/2021/03/02/linux/Linux_1.command/index.html","hash":"eb587b7cfc555a402281e7f17d80c5a54f0ebfa1","modified":1714977814509},{"_id":"public/2021/03/01/linux/Linux_0install/index.html","hash":"f9d977bd6a706c4d5fe2dc308376992a6975d448","modified":1714977814509},{"_id":"public/archives/page/10/index.html","hash":"5c88436123cf9649d78dc951eefe07a84d1b1794","modified":1714978435246},{"_id":"public/archives/page/11/index.html","hash":"9cff8ae2dad3092c9201e8869edad5c4a31ddfc1","modified":1714978435246},{"_id":"public/archives/page/12/index.html","hash":"36ce98f4248bd4a03a5b40cb39e9c507122fece4","modified":1714978435246},{"_id":"public/archives/page/13/index.html","hash":"99afae0bf6394c8546a465e23cda9714834fcdfd","modified":1714978435246},{"_id":"public/archives/page/14/index.html","hash":"181df6bea95ea534a6c02c3ffdaa32ce84724edf","modified":1714978435246},{"_id":"public/archives/page/15/index.html","hash":"94dd445bc19affb8ee8e2cc79233d5b484ade7ff","modified":1714978435246},{"_id":"public/archives/page/16/index.html","hash":"f13f1d949e0a8b25fda54d5f8f7de87087025686","modified":1714978435246},{"_id":"public/archives/page/17/index.html","hash":"bdcfc1f1a716350909a19480a1a4267507e3965a","modified":1714978435246},{"_id":"public/archives/page/18/index.html","hash":"5efda587486cdd713dd3172d2b03998836892926","modified":1714978435246},{"_id":"public/archives/page/19/index.html","hash":"82186723c4389d0c82980e70b78f6c738ff4142e","modified":1714978435246},{"_id":"public/archives/page/20/index.html","hash":"292a16bb4db0ea26d92c2604cce598a49aa3fb97","modified":1714978435246},{"_id":"public/archives/page/21/index.html","hash":"b2814505b9e3b6bbe2e7e1ee19c8732adf44bab1","modified":1714978435246},{"_id":"public/archives/page/22/index.html","hash":"1d65528df06f8841660a5b4b5e97c2dacc74998e","modified":1714978435246},{"_id":"public/archives/page/23/index.html","hash":"e04b01aebc4237bd4e329a087f42ed10580164a4","modified":1714978435246},{"_id":"public/archives/page/24/index.html","hash":"fcb6420212f26a59b675eea5b429a611f8926598","modified":1714978435246},{"_id":"public/archives/page/25/index.html","hash":"0304df5af0e9ada56aada7f8b7106869e764db04","modified":1714978435246},{"_id":"public/archives/2021/page/2/index.html","hash":"f5d91cd74cddd6f5d2fc395a3b9d3e3028961680","modified":1714977814509},{"_id":"public/archives/2021/03/page/2/index.html","hash":"d3d46b3dd0f2a0bf16da1be36e11890f82ad242a","modified":1714977814509},{"_id":"public/archives/2023/page/6/index.html","hash":"bd8eae6b1e42263955ffbf44bddc6fbfb64660e1","modified":1714978435246},{"_id":"public/archives/2024/page/3/index.html","hash":"b50d7c0d0daa21b7c1ad8ed1911c0f0f77d7d19c","modified":1714978435246},{"_id":"public/archives/2024/page/4/index.html","hash":"5c1b2975be3686084691ac4851e00a54f5e46538","modified":1714978435246},{"_id":"public/archives/2024/page/5/index.html","hash":"ad2445062897d38f0ed7d3930fd6c642d0ab3ae1","modified":1714978435246},{"_id":"public/archives/2024/page/6/index.html","hash":"153916427c63535af6224233470127731e17a764","modified":1714978435246},{"_id":"public/archives/2024/page/7/index.html","hash":"0e3fef8d9a68cdb5c9af4b38385f5ee4ce5450f4","modified":1714978435246},{"_id":"public/archives/2024/page/8/index.html","hash":"013e2538cf52bedc8dd31243403b5fbd795cc23d","modified":1714978435246},{"_id":"public/archives/2024/page/9/index.html","hash":"09a4aa5e4779a1374dbc2a83a83e673c00f98a44","modified":1714978435246},{"_id":"public/archives/2024/page/10/index.html","hash":"0dad42e5fbf09eaadf88dd66e9488a81615325fd","modified":1714978435246},{"_id":"public/archives/2024/page/11/index.html","hash":"bf6f0929b5392190e6b37afa6341193523926ca1","modified":1714978435246},{"_id":"public/archives/2024/page/12/index.html","hash":"b525ff08ffceb94dbd4cda47affed3352d5b2c6d","modified":1714978435246},{"_id":"public/archives/2024/page/13/index.html","hash":"44e6b4edc3661d9f0b1536dabaea66301f7bf7b7","modified":1714978435246},{"_id":"public/archives/2024/page/14/index.html","hash":"68c16d983052b78946a5065c8bda761dc383d253","modified":1714978435246},{"_id":"public/archives/2024/page/15/index.html","hash":"ced0443afb41ed6c2ffce3de8ec94f338efb7bcb","modified":1714978435246},{"_id":"public/archives/2024/page/16/index.html","hash":"779b0f5213af5f1f30aaa48b1b66c81282aa7b55","modified":1714978435246},{"_id":"public/archives/2024/page/17/index.html","hash":"8577f1d34dcf308f71534bb3dcae48429bd1ab21","modified":1714978435246},{"_id":"public/archives/2024/page/18/index.html","hash":"5ba8fcaad6ad07b3a40d0fe9aee3bf23ba45527f","modified":1714977814509},{"_id":"public/archives/2024/01/page/2/index.html","hash":"a02137b23557e0156f5e6f102fc6f4d7904e4895","modified":1714977814509},{"_id":"public/archives/2024/02/index.html","hash":"19f83581cca496acfc3e31066cff9c6e5a43599d","modified":1714977814509},{"_id":"public/archives/2024/02/page/2/index.html","hash":"920c99cb0f6403dbdf5ba2afddc23de28c196916","modified":1714977814509},{"_id":"public/archives/2024/02/page/3/index.html","hash":"b04611b354a056d5017c9aaad877fd644914d917","modified":1714977814509},{"_id":"public/archives/2024/02/page/4/index.html","hash":"678a37cbd8c7668c5c2ff534432ac9d9223c4798","modified":1714977814509},{"_id":"public/archives/2024/03/index.html","hash":"510b4bfd29acc62fb87d7628c679f0aedc6acd27","modified":1714977814509},{"_id":"public/archives/2024/03/page/2/index.html","hash":"c9738ca3a77e06a22932c80c2d5860bb821c6a84","modified":1714977814509},{"_id":"public/archives/2024/03/page/3/index.html","hash":"e5842248b52da4b4be2f3246b6b1bb9e9d54caaa","modified":1714977814509},{"_id":"public/archives/2024/03/page/4/index.html","hash":"e9a770b570c4279de46e922ad0f6e95780dcc83b","modified":1714977814509},{"_id":"public/archives/2024/03/page/5/index.html","hash":"4fe6719435526eff5d4ff21bee72578e7d354fe3","modified":1714977814509},{"_id":"public/archives/2024/04/index.html","hash":"e8478184b3e8098b3b92f62c5c3d4aa92b51ce57","modified":1714977814509},{"_id":"public/archives/2024/04/page/2/index.html","hash":"aaf3cccf31c35d6b47965d20607c6a85b7cfe4d5","modified":1714977814509},{"_id":"public/archives/2024/04/page/3/index.html","hash":"487e0c8b39504f03612f1f9047c95d0381dce717","modified":1714977814509},{"_id":"public/archives/2024/04/page/4/index.html","hash":"7cec3bcf5c7afa3435e4ea51111cc4888fc3dc26","modified":1714977814509},{"_id":"public/archives/2024/04/page/5/index.html","hash":"17da550c46eb3d2b6625aeb9a4a346e52d469fb0","modified":1714977814509},{"_id":"public/archives/2024/04/page/6/index.html","hash":"81a329494edeb5bee9adbeb7995da4f66f8eb3ad","modified":1714977814509},{"_id":"public/archives/2024/05/page/2/index.html","hash":"aa7ab539cf2ae144f887864fa954acd2351c5f9e","modified":1714978435246},{"_id":"public/archives/2024/05/page/3/index.html","hash":"055c4ca7ab6cf7da4425b9b79c1edaa6c6e8c622","modified":1714977814509},{"_id":"public/page/10/index.html","hash":"e2bf92ec43b505a4dd583289f39a8187099df17e","modified":1714978435246},{"_id":"public/page/11/index.html","hash":"d2e294a22d7dc8ed46b727cbe83921243e640f65","modified":1714978435246},{"_id":"public/page/12/index.html","hash":"6d183626a403a5708ab677716e2607a52035ce9a","modified":1714978435246},{"_id":"public/page/13/index.html","hash":"3231db027e022de2ba06dfcd16a8955f658754f7","modified":1714978435246},{"_id":"public/page/14/index.html","hash":"22d256153af5d6a6254b230959a994acc5eb2ab6","modified":1714978435246},{"_id":"public/page/15/index.html","hash":"502f10eb975a1f39f694a59d961b9e7d953d23c1","modified":1714978435246},{"_id":"public/page/16/index.html","hash":"ae29ceae42044a2b41a24daed0eb7e7e0aeebba9","modified":1714978435246},{"_id":"public/page/17/index.html","hash":"ca1dcea7b375abbba2f03f893d0bb6504e8fab3f","modified":1714978435246},{"_id":"public/page/18/index.html","hash":"70681ca8007625ee8fc35c60cd0b365a1a34ba16","modified":1714978435246},{"_id":"public/page/19/index.html","hash":"27239aad2abbd05de02f42745fce40fbfb4ee0c3","modified":1714978435246},{"_id":"public/page/20/index.html","hash":"23fe878f4291d0dbfa99faf95f7eb79630c7f451","modified":1714978435246},{"_id":"public/page/21/index.html","hash":"06976d1d1960fa41fe8bfff39aa7a4a7ff0b1f5e","modified":1714978435246},{"_id":"public/page/22/index.html","hash":"9d028a7ad5265d76a59de644c3055b9e8793edd0","modified":1714978435246},{"_id":"public/page/23/index.html","hash":"6d5bb7ee157f3c23c961b993e2cf2798d16e1760","modified":1714978435246},{"_id":"public/page/24/index.html","hash":"8a25f963dd1008a9302ebd168cadb9613a8ee58e","modified":1714978435246},{"_id":"public/page/25/index.html","hash":"b81c484228cc64c9abe70b2240b1c6a0ead01159","modified":1714978435246},{"_id":"public/categories/Linux/page/2/index.html","hash":"9cb2f47066502ee1065b1e3cacf573cdfde696dd","modified":1714977814509},{"_id":"public/categories/Linux/page/3/index.html","hash":"556116c0c7dc3a65962951a407616dba0c69576c","modified":1714977814509},{"_id":"public/categories/机器学习/index.html","hash":"33d18f9381cc7044da22b9f572379cac0504ef5b","modified":1714977814509},{"_id":"public/categories/机器学习/page/2/index.html","hash":"8e440269604dabcd67a6ac4df58fe75479b7b4a1","modified":1714977814509},{"_id":"public/categories/机器学习/page/3/index.html","hash":"7019132245aee0546f468f4e5bf463d8f2462cdf","modified":1714977814509},{"_id":"public/categories/机器学习/page/4/index.html","hash":"b4527f199e03c384f06a00e1e9a37064f15e491c","modified":1714977814509},{"_id":"public/categories/机器学习/page/5/index.html","hash":"eac9f87573f90a354d20aba6ef962ccc07a949cb","modified":1714977814509},{"_id":"public/categories/机器学习/page/6/index.html","hash":"7b55e8f42699c45d770f8274f7fabf8531518065","modified":1714977814509},{"_id":"public/categories/机器学习/page/7/index.html","hash":"9035d570ccd38c0ad24182433692c14271125533","modified":1714977814509},{"_id":"public/categories/机器学习/page/8/index.html","hash":"a5f09cf6152478390243f649b5c4e01add4fbd65","modified":1714977814509},{"_id":"public/categories/机器学习/page/9/index.html","hash":"e597ab728cb83ad7f365c19d48075578f92cd54f","modified":1714977814509},{"_id":"public/categories/机器学习/page/10/index.html","hash":"1552b01a8471bb81ee5f0015bec0dd5c63622ec6","modified":1714977814509},{"_id":"public/categories/经验/index.html","hash":"05ce97eb37504fb5b78cfa74476d39365a0f1223","modified":1714977814509},{"_id":"public/categories/经验/page/2/index.html","hash":"8ffa7080cb7bf3488dd551fffa3c4a540fb4e71a","modified":1714977814509},{"_id":"public/categories/博客/git/index.html","hash":"69f96aefcf8866b8ac82b0c51afad396ec3af721","modified":1714977814509},{"_id":"public/categories/博客/hexo/index.html","hash":"3c1939d272e8bcdfa26a35c856f6828aa74fd2ee","modified":1714977814509},{"_id":"public/categories/安装/index.html","hash":"21779e3b45181b318ba44824809b01841fd1c02b","modified":1714977814509},{"_id":"public/categories/python库/index.html","hash":"9ce1245acd3c5c065be55410be9800aee6dad208","modified":1714977814509},{"_id":"public/categories/python/index.html","hash":"1d1de4ed8afac197b1387f69f6417dd181618a17","modified":1714977814509},{"_id":"public/categories/windows/index.html","hash":"cdfb2227f6a9c9b4fe856fcc40b8e6627a49f4c7","modified":1714977814509},{"_id":"public/categories/java/index.html","hash":"f1a331903bbd41dc2eeffdd42bb57e2de2663dda","modified":1714977814509},{"_id":"public/categories/java/page/2/index.html","hash":"5c4a4ad96adae10389967e39dd9a3eb11717ab7e","modified":1714977814509},{"_id":"public/tags/Linux/page/2/index.html","hash":"635ea8aec43d684d7500e190771f45fb14ffe8f0","modified":1714977814509},{"_id":"public/tags/Linux/page/3/index.html","hash":"e2bb369c1ab6370af68c3cca4c8d85e91606d244","modified":1714977814509},{"_id":"public/tags/机器学习/page/2/index.html","hash":"8c8f98814b88a011f1535b97e8cabd5307a4e166","modified":1714977814509},{"_id":"public/tags/机器学习/page/3/index.html","hash":"1f0c13f269599b2bb8a129cc4c818c57cd801113","modified":1714977814509},{"_id":"public/tags/机器学习/page/4/index.html","hash":"b9e6e0bf498263e426f7ee3fc169de572f9c7491","modified":1714977814509},{"_id":"public/tags/机器学习/page/5/index.html","hash":"a2a047c056acfe9a8028caa460a78786d089fde4","modified":1714977814509},{"_id":"public/tags/机器学习/page/6/index.html","hash":"af90c9390e38e0ef6ceef0b1d55b5f81cd5f0cbe","modified":1714977814509},{"_id":"public/tags/机器学习/page/7/index.html","hash":"b8b4f9c1d519aa8b1fc73c0dcae4511e7aa68904","modified":1714977814509},{"_id":"public/tags/机器学习/page/8/index.html","hash":"5ac7af42bcce2b38b5d62129353a26f5f8705f27","modified":1714977814509},{"_id":"public/tags/机器学习/page/9/index.html","hash":"3b052df8fa22136ba97e4e9a7d828b65f47d5b88","modified":1714977814509},{"_id":"public/tags/机器学习/page/10/index.html","hash":"d971e58ab05d379384973b2869620c2852eb820d","modified":1714977814509},{"_id":"public/tags/pytorch/page/2/index.html","hash":"58c189e97fce38a5fca53bb6fed3d26743cdddfb","modified":1714977814509},{"_id":"public/tags/pytorch/page/3/index.html","hash":"8f41e1e9977a861c4e895c88912497e8101a0085","modified":1714977814509},{"_id":"public/tags/pytorch/page/4/index.html","hash":"eddbc31ed221e20565171ddac52ce1a6b06e03fc","modified":1714977814509},{"_id":"public/tags/pytorch/page/5/index.html","hash":"351a91a2f26e239c46f33c1bcd64cfec1e349bdc","modified":1714977814509},{"_id":"public/tags/pytorch/page/6/index.html","hash":"79393f03c0f16f091ebd89566b521ece6d900003","modified":1714977814509},{"_id":"public/tags/pytorch/page/7/index.html","hash":"409c4d2cec643db68bee4f641af220b79315bb62","modified":1714977814509},{"_id":"public/tags/pytorch/page/8/index.html","hash":"6ecb3ef23be2f2a375de2e9921ca6914664817de","modified":1714977814509},{"_id":"public/tags/pytorch/page/9/index.html","hash":"c54a053df8d2a03045e7f03992da83cbfb7531a1","modified":1714977814509},{"_id":"public/tags/pytorch/page/10/index.html","hash":"bcd685e75e54986ca6ddbaeb0e01fac88d8143d5","modified":1714977814509},{"_id":"public/tags/鼠标/index.html","hash":"45387254cc04be75b5745bd2df2b2f381094bd87","modified":1714977814509},{"_id":"public/tags/经验/index.html","hash":"529b45c297ddbbb8e43fe19a228265418054753c","modified":1714977814509},{"_id":"public/tags/kaggle/index.html","hash":"f48e32f9ed23ae1612af495f48ad8d3c58406bd7","modified":1714977814509},{"_id":"public/tags/键盘/index.html","hash":"3cb704d94f5fe44a2caf7f5b1b5e49753d86c1bb","modified":1714977814509},{"_id":"public/tags/安装/index.html","hash":"b4c44bc0681c473b8e934ef5252e88d01073e271","modified":1714977814509},{"_id":"public/tags/python/index.html","hash":"482fdfd76ea372288fb309320566c2110baf7af6","modified":1714977814509},{"_id":"public/tags/windows/index.html","hash":"43884900fb1c7bdadbb7b04923b909e0c491f188","modified":1714977814509},{"_id":"public/tags/生产实习/index.html","hash":"12da41a2e99e915539aeb675f1a030c9188b6692","modified":1714977814509},{"_id":"public/tags/生产实习/page/2/index.html","hash":"5e3c01d01baf99dfb4a2d6818355fa4090a8908a","modified":1714977814509},{"_id":"public/tags/java/index.html","hash":"7fc234d25aaa7dc8d17dad39bd20b8a6682361ca","modified":1714977814509},{"_id":"public/tags/java/page/2/index.html","hash":"d00ecaa7a444eb24604730ce254308d21aaefdcd","modified":1714977814509},{"_id":"public/tags/springboot/index.html","hash":"305d8b315bfc6d32163397478da77f146e3a7e15","modified":1714977814509},{"_id":"public/tags/springboot/page/2/index.html","hash":"61b57ae9f93e19c37bd9cf2140b8513408b5eeb4","modified":1714977814509},{"_id":"public/tags/Windows/index.html","hash":"da4e482b03d1f4f9aedcc0c77a104e1f971a9b3d","modified":1714977814509},{"_id":"public/2023/12/12/datastruct/7_search/3_AVL/index.html","hash":"7d23cdf0c452c1e5ef1c3b5d8d3432a895fc5c37","modified":1714978435246},{"_id":"public/2023/12/12/datastruct/7_search/4_redblacktree/index.html","hash":"88d4c3ffa74de2aa1c8fd950d8bef2d8b2be3303","modified":1714978435246},{"_id":"public/2023/12/12/datastruct/7_search/5_Btree/index.html","hash":"bdba240a3fe2f8053c11d944b85e2eaf533039eb","modified":1714978435246},{"_id":"public/2022/12/12/hexo/hexo搭建/index.html","hash":"2755d032b37f9aa44b4b46344cb0582c73549baa","modified":1714978435246},{"_id":"public/archives/2022/index.html","hash":"3ddd18a89fe0f7a16034788ea23ad039e4e74088","modified":1714978435246},{"_id":"public/archives/2022/12/index.html","hash":"ab946e39f48904ef0122f91a524160f3e8bd05be","modified":1714978435246}],"Category":[{"name":"博客","_id":"clvulgd5h00075ovw26ovfpxr"},{"name":"Linux","_id":"clvulgd5n000g5ovw3dvd4zg9"},{"name":"微信小程序","_id":"clvulgd5p000o5ovw1zz30awb"},{"name":"数据结构","_id":"clvulgd5u00155ovw5nj3asuk"},{"name":"算法","_id":"clvulgd67002o5ovw8wmy9d78"},{"name":"vscode","_id":"clvulgd69002y5ovw0q8e7i7m"},{"name":"其他","_id":"clvulgd6b00365ovwdqc9cvzx"},{"name":"深度学习","_id":"clvulgd6d003k5ovwai2g3qxs"},{"name":"机器学习","_id":"clvuli5qz000bzsvw3gat9qtb"},{"name":"经验","_id":"clvuli6ds004pzsvw3l15a6zm"},{"name":"git","parent":"clvulgd5h00075ovw26ovfpxr","_id":"clvuli6hm005bzsvwah283iiy"},{"name":"hexo","parent":"clvulgd5h00075ovw26ovfpxr","_id":"clvuli6ho005gzsvw0p8k8gdi"},{"name":"安装","_id":"clvuli6ih006dzsvwbvt7aghx"},{"name":"python库","_id":"clvuli6il006jzsvw6qrx5mo3"},{"name":"python","_id":"clvuli6io006yzsvw7ciihr6b"},{"name":"windows","_id":"clvuli6iv0075zsvwf73o8mtd"},{"name":"java","_id":"clvuli6rt007fzsvw7avxgv78"}],"Data":[],"Page":[{"title":"categories","date":"2023-07-20T02:18:50.000Z","type":"categories","layout":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2023-07-20 10:18:50\ntype: \"categories\"\nlayout: \"categories\"\ncomments: false\n---\n","updated":"2023-07-26T04:04:21.211Z","path":"categories/index.html","_id":"clvulgd5600005ovwap1ha74k","content":"","excerpt":"","more":""},{"title":"标签","date":"2020-02-14T14:20:43.000Z","type":"tags","layout":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: \"标签\"\ndate: \"2020-02-14 22:20:43\"\ntype: \"tags\"\nlayout: \"tags\"\ncomments: false\n\n---\n","updated":"2023-10-23T10:04:10.779Z","path":"tags/index.html","_id":"clvulgd5c00025ovwddgp3r6f","content":"","excerpt":"","more":""}],"Post":[{"title":"1. DDPM论文理解","date":"2023-10-24T02:28:00.000Z","comment":false,"toc":true,"_content":"#\n<!--more-->\n\n- 文章：[去噪扩散概率模型](https://arxiv.org/pdf/2006.11239.pdf)\n\n- 代码：https://github.com/hojonathanho/diffusion\n# 一. 论文摘要\n\n- 提出了扩散概率模型\n- 潜在变量模型来源于非平衡热力学\n- 在加权变分界限上进行训练（变分界限是根据“扩散概率模型” 和 “采用了去噪分数匹配与朗之万动力学的训练方法“之间的联系\n- 模型允许渐进的有损解压（可以解释为自回归解码的推广）\n- 在无条件CIFAR10上Inception=9.46，FID=3.17。在256*256 LSUN上与Progressive GAV相似\n","source":"_drafts/1_DDPM.md","raw":"---\ntitle: 1. DDPM论文理解\ndate: 2023-10-24 10:28:00\ntags: [深度学习,DDPM,生成模型]\ncategories: [深度学习]\ncomment: false\ntoc: true\n---\n#\n<!--more-->\n\n- 文章：[去噪扩散概率模型](https://arxiv.org/pdf/2006.11239.pdf)\n\n- 代码：https://github.com/hojonathanho/diffusion\n# 一. 论文摘要\n\n- 提出了扩散概率模型\n- 潜在变量模型来源于非平衡热力学\n- 在加权变分界限上进行训练（变分界限是根据“扩散概率模型” 和 “采用了去噪分数匹配与朗之万动力学的训练方法“之间的联系\n- 模型允许渐进的有损解压（可以解释为自回归解码的推广）\n- 在无条件CIFAR10上Inception=9.46，FID=3.17。在256*256 LSUN上与Progressive GAV相似\n","slug":"1_DDPM","published":0,"updated":"2023-10-24T03:38:31.023Z","comments":1,"layout":"post","photos":[],"_id":"clvulgd5900015ovwaxel8l6i","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<ul>\n<li><p>文章：<a href=\"https://arxiv.org/pdf/2006.11239.pdf\">去噪扩散概率模型</a></p>\n</li>\n<li><p>代码：<a href=\"https://github.com/hojonathanho/diffusion\">https://github.com/hojonathanho/diffusion</a></p>\n</li>\n</ul>\n<h1 id=\"一-论文摘要\"><a href=\"#一-论文摘要\" class=\"headerlink\" title=\"一. 论文摘要\"></a>一. 论文摘要</h1><ul>\n<li>提出了扩散概率模型</li>\n<li>潜在变量模型来源于非平衡热力学</li>\n<li>在加权变分界限上进行训练（变分界限是根据“扩散概率模型” 和 “采用了去噪分数匹配与朗之万动力学的训练方法“之间的联系</li>\n<li>模型允许渐进的有损解压（可以解释为自回归解码的推广）</li>\n<li>在无条件CIFAR10上Inception&#x3D;9.46，FID&#x3D;3.17。在256*256 LSUN上与Progressive GAV相似</li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<ul>\n<li><p>文章：<a href=\"https://arxiv.org/pdf/2006.11239.pdf\">去噪扩散概率模型</a></p>\n</li>\n<li><p>代码：<a href=\"https://github.com/hojonathanho/diffusion\">https://github.com/hojonathanho/diffusion</a></p>\n</li>\n</ul>\n<h1 id=\"一-论文摘要\"><a href=\"#一-论文摘要\" class=\"headerlink\" title=\"一. 论文摘要\"></a>一. 论文摘要</h1><ul>\n<li>提出了扩散概率模型</li>\n<li>潜在变量模型来源于非平衡热力学</li>\n<li>在加权变分界限上进行训练（变分界限是根据“扩散概率模型” 和 “采用了去噪分数匹配与朗之万动力学的训练方法“之间的联系</li>\n<li>模型允许渐进的有损解压（可以解释为自回归解码的推广）</li>\n<li>在无条件CIFAR10上Inception&#x3D;9.46，FID&#x3D;3.17。在256*256 LSUN上与Progressive GAV相似</li>\n</ul>"},{"_content":"## 1. pytorch网课\n### Introduction\n- format\n- calculate\n- variable\n- activation\n### Regression\n- 给定x , y（变量）\n- 搭建网络\n  - init重写（输入，输出，神经元个数）\n    - 每一层用Linear\n  - forward前向传递（输入数据）\n    - x依次经过init里面的层，最后返回\n- 定义网络、优化器、误差MSELoss\n- 训练（for)\n  - x输入网络得到y^\n  - 根据y与y^得到误差\n  - 优化器梯度为0\n  - backward\n  - 优化\n\n### Classification\n- 定义feature,label\n    - x0为y0类点，x1为y1类点\n    - 数据形式（[a,b] , 1)\n- 搭建网络\n    - init\n    - forward\n- 定义网络、优化器、误差CrossEntropyLoss\n- 训练（for)\n\n### quick_setup\n- net=torch.nn.Sequential()边定义边实例化\n\n### save and refine\n- save \n    - 训练完(for)后\n    - torch.save(net1,'net.pkl')#保留整个图\n    - torch.save(net1.state_dict(),'net_params.pkl')#保留结点参数\n- refine\n    - 直接提取：\n        - net2=torch.load('net.pkl')\n    - 参数提取：\n        - 先搭建一模一样的网络n\n        - 再n.load_state_dict(torch.load('net_params.pkl'))\n\n### minibize_train(批数据训练)\n- 定义batch_size，feature,label\n- 定义数据库：torch_dataset=Data.TensorDataset(x,y)\n- 定义loader：loader=Data.DataLoader( dataset,batch,shuffle,num_workers)\n- 定义网络等\n- 训练： for epoch in range(3):  \n&emsp;&nbsp;&emsp;&emsp;&emsp;for step,(batch_x,batch_y) in enumerate(loader):\n\n### optimizer\n- 用Adam\n\n### cnn\n- 下载数据集train_data,定义loader\n\n","source":"_drafts/progress1.md","raw":"## 1. pytorch网课\n### Introduction\n- format\n- calculate\n- variable\n- activation\n### Regression\n- 给定x , y（变量）\n- 搭建网络\n  - init重写（输入，输出，神经元个数）\n    - 每一层用Linear\n  - forward前向传递（输入数据）\n    - x依次经过init里面的层，最后返回\n- 定义网络、优化器、误差MSELoss\n- 训练（for)\n  - x输入网络得到y^\n  - 根据y与y^得到误差\n  - 优化器梯度为0\n  - backward\n  - 优化\n\n### Classification\n- 定义feature,label\n    - x0为y0类点，x1为y1类点\n    - 数据形式（[a,b] , 1)\n- 搭建网络\n    - init\n    - forward\n- 定义网络、优化器、误差CrossEntropyLoss\n- 训练（for)\n\n### quick_setup\n- net=torch.nn.Sequential()边定义边实例化\n\n### save and refine\n- save \n    - 训练完(for)后\n    - torch.save(net1,'net.pkl')#保留整个图\n    - torch.save(net1.state_dict(),'net_params.pkl')#保留结点参数\n- refine\n    - 直接提取：\n        - net2=torch.load('net.pkl')\n    - 参数提取：\n        - 先搭建一模一样的网络n\n        - 再n.load_state_dict(torch.load('net_params.pkl'))\n\n### minibize_train(批数据训练)\n- 定义batch_size，feature,label\n- 定义数据库：torch_dataset=Data.TensorDataset(x,y)\n- 定义loader：loader=Data.DataLoader( dataset,batch,shuffle,num_workers)\n- 定义网络等\n- 训练： for epoch in range(3):  \n&emsp;&nbsp;&emsp;&emsp;&emsp;for step,(batch_x,batch_y) in enumerate(loader):\n\n### optimizer\n- 用Adam\n\n### cnn\n- 下载数据集train_data,定义loader\n\n","slug":"progress1","published":0,"date":"2024-05-06T06:36:52.044Z","updated":"2023-02-23T10:16:58.035Z","title":"","comments":1,"layout":"post","photos":[],"_id":"clvulgd5d00035ovwgapi117b","content":"<h2 id=\"1-pytorch网课\"><a href=\"#1-pytorch网课\" class=\"headerlink\" title=\"1. pytorch网课\"></a>1. pytorch网课</h2><h3 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><ul>\n<li>format</li>\n<li>calculate</li>\n<li>variable</li>\n<li>activation</li>\n</ul>\n<h3 id=\"Regression\"><a href=\"#Regression\" class=\"headerlink\" title=\"Regression\"></a>Regression</h3><ul>\n<li>给定x , y（变量）</li>\n<li>搭建网络<ul>\n<li>init重写（输入，输出，神经元个数）<ul>\n<li>每一层用Linear</li>\n</ul>\n</li>\n<li>forward前向传递（输入数据）<ul>\n<li>x依次经过init里面的层，最后返回</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>定义网络、优化器、误差MSELoss</li>\n<li>训练（for)<ul>\n<li>x输入网络得到y^</li>\n<li>根据y与y^得到误差</li>\n<li>优化器梯度为0</li>\n<li>backward</li>\n<li>优化</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Classification\"><a href=\"#Classification\" class=\"headerlink\" title=\"Classification\"></a>Classification</h3><ul>\n<li>定义feature,label<ul>\n<li>x0为y0类点，x1为y1类点</li>\n<li>数据形式（[a,b] , 1)</li>\n</ul>\n</li>\n<li>搭建网络<ul>\n<li>init</li>\n<li>forward</li>\n</ul>\n</li>\n<li>定义网络、优化器、误差CrossEntropyLoss</li>\n<li>训练（for)</li>\n</ul>\n<h3 id=\"quick-setup\"><a href=\"#quick-setup\" class=\"headerlink\" title=\"quick_setup\"></a>quick_setup</h3><ul>\n<li>net&#x3D;torch.nn.Sequential()边定义边实例化</li>\n</ul>\n<h3 id=\"save-and-refine\"><a href=\"#save-and-refine\" class=\"headerlink\" title=\"save and refine\"></a>save and refine</h3><ul>\n<li>save <ul>\n<li>训练完(for)后</li>\n<li>torch.save(net1,’net.pkl’)#保留整个图</li>\n<li>torch.save(net1.state_dict(),’net_params.pkl’)#保留结点参数</li>\n</ul>\n</li>\n<li>refine<ul>\n<li>直接提取：<ul>\n<li>net2&#x3D;torch.load(‘net.pkl’)</li>\n</ul>\n</li>\n<li>参数提取：<ul>\n<li>先搭建一模一样的网络n</li>\n<li>再n.load_state_dict(torch.load(‘net_params.pkl’))</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"minibize-train-批数据训练\"><a href=\"#minibize-train-批数据训练\" class=\"headerlink\" title=\"minibize_train(批数据训练)\"></a>minibize_train(批数据训练)</h3><ul>\n<li>定义batch_size，feature,label</li>\n<li>定义数据库：torch_dataset&#x3D;Data.TensorDataset(x,y)</li>\n<li>定义loader：loader&#x3D;Data.DataLoader( dataset,batch,shuffle,num_workers)</li>\n<li>定义网络等</li>\n<li>训练： for epoch in range(3):<br>&emsp;&nbsp;&emsp;&emsp;&emsp;for step,(batch_x,batch_y) in enumerate(loader):</li>\n</ul>\n<h3 id=\"optimizer\"><a href=\"#optimizer\" class=\"headerlink\" title=\"optimizer\"></a>optimizer</h3><ul>\n<li>用Adam</li>\n</ul>\n<h3 id=\"cnn\"><a href=\"#cnn\" class=\"headerlink\" title=\"cnn\"></a>cnn</h3><ul>\n<li>下载数据集train_data,定义loader</li>\n</ul>\n","excerpt":"","more":"<h2 id=\"1-pytorch网课\"><a href=\"#1-pytorch网课\" class=\"headerlink\" title=\"1. pytorch网课\"></a>1. pytorch网课</h2><h3 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><ul>\n<li>format</li>\n<li>calculate</li>\n<li>variable</li>\n<li>activation</li>\n</ul>\n<h3 id=\"Regression\"><a href=\"#Regression\" class=\"headerlink\" title=\"Regression\"></a>Regression</h3><ul>\n<li>给定x , y（变量）</li>\n<li>搭建网络<ul>\n<li>init重写（输入，输出，神经元个数）<ul>\n<li>每一层用Linear</li>\n</ul>\n</li>\n<li>forward前向传递（输入数据）<ul>\n<li>x依次经过init里面的层，最后返回</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>定义网络、优化器、误差MSELoss</li>\n<li>训练（for)<ul>\n<li>x输入网络得到y^</li>\n<li>根据y与y^得到误差</li>\n<li>优化器梯度为0</li>\n<li>backward</li>\n<li>优化</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Classification\"><a href=\"#Classification\" class=\"headerlink\" title=\"Classification\"></a>Classification</h3><ul>\n<li>定义feature,label<ul>\n<li>x0为y0类点，x1为y1类点</li>\n<li>数据形式（[a,b] , 1)</li>\n</ul>\n</li>\n<li>搭建网络<ul>\n<li>init</li>\n<li>forward</li>\n</ul>\n</li>\n<li>定义网络、优化器、误差CrossEntropyLoss</li>\n<li>训练（for)</li>\n</ul>\n<h3 id=\"quick-setup\"><a href=\"#quick-setup\" class=\"headerlink\" title=\"quick_setup\"></a>quick_setup</h3><ul>\n<li>net&#x3D;torch.nn.Sequential()边定义边实例化</li>\n</ul>\n<h3 id=\"save-and-refine\"><a href=\"#save-and-refine\" class=\"headerlink\" title=\"save and refine\"></a>save and refine</h3><ul>\n<li>save <ul>\n<li>训练完(for)后</li>\n<li>torch.save(net1,’net.pkl’)#保留整个图</li>\n<li>torch.save(net1.state_dict(),’net_params.pkl’)#保留结点参数</li>\n</ul>\n</li>\n<li>refine<ul>\n<li>直接提取：<ul>\n<li>net2&#x3D;torch.load(‘net.pkl’)</li>\n</ul>\n</li>\n<li>参数提取：<ul>\n<li>先搭建一模一样的网络n</li>\n<li>再n.load_state_dict(torch.load(‘net_params.pkl’))</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"minibize-train-批数据训练\"><a href=\"#minibize-train-批数据训练\" class=\"headerlink\" title=\"minibize_train(批数据训练)\"></a>minibize_train(批数据训练)</h3><ul>\n<li>定义batch_size，feature,label</li>\n<li>定义数据库：torch_dataset&#x3D;Data.TensorDataset(x,y)</li>\n<li>定义loader：loader&#x3D;Data.DataLoader( dataset,batch,shuffle,num_workers)</li>\n<li>定义网络等</li>\n<li>训练： for epoch in range(3):<br>&emsp;&nbsp;&emsp;&emsp;&emsp;for step,(batch_x,batch_y) in enumerate(loader):</li>\n</ul>\n<h3 id=\"optimizer\"><a href=\"#optimizer\" class=\"headerlink\" title=\"optimizer\"></a>optimizer</h3><ul>\n<li>用Adam</li>\n</ul>\n<h3 id=\"cnn\"><a href=\"#cnn\" class=\"headerlink\" title=\"cnn\"></a>cnn</h3><ul>\n<li>下载数据集train_data,定义loader</li>\n</ul>\n"},{"title":"Hello World","date":"2014-12-24T15:57:18.000Z","toc":true,"_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n<!-- more -->\n\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","source":"_posts/hexo/hello-world.md","raw":"---\ntitle: Hello World\ndate: 2014-12-24 23:57:18\ntoc: true\ncategories: [博客]\ntags: [博客,hexo]\n\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n<!-- more -->\n\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","slug":"hexo/hello-world","published":1,"updated":"2023-10-23T09:55:30.261Z","comments":1,"layout":"post","photos":[],"_id":"clvulgd5f00055ovw7lud76kl","content":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<span id=\"more\"></span>\n\n\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n","excerpt":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>","more":"<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>"},{"_content":"(需要修改的地方)\n显示分类时修改source/category里的index:\n\ntitle: categories\ndate: 2023-07-20 10:18:50\ntype: \"categories\"\nlayout: \"categories\"\ncomments: false\n\n\n- 换头像\ntheme/yilia/config文件\n看到：avatar: img/head.png\n将head.png放到：blog/themes/yilia/source/img\n将blog/config文件中post_asset_folder: true\n\n\n\n\n\n- 代码块自动换行\n```\npre {\n    overflow: auto;\n    white-space: pre;\n    /*white-space: pre-wrap;*/\n    word-wrap: break-word\n}\n```\n- 代码块滚动条颜色：\n\n```\n::-webkit-scrollbar-thumb { /*滚动条的滑块*/\n\n  border-radius: 8px;\n\n  background-color: rgb(189, 189, 189)\n\n}```\n```","source":"_drafts/yilia配置.md","raw":"(需要修改的地方)\n显示分类时修改source/category里的index:\n\ntitle: categories\ndate: 2023-07-20 10:18:50\ntype: \"categories\"\nlayout: \"categories\"\ncomments: false\n\n\n- 换头像\ntheme/yilia/config文件\n看到：avatar: img/head.png\n将head.png放到：blog/themes/yilia/source/img\n将blog/config文件中post_asset_folder: true\n\n\n\n\n\n- 代码块自动换行\n```\npre {\n    overflow: auto;\n    white-space: pre;\n    /*white-space: pre-wrap;*/\n    word-wrap: break-word\n}\n```\n- 代码块滚动条颜色：\n\n```\n::-webkit-scrollbar-thumb { /*滚动条的滑块*/\n\n  border-radius: 8px;\n\n  background-color: rgb(189, 189, 189)\n\n}```\n```","slug":"yilia配置","published":0,"date":"2024-05-06T06:36:52.085Z","updated":"2024-02-22T04:05:09.517Z","_id":"clvulgd5g00065ovwb7h81t4z","title":"","comments":1,"layout":"post","photos":[],"content":"<p>(需要修改的地方)<br>显示分类时修改source&#x2F;category里的index:</p>\n<p>title: categories<br>date: 2023-07-20 10:18:50<br>type: “categories”<br>layout: “categories”<br>comments: false</p>\n<ul>\n<li><p>换头像<br>theme&#x2F;yilia&#x2F;config文件<br>看到：avatar: img&#x2F;head.png<br>将head.png放到：blog&#x2F;themes&#x2F;yilia&#x2F;source&#x2F;img<br>将blog&#x2F;config文件中post_asset_folder: true</p>\n</li>\n<li><p>代码块自动换行</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pre &#123;</span><br><span class=\"line\">    overflow: auto;</span><br><span class=\"line\">    white-space: pre;</span><br><span class=\"line\">    /*white-space: pre-wrap;*/</span><br><span class=\"line\">    word-wrap: break-word</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n<li><p>代码块滚动条颜色：</p>\n</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">::-webkit-scrollbar-thumb &#123; /*滚动条的滑块*/</span><br><span class=\"line\"></span><br><span class=\"line\">  border-radius: 8px;</span><br><span class=\"line\"></span><br><span class=\"line\">  background-color: rgb(189, 189, 189)</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;```</span><br></pre></td></tr></table></figure>","excerpt":"","more":"<p>(需要修改的地方)<br>显示分类时修改source&#x2F;category里的index:</p>\n<p>title: categories<br>date: 2023-07-20 10:18:50<br>type: “categories”<br>layout: “categories”<br>comments: false</p>\n<ul>\n<li><p>换头像<br>theme&#x2F;yilia&#x2F;config文件<br>看到：avatar: img&#x2F;head.png<br>将head.png放到：blog&#x2F;themes&#x2F;yilia&#x2F;source&#x2F;img<br>将blog&#x2F;config文件中post_asset_folder: true</p>\n</li>\n<li><p>代码块自动换行</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pre &#123;</span><br><span class=\"line\">    overflow: auto;</span><br><span class=\"line\">    white-space: pre;</span><br><span class=\"line\">    /*white-space: pre-wrap;*/</span><br><span class=\"line\">    word-wrap: break-word</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n<li><p>代码块滚动条颜色：</p>\n</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">::-webkit-scrollbar-thumb &#123; /*滚动条的滑块*/</span><br><span class=\"line\"></span><br><span class=\"line\">  border-radius: 8px;</span><br><span class=\"line\"></span><br><span class=\"line\">  background-color: rgb(189, 189, 189)</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;```</span><br></pre></td></tr></table></figure>"},{"title":"hexo博客搭建","toc":true,"date":"2022-12-11T16:00:00.000Z","_content":"#  \n\n <!-- more --> \n\n## 1.安装git\n\n- 输入网址https://npm.taobao.org/mirrors/git-for-windows/（你也可以去官网下载）\n- 点击你想下载的版本\n![](../../../themes/yilia/source/img/hexo/6.png)\n![](./img/hexo/6.png)\n- 下载exe文件\n![](../../../themes/yilia/source/img/hexo/7.png)\n![](./img/hexo/7.png)\n- 一路next即可\n![](../../../themes/yilia/source/img/hexo/8.png)\n![](./img/hexo/8.png)\n## 2.安装nodejs\n- 输入网址：nodejs.org\n- 按装左边长期支持版本\n![](../../../themes/yilia/source/img/hexo/1.png)\n![](./img/hexo/1.png)\n- 安装点击next即可\n## 3.通过cnpm安装hexo\n- 进入控制台使用管理员身份\n- 输入：npm install -g cnpm --registry=https://registry.npm.taobao.org<回车>\n![](../../../themes/yilia/source/img/hexo/2.png)\n![](./img/hexo/2.png)\n- 输入：cnpm install -g hexo-cli<回车>\n## 4.搭建博客\n- 创建一个空文件夹(d:/blog/test)\n- 进入该文件夹：cd /d d:/blog/test<回车>\n- 初始化：hexo init<回车>\n- 打开：hexo s<回车>\n- 浏览器访问：http://localhost:4000/\n- 命令行点击ctr+c输入y<回车>即可关闭\n## 5.操作博客\n- 新建博客：hexo n \"test1.md\"(保存在了blog\\test\\source\\_posts\\test1.md)（也可直接在该目录下新建.md文件）\n- 编辑博客：使用vscode/typora等\n## 6.将博客部署到GitHub\n- 登录GitHub\n- 新建仓库\n![](../../../themes/yilia/source/img/hexo/3.png)\n![](./img/hexo/3.png)\n- 注意前面部分要跟自己的名字一样（以后通过\n![](../../../themes/yilia/source/img/hexo/4.png)\n![](./img/hexo/4.png)\n- 安装插件：命令行输入：cnpm install --save hexo-deployer-git\n- 找到_config.yml文件,将最后面加上：\n```\ntype: git\n  repo: git@github.com:lankeren035/lankeren035.github.io.git\n  branch: main\n```\n- 输入hexo d即可部署到远端（在test文件夹下）\n- 浏览器输入仓库名：lankeren035.github.io即可访问\n\n## 7.换主题\n- 找到目标主题：github.com/litten/hexo-theme-yilia\n- 命令行输入：git clone https://github.com/litten/hexo-theme-yilia.git themes/yilia则会在theme下创建yilia文件夹\n- 在_config.yml中将theme后的改为yilia\n![](../../../themes/yilia/source/img/hexo/5.png)\n![](./img/hexo/5.png)\n- hexo g再hexo s通过本地看看\n- 最后hexo d再把远端的也更新了","source":"_posts/hexo/hexo搭建.md","raw":"---\ntitle: hexo博客搭建\ntoc: true\ndate: 2022-12-12\ncategories: [博客]\ntags: [博客,hexo]\n---\n#  \n\n <!-- more --> \n\n## 1.安装git\n\n- 输入网址https://npm.taobao.org/mirrors/git-for-windows/（你也可以去官网下载）\n- 点击你想下载的版本\n![](../../../themes/yilia/source/img/hexo/6.png)\n![](./img/hexo/6.png)\n- 下载exe文件\n![](../../../themes/yilia/source/img/hexo/7.png)\n![](./img/hexo/7.png)\n- 一路next即可\n![](../../../themes/yilia/source/img/hexo/8.png)\n![](./img/hexo/8.png)\n## 2.安装nodejs\n- 输入网址：nodejs.org\n- 按装左边长期支持版本\n![](../../../themes/yilia/source/img/hexo/1.png)\n![](./img/hexo/1.png)\n- 安装点击next即可\n## 3.通过cnpm安装hexo\n- 进入控制台使用管理员身份\n- 输入：npm install -g cnpm --registry=https://registry.npm.taobao.org<回车>\n![](../../../themes/yilia/source/img/hexo/2.png)\n![](./img/hexo/2.png)\n- 输入：cnpm install -g hexo-cli<回车>\n## 4.搭建博客\n- 创建一个空文件夹(d:/blog/test)\n- 进入该文件夹：cd /d d:/blog/test<回车>\n- 初始化：hexo init<回车>\n- 打开：hexo s<回车>\n- 浏览器访问：http://localhost:4000/\n- 命令行点击ctr+c输入y<回车>即可关闭\n## 5.操作博客\n- 新建博客：hexo n \"test1.md\"(保存在了blog\\test\\source\\_posts\\test1.md)（也可直接在该目录下新建.md文件）\n- 编辑博客：使用vscode/typora等\n## 6.将博客部署到GitHub\n- 登录GitHub\n- 新建仓库\n![](../../../themes/yilia/source/img/hexo/3.png)\n![](./img/hexo/3.png)\n- 注意前面部分要跟自己的名字一样（以后通过\n![](../../../themes/yilia/source/img/hexo/4.png)\n![](./img/hexo/4.png)\n- 安装插件：命令行输入：cnpm install --save hexo-deployer-git\n- 找到_config.yml文件,将最后面加上：\n```\ntype: git\n  repo: git@github.com:lankeren035/lankeren035.github.io.git\n  branch: main\n```\n- 输入hexo d即可部署到远端（在test文件夹下）\n- 浏览器输入仓库名：lankeren035.github.io即可访问\n\n## 7.换主题\n- 找到目标主题：github.com/litten/hexo-theme-yilia\n- 命令行输入：git clone https://github.com/litten/hexo-theme-yilia.git themes/yilia则会在theme下创建yilia文件夹\n- 在_config.yml中将theme后的改为yilia\n![](../../../themes/yilia/source/img/hexo/5.png)\n![](./img/hexo/5.png)\n- hexo g再hexo s通过本地看看\n- 最后hexo d再把远端的也更新了","slug":"hexo/hexo搭建","published":1,"updated":"2024-05-06T06:50:06.927Z","_id":"clvulgd5i00095ovw1otn5y7u","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1> <span id=\"more\"></span> \n\n<h2 id=\"1-安装git\"><a href=\"#1-安装git\" class=\"headerlink\" title=\"1.安装git\"></a>1.安装git</h2><ul>\n<li>输入网址<a href=\"https://npm.taobao.org/mirrors/git-for-windows/%EF%BC%88%E4%BD%A0%E4%B9%9F%E5%8F%AF%E4%BB%A5%E5%8E%BB%E5%AE%98%E7%BD%91%E4%B8%8B%E8%BD%BD%EF%BC%89\">https://npm.taobao.org/mirrors/git-for-windows/（你也可以去官网下载）</a></li>\n<li>点击你想下载的版本<br><img src=\"/../../../themes/yilia/source/img/hexo/6.png\"><br><img src=\"/./img/hexo/6.png\"></li>\n<li>下载exe文件<br><img src=\"/../../../themes/yilia/source/img/hexo/7.png\"><br><img src=\"/./img/hexo/7.png\"></li>\n<li>一路next即可<br><img src=\"/../../../themes/yilia/source/img/hexo/8.png\"><br><img src=\"/./img/hexo/8.png\"></li>\n</ul>\n<h2 id=\"2-安装nodejs\"><a href=\"#2-安装nodejs\" class=\"headerlink\" title=\"2.安装nodejs\"></a>2.安装nodejs</h2><ul>\n<li>输入网址：nodejs.org</li>\n<li>按装左边长期支持版本<br><img src=\"/../../../themes/yilia/source/img/hexo/1.png\"><br><img src=\"/./img/hexo/1.png\"></li>\n<li>安装点击next即可</li>\n</ul>\n<h2 id=\"3-通过cnpm安装hexo\"><a href=\"#3-通过cnpm安装hexo\" class=\"headerlink\" title=\"3.通过cnpm安装hexo\"></a>3.通过cnpm安装hexo</h2><ul>\n<li>进入控制台使用管理员身份</li>\n<li>输入：npm install -g cnpm –registry&#x3D;<a href=\"https://registry.npm.taobao.org/\">https://registry.npm.taobao.org</a>&lt;回车&gt;<br><img src=\"/../../../themes/yilia/source/img/hexo/2.png\"><br><img src=\"/./img/hexo/2.png\"></li>\n<li>输入：cnpm install -g hexo-cli&lt;回车&gt;</li>\n</ul>\n<h2 id=\"4-搭建博客\"><a href=\"#4-搭建博客\" class=\"headerlink\" title=\"4.搭建博客\"></a>4.搭建博客</h2><ul>\n<li>创建一个空文件夹(d:&#x2F;blog&#x2F;test)</li>\n<li>进入该文件夹：cd &#x2F;d d:&#x2F;blog&#x2F;test&lt;回车&gt;</li>\n<li>初始化：hexo init&lt;回车&gt;</li>\n<li>打开：hexo s&lt;回车&gt;</li>\n<li>浏览器访问：<a href=\"http://localhost:4000/\">http://localhost:4000/</a></li>\n<li>命令行点击ctr+c输入y&lt;回车&gt;即可关闭</li>\n</ul>\n<h2 id=\"5-操作博客\"><a href=\"#5-操作博客\" class=\"headerlink\" title=\"5.操作博客\"></a>5.操作博客</h2><ul>\n<li>新建博客：hexo n “test1.md”(保存在了blog\\test\\source_posts\\test1.md)（也可直接在该目录下新建.md文件）</li>\n<li>编辑博客：使用vscode&#x2F;typora等</li>\n</ul>\n<h2 id=\"6-将博客部署到GitHub\"><a href=\"#6-将博客部署到GitHub\" class=\"headerlink\" title=\"6.将博客部署到GitHub\"></a>6.将博客部署到GitHub</h2><ul>\n<li>登录GitHub</li>\n<li>新建仓库<br><img src=\"/../../../themes/yilia/source/img/hexo/3.png\"><br><img src=\"/./img/hexo/3.png\"></li>\n<li>注意前面部分要跟自己的名字一样（以后通过<br><img src=\"/../../../themes/yilia/source/img/hexo/4.png\"><br><img src=\"/./img/hexo/4.png\"></li>\n<li>安装插件：命令行输入：cnpm install –save hexo-deployer-git</li>\n<li>找到_config.yml文件,将最后面加上：<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">type: git</span><br><span class=\"line\">  repo: git@github.com:lankeren035/lankeren035.github.io.git</span><br><span class=\"line\">  branch: main</span><br></pre></td></tr></table></figure></li>\n<li>输入hexo d即可部署到远端（在test文件夹下）</li>\n<li>浏览器输入仓库名：lankeren035.github.io即可访问</li>\n</ul>\n<h2 id=\"7-换主题\"><a href=\"#7-换主题\" class=\"headerlink\" title=\"7.换主题\"></a>7.换主题</h2><ul>\n<li>找到目标主题：github.com&#x2F;litten&#x2F;hexo-theme-yilia</li>\n<li>命令行输入：git clone <a href=\"https://github.com/litten/hexo-theme-yilia.git\">https://github.com/litten/hexo-theme-yilia.git</a> themes&#x2F;yilia则会在theme下创建yilia文件夹</li>\n<li>在_config.yml中将theme后的改为yilia<br><img src=\"/../../../themes/yilia/source/img/hexo/5.png\"><br><img src=\"/./img/hexo/5.png\"></li>\n<li>hexo g再hexo s通过本地看看</li>\n<li>最后hexo d再把远端的也更新了</li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h2 id=\"1-安装git\"><a href=\"#1-安装git\" class=\"headerlink\" title=\"1.安装git\"></a>1.安装git</h2><ul>\n<li>输入网址<a href=\"https://npm.taobao.org/mirrors/git-for-windows/%EF%BC%88%E4%BD%A0%E4%B9%9F%E5%8F%AF%E4%BB%A5%E5%8E%BB%E5%AE%98%E7%BD%91%E4%B8%8B%E8%BD%BD%EF%BC%89\">https://npm.taobao.org/mirrors/git-for-windows/（你也可以去官网下载）</a></li>\n<li>点击你想下载的版本<br><img src=\"/../../../themes/yilia/source/img/hexo/6.png\"><br><img src=\"/./img/hexo/6.png\"></li>\n<li>下载exe文件<br><img src=\"/../../../themes/yilia/source/img/hexo/7.png\"><br><img src=\"/./img/hexo/7.png\"></li>\n<li>一路next即可<br><img src=\"/../../../themes/yilia/source/img/hexo/8.png\"><br><img src=\"/./img/hexo/8.png\"></li>\n</ul>\n<h2 id=\"2-安装nodejs\"><a href=\"#2-安装nodejs\" class=\"headerlink\" title=\"2.安装nodejs\"></a>2.安装nodejs</h2><ul>\n<li>输入网址：nodejs.org</li>\n<li>按装左边长期支持版本<br><img src=\"/../../../themes/yilia/source/img/hexo/1.png\"><br><img src=\"/./img/hexo/1.png\"></li>\n<li>安装点击next即可</li>\n</ul>\n<h2 id=\"3-通过cnpm安装hexo\"><a href=\"#3-通过cnpm安装hexo\" class=\"headerlink\" title=\"3.通过cnpm安装hexo\"></a>3.通过cnpm安装hexo</h2><ul>\n<li>进入控制台使用管理员身份</li>\n<li>输入：npm install -g cnpm –registry&#x3D;<a href=\"https://registry.npm.taobao.org/\">https://registry.npm.taobao.org</a>&lt;回车&gt;<br><img src=\"/../../../themes/yilia/source/img/hexo/2.png\"><br><img src=\"/./img/hexo/2.png\"></li>\n<li>输入：cnpm install -g hexo-cli&lt;回车&gt;</li>\n</ul>\n<h2 id=\"4-搭建博客\"><a href=\"#4-搭建博客\" class=\"headerlink\" title=\"4.搭建博客\"></a>4.搭建博客</h2><ul>\n<li>创建一个空文件夹(d:&#x2F;blog&#x2F;test)</li>\n<li>进入该文件夹：cd &#x2F;d d:&#x2F;blog&#x2F;test&lt;回车&gt;</li>\n<li>初始化：hexo init&lt;回车&gt;</li>\n<li>打开：hexo s&lt;回车&gt;</li>\n<li>浏览器访问：<a href=\"http://localhost:4000/\">http://localhost:4000/</a></li>\n<li>命令行点击ctr+c输入y&lt;回车&gt;即可关闭</li>\n</ul>\n<h2 id=\"5-操作博客\"><a href=\"#5-操作博客\" class=\"headerlink\" title=\"5.操作博客\"></a>5.操作博客</h2><ul>\n<li>新建博客：hexo n “test1.md”(保存在了blog\\test\\source_posts\\test1.md)（也可直接在该目录下新建.md文件）</li>\n<li>编辑博客：使用vscode&#x2F;typora等</li>\n</ul>\n<h2 id=\"6-将博客部署到GitHub\"><a href=\"#6-将博客部署到GitHub\" class=\"headerlink\" title=\"6.将博客部署到GitHub\"></a>6.将博客部署到GitHub</h2><ul>\n<li>登录GitHub</li>\n<li>新建仓库<br><img src=\"/../../../themes/yilia/source/img/hexo/3.png\"><br><img src=\"/./img/hexo/3.png\"></li>\n<li>注意前面部分要跟自己的名字一样（以后通过<br><img src=\"/../../../themes/yilia/source/img/hexo/4.png\"><br><img src=\"/./img/hexo/4.png\"></li>\n<li>安装插件：命令行输入：cnpm install –save hexo-deployer-git</li>\n<li>找到_config.yml文件,将最后面加上：<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">type: git</span><br><span class=\"line\">  repo: git@github.com:lankeren035/lankeren035.github.io.git</span><br><span class=\"line\">  branch: main</span><br></pre></td></tr></table></figure></li>\n<li>输入hexo d即可部署到远端（在test文件夹下）</li>\n<li>浏览器输入仓库名：lankeren035.github.io即可访问</li>\n</ul>\n<h2 id=\"7-换主题\"><a href=\"#7-换主题\" class=\"headerlink\" title=\"7.换主题\"></a>7.换主题</h2><ul>\n<li>找到目标主题：github.com&#x2F;litten&#x2F;hexo-theme-yilia</li>\n<li>命令行输入：git clone <a href=\"https://github.com/litten/hexo-theme-yilia.git\">https://github.com/litten/hexo-theme-yilia.git</a> themes&#x2F;yilia则会在theme下创建yilia文件夹</li>\n<li>在_config.yml中将theme后的改为yilia<br><img src=\"/../../../themes/yilia/source/img/hexo/5.png\"><br><img src=\"/./img/hexo/5.png\"></li>\n<li>hexo g再hexo s通过本地看看</li>\n<li>最后hexo d再把远端的也更新了</li>\n</ul>"},{"title":"Linux入门-0配置","date":"2021-03-01T12:00:00.000Z","toc":true,"_content":"#\n\n<!-- more -->\n\n## 安装\n\n\n\n### 1 虚拟机\n### 2 [下载finalshell](https://www.hostbuf.com/downloads/finalshell_install.exe)\n### 3 连接finalshell\n1. 在linux中右键，打开终端\n\n2. 输入：ifconfig ,找到ens下的inet后面的数字，复制![](./img/linux/install/1.png)\n\n   ![](D:\\blog\\themes\\yilia\\source\\img\\linux\\install\\1.png)\n1. 打开finalshell点击左上文件图标，点击新串口左上带加号的文件图标，点击ssh连接，输入自定义的名称，主机输入刚刚的复制的一串数字，用户名、密码为linux用户的，点击确定\n  ![](./img/linux/install/2.png)\n\n  ![](D:\\blog\\themes\\yilia\\source\\img\\linux\\install\\2.png)\n### 4（windows中使用ubuntu）\n1. windows中搜索：windows功能，点击：启动或关闭windows功能，将：适用于Linux的子系统勾选并重启\n\n2. 微软应用商店搜索ubuntu并下载，win10还需下载terminal\n\n3. 在terminal中点击向下箭头，选择Ubuntu\n  ![](./img/linux/install/3.png)\n\n  ![](D:\\blog\\themes\\yilia\\source\\img\\linux\\install\\3.png)\n\n4. 如果显示Error: 0x800701bc WSL 2 ?????????????????? 则返回windows命令行输入wsl --update\n\n5. 完成后打开Ubuntu窗口，输入用户名，密码，密码\n\n","source":"_posts/linux/Linux_0安装.md","raw":"---\ntitle: Linux入门-0配置\ndate: 2021-03-01 20:00:00\ntoc: true\ntags: [Linux]\ncategories: [Linux]\n\n---\n#\n\n<!-- more -->\n\n## 安装\n\n\n\n### 1 虚拟机\n### 2 [下载finalshell](https://www.hostbuf.com/downloads/finalshell_install.exe)\n### 3 连接finalshell\n1. 在linux中右键，打开终端\n\n2. 输入：ifconfig ,找到ens下的inet后面的数字，复制![](./img/linux/install/1.png)\n\n   ![](D:\\blog\\themes\\yilia\\source\\img\\linux\\install\\1.png)\n1. 打开finalshell点击左上文件图标，点击新串口左上带加号的文件图标，点击ssh连接，输入自定义的名称，主机输入刚刚的复制的一串数字，用户名、密码为linux用户的，点击确定\n  ![](./img/linux/install/2.png)\n\n  ![](D:\\blog\\themes\\yilia\\source\\img\\linux\\install\\2.png)\n### 4（windows中使用ubuntu）\n1. windows中搜索：windows功能，点击：启动或关闭windows功能，将：适用于Linux的子系统勾选并重启\n\n2. 微软应用商店搜索ubuntu并下载，win10还需下载terminal\n\n3. 在terminal中点击向下箭头，选择Ubuntu\n  ![](./img/linux/install/3.png)\n\n  ![](D:\\blog\\themes\\yilia\\source\\img\\linux\\install\\3.png)\n\n4. 如果显示Error: 0x800701bc WSL 2 ?????????????????? 则返回windows命令行输入wsl --update\n\n5. 完成后打开Ubuntu窗口，输入用户名，密码，密码\n\n","slug":"linux/Linux_0安装","published":1,"updated":"2024-05-06T05:52:09.458Z","comments":1,"layout":"post","photos":[],"_id":"clvulgd5j000a5ovw5s6w9cbx","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h2 id=\"安装\"><a href=\"#安装\" class=\"headerlink\" title=\"安装\"></a>安装</h2><h3 id=\"1-虚拟机\"><a href=\"#1-虚拟机\" class=\"headerlink\" title=\"1 虚拟机\"></a>1 虚拟机</h3><h3 id=\"2-下载finalshell\"><a href=\"#2-下载finalshell\" class=\"headerlink\" title=\"2 下载finalshell\"></a>2 <a href=\"https://www.hostbuf.com/downloads/finalshell_install.exe\">下载finalshell</a></h3><h3 id=\"3-连接finalshell\"><a href=\"#3-连接finalshell\" class=\"headerlink\" title=\"3 连接finalshell\"></a>3 连接finalshell</h3><ol>\n<li><p>在linux中右键，打开终端</p>\n</li>\n<li><p>输入：ifconfig ,找到ens下的inet后面的数字，复制<img src=\"/./img/linux/install/1.png\"></p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\linux\\install\\1.png\"></p>\n</li>\n<li><p>打开finalshell点击左上文件图标，点击新串口左上带加号的文件图标，点击ssh连接，输入自定义的名称，主机输入刚刚的复制的一串数字，用户名、密码为linux用户的，点击确定<br>  <img src=\"/./img/linux/install/2.png\"></p>\n</li>\n</ol>\n<p>  <img src=\"D:\\blog\\themes\\yilia\\source\\img\\linux\\install\\2.png\"></p>\n<h3 id=\"4（windows中使用ubuntu）\"><a href=\"#4（windows中使用ubuntu）\" class=\"headerlink\" title=\"4（windows中使用ubuntu）\"></a>4（windows中使用ubuntu）</h3><ol>\n<li><p>windows中搜索：windows功能，点击：启动或关闭windows功能，将：适用于Linux的子系统勾选并重启</p>\n</li>\n<li><p>微软应用商店搜索ubuntu并下载，win10还需下载terminal</p>\n</li>\n<li><p>在terminal中点击向下箭头，选择Ubuntu<br>  <img src=\"/./img/linux/install/3.png\"></p>\n</li>\n</ol>\n<p>  <img src=\"D:\\blog\\themes\\yilia\\source\\img\\linux\\install\\3.png\"></p>\n<ol start=\"4\">\n<li><p>如果显示Error: 0x800701bc WSL 2 ?????????????????? 则返回windows命令行输入wsl –update</p>\n</li>\n<li><p>完成后打开Ubuntu窗口，输入用户名，密码，密码</p>\n</li>\n</ol>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h2 id=\"安装\"><a href=\"#安装\" class=\"headerlink\" title=\"安装\"></a>安装</h2><h3 id=\"1-虚拟机\"><a href=\"#1-虚拟机\" class=\"headerlink\" title=\"1 虚拟机\"></a>1 虚拟机</h3><h3 id=\"2-下载finalshell\"><a href=\"#2-下载finalshell\" class=\"headerlink\" title=\"2 下载finalshell\"></a>2 <a href=\"https://www.hostbuf.com/downloads/finalshell_install.exe\">下载finalshell</a></h3><h3 id=\"3-连接finalshell\"><a href=\"#3-连接finalshell\" class=\"headerlink\" title=\"3 连接finalshell\"></a>3 连接finalshell</h3><ol>\n<li><p>在linux中右键，打开终端</p>\n</li>\n<li><p>输入：ifconfig ,找到ens下的inet后面的数字，复制<img src=\"/./img/linux/install/1.png\"></p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\linux\\install\\1.png\"></p>\n</li>\n<li><p>打开finalshell点击左上文件图标，点击新串口左上带加号的文件图标，点击ssh连接，输入自定义的名称，主机输入刚刚的复制的一串数字，用户名、密码为linux用户的，点击确定<br>  <img src=\"/./img/linux/install/2.png\"></p>\n</li>\n</ol>\n<p>  <img src=\"D:\\blog\\themes\\yilia\\source\\img\\linux\\install\\2.png\"></p>\n<h3 id=\"4（windows中使用ubuntu）\"><a href=\"#4（windows中使用ubuntu）\" class=\"headerlink\" title=\"4（windows中使用ubuntu）\"></a>4（windows中使用ubuntu）</h3><ol>\n<li><p>windows中搜索：windows功能，点击：启动或关闭windows功能，将：适用于Linux的子系统勾选并重启</p>\n</li>\n<li><p>微软应用商店搜索ubuntu并下载，win10还需下载terminal</p>\n</li>\n<li><p>在terminal中点击向下箭头，选择Ubuntu<br>  <img src=\"/./img/linux/install/3.png\"></p>\n</li>\n</ol>\n<p>  <img src=\"D:\\blog\\themes\\yilia\\source\\img\\linux\\install\\3.png\"></p>\n<ol start=\"4\">\n<li><p>如果显示Error: 0x800701bc WSL 2 ?????????????????? 则返回windows命令行输入wsl –update</p>\n</li>\n<li><p>完成后打开Ubuntu窗口，输入用户名，密码，密码</p>\n</li>\n</ol>"},{"title":"微信小程序基础","date":"2021-03-01T12:00:00.000Z","toc":true,"_content":"#\n\n<!--more-->\n\n## 1.总体结构\n\n### （1）概述\n#### 1)项目结构\n- page：存放所有页面\n- utils: 存放工具模块（如格式化时间的自定义模块）\n- app.js: 项目入口\n- app.json: 项目全局配置\n- app.wxss: 项目全局样式\n- project.config.json: 项目配置文件\n- sitemap.json: 配置小程序及其页面是否允许被微信索引\n\n#### 2）页面组成\n- .js文件：页面脚本，存放页面数据、事件处理函数等\n- .json文件：配置文件，配置窗口外观、表现等\n- .wxml文件：模板结构文件\n- .wxss文件: 样式表文件\n\n### （2）详述\n#### 1）json文件\n json是一种数据格式，在开发中json总是以配置文件的形式出现。\n|app.json|afadsf|\n|-------------------|------|\n|project.config.json|\n|sitemap.json|\n|每个页面的json|","source":"_posts/wechartapp/weixin.md","raw":"---\ntitle: 微信小程序基础\ndate: 2021-03-01 20:00:00\ntoc: true\ntags: [微信小程序]\ncategories: [微信小程序]\n\n---\n#\n\n<!--more-->\n\n## 1.总体结构\n\n### （1）概述\n#### 1)项目结构\n- page：存放所有页面\n- utils: 存放工具模块（如格式化时间的自定义模块）\n- app.js: 项目入口\n- app.json: 项目全局配置\n- app.wxss: 项目全局样式\n- project.config.json: 项目配置文件\n- sitemap.json: 配置小程序及其页面是否允许被微信索引\n\n#### 2）页面组成\n- .js文件：页面脚本，存放页面数据、事件处理函数等\n- .json文件：配置文件，配置窗口外观、表现等\n- .wxml文件：模板结构文件\n- .wxss文件: 样式表文件\n\n### （2）详述\n#### 1）json文件\n json是一种数据格式，在开发中json总是以配置文件的形式出现。\n|app.json|afadsf|\n|-------------------|------|\n|project.config.json|\n|sitemap.json|\n|每个页面的json|","slug":"wechartapp/weixin","published":1,"updated":"2024-01-18T04:36:36.694Z","comments":1,"layout":"post","photos":[],"_id":"clvulgd5k000b5ovw5zpo42gm","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h2 id=\"1-总体结构\"><a href=\"#1-总体结构\" class=\"headerlink\" title=\"1.总体结构\"></a>1.总体结构</h2><h3 id=\"（1）概述\"><a href=\"#（1）概述\" class=\"headerlink\" title=\"（1）概述\"></a>（1）概述</h3><h4 id=\"1-项目结构\"><a href=\"#1-项目结构\" class=\"headerlink\" title=\"1)项目结构\"></a>1)项目结构</h4><ul>\n<li>page：存放所有页面</li>\n<li>utils: 存放工具模块（如格式化时间的自定义模块）</li>\n<li>app.js: 项目入口</li>\n<li>app.json: 项目全局配置</li>\n<li>app.wxss: 项目全局样式</li>\n<li>project.config.json: 项目配置文件</li>\n<li>sitemap.json: 配置小程序及其页面是否允许被微信索引</li>\n</ul>\n<h4 id=\"2）页面组成\"><a href=\"#2）页面组成\" class=\"headerlink\" title=\"2）页面组成\"></a>2）页面组成</h4><ul>\n<li>.js文件：页面脚本，存放页面数据、事件处理函数等</li>\n<li>.json文件：配置文件，配置窗口外观、表现等</li>\n<li>.wxml文件：模板结构文件</li>\n<li>.wxss文件: 样式表文件</li>\n</ul>\n<h3 id=\"（2）详述\"><a href=\"#（2）详述\" class=\"headerlink\" title=\"（2）详述\"></a>（2）详述</h3><h4 id=\"1）json文件\"><a href=\"#1）json文件\" class=\"headerlink\" title=\"1）json文件\"></a>1）json文件</h4><p> json是一种数据格式，在开发中json总是以配置文件的形式出现。</p>\n<table>\n<thead>\n<tr>\n<th>app.json</th>\n<th>afadsf</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>project.config.json</td>\n<td></td>\n</tr>\n<tr>\n<td>sitemap.json</td>\n<td></td>\n</tr>\n<tr>\n<td>每个页面的json</td>\n<td></td>\n</tr>\n</tbody></table>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h2 id=\"1-总体结构\"><a href=\"#1-总体结构\" class=\"headerlink\" title=\"1.总体结构\"></a>1.总体结构</h2><h3 id=\"（1）概述\"><a href=\"#（1）概述\" class=\"headerlink\" title=\"（1）概述\"></a>（1）概述</h3><h4 id=\"1-项目结构\"><a href=\"#1-项目结构\" class=\"headerlink\" title=\"1)项目结构\"></a>1)项目结构</h4><ul>\n<li>page：存放所有页面</li>\n<li>utils: 存放工具模块（如格式化时间的自定义模块）</li>\n<li>app.js: 项目入口</li>\n<li>app.json: 项目全局配置</li>\n<li>app.wxss: 项目全局样式</li>\n<li>project.config.json: 项目配置文件</li>\n<li>sitemap.json: 配置小程序及其页面是否允许被微信索引</li>\n</ul>\n<h4 id=\"2）页面组成\"><a href=\"#2）页面组成\" class=\"headerlink\" title=\"2）页面组成\"></a>2）页面组成</h4><ul>\n<li>.js文件：页面脚本，存放页面数据、事件处理函数等</li>\n<li>.json文件：配置文件，配置窗口外观、表现等</li>\n<li>.wxml文件：模板结构文件</li>\n<li>.wxss文件: 样式表文件</li>\n</ul>\n<h3 id=\"（2）详述\"><a href=\"#（2）详述\" class=\"headerlink\" title=\"（2）详述\"></a>（2）详述</h3><h4 id=\"1）json文件\"><a href=\"#1）json文件\" class=\"headerlink\" title=\"1）json文件\"></a>1）json文件</h4><p> json是一种数据格式，在开发中json总是以配置文件的形式出现。</p>\n<table>\n<thead>\n<tr>\n<th>app.json</th>\n<th>afadsf</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>project.config.json</td>\n<td></td>\n</tr>\n<tr>\n<td>sitemap.json</td>\n<td></td>\n</tr>\n<tr>\n<td>每个页面的json</td>\n<td></td>\n</tr>\n</tbody></table>"},{"title":"Linux入门-1基础命令","date":"2021-03-01T12:00:00.000Z","toc":true,"_content":"#\n\n<!--more-->\n\n## 1. 目录结构\n根目录：/\n/home/hellow/test.txt\n\n## 2. 小tip\n### 2-1 特殊符号\n- . 当前目录\n\n- .. 上级目录\n\n- ~ home\n- `作为命令执行\n### 2-2 进入root用户\n- 进入\n\nsu - root\n\n123456\n\n- 退出\nexit\n### 2-3管道符\n|\ncat 1.txt | grep \"hello\"\n\n### 2-4重定向符\n\\>：右边文件内容清空，左边内容写入文件\n\n\\>>：追加\n## 3. 命令\n命令 [选项] [参数]\n### 1. ls命令（显示内容）\nls [-a -l -h] [路径]\n- a: all，列出全部文件（包括隐藏文件）\n- l: list，以列表展现（权限，用户和用户组，大小，创建日期）\n- h: 以易于阅读的形式，列出文件大小（搭配-l)\n- 组合使用：如-la\n### 2. cd命令（change directory)\ncd [路径]\n\n- cd 回到用户HOME目录\n- cd ../.. 返回上两级\n- cd 路径\n### 3. pwd命令（print work directory)\npwd\n### 4. mkdir命令（make directory)\nmkdir [-p] 路径\n\n- 多层创建\n### 5. touch命令（创建文件）\ntouch 路径\n### 6. cat命令（查看文件）\ncat 路径\n### 7. more命令（查看文件）\nmore 路径（分页看，空格下一页，q退出）\n### 8. cp命令（copy)\ncp [-r] 源路径 目的地\n- r 递归，用于文件夹的复制\n### 9. mv命令（move)\nmv 源路径 目的地/（可重命名）\n### 10. rm命令（remove)\nrm [-r -f] 参数1 参数2...\n\n- r 递归，用于删除文件夹\n- f (force)强制删除\n- 支持通配符：test* 表示以test开头\n### 11. grep命令（文件中通过关键字搜索行）\ngrep [-n] \"关键字\" 文件路径\n\n- n：结果中显示行号\n- 关键字：用于查找\n- 路径：可做输入端口\n### 12. wc命令（文件内容统计）\nwc [-cmlw] 文件路径\n- c：统计bytes数量\n- m：统计字符数量\n- l：统计行数\n- w：统计单词数\n- 路径：可做输入端口\n### 13. which命令（查看命令的源文件）\nwhich Linux命令\n### 14. find命令（文件搜索）\nfind 起始路径 -name \"test*\"\nfind 起始路径 -size +100k(大于100kb)\n### 15. echo命令（打印）\necho 输出内容\n- echo `pwd`：将pwd作为命令执行\n### 16. tail命令（从尾部查看）\ntail [-f -数字] 路径\n- f 持续跟踪\n- 数字 查看多少行（默认10）\n\n## 4. vim编辑器\nvi 文件路径\n\nvim 文件路径\n\n![](./img/linux/commend/4.png)\n![](./img/linux/commend/5.png)\n![](./img/linux/commend/6.png)\n![](./img/linux/commend/7.png)\n![](../../../themes/yilia/source/img/linux/commend/4.png)\n![](../../../themes/yilia/source/img/linux/commend/5.png)\n![](../../../themes/yilia/source/img/linux/commend/6.png)\n![](../../../themes/yilia/source/img/linux/commend/7.png)","source":"_posts/linux/Linux_1.基础命令.md","raw":"---\ntitle: Linux入门-1基础命令\ndate: 2021-03-01 20:00:00\ntoc: true\ntags: [Linux]\ncategories: [Linux]\n---\n#\n\n<!--more-->\n\n## 1. 目录结构\n根目录：/\n/home/hellow/test.txt\n\n## 2. 小tip\n### 2-1 特殊符号\n- . 当前目录\n\n- .. 上级目录\n\n- ~ home\n- `作为命令执行\n### 2-2 进入root用户\n- 进入\n\nsu - root\n\n123456\n\n- 退出\nexit\n### 2-3管道符\n|\ncat 1.txt | grep \"hello\"\n\n### 2-4重定向符\n\\>：右边文件内容清空，左边内容写入文件\n\n\\>>：追加\n## 3. 命令\n命令 [选项] [参数]\n### 1. ls命令（显示内容）\nls [-a -l -h] [路径]\n- a: all，列出全部文件（包括隐藏文件）\n- l: list，以列表展现（权限，用户和用户组，大小，创建日期）\n- h: 以易于阅读的形式，列出文件大小（搭配-l)\n- 组合使用：如-la\n### 2. cd命令（change directory)\ncd [路径]\n\n- cd 回到用户HOME目录\n- cd ../.. 返回上两级\n- cd 路径\n### 3. pwd命令（print work directory)\npwd\n### 4. mkdir命令（make directory)\nmkdir [-p] 路径\n\n- 多层创建\n### 5. touch命令（创建文件）\ntouch 路径\n### 6. cat命令（查看文件）\ncat 路径\n### 7. more命令（查看文件）\nmore 路径（分页看，空格下一页，q退出）\n### 8. cp命令（copy)\ncp [-r] 源路径 目的地\n- r 递归，用于文件夹的复制\n### 9. mv命令（move)\nmv 源路径 目的地/（可重命名）\n### 10. rm命令（remove)\nrm [-r -f] 参数1 参数2...\n\n- r 递归，用于删除文件夹\n- f (force)强制删除\n- 支持通配符：test* 表示以test开头\n### 11. grep命令（文件中通过关键字搜索行）\ngrep [-n] \"关键字\" 文件路径\n\n- n：结果中显示行号\n- 关键字：用于查找\n- 路径：可做输入端口\n### 12. wc命令（文件内容统计）\nwc [-cmlw] 文件路径\n- c：统计bytes数量\n- m：统计字符数量\n- l：统计行数\n- w：统计单词数\n- 路径：可做输入端口\n### 13. which命令（查看命令的源文件）\nwhich Linux命令\n### 14. find命令（文件搜索）\nfind 起始路径 -name \"test*\"\nfind 起始路径 -size +100k(大于100kb)\n### 15. echo命令（打印）\necho 输出内容\n- echo `pwd`：将pwd作为命令执行\n### 16. tail命令（从尾部查看）\ntail [-f -数字] 路径\n- f 持续跟踪\n- 数字 查看多少行（默认10）\n\n## 4. vim编辑器\nvi 文件路径\n\nvim 文件路径\n\n![](./img/linux/commend/4.png)\n![](./img/linux/commend/5.png)\n![](./img/linux/commend/6.png)\n![](./img/linux/commend/7.png)\n![](../../../themes/yilia/source/img/linux/commend/4.png)\n![](../../../themes/yilia/source/img/linux/commend/5.png)\n![](../../../themes/yilia/source/img/linux/commend/6.png)\n![](../../../themes/yilia/source/img/linux/commend/7.png)","slug":"linux/Linux_1.基础命令","published":1,"updated":"2024-05-06T05:52:09.458Z","comments":1,"layout":"post","photos":[],"_id":"clvulgd5m000e5ovwh89e8uhh","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h2 id=\"1-目录结构\"><a href=\"#1-目录结构\" class=\"headerlink\" title=\"1. 目录结构\"></a>1. 目录结构</h2><p>根目录：&#x2F;<br>&#x2F;home&#x2F;hellow&#x2F;test.txt</p>\n<h2 id=\"2-小tip\"><a href=\"#2-小tip\" class=\"headerlink\" title=\"2. 小tip\"></a>2. 小tip</h2><h3 id=\"2-1-特殊符号\"><a href=\"#2-1-特殊符号\" class=\"headerlink\" title=\"2-1 特殊符号\"></a>2-1 特殊符号</h3><ul>\n<li><p>. 当前目录</p>\n</li>\n<li><p>.. 上级目录</p>\n</li>\n<li><p>~ home</p>\n</li>\n<li><p>&#96;作为命令执行</p>\n</li>\n</ul>\n<h3 id=\"2-2-进入root用户\"><a href=\"#2-2-进入root用户\" class=\"headerlink\" title=\"2-2 进入root用户\"></a>2-2 进入root用户</h3><ul>\n<li>进入</li>\n</ul>\n<p>su - root</p>\n<p>123456</p>\n<ul>\n<li>退出<br>exit</li>\n</ul>\n<h3 id=\"2-3管道符\"><a href=\"#2-3管道符\" class=\"headerlink\" title=\"2-3管道符\"></a>2-3管道符</h3><p>|<br>cat 1.txt | grep “hello”</p>\n<h3 id=\"2-4重定向符\"><a href=\"#2-4重定向符\" class=\"headerlink\" title=\"2-4重定向符\"></a>2-4重定向符</h3><p>&gt;：右边文件内容清空，左边内容写入文件</p>\n<p>&gt;&gt;：追加</p>\n<h2 id=\"3-命令\"><a href=\"#3-命令\" class=\"headerlink\" title=\"3. 命令\"></a>3. 命令</h2><p>命令 [选项] [参数]</p>\n<h3 id=\"1-ls命令（显示内容）\"><a href=\"#1-ls命令（显示内容）\" class=\"headerlink\" title=\"1. ls命令（显示内容）\"></a>1. ls命令（显示内容）</h3><p>ls [-a -l -h] [路径]</p>\n<ul>\n<li>a: all，列出全部文件（包括隐藏文件）</li>\n<li>l: list，以列表展现（权限，用户和用户组，大小，创建日期）</li>\n<li>h: 以易于阅读的形式，列出文件大小（搭配-l)</li>\n<li>组合使用：如-la</li>\n</ul>\n<h3 id=\"2-cd命令（change-directory\"><a href=\"#2-cd命令（change-directory\" class=\"headerlink\" title=\"2. cd命令（change directory)\"></a>2. cd命令（change directory)</h3><p>cd [路径]</p>\n<ul>\n<li>cd 回到用户HOME目录</li>\n<li>cd ..&#x2F;.. 返回上两级</li>\n<li>cd 路径</li>\n</ul>\n<h3 id=\"3-pwd命令（print-work-directory\"><a href=\"#3-pwd命令（print-work-directory\" class=\"headerlink\" title=\"3. pwd命令（print work directory)\"></a>3. pwd命令（print work directory)</h3><p>pwd</p>\n<h3 id=\"4-mkdir命令（make-directory\"><a href=\"#4-mkdir命令（make-directory\" class=\"headerlink\" title=\"4. mkdir命令（make directory)\"></a>4. mkdir命令（make directory)</h3><p>mkdir [-p] 路径</p>\n<ul>\n<li>多层创建</li>\n</ul>\n<h3 id=\"5-touch命令（创建文件）\"><a href=\"#5-touch命令（创建文件）\" class=\"headerlink\" title=\"5. touch命令（创建文件）\"></a>5. touch命令（创建文件）</h3><p>touch 路径</p>\n<h3 id=\"6-cat命令（查看文件）\"><a href=\"#6-cat命令（查看文件）\" class=\"headerlink\" title=\"6. cat命令（查看文件）\"></a>6. cat命令（查看文件）</h3><p>cat 路径</p>\n<h3 id=\"7-more命令（查看文件）\"><a href=\"#7-more命令（查看文件）\" class=\"headerlink\" title=\"7. more命令（查看文件）\"></a>7. more命令（查看文件）</h3><p>more 路径（分页看，空格下一页，q退出）</p>\n<h3 id=\"8-cp命令（copy\"><a href=\"#8-cp命令（copy\" class=\"headerlink\" title=\"8. cp命令（copy)\"></a>8. cp命令（copy)</h3><p>cp [-r] 源路径 目的地</p>\n<ul>\n<li>r 递归，用于文件夹的复制</li>\n</ul>\n<h3 id=\"9-mv命令（move\"><a href=\"#9-mv命令（move\" class=\"headerlink\" title=\"9. mv命令（move)\"></a>9. mv命令（move)</h3><p>mv 源路径 目的地&#x2F;（可重命名）</p>\n<h3 id=\"10-rm命令（remove\"><a href=\"#10-rm命令（remove\" class=\"headerlink\" title=\"10. rm命令（remove)\"></a>10. rm命令（remove)</h3><p>rm [-r -f] 参数1 参数2…</p>\n<ul>\n<li>r 递归，用于删除文件夹</li>\n<li>f (force)强制删除</li>\n<li>支持通配符：test* 表示以test开头</li>\n</ul>\n<h3 id=\"11-grep命令（文件中通过关键字搜索行）\"><a href=\"#11-grep命令（文件中通过关键字搜索行）\" class=\"headerlink\" title=\"11. grep命令（文件中通过关键字搜索行）\"></a>11. grep命令（文件中通过关键字搜索行）</h3><p>grep [-n] “关键字” 文件路径</p>\n<ul>\n<li>n：结果中显示行号</li>\n<li>关键字：用于查找</li>\n<li>路径：可做输入端口</li>\n</ul>\n<h3 id=\"12-wc命令（文件内容统计）\"><a href=\"#12-wc命令（文件内容统计）\" class=\"headerlink\" title=\"12. wc命令（文件内容统计）\"></a>12. wc命令（文件内容统计）</h3><p>wc [-cmlw] 文件路径</p>\n<ul>\n<li>c：统计bytes数量</li>\n<li>m：统计字符数量</li>\n<li>l：统计行数</li>\n<li>w：统计单词数</li>\n<li>路径：可做输入端口</li>\n</ul>\n<h3 id=\"13-which命令（查看命令的源文件）\"><a href=\"#13-which命令（查看命令的源文件）\" class=\"headerlink\" title=\"13. which命令（查看命令的源文件）\"></a>13. which命令（查看命令的源文件）</h3><p>which Linux命令</p>\n<h3 id=\"14-find命令（文件搜索）\"><a href=\"#14-find命令（文件搜索）\" class=\"headerlink\" title=\"14. find命令（文件搜索）\"></a>14. find命令（文件搜索）</h3><p>find 起始路径 -name “test*”<br>find 起始路径 -size +100k(大于100kb)</p>\n<h3 id=\"15-echo命令（打印）\"><a href=\"#15-echo命令（打印）\" class=\"headerlink\" title=\"15. echo命令（打印）\"></a>15. echo命令（打印）</h3><p>echo 输出内容</p>\n<ul>\n<li>echo <code>pwd</code>：将pwd作为命令执行</li>\n</ul>\n<h3 id=\"16-tail命令（从尾部查看）\"><a href=\"#16-tail命令（从尾部查看）\" class=\"headerlink\" title=\"16. tail命令（从尾部查看）\"></a>16. tail命令（从尾部查看）</h3><p>tail [-f -数字] 路径</p>\n<ul>\n<li>f 持续跟踪</li>\n<li>数字 查看多少行（默认10）</li>\n</ul>\n<h2 id=\"4-vim编辑器\"><a href=\"#4-vim编辑器\" class=\"headerlink\" title=\"4. vim编辑器\"></a>4. vim编辑器</h2><p>vi 文件路径</p>\n<p>vim 文件路径</p>\n<p><img src=\"/./img/linux/commend/4.png\"><br><img src=\"/./img/linux/commend/5.png\"><br><img src=\"/./img/linux/commend/6.png\"><br><img src=\"/./img/linux/commend/7.png\"><br><img src=\"/../../../themes/yilia/source/img/linux/commend/4.png\"><br><img src=\"/../../../themes/yilia/source/img/linux/commend/5.png\"><br><img src=\"/../../../themes/yilia/source/img/linux/commend/6.png\"><br><img src=\"/../../../themes/yilia/source/img/linux/commend/7.png\"></p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h2 id=\"1-目录结构\"><a href=\"#1-目录结构\" class=\"headerlink\" title=\"1. 目录结构\"></a>1. 目录结构</h2><p>根目录：&#x2F;<br>&#x2F;home&#x2F;hellow&#x2F;test.txt</p>\n<h2 id=\"2-小tip\"><a href=\"#2-小tip\" class=\"headerlink\" title=\"2. 小tip\"></a>2. 小tip</h2><h3 id=\"2-1-特殊符号\"><a href=\"#2-1-特殊符号\" class=\"headerlink\" title=\"2-1 特殊符号\"></a>2-1 特殊符号</h3><ul>\n<li><p>. 当前目录</p>\n</li>\n<li><p>.. 上级目录</p>\n</li>\n<li><p>~ home</p>\n</li>\n<li><p>&#96;作为命令执行</p>\n</li>\n</ul>\n<h3 id=\"2-2-进入root用户\"><a href=\"#2-2-进入root用户\" class=\"headerlink\" title=\"2-2 进入root用户\"></a>2-2 进入root用户</h3><ul>\n<li>进入</li>\n</ul>\n<p>su - root</p>\n<p>123456</p>\n<ul>\n<li>退出<br>exit</li>\n</ul>\n<h3 id=\"2-3管道符\"><a href=\"#2-3管道符\" class=\"headerlink\" title=\"2-3管道符\"></a>2-3管道符</h3><p>|<br>cat 1.txt | grep “hello”</p>\n<h3 id=\"2-4重定向符\"><a href=\"#2-4重定向符\" class=\"headerlink\" title=\"2-4重定向符\"></a>2-4重定向符</h3><p>&gt;：右边文件内容清空，左边内容写入文件</p>\n<p>&gt;&gt;：追加</p>\n<h2 id=\"3-命令\"><a href=\"#3-命令\" class=\"headerlink\" title=\"3. 命令\"></a>3. 命令</h2><p>命令 [选项] [参数]</p>\n<h3 id=\"1-ls命令（显示内容）\"><a href=\"#1-ls命令（显示内容）\" class=\"headerlink\" title=\"1. ls命令（显示内容）\"></a>1. ls命令（显示内容）</h3><p>ls [-a -l -h] [路径]</p>\n<ul>\n<li>a: all，列出全部文件（包括隐藏文件）</li>\n<li>l: list，以列表展现（权限，用户和用户组，大小，创建日期）</li>\n<li>h: 以易于阅读的形式，列出文件大小（搭配-l)</li>\n<li>组合使用：如-la</li>\n</ul>\n<h3 id=\"2-cd命令（change-directory\"><a href=\"#2-cd命令（change-directory\" class=\"headerlink\" title=\"2. cd命令（change directory)\"></a>2. cd命令（change directory)</h3><p>cd [路径]</p>\n<ul>\n<li>cd 回到用户HOME目录</li>\n<li>cd ..&#x2F;.. 返回上两级</li>\n<li>cd 路径</li>\n</ul>\n<h3 id=\"3-pwd命令（print-work-directory\"><a href=\"#3-pwd命令（print-work-directory\" class=\"headerlink\" title=\"3. pwd命令（print work directory)\"></a>3. pwd命令（print work directory)</h3><p>pwd</p>\n<h3 id=\"4-mkdir命令（make-directory\"><a href=\"#4-mkdir命令（make-directory\" class=\"headerlink\" title=\"4. mkdir命令（make directory)\"></a>4. mkdir命令（make directory)</h3><p>mkdir [-p] 路径</p>\n<ul>\n<li>多层创建</li>\n</ul>\n<h3 id=\"5-touch命令（创建文件）\"><a href=\"#5-touch命令（创建文件）\" class=\"headerlink\" title=\"5. touch命令（创建文件）\"></a>5. touch命令（创建文件）</h3><p>touch 路径</p>\n<h3 id=\"6-cat命令（查看文件）\"><a href=\"#6-cat命令（查看文件）\" class=\"headerlink\" title=\"6. cat命令（查看文件）\"></a>6. cat命令（查看文件）</h3><p>cat 路径</p>\n<h3 id=\"7-more命令（查看文件）\"><a href=\"#7-more命令（查看文件）\" class=\"headerlink\" title=\"7. more命令（查看文件）\"></a>7. more命令（查看文件）</h3><p>more 路径（分页看，空格下一页，q退出）</p>\n<h3 id=\"8-cp命令（copy\"><a href=\"#8-cp命令（copy\" class=\"headerlink\" title=\"8. cp命令（copy)\"></a>8. cp命令（copy)</h3><p>cp [-r] 源路径 目的地</p>\n<ul>\n<li>r 递归，用于文件夹的复制</li>\n</ul>\n<h3 id=\"9-mv命令（move\"><a href=\"#9-mv命令（move\" class=\"headerlink\" title=\"9. mv命令（move)\"></a>9. mv命令（move)</h3><p>mv 源路径 目的地&#x2F;（可重命名）</p>\n<h3 id=\"10-rm命令（remove\"><a href=\"#10-rm命令（remove\" class=\"headerlink\" title=\"10. rm命令（remove)\"></a>10. rm命令（remove)</h3><p>rm [-r -f] 参数1 参数2…</p>\n<ul>\n<li>r 递归，用于删除文件夹</li>\n<li>f (force)强制删除</li>\n<li>支持通配符：test* 表示以test开头</li>\n</ul>\n<h3 id=\"11-grep命令（文件中通过关键字搜索行）\"><a href=\"#11-grep命令（文件中通过关键字搜索行）\" class=\"headerlink\" title=\"11. grep命令（文件中通过关键字搜索行）\"></a>11. grep命令（文件中通过关键字搜索行）</h3><p>grep [-n] “关键字” 文件路径</p>\n<ul>\n<li>n：结果中显示行号</li>\n<li>关键字：用于查找</li>\n<li>路径：可做输入端口</li>\n</ul>\n<h3 id=\"12-wc命令（文件内容统计）\"><a href=\"#12-wc命令（文件内容统计）\" class=\"headerlink\" title=\"12. wc命令（文件内容统计）\"></a>12. wc命令（文件内容统计）</h3><p>wc [-cmlw] 文件路径</p>\n<ul>\n<li>c：统计bytes数量</li>\n<li>m：统计字符数量</li>\n<li>l：统计行数</li>\n<li>w：统计单词数</li>\n<li>路径：可做输入端口</li>\n</ul>\n<h3 id=\"13-which命令（查看命令的源文件）\"><a href=\"#13-which命令（查看命令的源文件）\" class=\"headerlink\" title=\"13. which命令（查看命令的源文件）\"></a>13. which命令（查看命令的源文件）</h3><p>which Linux命令</p>\n<h3 id=\"14-find命令（文件搜索）\"><a href=\"#14-find命令（文件搜索）\" class=\"headerlink\" title=\"14. find命令（文件搜索）\"></a>14. find命令（文件搜索）</h3><p>find 起始路径 -name “test*”<br>find 起始路径 -size +100k(大于100kb)</p>\n<h3 id=\"15-echo命令（打印）\"><a href=\"#15-echo命令（打印）\" class=\"headerlink\" title=\"15. echo命令（打印）\"></a>15. echo命令（打印）</h3><p>echo 输出内容</p>\n<ul>\n<li>echo <code>pwd</code>：将pwd作为命令执行</li>\n</ul>\n<h3 id=\"16-tail命令（从尾部查看）\"><a href=\"#16-tail命令（从尾部查看）\" class=\"headerlink\" title=\"16. tail命令（从尾部查看）\"></a>16. tail命令（从尾部查看）</h3><p>tail [-f -数字] 路径</p>\n<ul>\n<li>f 持续跟踪</li>\n<li>数字 查看多少行（默认10）</li>\n</ul>\n<h2 id=\"4-vim编辑器\"><a href=\"#4-vim编辑器\" class=\"headerlink\" title=\"4. vim编辑器\"></a>4. vim编辑器</h2><p>vi 文件路径</p>\n<p>vim 文件路径</p>\n<p><img src=\"/./img/linux/commend/4.png\"><br><img src=\"/./img/linux/commend/5.png\"><br><img src=\"/./img/linux/commend/6.png\"><br><img src=\"/./img/linux/commend/7.png\"><br><img src=\"/../../../themes/yilia/source/img/linux/commend/4.png\"><br><img src=\"/../../../themes/yilia/source/img/linux/commend/5.png\"><br><img src=\"/../../../themes/yilia/source/img/linux/commend/6.png\"><br><img src=\"/../../../themes/yilia/source/img/linux/commend/7.png\"></p>"},{"title":"Linux入门-2用户和权限","date":"2021-03-01T12:00:00.000Z","toc":true,"_content":"#\n\n<!--more-->\n\n### 1. su命令\nsu [-] [用户名]\n\n### 2. sudo命令（以管理员身份运行）\n\nsudo Linux命令\n- 先进入root用户\n- 再visudo，G到最后一行，o到末尾，再末尾输入：用户名 ALL=(ALL)      NOPASSWD: ALL\n- 再esc，:wq退出 \n\n### 3. 用户组\n- 创建：groupadd 组名\n- 删除：groupdel 组名\n\n### 4. 用户\n#### 4-1创建用户\nuseradd [-g -d] 用户名\n（useradd 用户名 -g 组名 -d 目录）\n- g 指定用户的组，不写则组与用户名同\n- d指定用户home目录位置\n#### 4-2 删除用户\nuserdel [-r] 用户名\n- r 删除home目录\n#### 4-3 查看用户所属组\nid [用户名]\n#### 4-4 修改用户所属组\nusermod -aG 组名 用户名\n#### 4-5 查看系统用户(组)\ngetent passwd或group\n- 用户显示的信息：用户名:密码:用户组:描述信息:HOME目录:执行终端（默认bash)\n- 组显示信息：组名:组认证:组ID\n\n### 5 权限\n\n![](./lmg/linux/user/8.png)\n\n![](D:\\blog\\themes\\yilia\\source\\img\\linux\\user\\8.png)","source":"_posts/linux/Linux_2.用户和权限.md","raw":"---\ntitle: Linux入门-2用户和权限\ndate: 2021-03-01 20:00:00\ntoc: true\ntags: [Linux]\ncategories: [Linux]\n\n---\n#\n\n<!--more-->\n\n### 1. su命令\nsu [-] [用户名]\n\n### 2. sudo命令（以管理员身份运行）\n\nsudo Linux命令\n- 先进入root用户\n- 再visudo，G到最后一行，o到末尾，再末尾输入：用户名 ALL=(ALL)      NOPASSWD: ALL\n- 再esc，:wq退出 \n\n### 3. 用户组\n- 创建：groupadd 组名\n- 删除：groupdel 组名\n\n### 4. 用户\n#### 4-1创建用户\nuseradd [-g -d] 用户名\n（useradd 用户名 -g 组名 -d 目录）\n- g 指定用户的组，不写则组与用户名同\n- d指定用户home目录位置\n#### 4-2 删除用户\nuserdel [-r] 用户名\n- r 删除home目录\n#### 4-3 查看用户所属组\nid [用户名]\n#### 4-4 修改用户所属组\nusermod -aG 组名 用户名\n#### 4-5 查看系统用户(组)\ngetent passwd或group\n- 用户显示的信息：用户名:密码:用户组:描述信息:HOME目录:执行终端（默认bash)\n- 组显示信息：组名:组认证:组ID\n\n### 5 权限\n\n![](./lmg/linux/user/8.png)\n\n![](D:\\blog\\themes\\yilia\\source\\img\\linux\\user\\8.png)","slug":"linux/Linux_2.用户和权限","published":1,"updated":"2024-05-06T05:52:09.459Z","comments":1,"layout":"post","photos":[],"_id":"clvulgd5m000f5ovw92jebf5u","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h3 id=\"1-su命令\"><a href=\"#1-su命令\" class=\"headerlink\" title=\"1. su命令\"></a>1. su命令</h3><p>su [-] [用户名]</p>\n<h3 id=\"2-sudo命令（以管理员身份运行）\"><a href=\"#2-sudo命令（以管理员身份运行）\" class=\"headerlink\" title=\"2. sudo命令（以管理员身份运行）\"></a>2. sudo命令（以管理员身份运行）</h3><p>sudo Linux命令</p>\n<ul>\n<li>先进入root用户</li>\n<li>再visudo，G到最后一行，o到末尾，再末尾输入：用户名 ALL&#x3D;(ALL)      NOPASSWD: ALL</li>\n<li>再esc，:wq退出</li>\n</ul>\n<h3 id=\"3-用户组\"><a href=\"#3-用户组\" class=\"headerlink\" title=\"3. 用户组\"></a>3. 用户组</h3><ul>\n<li>创建：groupadd 组名</li>\n<li>删除：groupdel 组名</li>\n</ul>\n<h3 id=\"4-用户\"><a href=\"#4-用户\" class=\"headerlink\" title=\"4. 用户\"></a>4. 用户</h3><h4 id=\"4-1创建用户\"><a href=\"#4-1创建用户\" class=\"headerlink\" title=\"4-1创建用户\"></a>4-1创建用户</h4><p>useradd [-g -d] 用户名<br>（useradd 用户名 -g 组名 -d 目录）</p>\n<ul>\n<li>g 指定用户的组，不写则组与用户名同</li>\n<li>d指定用户home目录位置</li>\n</ul>\n<h4 id=\"4-2-删除用户\"><a href=\"#4-2-删除用户\" class=\"headerlink\" title=\"4-2 删除用户\"></a>4-2 删除用户</h4><p>userdel [-r] 用户名</p>\n<ul>\n<li>r 删除home目录</li>\n</ul>\n<h4 id=\"4-3-查看用户所属组\"><a href=\"#4-3-查看用户所属组\" class=\"headerlink\" title=\"4-3 查看用户所属组\"></a>4-3 查看用户所属组</h4><p>id [用户名]</p>\n<h4 id=\"4-4-修改用户所属组\"><a href=\"#4-4-修改用户所属组\" class=\"headerlink\" title=\"4-4 修改用户所属组\"></a>4-4 修改用户所属组</h4><p>usermod -aG 组名 用户名</p>\n<h4 id=\"4-5-查看系统用户-组\"><a href=\"#4-5-查看系统用户-组\" class=\"headerlink\" title=\"4-5 查看系统用户(组)\"></a>4-5 查看系统用户(组)</h4><p>getent passwd或group</p>\n<ul>\n<li>用户显示的信息：用户名:密码:用户组:描述信息:HOME目录:执行终端（默认bash)</li>\n<li>组显示信息：组名:组认证:组ID</li>\n</ul>\n<h3 id=\"5-权限\"><a href=\"#5-权限\" class=\"headerlink\" title=\"5 权限\"></a>5 权限</h3><p><img src=\"/./lmg/linux/user/8.png\"></p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\linux\\user\\8.png\"></p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h3 id=\"1-su命令\"><a href=\"#1-su命令\" class=\"headerlink\" title=\"1. su命令\"></a>1. su命令</h3><p>su [-] [用户名]</p>\n<h3 id=\"2-sudo命令（以管理员身份运行）\"><a href=\"#2-sudo命令（以管理员身份运行）\" class=\"headerlink\" title=\"2. sudo命令（以管理员身份运行）\"></a>2. sudo命令（以管理员身份运行）</h3><p>sudo Linux命令</p>\n<ul>\n<li>先进入root用户</li>\n<li>再visudo，G到最后一行，o到末尾，再末尾输入：用户名 ALL&#x3D;(ALL)      NOPASSWD: ALL</li>\n<li>再esc，:wq退出</li>\n</ul>\n<h3 id=\"3-用户组\"><a href=\"#3-用户组\" class=\"headerlink\" title=\"3. 用户组\"></a>3. 用户组</h3><ul>\n<li>创建：groupadd 组名</li>\n<li>删除：groupdel 组名</li>\n</ul>\n<h3 id=\"4-用户\"><a href=\"#4-用户\" class=\"headerlink\" title=\"4. 用户\"></a>4. 用户</h3><h4 id=\"4-1创建用户\"><a href=\"#4-1创建用户\" class=\"headerlink\" title=\"4-1创建用户\"></a>4-1创建用户</h4><p>useradd [-g -d] 用户名<br>（useradd 用户名 -g 组名 -d 目录）</p>\n<ul>\n<li>g 指定用户的组，不写则组与用户名同</li>\n<li>d指定用户home目录位置</li>\n</ul>\n<h4 id=\"4-2-删除用户\"><a href=\"#4-2-删除用户\" class=\"headerlink\" title=\"4-2 删除用户\"></a>4-2 删除用户</h4><p>userdel [-r] 用户名</p>\n<ul>\n<li>r 删除home目录</li>\n</ul>\n<h4 id=\"4-3-查看用户所属组\"><a href=\"#4-3-查看用户所属组\" class=\"headerlink\" title=\"4-3 查看用户所属组\"></a>4-3 查看用户所属组</h4><p>id [用户名]</p>\n<h4 id=\"4-4-修改用户所属组\"><a href=\"#4-4-修改用户所属组\" class=\"headerlink\" title=\"4-4 修改用户所属组\"></a>4-4 修改用户所属组</h4><p>usermod -aG 组名 用户名</p>\n<h4 id=\"4-5-查看系统用户-组\"><a href=\"#4-5-查看系统用户-组\" class=\"headerlink\" title=\"4-5 查看系统用户(组)\"></a>4-5 查看系统用户(组)</h4><p>getent passwd或group</p>\n<ul>\n<li>用户显示的信息：用户名:密码:用户组:描述信息:HOME目录:执行终端（默认bash)</li>\n<li>组显示信息：组名:组认证:组ID</li>\n</ul>\n<h3 id=\"5-权限\"><a href=\"#5-权限\" class=\"headerlink\" title=\"5 权限\"></a>5 权限</h3><p><img src=\"/./lmg/linux/user/8.png\"></p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\linux\\user\\8.png\"></p>"},{"title":"2.3 双链表","date":"2023-08-04T16:00:00.000Z","comment":false,"toc":true,"_content":"#\n<!--more-->\n![](../../../../themes/yilia/source/img/datastruct/2_linearlist/1.png)\n![数据结构](/img/datastruct/2_linearlist/1.png)\n\n# 2 线性表\n\n\n|单链表|双链表\n|:---:|:---:|\n|无法逆向检索，不太方便|可进可退，存储密度更低\n\n\n\n### 2.4.1 基本操作\n![](../../../../themes/yilia/source/img/datastruct/2_linearlist/doublelinklist/1.png)\n![数据结构](/img/datastruct/2_linearlist/doublelinklist/1.png)\n\n- 插入\n- 删除\n- 遍历\n```c\n#include<stdio.h>\n#include<stdlib.h>\ntypedef struct DNode{\n    int data;\n    struct DNode *prior,*next;\n} DNode, *DLinklist;\n\n//创建一个结点\nDNode* CreateNode(int data){\n    DNode *p;\n    p=(DNode*)malloc(sizeof(DNode));\n    p->data=data;\n    p->prior=NULL;\n    p->next=NULL;\n    return p;\n}\n\n//输出链表\nvoid printlist(DLinklist L){\n    DNode* p=L;\n    for(;p!=NULL;p=p->next)\n        printf(\"%d \",p->data);\n}\n\nint main(){\n    DLinklist L=NULL;\n    //创建一个结点\n    DNode *p=CreateNode(1);\n    //插入\n    InsertNextNode(p,CreateNode(2));\n    InsertNextNode(p->next,CreateNode(3));\n    InsertNextNode(p->next->next,CreateNode(4));\n    InsertNextNode(p->next->next->next,CreateNode(5));\n\n    //输出链表\n    printlist(p);\n    return 0;\n}\n\n//后插\nint InsertNextNode(DNode *p, DNode *s){\n    if(p==NULL||s==NULL)\n        return 0;\n    s->next=p->next;\n    if(p->next!=NULL)\n        p->next->prior=s;\n    s->prior=p;\n    p->next=s;\n    return 1;\n}\n\n//删除\nint DeleteNode(DNode *p){\n    if(p==NULL)\n        return 0;\n    p->prior->next=p->next;\n    p->next->prior=p->prior;\n    free(p);\n    return 1;\n}\n\n```\n\n## 2.5 循环链表\n![](../../../../themes/yilia/source/img/datastruct/2_linearlist/doublelinklist/2.png)\n![数据结构](/img/datastruct/2_linearlist/doublelinklist/2.png)\n\n## 2.6 静态链表\n![](../../../../themes/yilia/source/img/datastruct/2_linearlist/doublelinklist/3.png)\n![数据结构](/img/datastruct/2_linearlist/doublelinklist/3.png)\n\n![](../../../../themes/yilia/source/img/datastruct/2_linearlist/doublelinklist/4.png)\n![数据结构](/img/datastruct/2_linearlist/doublelinklist/4.png)\n\n### 2.6.1 代码实现\n- 插入\n- 删除\n\n## 2.7 顺序表vs链表\n\n||顺序表|链表\n|:---:|:---:|:---:|\n|逻辑结构|线性结构|线性结构\n|存储结构|顺序存储|链式存储\n|优点|随机存取，存储密度高|插入和删除\n\n![](../../../../themes/yilia/source/img/datastruct/2_linearlist/doublelinklist/5.png)\n![数据结构](/img/datastruct/2_linearlist/doublelinklist/5.png)","source":"_posts/datastruct/2_linearlist/3_doublelinklist.md","raw":"---\ntitle: 2.3 双链表\ndate: 2023-08-05 00:00:00\ntags: [数据结构,线性表,双链表]\ncategories: [数据结构]\ncomment: false\ntoc: true\n---\n#\n<!--more-->\n![](../../../../themes/yilia/source/img/datastruct/2_linearlist/1.png)\n![数据结构](/img/datastruct/2_linearlist/1.png)\n\n# 2 线性表\n\n\n|单链表|双链表\n|:---:|:---:|\n|无法逆向检索，不太方便|可进可退，存储密度更低\n\n\n\n### 2.4.1 基本操作\n![](../../../../themes/yilia/source/img/datastruct/2_linearlist/doublelinklist/1.png)\n![数据结构](/img/datastruct/2_linearlist/doublelinklist/1.png)\n\n- 插入\n- 删除\n- 遍历\n```c\n#include<stdio.h>\n#include<stdlib.h>\ntypedef struct DNode{\n    int data;\n    struct DNode *prior,*next;\n} DNode, *DLinklist;\n\n//创建一个结点\nDNode* CreateNode(int data){\n    DNode *p;\n    p=(DNode*)malloc(sizeof(DNode));\n    p->data=data;\n    p->prior=NULL;\n    p->next=NULL;\n    return p;\n}\n\n//输出链表\nvoid printlist(DLinklist L){\n    DNode* p=L;\n    for(;p!=NULL;p=p->next)\n        printf(\"%d \",p->data);\n}\n\nint main(){\n    DLinklist L=NULL;\n    //创建一个结点\n    DNode *p=CreateNode(1);\n    //插入\n    InsertNextNode(p,CreateNode(2));\n    InsertNextNode(p->next,CreateNode(3));\n    InsertNextNode(p->next->next,CreateNode(4));\n    InsertNextNode(p->next->next->next,CreateNode(5));\n\n    //输出链表\n    printlist(p);\n    return 0;\n}\n\n//后插\nint InsertNextNode(DNode *p, DNode *s){\n    if(p==NULL||s==NULL)\n        return 0;\n    s->next=p->next;\n    if(p->next!=NULL)\n        p->next->prior=s;\n    s->prior=p;\n    p->next=s;\n    return 1;\n}\n\n//删除\nint DeleteNode(DNode *p){\n    if(p==NULL)\n        return 0;\n    p->prior->next=p->next;\n    p->next->prior=p->prior;\n    free(p);\n    return 1;\n}\n\n```\n\n## 2.5 循环链表\n![](../../../../themes/yilia/source/img/datastruct/2_linearlist/doublelinklist/2.png)\n![数据结构](/img/datastruct/2_linearlist/doublelinklist/2.png)\n\n## 2.6 静态链表\n![](../../../../themes/yilia/source/img/datastruct/2_linearlist/doublelinklist/3.png)\n![数据结构](/img/datastruct/2_linearlist/doublelinklist/3.png)\n\n![](../../../../themes/yilia/source/img/datastruct/2_linearlist/doublelinklist/4.png)\n![数据结构](/img/datastruct/2_linearlist/doublelinklist/4.png)\n\n### 2.6.1 代码实现\n- 插入\n- 删除\n\n## 2.7 顺序表vs链表\n\n||顺序表|链表\n|:---:|:---:|:---:|\n|逻辑结构|线性结构|线性结构\n|存储结构|顺序存储|链式存储\n|优点|随机存取，存储密度高|插入和删除\n\n![](../../../../themes/yilia/source/img/datastruct/2_linearlist/doublelinklist/5.png)\n![数据结构](/img/datastruct/2_linearlist/doublelinklist/5.png)","slug":"datastruct/2_linearlist/3_doublelinklist","published":1,"updated":"2023-10-23T12:03:40.512Z","comments":1,"layout":"post","photos":[],"_id":"clvulgd5o000j5ovw9s5o0sam","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/2_linearlist/1.png\"><br><img src=\"/img/datastruct/2_linearlist/1.png\" alt=\"数据结构\"></p>\n<h1 id=\"2-线性表\"><a href=\"#2-线性表\" class=\"headerlink\" title=\"2 线性表\"></a>2 线性表</h1><table>\n<thead>\n<tr>\n<th align=\"center\">单链表</th>\n<th align=\"center\">双链表</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">无法逆向检索，不太方便</td>\n<td align=\"center\">可进可退，存储密度更低</td>\n</tr>\n</tbody></table>\n<h3 id=\"2-4-1-基本操作\"><a href=\"#2-4-1-基本操作\" class=\"headerlink\" title=\"2.4.1 基本操作\"></a>2.4.1 基本操作</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/2_linearlist/doublelinklist/1.png\"><br><img src=\"/img/datastruct/2_linearlist/doublelinklist/1.png\" alt=\"数据结构\"></p>\n<ul>\n<li>插入</li>\n<li>删除</li>\n<li>遍历<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;stdlib.h&gt;</span></span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">DNode</span>&#123;</span></span><br><span class=\"line\">    <span class=\"type\">int</span> data;</span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">DNode</span> *<span class=\"title\">prior</span>,*<span class=\"title\">next</span>;</span></span><br><span class=\"line\">&#125; DNode, *DLinklist;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//创建一个结点</span></span><br><span class=\"line\">DNode* <span class=\"title function_\">CreateNode</span><span class=\"params\">(<span class=\"type\">int</span> data)</span>&#123;</span><br><span class=\"line\">    DNode *p;</span><br><span class=\"line\">    p=(DNode*)<span class=\"built_in\">malloc</span>(<span class=\"keyword\">sizeof</span>(DNode));</span><br><span class=\"line\">    p-&gt;data=data;</span><br><span class=\"line\">    p-&gt;prior=<span class=\"literal\">NULL</span>;</span><br><span class=\"line\">    p-&gt;next=<span class=\"literal\">NULL</span>;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> p;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//输出链表</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">printlist</span><span class=\"params\">(DLinklist L)</span>&#123;</span><br><span class=\"line\">    DNode* p=L;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(;p!=<span class=\"literal\">NULL</span>;p=p-&gt;next)</span><br><span class=\"line\">        <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;%d &quot;</span>,p-&gt;data);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">    DLinklist L=<span class=\"literal\">NULL</span>;</span><br><span class=\"line\">    <span class=\"comment\">//创建一个结点</span></span><br><span class=\"line\">    DNode *p=CreateNode(<span class=\"number\">1</span>);</span><br><span class=\"line\">    <span class=\"comment\">//插入</span></span><br><span class=\"line\">    InsertNextNode(p,CreateNode(<span class=\"number\">2</span>));</span><br><span class=\"line\">    InsertNextNode(p-&gt;next,CreateNode(<span class=\"number\">3</span>));</span><br><span class=\"line\">    InsertNextNode(p-&gt;next-&gt;next,CreateNode(<span class=\"number\">4</span>));</span><br><span class=\"line\">    InsertNextNode(p-&gt;next-&gt;next-&gt;next,CreateNode(<span class=\"number\">5</span>));</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//输出链表</span></span><br><span class=\"line\">    printlist(p);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//后插</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">InsertNextNode</span><span class=\"params\">(DNode *p, DNode *s)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(p==<span class=\"literal\">NULL</span>||s==<span class=\"literal\">NULL</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    s-&gt;next=p-&gt;next;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(p-&gt;next!=<span class=\"literal\">NULL</span>)</span><br><span class=\"line\">        p-&gt;next-&gt;prior=s;</span><br><span class=\"line\">    s-&gt;prior=p;</span><br><span class=\"line\">    p-&gt;next=s;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//删除</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">DeleteNode</span><span class=\"params\">(DNode *p)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(p==<span class=\"literal\">NULL</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    p-&gt;prior-&gt;next=p-&gt;next;</span><br><span class=\"line\">    p-&gt;next-&gt;prior=p-&gt;prior;</span><br><span class=\"line\">    <span class=\"built_in\">free</span>(p);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></li>\n</ul>\n<h2 id=\"2-5-循环链表\"><a href=\"#2-5-循环链表\" class=\"headerlink\" title=\"2.5 循环链表\"></a>2.5 循环链表</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/2_linearlist/doublelinklist/2.png\"><br><img src=\"/img/datastruct/2_linearlist/doublelinklist/2.png\" alt=\"数据结构\"></p>\n<h2 id=\"2-6-静态链表\"><a href=\"#2-6-静态链表\" class=\"headerlink\" title=\"2.6 静态链表\"></a>2.6 静态链表</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/2_linearlist/doublelinklist/3.png\"><br><img src=\"/img/datastruct/2_linearlist/doublelinklist/3.png\" alt=\"数据结构\"></p>\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/2_linearlist/doublelinklist/4.png\"><br><img src=\"/img/datastruct/2_linearlist/doublelinklist/4.png\" alt=\"数据结构\"></p>\n<h3 id=\"2-6-1-代码实现\"><a href=\"#2-6-1-代码实现\" class=\"headerlink\" title=\"2.6.1 代码实现\"></a>2.6.1 代码实现</h3><ul>\n<li>插入</li>\n<li>删除</li>\n</ul>\n<h2 id=\"2-7-顺序表vs链表\"><a href=\"#2-7-顺序表vs链表\" class=\"headerlink\" title=\"2.7 顺序表vs链表\"></a>2.7 顺序表vs链表</h2><table>\n<thead>\n<tr>\n<th align=\"center\"></th>\n<th align=\"center\">顺序表</th>\n<th align=\"center\">链表</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">逻辑结构</td>\n<td align=\"center\">线性结构</td>\n<td align=\"center\">线性结构</td>\n</tr>\n<tr>\n<td align=\"center\">存储结构</td>\n<td align=\"center\">顺序存储</td>\n<td align=\"center\">链式存储</td>\n</tr>\n<tr>\n<td align=\"center\">优点</td>\n<td align=\"center\">随机存取，存储密度高</td>\n<td align=\"center\">插入和删除</td>\n</tr>\n</tbody></table>\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/2_linearlist/doublelinklist/5.png\"><br><img src=\"/img/datastruct/2_linearlist/doublelinklist/5.png\" alt=\"数据结构\"></p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<p><img src=\"/../../../../themes/yilia/source/img/datastruct/2_linearlist/1.png\"><br><img src=\"/img/datastruct/2_linearlist/1.png\" alt=\"数据结构\"></p>\n<h1 id=\"2-线性表\"><a href=\"#2-线性表\" class=\"headerlink\" title=\"2 线性表\"></a>2 线性表</h1><table>\n<thead>\n<tr>\n<th align=\"center\">单链表</th>\n<th align=\"center\">双链表</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">无法逆向检索，不太方便</td>\n<td align=\"center\">可进可退，存储密度更低</td>\n</tr>\n</tbody></table>\n<h3 id=\"2-4-1-基本操作\"><a href=\"#2-4-1-基本操作\" class=\"headerlink\" title=\"2.4.1 基本操作\"></a>2.4.1 基本操作</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/2_linearlist/doublelinklist/1.png\"><br><img src=\"/img/datastruct/2_linearlist/doublelinklist/1.png\" alt=\"数据结构\"></p>\n<ul>\n<li>插入</li>\n<li>删除</li>\n<li>遍历<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;stdlib.h&gt;</span></span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">DNode</span>&#123;</span></span><br><span class=\"line\">    <span class=\"type\">int</span> data;</span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">DNode</span> *<span class=\"title\">prior</span>,*<span class=\"title\">next</span>;</span></span><br><span class=\"line\">&#125; DNode, *DLinklist;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//创建一个结点</span></span><br><span class=\"line\">DNode* <span class=\"title function_\">CreateNode</span><span class=\"params\">(<span class=\"type\">int</span> data)</span>&#123;</span><br><span class=\"line\">    DNode *p;</span><br><span class=\"line\">    p=(DNode*)<span class=\"built_in\">malloc</span>(<span class=\"keyword\">sizeof</span>(DNode));</span><br><span class=\"line\">    p-&gt;data=data;</span><br><span class=\"line\">    p-&gt;prior=<span class=\"literal\">NULL</span>;</span><br><span class=\"line\">    p-&gt;next=<span class=\"literal\">NULL</span>;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> p;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//输出链表</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">printlist</span><span class=\"params\">(DLinklist L)</span>&#123;</span><br><span class=\"line\">    DNode* p=L;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(;p!=<span class=\"literal\">NULL</span>;p=p-&gt;next)</span><br><span class=\"line\">        <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;%d &quot;</span>,p-&gt;data);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">    DLinklist L=<span class=\"literal\">NULL</span>;</span><br><span class=\"line\">    <span class=\"comment\">//创建一个结点</span></span><br><span class=\"line\">    DNode *p=CreateNode(<span class=\"number\">1</span>);</span><br><span class=\"line\">    <span class=\"comment\">//插入</span></span><br><span class=\"line\">    InsertNextNode(p,CreateNode(<span class=\"number\">2</span>));</span><br><span class=\"line\">    InsertNextNode(p-&gt;next,CreateNode(<span class=\"number\">3</span>));</span><br><span class=\"line\">    InsertNextNode(p-&gt;next-&gt;next,CreateNode(<span class=\"number\">4</span>));</span><br><span class=\"line\">    InsertNextNode(p-&gt;next-&gt;next-&gt;next,CreateNode(<span class=\"number\">5</span>));</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//输出链表</span></span><br><span class=\"line\">    printlist(p);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//后插</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">InsertNextNode</span><span class=\"params\">(DNode *p, DNode *s)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(p==<span class=\"literal\">NULL</span>||s==<span class=\"literal\">NULL</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    s-&gt;next=p-&gt;next;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(p-&gt;next!=<span class=\"literal\">NULL</span>)</span><br><span class=\"line\">        p-&gt;next-&gt;prior=s;</span><br><span class=\"line\">    s-&gt;prior=p;</span><br><span class=\"line\">    p-&gt;next=s;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//删除</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">DeleteNode</span><span class=\"params\">(DNode *p)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(p==<span class=\"literal\">NULL</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    p-&gt;prior-&gt;next=p-&gt;next;</span><br><span class=\"line\">    p-&gt;next-&gt;prior=p-&gt;prior;</span><br><span class=\"line\">    <span class=\"built_in\">free</span>(p);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></li>\n</ul>\n<h2 id=\"2-5-循环链表\"><a href=\"#2-5-循环链表\" class=\"headerlink\" title=\"2.5 循环链表\"></a>2.5 循环链表</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/2_linearlist/doublelinklist/2.png\"><br><img src=\"/img/datastruct/2_linearlist/doublelinklist/2.png\" alt=\"数据结构\"></p>\n<h2 id=\"2-6-静态链表\"><a href=\"#2-6-静态链表\" class=\"headerlink\" title=\"2.6 静态链表\"></a>2.6 静态链表</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/2_linearlist/doublelinklist/3.png\"><br><img src=\"/img/datastruct/2_linearlist/doublelinklist/3.png\" alt=\"数据结构\"></p>\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/2_linearlist/doublelinklist/4.png\"><br><img src=\"/img/datastruct/2_linearlist/doublelinklist/4.png\" alt=\"数据结构\"></p>\n<h3 id=\"2-6-1-代码实现\"><a href=\"#2-6-1-代码实现\" class=\"headerlink\" title=\"2.6.1 代码实现\"></a>2.6.1 代码实现</h3><ul>\n<li>插入</li>\n<li>删除</li>\n</ul>\n<h2 id=\"2-7-顺序表vs链表\"><a href=\"#2-7-顺序表vs链表\" class=\"headerlink\" title=\"2.7 顺序表vs链表\"></a>2.7 顺序表vs链表</h2><table>\n<thead>\n<tr>\n<th align=\"center\"></th>\n<th align=\"center\">顺序表</th>\n<th align=\"center\">链表</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">逻辑结构</td>\n<td align=\"center\">线性结构</td>\n<td align=\"center\">线性结构</td>\n</tr>\n<tr>\n<td align=\"center\">存储结构</td>\n<td align=\"center\">顺序存储</td>\n<td align=\"center\">链式存储</td>\n</tr>\n<tr>\n<td align=\"center\">优点</td>\n<td align=\"center\">随机存取，存储密度高</td>\n<td align=\"center\">插入和删除</td>\n</tr>\n</tbody></table>\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/2_linearlist/doublelinklist/5.png\"><br><img src=\"/img/datastruct/2_linearlist/doublelinklist/5.png\" alt=\"数据结构\"></p>"},{"title":"2.2 单链表","date":"2023-08-04T16:00:00.000Z","comment":false,"toc":true,"_content":"#\n<!--more-->\n![](../../../../themes/yilia/source/img/datastruct/2_linearlist/1.png)\n![数据结构](/img/datastruct/2_linearlist/1.png)\n\n\n![](../../../../themes/yilia/source/img/datastruct/2_linearlist/linklist/1.png)\n![数据结构](/img/datastruct/2_linearlist/linklist/1.png)\n### 2.4.1 定义\n![](../../../../themes/yilia/source/img/datastruct/2_linearlist/linklist/2.png)\n![数据结构](/img/datastruct/2_linearlist/linklist/2.png)\n\n||顺序表|单链表|\n|:---:|:---:|:---:|\n|结构|![](../../../../themes/yilia/source/img/datastruct/2_linearlist/linklist/3.png)![数据结构](/img/datastruct/2_linearlist/linklist/3.png)|![](../../../../themes/yilia/source/img/datastruct/2_linearlist/linklist/4.png)![数据结构](/img/datastruct/2_linearlist/linklist/4.png)|\n|优点|随机存取，存储密度高|不要求大片连续空间，方便改容量|\n|缺点|要求大片连续空间，改容量不便|不可随机存取，额外指针空间|\n\n\n|带头结点|不带头结点|\n|:---:|:---:|\n||写代码更麻烦|\n||对第一个结点和后续结点的处理逻辑不同|\n||对空表和非空表的处理需要用不同的逻辑|\n\n### 2.4.2基本操作\n![](../../../../themes/yilia/source/img/datastruct/2_linearlist/linklist/5.png)\n![数据结构](/img/datastruct/2_linearlist/linklist/5.png)\n![](../../../../themes/yilia/source/img/datastruct/2_linearlist/linklist/6.png)\n![数据结构](/img/datastruct/2_linearlist/linklist/6.png)\n\n- 查找\n    - 按位查找\n    - 按值查找\n- 插入\n    - 位序插入\n    - 指定结点后插\n- 删除\n    - 按位删除\n    - 删除指定节点\n\n```c\n#include<stdio.h>\n#include<stdlib.h>\ntypedef struct LNode{\n    int data;\n    struct LNode *next;\n} LNode, *Linklist;\n//输出链表\nvoid printlist(LNode L){\n    LNode* p=&L;\n    for(;p!=NULL;p=p->next)\n        printf(\"%d \",p->data);\n}\n\n//头插法-带头结点\nLinklist HeadInsert(Linklist L){\n    LNode *s;\n    int i=0,x=0;\n    int input[6]={5,4,3,2,1,-1};\n    L=(Linklist)malloc(sizeof(LNode));\n    L->next=NULL;\n    for(;input[i]!=-1;i++){\n        InsertNextNode(L,input[i]);\n    }\n    return L;\n}\n\n//尾插法-带头结点\nLinklist TailInsert(Linklist L){\n    LNode *s,*r;\n    int i=0,x=0;\n    int input[6]={5,4,3,2,1,-1};\n    L=(Linklist)malloc(sizeof(LNode));\n    L->next=NULL;\n    r=L;\n    for(;input[i]!=-1;i++){\n        s=(LNode*)malloc(sizeof(LNode));\n        s->data=input[i];\n        r->next=s;\n        r=s;\n    }\n    r->next=NULL;\n    return L;\n}\n\n//尾插法-不带头结点\nLinklist HeadInsertNoHead(Linklist L){\n    LNode *s;\n    int i=1,x=0;\n    int input[6]={5,4,3,2,1,-1};\n    L=(Linklist)malloc(sizeof(LNode));\n    L->data=input[0];\n    L->next=NULL;\n    for(;input[i]!=-1;i++){\n        s=(LNode*)malloc(sizeof(LNode));\n        s->next=L->next;\n        L->next=s;\n        s->data=input[i];\n    }\n    return L;\n}\n\n\n\n//按位查找\nLNode* GetElement(Linklist L,int i){\n    if(i<0) return NULL;\n    int j=0;\n    LNode* p=L;\n    for(;p!=NULL && j<i;j++)\n        p=p->next;\n    return p;\n}\n\n//按值查找\nLNode* LocateElem(Linklist L,int e){\n    LNode* p=L->next;\n    for(;p!=NULL && p->data!=e;p=p->next);\n    return p;\n}\n\n//求\nint Length(Linklist L){\n    int i=0;\n    LNode* p=L;\n    for(;p!=NULL;p=p->next)\n        i++;\n    return i;\n}\n\nvoid test_head(){\n    printf(\"带头结点测试：\");\n    Linklist L=NULL;\n    L=HeadInsert(L);\n    printlist(*(L->next));\n\n    //按位查找\n    printf(\"\\n第3个位置的元素为：%d\\n\",GetElement(L,3)->data);\n\n    //按位插入\n    printf(\"\\n在第1个位置插入11\\n\");\n    ListInsert_head(L,1,11);\n    printf(\"插入后：\");\n    printlist(*(L->next));    \n\n    //后插\n    printf(\"\\n在第2个位置后插22\\n\");\n    LNode *p=GetElement(L,2);\n    InsertNextNode(p,22);\n    printf(\"插入后：\");\n    printlist(*(L->next));\n\n    //前插\n    printf(\"\\n在第3个位置前插33\\n\");\n    p=GetElement(L,3);\n    InsertPriorNode(p,33);\n    printf(\"插入后：\");\n    printlist(*(L->next));\n\n    //按位删除\n    printf(\"\\n删除第1个位置\\n\");\n    int e;\n    ListDelete(L,1,&e);\n    printf(\"删除后：\");\n    printlist(*(L->next));\n\n    //逆置\n    printf(\"\\n逆置\\n\");\n    Reverse(&L);\n    printf(\"逆置后：\");\n    printlist(*(L->next));\n    \n}\nvoid test_nohead(){\n    Linklist L=NULL;\n    L=HeadInsertNoHead(L);\n    printf(\"\\n\\n\\n不带头结点测试：\");\n    printlist(*L);\n\n    //按位插入\n    printf(\"\\n在第1个位置插入11\\n\");\n    ListInsert_nohead(&L,1,11);\n    printf(\"插入后：\");\n    printlist(*L);\n\n    printf(\"\\n在第2个位置插入22\\n\");\n    ListInsert_nohead(&L,2,22);\n    printf(\"插入后：\");\n    printlist(*L);\n\n    \n}\nint main(){\n    test_head();\n    test_nohead();\n    return 0;\n}\n\n\n\n\n\n//按位插入_头\nint ListInsert_head(Linklist L, int i, int e){\n    if(i<1) return 0;\n    LNode *p=GetElement(L,i-1);\n    return InsertNextNode(p,e);\n}\n\n//按位插入_不头\nint ListInsert_nohead(Linklist *L,int i, int e){\n    if(i<1) return 0;\n    if(i==1){\n        LNode *s=(LNode*)malloc(sizeof(LNode));\n        s->data=e;\n        s->next=*L;\n        *L=s;\n        return 1;\n    }\n    LNode *p=*L;\n    int j=1;\n    for(;p!=NULL && j<i-1;j++) p=p->next;\n    if(p==NULL) return 0;\n    LNode *s=(LNode*)malloc(sizeof(LNode));\n    s->data=e;\n    s->next=p->next;//顺序不能换        \n    p->next=s;\n    return 1;\n}\n\n//后插\nint InsertNextNode(LNode *p, int e){\n    if(p==NULL) return 0;\n    LNode *s=(LNode*)malloc(sizeof(LNode));\n    s->data=e;\n    s->next=p->next;\n    p->next=s;\n    return 1;\n}\n\n//前插\nint InsertPriorNode(LNode *p, int e){//只需交换数据域\n    if(p==NULL) return 0;\n    LNode *s=(LNode*)malloc(sizeof(LNode));\n    if(s==NULL) return 0;\n    s->next=p->next;\n    p->next=s;\n    s->data=p->data;\n    p->data=e;\n    return 1;\n}\n\n//按位删除\nint ListDelete(Linklist L, int i, int *e){\n    if(i<1) return 0;\n    LNode *p=L;\n    int j=0;\n    for(;p!=NULL && j<i-1;j++) p=p->next;\n    if(p==NULL) return 0;\n    LNode *q=p->next;\n    if(q==NULL) return 0;\n    p->next=q->next;\n    *e=q->data;\n    free(q);\n    return 1;\n}\n\n//逆置\nvoid Reverse(Linklist *L){\n    LNode *p=(*L)->next;\n    (*L)->next=NULL;\n    LNode *q;\n    while(p!=NULL){\n        q=p->next;\n        p->next=(*L)->next;\n        (*L)->next=p;\n        p=q;\n    }\n}\n```","source":"_posts/datastruct/2_linearlist/2_linklist.md","raw":"---\ntitle: 2.2 单链表\ndate: 2023-08-05 00:00:00\ntags: [数据结构,线性表,单链表]\ncategories: [数据结构]\ncomment: false\ntoc: true\n---\n#\n<!--more-->\n![](../../../../themes/yilia/source/img/datastruct/2_linearlist/1.png)\n![数据结构](/img/datastruct/2_linearlist/1.png)\n\n\n![](../../../../themes/yilia/source/img/datastruct/2_linearlist/linklist/1.png)\n![数据结构](/img/datastruct/2_linearlist/linklist/1.png)\n### 2.4.1 定义\n![](../../../../themes/yilia/source/img/datastruct/2_linearlist/linklist/2.png)\n![数据结构](/img/datastruct/2_linearlist/linklist/2.png)\n\n||顺序表|单链表|\n|:---:|:---:|:---:|\n|结构|![](../../../../themes/yilia/source/img/datastruct/2_linearlist/linklist/3.png)![数据结构](/img/datastruct/2_linearlist/linklist/3.png)|![](../../../../themes/yilia/source/img/datastruct/2_linearlist/linklist/4.png)![数据结构](/img/datastruct/2_linearlist/linklist/4.png)|\n|优点|随机存取，存储密度高|不要求大片连续空间，方便改容量|\n|缺点|要求大片连续空间，改容量不便|不可随机存取，额外指针空间|\n\n\n|带头结点|不带头结点|\n|:---:|:---:|\n||写代码更麻烦|\n||对第一个结点和后续结点的处理逻辑不同|\n||对空表和非空表的处理需要用不同的逻辑|\n\n### 2.4.2基本操作\n![](../../../../themes/yilia/source/img/datastruct/2_linearlist/linklist/5.png)\n![数据结构](/img/datastruct/2_linearlist/linklist/5.png)\n![](../../../../themes/yilia/source/img/datastruct/2_linearlist/linklist/6.png)\n![数据结构](/img/datastruct/2_linearlist/linklist/6.png)\n\n- 查找\n    - 按位查找\n    - 按值查找\n- 插入\n    - 位序插入\n    - 指定结点后插\n- 删除\n    - 按位删除\n    - 删除指定节点\n\n```c\n#include<stdio.h>\n#include<stdlib.h>\ntypedef struct LNode{\n    int data;\n    struct LNode *next;\n} LNode, *Linklist;\n//输出链表\nvoid printlist(LNode L){\n    LNode* p=&L;\n    for(;p!=NULL;p=p->next)\n        printf(\"%d \",p->data);\n}\n\n//头插法-带头结点\nLinklist HeadInsert(Linklist L){\n    LNode *s;\n    int i=0,x=0;\n    int input[6]={5,4,3,2,1,-1};\n    L=(Linklist)malloc(sizeof(LNode));\n    L->next=NULL;\n    for(;input[i]!=-1;i++){\n        InsertNextNode(L,input[i]);\n    }\n    return L;\n}\n\n//尾插法-带头结点\nLinklist TailInsert(Linklist L){\n    LNode *s,*r;\n    int i=0,x=0;\n    int input[6]={5,4,3,2,1,-1};\n    L=(Linklist)malloc(sizeof(LNode));\n    L->next=NULL;\n    r=L;\n    for(;input[i]!=-1;i++){\n        s=(LNode*)malloc(sizeof(LNode));\n        s->data=input[i];\n        r->next=s;\n        r=s;\n    }\n    r->next=NULL;\n    return L;\n}\n\n//尾插法-不带头结点\nLinklist HeadInsertNoHead(Linklist L){\n    LNode *s;\n    int i=1,x=0;\n    int input[6]={5,4,3,2,1,-1};\n    L=(Linklist)malloc(sizeof(LNode));\n    L->data=input[0];\n    L->next=NULL;\n    for(;input[i]!=-1;i++){\n        s=(LNode*)malloc(sizeof(LNode));\n        s->next=L->next;\n        L->next=s;\n        s->data=input[i];\n    }\n    return L;\n}\n\n\n\n//按位查找\nLNode* GetElement(Linklist L,int i){\n    if(i<0) return NULL;\n    int j=0;\n    LNode* p=L;\n    for(;p!=NULL && j<i;j++)\n        p=p->next;\n    return p;\n}\n\n//按值查找\nLNode* LocateElem(Linklist L,int e){\n    LNode* p=L->next;\n    for(;p!=NULL && p->data!=e;p=p->next);\n    return p;\n}\n\n//求\nint Length(Linklist L){\n    int i=0;\n    LNode* p=L;\n    for(;p!=NULL;p=p->next)\n        i++;\n    return i;\n}\n\nvoid test_head(){\n    printf(\"带头结点测试：\");\n    Linklist L=NULL;\n    L=HeadInsert(L);\n    printlist(*(L->next));\n\n    //按位查找\n    printf(\"\\n第3个位置的元素为：%d\\n\",GetElement(L,3)->data);\n\n    //按位插入\n    printf(\"\\n在第1个位置插入11\\n\");\n    ListInsert_head(L,1,11);\n    printf(\"插入后：\");\n    printlist(*(L->next));    \n\n    //后插\n    printf(\"\\n在第2个位置后插22\\n\");\n    LNode *p=GetElement(L,2);\n    InsertNextNode(p,22);\n    printf(\"插入后：\");\n    printlist(*(L->next));\n\n    //前插\n    printf(\"\\n在第3个位置前插33\\n\");\n    p=GetElement(L,3);\n    InsertPriorNode(p,33);\n    printf(\"插入后：\");\n    printlist(*(L->next));\n\n    //按位删除\n    printf(\"\\n删除第1个位置\\n\");\n    int e;\n    ListDelete(L,1,&e);\n    printf(\"删除后：\");\n    printlist(*(L->next));\n\n    //逆置\n    printf(\"\\n逆置\\n\");\n    Reverse(&L);\n    printf(\"逆置后：\");\n    printlist(*(L->next));\n    \n}\nvoid test_nohead(){\n    Linklist L=NULL;\n    L=HeadInsertNoHead(L);\n    printf(\"\\n\\n\\n不带头结点测试：\");\n    printlist(*L);\n\n    //按位插入\n    printf(\"\\n在第1个位置插入11\\n\");\n    ListInsert_nohead(&L,1,11);\n    printf(\"插入后：\");\n    printlist(*L);\n\n    printf(\"\\n在第2个位置插入22\\n\");\n    ListInsert_nohead(&L,2,22);\n    printf(\"插入后：\");\n    printlist(*L);\n\n    \n}\nint main(){\n    test_head();\n    test_nohead();\n    return 0;\n}\n\n\n\n\n\n//按位插入_头\nint ListInsert_head(Linklist L, int i, int e){\n    if(i<1) return 0;\n    LNode *p=GetElement(L,i-1);\n    return InsertNextNode(p,e);\n}\n\n//按位插入_不头\nint ListInsert_nohead(Linklist *L,int i, int e){\n    if(i<1) return 0;\n    if(i==1){\n        LNode *s=(LNode*)malloc(sizeof(LNode));\n        s->data=e;\n        s->next=*L;\n        *L=s;\n        return 1;\n    }\n    LNode *p=*L;\n    int j=1;\n    for(;p!=NULL && j<i-1;j++) p=p->next;\n    if(p==NULL) return 0;\n    LNode *s=(LNode*)malloc(sizeof(LNode));\n    s->data=e;\n    s->next=p->next;//顺序不能换        \n    p->next=s;\n    return 1;\n}\n\n//后插\nint InsertNextNode(LNode *p, int e){\n    if(p==NULL) return 0;\n    LNode *s=(LNode*)malloc(sizeof(LNode));\n    s->data=e;\n    s->next=p->next;\n    p->next=s;\n    return 1;\n}\n\n//前插\nint InsertPriorNode(LNode *p, int e){//只需交换数据域\n    if(p==NULL) return 0;\n    LNode *s=(LNode*)malloc(sizeof(LNode));\n    if(s==NULL) return 0;\n    s->next=p->next;\n    p->next=s;\n    s->data=p->data;\n    p->data=e;\n    return 1;\n}\n\n//按位删除\nint ListDelete(Linklist L, int i, int *e){\n    if(i<1) return 0;\n    LNode *p=L;\n    int j=0;\n    for(;p!=NULL && j<i-1;j++) p=p->next;\n    if(p==NULL) return 0;\n    LNode *q=p->next;\n    if(q==NULL) return 0;\n    p->next=q->next;\n    *e=q->data;\n    free(q);\n    return 1;\n}\n\n//逆置\nvoid Reverse(Linklist *L){\n    LNode *p=(*L)->next;\n    (*L)->next=NULL;\n    LNode *q;\n    while(p!=NULL){\n        q=p->next;\n        p->next=(*L)->next;\n        (*L)->next=p;\n        p=q;\n    }\n}\n```","slug":"datastruct/2_linearlist/2_linklist","published":1,"updated":"2023-10-23T12:02:48.746Z","comments":1,"layout":"post","photos":[],"_id":"clvulgd5p000l5ovwdki71pig","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/2_linearlist/1.png\"><br><img src=\"/img/datastruct/2_linearlist/1.png\" alt=\"数据结构\"></p>\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/2_linearlist/linklist/1.png\"><br><img src=\"/img/datastruct/2_linearlist/linklist/1.png\" alt=\"数据结构\"></p>\n<h3 id=\"2-4-1-定义\"><a href=\"#2-4-1-定义\" class=\"headerlink\" title=\"2.4.1 定义\"></a>2.4.1 定义</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/2_linearlist/linklist/2.png\"><br><img src=\"/img/datastruct/2_linearlist/linklist/2.png\" alt=\"数据结构\"></p>\n<table>\n<thead>\n<tr>\n<th align=\"center\"></th>\n<th align=\"center\">顺序表</th>\n<th align=\"center\">单链表</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">结构</td>\n<td align=\"center\"><img src=\"/../../../../themes/yilia/source/img/datastruct/2_linearlist/linklist/3.png\"><img src=\"/img/datastruct/2_linearlist/linklist/3.png\" alt=\"数据结构\"></td>\n<td align=\"center\"><img src=\"/../../../../themes/yilia/source/img/datastruct/2_linearlist/linklist/4.png\"><img src=\"/img/datastruct/2_linearlist/linklist/4.png\" alt=\"数据结构\"></td>\n</tr>\n<tr>\n<td align=\"center\">优点</td>\n<td align=\"center\">随机存取，存储密度高</td>\n<td align=\"center\">不要求大片连续空间，方便改容量</td>\n</tr>\n<tr>\n<td align=\"center\">缺点</td>\n<td align=\"center\">要求大片连续空间，改容量不便</td>\n<td align=\"center\">不可随机存取，额外指针空间</td>\n</tr>\n</tbody></table>\n<table>\n<thead>\n<tr>\n<th align=\"center\">带头结点</th>\n<th align=\"center\">不带头结点</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\"></td>\n<td align=\"center\">写代码更麻烦</td>\n</tr>\n<tr>\n<td align=\"center\"></td>\n<td align=\"center\">对第一个结点和后续结点的处理逻辑不同</td>\n</tr>\n<tr>\n<td align=\"center\"></td>\n<td align=\"center\">对空表和非空表的处理需要用不同的逻辑</td>\n</tr>\n</tbody></table>\n<h3 id=\"2-4-2基本操作\"><a href=\"#2-4-2基本操作\" class=\"headerlink\" title=\"2.4.2基本操作\"></a>2.4.2基本操作</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/2_linearlist/linklist/5.png\"><br><img src=\"/img/datastruct/2_linearlist/linklist/5.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/2_linearlist/linklist/6.png\"><br><img src=\"/img/datastruct/2_linearlist/linklist/6.png\" alt=\"数据结构\"></p>\n<ul>\n<li>查找<ul>\n<li>按位查找</li>\n<li>按值查找</li>\n</ul>\n</li>\n<li>插入<ul>\n<li>位序插入</li>\n<li>指定结点后插</li>\n</ul>\n</li>\n<li>删除<ul>\n<li>按位删除</li>\n<li>删除指定节点</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br><span class=\"line\">194</span><br><span class=\"line\">195</span><br><span class=\"line\">196</span><br><span class=\"line\">197</span><br><span class=\"line\">198</span><br><span class=\"line\">199</span><br><span class=\"line\">200</span><br><span class=\"line\">201</span><br><span class=\"line\">202</span><br><span class=\"line\">203</span><br><span class=\"line\">204</span><br><span class=\"line\">205</span><br><span class=\"line\">206</span><br><span class=\"line\">207</span><br><span class=\"line\">208</span><br><span class=\"line\">209</span><br><span class=\"line\">210</span><br><span class=\"line\">211</span><br><span class=\"line\">212</span><br><span class=\"line\">213</span><br><span class=\"line\">214</span><br><span class=\"line\">215</span><br><span class=\"line\">216</span><br><span class=\"line\">217</span><br><span class=\"line\">218</span><br><span class=\"line\">219</span><br><span class=\"line\">220</span><br><span class=\"line\">221</span><br><span class=\"line\">222</span><br><span class=\"line\">223</span><br><span class=\"line\">224</span><br><span class=\"line\">225</span><br><span class=\"line\">226</span><br><span class=\"line\">227</span><br><span class=\"line\">228</span><br><span class=\"line\">229</span><br><span class=\"line\">230</span><br><span class=\"line\">231</span><br><span class=\"line\">232</span><br><span class=\"line\">233</span><br><span class=\"line\">234</span><br><span class=\"line\">235</span><br><span class=\"line\">236</span><br><span class=\"line\">237</span><br><span class=\"line\">238</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;stdlib.h&gt;</span></span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">LNode</span>&#123;</span></span><br><span class=\"line\">    <span class=\"type\">int</span> data;</span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">LNode</span> *<span class=\"title\">next</span>;</span></span><br><span class=\"line\">&#125; LNode, *Linklist;</span><br><span class=\"line\"><span class=\"comment\">//输出链表</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">printlist</span><span class=\"params\">(LNode L)</span>&#123;</span><br><span class=\"line\">    LNode* p=&amp;L;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(;p!=<span class=\"literal\">NULL</span>;p=p-&gt;next)</span><br><span class=\"line\">        <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;%d &quot;</span>,p-&gt;data);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//头插法-带头结点</span></span><br><span class=\"line\">Linklist <span class=\"title function_\">HeadInsert</span><span class=\"params\">(Linklist L)</span>&#123;</span><br><span class=\"line\">    LNode *s;</span><br><span class=\"line\">    <span class=\"type\">int</span> i=<span class=\"number\">0</span>,x=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"type\">int</span> input[<span class=\"number\">6</span>]=&#123;<span class=\"number\">5</span>,<span class=\"number\">4</span>,<span class=\"number\">3</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>,<span class=\"number\">-1</span>&#125;;</span><br><span class=\"line\">    L=(Linklist)<span class=\"built_in\">malloc</span>(<span class=\"keyword\">sizeof</span>(LNode));</span><br><span class=\"line\">    L-&gt;next=<span class=\"literal\">NULL</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(;input[i]!=<span class=\"number\">-1</span>;i++)&#123;</span><br><span class=\"line\">        InsertNextNode(L,input[i]);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> L;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//尾插法-带头结点</span></span><br><span class=\"line\">Linklist <span class=\"title function_\">TailInsert</span><span class=\"params\">(Linklist L)</span>&#123;</span><br><span class=\"line\">    LNode *s,*r;</span><br><span class=\"line\">    <span class=\"type\">int</span> i=<span class=\"number\">0</span>,x=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"type\">int</span> input[<span class=\"number\">6</span>]=&#123;<span class=\"number\">5</span>,<span class=\"number\">4</span>,<span class=\"number\">3</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>,<span class=\"number\">-1</span>&#125;;</span><br><span class=\"line\">    L=(Linklist)<span class=\"built_in\">malloc</span>(<span class=\"keyword\">sizeof</span>(LNode));</span><br><span class=\"line\">    L-&gt;next=<span class=\"literal\">NULL</span>;</span><br><span class=\"line\">    r=L;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(;input[i]!=<span class=\"number\">-1</span>;i++)&#123;</span><br><span class=\"line\">        s=(LNode*)<span class=\"built_in\">malloc</span>(<span class=\"keyword\">sizeof</span>(LNode));</span><br><span class=\"line\">        s-&gt;data=input[i];</span><br><span class=\"line\">        r-&gt;next=s;</span><br><span class=\"line\">        r=s;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    r-&gt;next=<span class=\"literal\">NULL</span>;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> L;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//尾插法-不带头结点</span></span><br><span class=\"line\">Linklist <span class=\"title function_\">HeadInsertNoHead</span><span class=\"params\">(Linklist L)</span>&#123;</span><br><span class=\"line\">    LNode *s;</span><br><span class=\"line\">    <span class=\"type\">int</span> i=<span class=\"number\">1</span>,x=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"type\">int</span> input[<span class=\"number\">6</span>]=&#123;<span class=\"number\">5</span>,<span class=\"number\">4</span>,<span class=\"number\">3</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>,<span class=\"number\">-1</span>&#125;;</span><br><span class=\"line\">    L=(Linklist)<span class=\"built_in\">malloc</span>(<span class=\"keyword\">sizeof</span>(LNode));</span><br><span class=\"line\">    L-&gt;data=input[<span class=\"number\">0</span>];</span><br><span class=\"line\">    L-&gt;next=<span class=\"literal\">NULL</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(;input[i]!=<span class=\"number\">-1</span>;i++)&#123;</span><br><span class=\"line\">        s=(LNode*)<span class=\"built_in\">malloc</span>(<span class=\"keyword\">sizeof</span>(LNode));</span><br><span class=\"line\">        s-&gt;next=L-&gt;next;</span><br><span class=\"line\">        L-&gt;next=s;</span><br><span class=\"line\">        s-&gt;data=input[i];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> L;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//按位查找</span></span><br><span class=\"line\">LNode* <span class=\"title function_\">GetElement</span><span class=\"params\">(Linklist L,<span class=\"type\">int</span> i)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(i&lt;<span class=\"number\">0</span>) <span class=\"keyword\">return</span> <span class=\"literal\">NULL</span>;</span><br><span class=\"line\">    <span class=\"type\">int</span> j=<span class=\"number\">0</span>;</span><br><span class=\"line\">    LNode* p=L;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(;p!=<span class=\"literal\">NULL</span> &amp;&amp; j&lt;i;j++)</span><br><span class=\"line\">        p=p-&gt;next;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> p;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//按值查找</span></span><br><span class=\"line\">LNode* <span class=\"title function_\">LocateElem</span><span class=\"params\">(Linklist L,<span class=\"type\">int</span> e)</span>&#123;</span><br><span class=\"line\">    LNode* p=L-&gt;next;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(;p!=<span class=\"literal\">NULL</span> &amp;&amp; p-&gt;data!=e;p=p-&gt;next);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> p;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//求</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">Length</span><span class=\"params\">(Linklist L)</span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> i=<span class=\"number\">0</span>;</span><br><span class=\"line\">    LNode* p=L;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(;p!=<span class=\"literal\">NULL</span>;p=p-&gt;next)</span><br><span class=\"line\">        i++;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> i;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">test_head</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;带头结点测试：&quot;</span>);</span><br><span class=\"line\">    Linklist L=<span class=\"literal\">NULL</span>;</span><br><span class=\"line\">    L=HeadInsert(L);</span><br><span class=\"line\">    printlist(*(L-&gt;next));</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//按位查找</span></span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n第3个位置的元素为：%d\\n&quot;</span>,GetElement(L,<span class=\"number\">3</span>)-&gt;data);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//按位插入</span></span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n在第1个位置插入11\\n&quot;</span>);</span><br><span class=\"line\">    ListInsert_head(L,<span class=\"number\">1</span>,<span class=\"number\">11</span>);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;插入后：&quot;</span>);</span><br><span class=\"line\">    printlist(*(L-&gt;next));    </span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//后插</span></span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n在第2个位置后插22\\n&quot;</span>);</span><br><span class=\"line\">    LNode *p=GetElement(L,<span class=\"number\">2</span>);</span><br><span class=\"line\">    InsertNextNode(p,<span class=\"number\">22</span>);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;插入后：&quot;</span>);</span><br><span class=\"line\">    printlist(*(L-&gt;next));</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//前插</span></span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n在第3个位置前插33\\n&quot;</span>);</span><br><span class=\"line\">    p=GetElement(L,<span class=\"number\">3</span>);</span><br><span class=\"line\">    InsertPriorNode(p,<span class=\"number\">33</span>);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;插入后：&quot;</span>);</span><br><span class=\"line\">    printlist(*(L-&gt;next));</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//按位删除</span></span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n删除第1个位置\\n&quot;</span>);</span><br><span class=\"line\">    <span class=\"type\">int</span> e;</span><br><span class=\"line\">    ListDelete(L,<span class=\"number\">1</span>,&amp;e);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;删除后：&quot;</span>);</span><br><span class=\"line\">    printlist(*(L-&gt;next));</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//逆置</span></span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n逆置\\n&quot;</span>);</span><br><span class=\"line\">    Reverse(&amp;L);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;逆置后：&quot;</span>);</span><br><span class=\"line\">    printlist(*(L-&gt;next));</span><br><span class=\"line\">    </span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">test_nohead</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">    Linklist L=<span class=\"literal\">NULL</span>;</span><br><span class=\"line\">    L=HeadInsertNoHead(L);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n\\n\\n不带头结点测试：&quot;</span>);</span><br><span class=\"line\">    printlist(*L);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//按位插入</span></span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n在第1个位置插入11\\n&quot;</span>);</span><br><span class=\"line\">    ListInsert_nohead(&amp;L,<span class=\"number\">1</span>,<span class=\"number\">11</span>);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;插入后：&quot;</span>);</span><br><span class=\"line\">    printlist(*L);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n在第2个位置插入22\\n&quot;</span>);</span><br><span class=\"line\">    ListInsert_nohead(&amp;L,<span class=\"number\">2</span>,<span class=\"number\">22</span>);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;插入后：&quot;</span>);</span><br><span class=\"line\">    printlist(*L);</span><br><span class=\"line\"></span><br><span class=\"line\">    </span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">    test_head();</span><br><span class=\"line\">    test_nohead();</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//按位插入_头</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">ListInsert_head</span><span class=\"params\">(Linklist L, <span class=\"type\">int</span> i, <span class=\"type\">int</span> e)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(i&lt;<span class=\"number\">1</span>) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    LNode *p=GetElement(L,i<span class=\"number\">-1</span>);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> InsertNextNode(p,e);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//按位插入_不头</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">ListInsert_nohead</span><span class=\"params\">(Linklist *L,<span class=\"type\">int</span> i, <span class=\"type\">int</span> e)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(i&lt;<span class=\"number\">1</span>) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(i==<span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        LNode *s=(LNode*)<span class=\"built_in\">malloc</span>(<span class=\"keyword\">sizeof</span>(LNode));</span><br><span class=\"line\">        s-&gt;data=e;</span><br><span class=\"line\">        s-&gt;next=*L;</span><br><span class=\"line\">        *L=s;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    LNode *p=*L;</span><br><span class=\"line\">    <span class=\"type\">int</span> j=<span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(;p!=<span class=\"literal\">NULL</span> &amp;&amp; j&lt;i<span class=\"number\">-1</span>;j++) p=p-&gt;next;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(p==<span class=\"literal\">NULL</span>) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    LNode *s=(LNode*)<span class=\"built_in\">malloc</span>(<span class=\"keyword\">sizeof</span>(LNode));</span><br><span class=\"line\">    s-&gt;data=e;</span><br><span class=\"line\">    s-&gt;next=p-&gt;next;<span class=\"comment\">//顺序不能换        </span></span><br><span class=\"line\">    p-&gt;next=s;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//后插</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">InsertNextNode</span><span class=\"params\">(LNode *p, <span class=\"type\">int</span> e)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(p==<span class=\"literal\">NULL</span>) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    LNode *s=(LNode*)<span class=\"built_in\">malloc</span>(<span class=\"keyword\">sizeof</span>(LNode));</span><br><span class=\"line\">    s-&gt;data=e;</span><br><span class=\"line\">    s-&gt;next=p-&gt;next;</span><br><span class=\"line\">    p-&gt;next=s;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//前插</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">InsertPriorNode</span><span class=\"params\">(LNode *p, <span class=\"type\">int</span> e)</span>&#123;<span class=\"comment\">//只需交换数据域</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span>(p==<span class=\"literal\">NULL</span>) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    LNode *s=(LNode*)<span class=\"built_in\">malloc</span>(<span class=\"keyword\">sizeof</span>(LNode));</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(s==<span class=\"literal\">NULL</span>) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    s-&gt;next=p-&gt;next;</span><br><span class=\"line\">    p-&gt;next=s;</span><br><span class=\"line\">    s-&gt;data=p-&gt;data;</span><br><span class=\"line\">    p-&gt;data=e;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//按位删除</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">ListDelete</span><span class=\"params\">(Linklist L, <span class=\"type\">int</span> i, <span class=\"type\">int</span> *e)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(i&lt;<span class=\"number\">1</span>) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    LNode *p=L;</span><br><span class=\"line\">    <span class=\"type\">int</span> j=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(;p!=<span class=\"literal\">NULL</span> &amp;&amp; j&lt;i<span class=\"number\">-1</span>;j++) p=p-&gt;next;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(p==<span class=\"literal\">NULL</span>) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    LNode *q=p-&gt;next;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(q==<span class=\"literal\">NULL</span>) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    p-&gt;next=q-&gt;next;</span><br><span class=\"line\">    *e=q-&gt;data;</span><br><span class=\"line\">    <span class=\"built_in\">free</span>(q);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//逆置</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">Reverse</span><span class=\"params\">(Linklist *L)</span>&#123;</span><br><span class=\"line\">    LNode *p=(*L)-&gt;next;</span><br><span class=\"line\">    (*L)-&gt;next=<span class=\"literal\">NULL</span>;</span><br><span class=\"line\">    LNode *q;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(p!=<span class=\"literal\">NULL</span>)&#123;</span><br><span class=\"line\">        q=p-&gt;next;</span><br><span class=\"line\">        p-&gt;next=(*L)-&gt;next;</span><br><span class=\"line\">        (*L)-&gt;next=p;</span><br><span class=\"line\">        p=q;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<p><img src=\"/../../../../themes/yilia/source/img/datastruct/2_linearlist/1.png\"><br><img src=\"/img/datastruct/2_linearlist/1.png\" alt=\"数据结构\"></p>\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/2_linearlist/linklist/1.png\"><br><img src=\"/img/datastruct/2_linearlist/linklist/1.png\" alt=\"数据结构\"></p>\n<h3 id=\"2-4-1-定义\"><a href=\"#2-4-1-定义\" class=\"headerlink\" title=\"2.4.1 定义\"></a>2.4.1 定义</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/2_linearlist/linklist/2.png\"><br><img src=\"/img/datastruct/2_linearlist/linklist/2.png\" alt=\"数据结构\"></p>\n<table>\n<thead>\n<tr>\n<th align=\"center\"></th>\n<th align=\"center\">顺序表</th>\n<th align=\"center\">单链表</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">结构</td>\n<td align=\"center\"><img src=\"/../../../../themes/yilia/source/img/datastruct/2_linearlist/linklist/3.png\"><img src=\"/img/datastruct/2_linearlist/linklist/3.png\" alt=\"数据结构\"></td>\n<td align=\"center\"><img src=\"/../../../../themes/yilia/source/img/datastruct/2_linearlist/linklist/4.png\"><img src=\"/img/datastruct/2_linearlist/linklist/4.png\" alt=\"数据结构\"></td>\n</tr>\n<tr>\n<td align=\"center\">优点</td>\n<td align=\"center\">随机存取，存储密度高</td>\n<td align=\"center\">不要求大片连续空间，方便改容量</td>\n</tr>\n<tr>\n<td align=\"center\">缺点</td>\n<td align=\"center\">要求大片连续空间，改容量不便</td>\n<td align=\"center\">不可随机存取，额外指针空间</td>\n</tr>\n</tbody></table>\n<table>\n<thead>\n<tr>\n<th align=\"center\">带头结点</th>\n<th align=\"center\">不带头结点</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\"></td>\n<td align=\"center\">写代码更麻烦</td>\n</tr>\n<tr>\n<td align=\"center\"></td>\n<td align=\"center\">对第一个结点和后续结点的处理逻辑不同</td>\n</tr>\n<tr>\n<td align=\"center\"></td>\n<td align=\"center\">对空表和非空表的处理需要用不同的逻辑</td>\n</tr>\n</tbody></table>\n<h3 id=\"2-4-2基本操作\"><a href=\"#2-4-2基本操作\" class=\"headerlink\" title=\"2.4.2基本操作\"></a>2.4.2基本操作</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/2_linearlist/linklist/5.png\"><br><img src=\"/img/datastruct/2_linearlist/linklist/5.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/2_linearlist/linklist/6.png\"><br><img src=\"/img/datastruct/2_linearlist/linklist/6.png\" alt=\"数据结构\"></p>\n<ul>\n<li>查找<ul>\n<li>按位查找</li>\n<li>按值查找</li>\n</ul>\n</li>\n<li>插入<ul>\n<li>位序插入</li>\n<li>指定结点后插</li>\n</ul>\n</li>\n<li>删除<ul>\n<li>按位删除</li>\n<li>删除指定节点</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br><span class=\"line\">194</span><br><span class=\"line\">195</span><br><span class=\"line\">196</span><br><span class=\"line\">197</span><br><span class=\"line\">198</span><br><span class=\"line\">199</span><br><span class=\"line\">200</span><br><span class=\"line\">201</span><br><span class=\"line\">202</span><br><span class=\"line\">203</span><br><span class=\"line\">204</span><br><span class=\"line\">205</span><br><span class=\"line\">206</span><br><span class=\"line\">207</span><br><span class=\"line\">208</span><br><span class=\"line\">209</span><br><span class=\"line\">210</span><br><span class=\"line\">211</span><br><span class=\"line\">212</span><br><span class=\"line\">213</span><br><span class=\"line\">214</span><br><span class=\"line\">215</span><br><span class=\"line\">216</span><br><span class=\"line\">217</span><br><span class=\"line\">218</span><br><span class=\"line\">219</span><br><span class=\"line\">220</span><br><span class=\"line\">221</span><br><span class=\"line\">222</span><br><span class=\"line\">223</span><br><span class=\"line\">224</span><br><span class=\"line\">225</span><br><span class=\"line\">226</span><br><span class=\"line\">227</span><br><span class=\"line\">228</span><br><span class=\"line\">229</span><br><span class=\"line\">230</span><br><span class=\"line\">231</span><br><span class=\"line\">232</span><br><span class=\"line\">233</span><br><span class=\"line\">234</span><br><span class=\"line\">235</span><br><span class=\"line\">236</span><br><span class=\"line\">237</span><br><span class=\"line\">238</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;stdlib.h&gt;</span></span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">LNode</span>&#123;</span></span><br><span class=\"line\">    <span class=\"type\">int</span> data;</span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">LNode</span> *<span class=\"title\">next</span>;</span></span><br><span class=\"line\">&#125; LNode, *Linklist;</span><br><span class=\"line\"><span class=\"comment\">//输出链表</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">printlist</span><span class=\"params\">(LNode L)</span>&#123;</span><br><span class=\"line\">    LNode* p=&amp;L;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(;p!=<span class=\"literal\">NULL</span>;p=p-&gt;next)</span><br><span class=\"line\">        <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;%d &quot;</span>,p-&gt;data);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//头插法-带头结点</span></span><br><span class=\"line\">Linklist <span class=\"title function_\">HeadInsert</span><span class=\"params\">(Linklist L)</span>&#123;</span><br><span class=\"line\">    LNode *s;</span><br><span class=\"line\">    <span class=\"type\">int</span> i=<span class=\"number\">0</span>,x=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"type\">int</span> input[<span class=\"number\">6</span>]=&#123;<span class=\"number\">5</span>,<span class=\"number\">4</span>,<span class=\"number\">3</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>,<span class=\"number\">-1</span>&#125;;</span><br><span class=\"line\">    L=(Linklist)<span class=\"built_in\">malloc</span>(<span class=\"keyword\">sizeof</span>(LNode));</span><br><span class=\"line\">    L-&gt;next=<span class=\"literal\">NULL</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(;input[i]!=<span class=\"number\">-1</span>;i++)&#123;</span><br><span class=\"line\">        InsertNextNode(L,input[i]);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> L;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//尾插法-带头结点</span></span><br><span class=\"line\">Linklist <span class=\"title function_\">TailInsert</span><span class=\"params\">(Linklist L)</span>&#123;</span><br><span class=\"line\">    LNode *s,*r;</span><br><span class=\"line\">    <span class=\"type\">int</span> i=<span class=\"number\">0</span>,x=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"type\">int</span> input[<span class=\"number\">6</span>]=&#123;<span class=\"number\">5</span>,<span class=\"number\">4</span>,<span class=\"number\">3</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>,<span class=\"number\">-1</span>&#125;;</span><br><span class=\"line\">    L=(Linklist)<span class=\"built_in\">malloc</span>(<span class=\"keyword\">sizeof</span>(LNode));</span><br><span class=\"line\">    L-&gt;next=<span class=\"literal\">NULL</span>;</span><br><span class=\"line\">    r=L;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(;input[i]!=<span class=\"number\">-1</span>;i++)&#123;</span><br><span class=\"line\">        s=(LNode*)<span class=\"built_in\">malloc</span>(<span class=\"keyword\">sizeof</span>(LNode));</span><br><span class=\"line\">        s-&gt;data=input[i];</span><br><span class=\"line\">        r-&gt;next=s;</span><br><span class=\"line\">        r=s;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    r-&gt;next=<span class=\"literal\">NULL</span>;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> L;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//尾插法-不带头结点</span></span><br><span class=\"line\">Linklist <span class=\"title function_\">HeadInsertNoHead</span><span class=\"params\">(Linklist L)</span>&#123;</span><br><span class=\"line\">    LNode *s;</span><br><span class=\"line\">    <span class=\"type\">int</span> i=<span class=\"number\">1</span>,x=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"type\">int</span> input[<span class=\"number\">6</span>]=&#123;<span class=\"number\">5</span>,<span class=\"number\">4</span>,<span class=\"number\">3</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>,<span class=\"number\">-1</span>&#125;;</span><br><span class=\"line\">    L=(Linklist)<span class=\"built_in\">malloc</span>(<span class=\"keyword\">sizeof</span>(LNode));</span><br><span class=\"line\">    L-&gt;data=input[<span class=\"number\">0</span>];</span><br><span class=\"line\">    L-&gt;next=<span class=\"literal\">NULL</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(;input[i]!=<span class=\"number\">-1</span>;i++)&#123;</span><br><span class=\"line\">        s=(LNode*)<span class=\"built_in\">malloc</span>(<span class=\"keyword\">sizeof</span>(LNode));</span><br><span class=\"line\">        s-&gt;next=L-&gt;next;</span><br><span class=\"line\">        L-&gt;next=s;</span><br><span class=\"line\">        s-&gt;data=input[i];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> L;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//按位查找</span></span><br><span class=\"line\">LNode* <span class=\"title function_\">GetElement</span><span class=\"params\">(Linklist L,<span class=\"type\">int</span> i)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(i&lt;<span class=\"number\">0</span>) <span class=\"keyword\">return</span> <span class=\"literal\">NULL</span>;</span><br><span class=\"line\">    <span class=\"type\">int</span> j=<span class=\"number\">0</span>;</span><br><span class=\"line\">    LNode* p=L;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(;p!=<span class=\"literal\">NULL</span> &amp;&amp; j&lt;i;j++)</span><br><span class=\"line\">        p=p-&gt;next;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> p;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//按值查找</span></span><br><span class=\"line\">LNode* <span class=\"title function_\">LocateElem</span><span class=\"params\">(Linklist L,<span class=\"type\">int</span> e)</span>&#123;</span><br><span class=\"line\">    LNode* p=L-&gt;next;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(;p!=<span class=\"literal\">NULL</span> &amp;&amp; p-&gt;data!=e;p=p-&gt;next);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> p;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//求</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">Length</span><span class=\"params\">(Linklist L)</span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> i=<span class=\"number\">0</span>;</span><br><span class=\"line\">    LNode* p=L;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(;p!=<span class=\"literal\">NULL</span>;p=p-&gt;next)</span><br><span class=\"line\">        i++;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> i;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">test_head</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;带头结点测试：&quot;</span>);</span><br><span class=\"line\">    Linklist L=<span class=\"literal\">NULL</span>;</span><br><span class=\"line\">    L=HeadInsert(L);</span><br><span class=\"line\">    printlist(*(L-&gt;next));</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//按位查找</span></span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n第3个位置的元素为：%d\\n&quot;</span>,GetElement(L,<span class=\"number\">3</span>)-&gt;data);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//按位插入</span></span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n在第1个位置插入11\\n&quot;</span>);</span><br><span class=\"line\">    ListInsert_head(L,<span class=\"number\">1</span>,<span class=\"number\">11</span>);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;插入后：&quot;</span>);</span><br><span class=\"line\">    printlist(*(L-&gt;next));    </span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//后插</span></span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n在第2个位置后插22\\n&quot;</span>);</span><br><span class=\"line\">    LNode *p=GetElement(L,<span class=\"number\">2</span>);</span><br><span class=\"line\">    InsertNextNode(p,<span class=\"number\">22</span>);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;插入后：&quot;</span>);</span><br><span class=\"line\">    printlist(*(L-&gt;next));</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//前插</span></span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n在第3个位置前插33\\n&quot;</span>);</span><br><span class=\"line\">    p=GetElement(L,<span class=\"number\">3</span>);</span><br><span class=\"line\">    InsertPriorNode(p,<span class=\"number\">33</span>);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;插入后：&quot;</span>);</span><br><span class=\"line\">    printlist(*(L-&gt;next));</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//按位删除</span></span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n删除第1个位置\\n&quot;</span>);</span><br><span class=\"line\">    <span class=\"type\">int</span> e;</span><br><span class=\"line\">    ListDelete(L,<span class=\"number\">1</span>,&amp;e);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;删除后：&quot;</span>);</span><br><span class=\"line\">    printlist(*(L-&gt;next));</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//逆置</span></span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n逆置\\n&quot;</span>);</span><br><span class=\"line\">    Reverse(&amp;L);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;逆置后：&quot;</span>);</span><br><span class=\"line\">    printlist(*(L-&gt;next));</span><br><span class=\"line\">    </span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">test_nohead</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">    Linklist L=<span class=\"literal\">NULL</span>;</span><br><span class=\"line\">    L=HeadInsertNoHead(L);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n\\n\\n不带头结点测试：&quot;</span>);</span><br><span class=\"line\">    printlist(*L);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//按位插入</span></span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n在第1个位置插入11\\n&quot;</span>);</span><br><span class=\"line\">    ListInsert_nohead(&amp;L,<span class=\"number\">1</span>,<span class=\"number\">11</span>);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;插入后：&quot;</span>);</span><br><span class=\"line\">    printlist(*L);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n在第2个位置插入22\\n&quot;</span>);</span><br><span class=\"line\">    ListInsert_nohead(&amp;L,<span class=\"number\">2</span>,<span class=\"number\">22</span>);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;插入后：&quot;</span>);</span><br><span class=\"line\">    printlist(*L);</span><br><span class=\"line\"></span><br><span class=\"line\">    </span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">    test_head();</span><br><span class=\"line\">    test_nohead();</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//按位插入_头</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">ListInsert_head</span><span class=\"params\">(Linklist L, <span class=\"type\">int</span> i, <span class=\"type\">int</span> e)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(i&lt;<span class=\"number\">1</span>) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    LNode *p=GetElement(L,i<span class=\"number\">-1</span>);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> InsertNextNode(p,e);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//按位插入_不头</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">ListInsert_nohead</span><span class=\"params\">(Linklist *L,<span class=\"type\">int</span> i, <span class=\"type\">int</span> e)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(i&lt;<span class=\"number\">1</span>) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(i==<span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        LNode *s=(LNode*)<span class=\"built_in\">malloc</span>(<span class=\"keyword\">sizeof</span>(LNode));</span><br><span class=\"line\">        s-&gt;data=e;</span><br><span class=\"line\">        s-&gt;next=*L;</span><br><span class=\"line\">        *L=s;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    LNode *p=*L;</span><br><span class=\"line\">    <span class=\"type\">int</span> j=<span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(;p!=<span class=\"literal\">NULL</span> &amp;&amp; j&lt;i<span class=\"number\">-1</span>;j++) p=p-&gt;next;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(p==<span class=\"literal\">NULL</span>) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    LNode *s=(LNode*)<span class=\"built_in\">malloc</span>(<span class=\"keyword\">sizeof</span>(LNode));</span><br><span class=\"line\">    s-&gt;data=e;</span><br><span class=\"line\">    s-&gt;next=p-&gt;next;<span class=\"comment\">//顺序不能换        </span></span><br><span class=\"line\">    p-&gt;next=s;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//后插</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">InsertNextNode</span><span class=\"params\">(LNode *p, <span class=\"type\">int</span> e)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(p==<span class=\"literal\">NULL</span>) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    LNode *s=(LNode*)<span class=\"built_in\">malloc</span>(<span class=\"keyword\">sizeof</span>(LNode));</span><br><span class=\"line\">    s-&gt;data=e;</span><br><span class=\"line\">    s-&gt;next=p-&gt;next;</span><br><span class=\"line\">    p-&gt;next=s;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//前插</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">InsertPriorNode</span><span class=\"params\">(LNode *p, <span class=\"type\">int</span> e)</span>&#123;<span class=\"comment\">//只需交换数据域</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span>(p==<span class=\"literal\">NULL</span>) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    LNode *s=(LNode*)<span class=\"built_in\">malloc</span>(<span class=\"keyword\">sizeof</span>(LNode));</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(s==<span class=\"literal\">NULL</span>) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    s-&gt;next=p-&gt;next;</span><br><span class=\"line\">    p-&gt;next=s;</span><br><span class=\"line\">    s-&gt;data=p-&gt;data;</span><br><span class=\"line\">    p-&gt;data=e;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//按位删除</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">ListDelete</span><span class=\"params\">(Linklist L, <span class=\"type\">int</span> i, <span class=\"type\">int</span> *e)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(i&lt;<span class=\"number\">1</span>) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    LNode *p=L;</span><br><span class=\"line\">    <span class=\"type\">int</span> j=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(;p!=<span class=\"literal\">NULL</span> &amp;&amp; j&lt;i<span class=\"number\">-1</span>;j++) p=p-&gt;next;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(p==<span class=\"literal\">NULL</span>) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    LNode *q=p-&gt;next;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(q==<span class=\"literal\">NULL</span>) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    p-&gt;next=q-&gt;next;</span><br><span class=\"line\">    *e=q-&gt;data;</span><br><span class=\"line\">    <span class=\"built_in\">free</span>(q);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//逆置</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">Reverse</span><span class=\"params\">(Linklist *L)</span>&#123;</span><br><span class=\"line\">    LNode *p=(*L)-&gt;next;</span><br><span class=\"line\">    (*L)-&gt;next=<span class=\"literal\">NULL</span>;</span><br><span class=\"line\">    LNode *q;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(p!=<span class=\"literal\">NULL</span>)&#123;</span><br><span class=\"line\">        q=p-&gt;next;</span><br><span class=\"line\">        p-&gt;next=(*L)-&gt;next;</span><br><span class=\"line\">        (*L)-&gt;next=p;</span><br><span class=\"line\">        p=q;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>"},{"title":"2.1 线性表的定义","date":"2023-01-31T16:00:00.000Z","comment":false,"toc":true,"_content":"#\n<!--more-->\n![](../../../../themes/yilia/source/img/datastruct/2_linearlist/1.png)\n![数据结构](/img/datastruct/2_linearlist/1.png)\n\n\n## 2.1 定义\n- L=(a<sub>1</sub>,a<sub>2</sub>,a<sub>3</sub>,...,a<sub>n</sub>)\n- 有限个相同数据类型的数据元素的有序序列\n- 一些概念\n   \n   ·位序：数据元素在线性表中的位置\n   \n   ·表头：第一个元素a<sub>1</sub>\n\n   ·表尾：最后一个元素a<sub>n</sub>\n\n- 一些性质\n\n   ·除第一个元素外，每个元素有且仅有一个直接前驱\n   ·除最后一个元素外，每个元素有且仅有一个直接后继\n## 2.2 基本操作\n\n|函数|功能|说明|\n|:---:|:---:|:---:|\n|InitList(&L)|初始化|构造一个空的线性表L，分配内存空间|\n|DestroyList(&L)|销毁|销毁线性表，并释放内存空间|\n|ClearList(&L)|清空|清空线性表，保留内存空间|\n|Empty(L)|判空|判断线性表是否为空|\n|Length(L)|求长|返回线性表的长度|\n|GetElem(L,i,&e)|取值|返回线性表中第i个元素的值|\n|LocateElem(L,e,compare())|查找|返回线性表中第一个与e满足compare()的元素的位序|\n|PriorElem(L,cur_e,&pre_e)|前驱|返回线性表中元素cur_e的前驱元素的值|\n|NextElem(L,cur_e,&next_e)|后继|返回线性表中元素cur_e的后继元素的值|\n|ListInsert(&L,i,e)|插入|在线性表的第i个位置插入元素e|\n|ListDelete(&L,i,&e)|删除|删除线性表中第i个位置的元素，并返回其值|\n|ListTraverse(L,visit())|遍历|依次对线性表中每个元素调用visit()函数|\n\n## 2.3 顺序表\n![](../../../../themes/yilia/source/img/datastruct/2_linearlist/2.png)\n![数据结构](/img/datastruct/2_linearlist/2.png)\n### 2.3.1 顺序存储实现线性表\n- 静态分配\n```c\n/*\n实现顺序表\n静态分配\n*/\n#include<stdio.h>\n#define MaxSize 10\ntypedef struct\n{\n    int data[MaxSize];\n    int length;\n}SqList;\n\n//初始化\nvoid InitList(SqList* L){\n    L->length=0;//逻辑归零\n}\n\nint main(){\n    SqList L;\n    InitList(&L);\n    //违规访问\n    for(int i=0;i<MaxSize;i++){\n        printf(\"ta[%d]=%d\\n\",i,L.data[i]);\n    }\n    return 0;\n}\n```\n- 动态分配\n```c\n/*\n顺序表\n动态分配\n*/\n#include<stdio.h>\n#include<stdlib.h>\n#define InitSize 10 //默认最大长度\ntypedef struct\n{\n    int *data ;//指针，用于动态分配\n    int MaxSize; \n    int length;\n}SeqList;\n//初始化\nvoid InitList(SeqList *L){\n    //申请空间\n    L->data=(int*)malloc(InitSize*sizeof(int));\n    L->length=0;\n    L->MaxSize=InitSize;\n}\n\n//动态增长\nvoid IncreaseSize(SeqList *L, int len){\n    int* p=L->data;\n    L->data=(int *)malloc((L->MaxSize+len)*sizeof(int));\n    //将数据复制到新的区域\n    int i=0;\n    for(i=0;i<L->length;i++){\n        L->data[i]=p[i];\n    }\n    L->MaxSize=L->MaxSize+len;\n    free(p);\n}\n\nint main(){\n    SeqList L;\n    //初始化\n    InitList(&L);\n    printf(\"最大长度：%d\\n\",L.MaxSize);\n    //增长\n    IncreaseSize(&L,5);\n    printf(\"+5\\n\");\n    printf(\"最大长度：%d\",L.MaxSize);\n    return 0;\n}\n```\n- 基本操作\n![](../../../../themes/yilia/source/img/datastruct/2_linearlist/3.png)\n![数据结构](/img/datastruct/2_linearlist/3.png)\n![](../../../../themes/yilia/source/img/datastruct/2_linearlist/4.png)\n![数据结构](/img/datastruct/2_linearlist/4.png)\n```c\n/*\n顺序表\n静态分配\n*/\n#include<stdio.h>\n#define MaxSize 10\ntypedef struct\n{\n    int data[MaxSize];\n    int length;\n}SqList;\n//初始化\nvoid InitList(SqList* L){\n    L->length=0;//逻辑上置零\n}\nvoid PrintList(SqList L){\n    //判空\n    if(L.data==NULL || L.length==0)\n        return;\n    int i=0;\n    for(i=0;i<L.length;i++)\n        printf(\"%d \",L.data[i]);\n}\n\nint main(){\n    SqList L;\n    InitList(&L);\n    ListInsert(&L,1,1);\n    ListInsert(&L,2,2);\n    ListInsert(&L,3,4);\n    ListInsert(&L,4,5);\n    printf(\"前：\");\n    PrintList(L);\n    ListInsert(&L,3,3);\n    printf(\"\\n位序：3，插入：3\");\n    printf(\"\\n后：\");\n    PrintList(L);\n\n    //删除\n    int x=0;\n    ListDelete(&L,1,&x);\n    printf(\"\\n删除第%d个元素：%d\",1,x);\n    printf(\"\\n删除后：\");\n    PrintList(L);\n\n    ListDelete(&L,2,&x);\n    printf(\"\\n删除第%d个元素：%d\",2,x);\n    printf(\"\\n删除后：\");\n    PrintList(L);\n\n    printf(\"\\n删除第%d个元素：%d\",L.length,x);\n    ListDelete(&L,L.length,&x);\n    printf(\"\\n删除后：\");\n    PrintList(L);\n\n    //按位查找\n    printf(\"\\n第1个：%d\",GetElement(L,0));\n    printf(\"\\n第2个：%d\",GetElement(L,1));\n\n    //按值查找\n    printf(\"\\n2的位序：%d\",LocateElement(L,2));\n    printf(\"\\n4的位序：：%d\",LocateElement(L,4));\n    printf(\"\\n5的位序：：%d\",LocateElement(L,5));\n    return 0;\n}\n\n//位序插入\nint ListInsert(SqList *L,int i, int e){\n    //满否\n    if (L->length>=MaxSize || i<1 || i>L->length+1) return 0;\n    int j=0;\n    for(j=L->length;j>=i;j--)//后移\n        L->data[j]=L->data[j-1];\n    L->data[j]=e;//插入\n    return ++(L->length);\n}\n\n//删除\nint ListDelete(SqList *L,int i, int *e){\n    if(i<1|| i>L->length) return 0;\n    *e=L->data[i-1];\n    int j=i;\n    for(;j<L->length;j++) L->data[j-1]=L->data[j];\n    L->length--;\n    return 1;\n}\n\n//按位查找\nint GetElement(SqList L,int i){\n    if(i<1 || i>L.length) return 0;\n    return L.data[i-1];\n}\n\n//按值查找\nint LocateElement(SqList L,int e){\n    int i=0;\n    for(;i<L.length;i++)\n        if(L.data[i]==e)\n            return i+1;\n    return -1;\n}\n```\n\n\n||按位插入/删除||\n|:---:|:---:|:---:|\n|最好|尾插|O(1)|\n|最坏|头插|O(n)|\n|平均|$$p=\\frac{1}{n+1}$$ $$1p+2p+...+np$$|O(n)|\n\n||按位查找|按值查找|\n|:---:|:---:|:---:|\n|最好|O(1)|O(1)|\n|最坏|O(1)|O(n)|\n|平均|O(1)|O(n)|","source":"_posts/datastruct/2_linearlist/1_linearlist.md","raw":"---\ntitle: 2.1 线性表的定义\ndate: 2023-02-01 00:00:00\ntags: [数据结构,线性表]\ncategories: [数据结构]\ncomment: false\ntoc: true\n---\n#\n<!--more-->\n![](../../../../themes/yilia/source/img/datastruct/2_linearlist/1.png)\n![数据结构](/img/datastruct/2_linearlist/1.png)\n\n\n## 2.1 定义\n- L=(a<sub>1</sub>,a<sub>2</sub>,a<sub>3</sub>,...,a<sub>n</sub>)\n- 有限个相同数据类型的数据元素的有序序列\n- 一些概念\n   \n   ·位序：数据元素在线性表中的位置\n   \n   ·表头：第一个元素a<sub>1</sub>\n\n   ·表尾：最后一个元素a<sub>n</sub>\n\n- 一些性质\n\n   ·除第一个元素外，每个元素有且仅有一个直接前驱\n   ·除最后一个元素外，每个元素有且仅有一个直接后继\n## 2.2 基本操作\n\n|函数|功能|说明|\n|:---:|:---:|:---:|\n|InitList(&L)|初始化|构造一个空的线性表L，分配内存空间|\n|DestroyList(&L)|销毁|销毁线性表，并释放内存空间|\n|ClearList(&L)|清空|清空线性表，保留内存空间|\n|Empty(L)|判空|判断线性表是否为空|\n|Length(L)|求长|返回线性表的长度|\n|GetElem(L,i,&e)|取值|返回线性表中第i个元素的值|\n|LocateElem(L,e,compare())|查找|返回线性表中第一个与e满足compare()的元素的位序|\n|PriorElem(L,cur_e,&pre_e)|前驱|返回线性表中元素cur_e的前驱元素的值|\n|NextElem(L,cur_e,&next_e)|后继|返回线性表中元素cur_e的后继元素的值|\n|ListInsert(&L,i,e)|插入|在线性表的第i个位置插入元素e|\n|ListDelete(&L,i,&e)|删除|删除线性表中第i个位置的元素，并返回其值|\n|ListTraverse(L,visit())|遍历|依次对线性表中每个元素调用visit()函数|\n\n## 2.3 顺序表\n![](../../../../themes/yilia/source/img/datastruct/2_linearlist/2.png)\n![数据结构](/img/datastruct/2_linearlist/2.png)\n### 2.3.1 顺序存储实现线性表\n- 静态分配\n```c\n/*\n实现顺序表\n静态分配\n*/\n#include<stdio.h>\n#define MaxSize 10\ntypedef struct\n{\n    int data[MaxSize];\n    int length;\n}SqList;\n\n//初始化\nvoid InitList(SqList* L){\n    L->length=0;//逻辑归零\n}\n\nint main(){\n    SqList L;\n    InitList(&L);\n    //违规访问\n    for(int i=0;i<MaxSize;i++){\n        printf(\"ta[%d]=%d\\n\",i,L.data[i]);\n    }\n    return 0;\n}\n```\n- 动态分配\n```c\n/*\n顺序表\n动态分配\n*/\n#include<stdio.h>\n#include<stdlib.h>\n#define InitSize 10 //默认最大长度\ntypedef struct\n{\n    int *data ;//指针，用于动态分配\n    int MaxSize; \n    int length;\n}SeqList;\n//初始化\nvoid InitList(SeqList *L){\n    //申请空间\n    L->data=(int*)malloc(InitSize*sizeof(int));\n    L->length=0;\n    L->MaxSize=InitSize;\n}\n\n//动态增长\nvoid IncreaseSize(SeqList *L, int len){\n    int* p=L->data;\n    L->data=(int *)malloc((L->MaxSize+len)*sizeof(int));\n    //将数据复制到新的区域\n    int i=0;\n    for(i=0;i<L->length;i++){\n        L->data[i]=p[i];\n    }\n    L->MaxSize=L->MaxSize+len;\n    free(p);\n}\n\nint main(){\n    SeqList L;\n    //初始化\n    InitList(&L);\n    printf(\"最大长度：%d\\n\",L.MaxSize);\n    //增长\n    IncreaseSize(&L,5);\n    printf(\"+5\\n\");\n    printf(\"最大长度：%d\",L.MaxSize);\n    return 0;\n}\n```\n- 基本操作\n![](../../../../themes/yilia/source/img/datastruct/2_linearlist/3.png)\n![数据结构](/img/datastruct/2_linearlist/3.png)\n![](../../../../themes/yilia/source/img/datastruct/2_linearlist/4.png)\n![数据结构](/img/datastruct/2_linearlist/4.png)\n```c\n/*\n顺序表\n静态分配\n*/\n#include<stdio.h>\n#define MaxSize 10\ntypedef struct\n{\n    int data[MaxSize];\n    int length;\n}SqList;\n//初始化\nvoid InitList(SqList* L){\n    L->length=0;//逻辑上置零\n}\nvoid PrintList(SqList L){\n    //判空\n    if(L.data==NULL || L.length==0)\n        return;\n    int i=0;\n    for(i=0;i<L.length;i++)\n        printf(\"%d \",L.data[i]);\n}\n\nint main(){\n    SqList L;\n    InitList(&L);\n    ListInsert(&L,1,1);\n    ListInsert(&L,2,2);\n    ListInsert(&L,3,4);\n    ListInsert(&L,4,5);\n    printf(\"前：\");\n    PrintList(L);\n    ListInsert(&L,3,3);\n    printf(\"\\n位序：3，插入：3\");\n    printf(\"\\n后：\");\n    PrintList(L);\n\n    //删除\n    int x=0;\n    ListDelete(&L,1,&x);\n    printf(\"\\n删除第%d个元素：%d\",1,x);\n    printf(\"\\n删除后：\");\n    PrintList(L);\n\n    ListDelete(&L,2,&x);\n    printf(\"\\n删除第%d个元素：%d\",2,x);\n    printf(\"\\n删除后：\");\n    PrintList(L);\n\n    printf(\"\\n删除第%d个元素：%d\",L.length,x);\n    ListDelete(&L,L.length,&x);\n    printf(\"\\n删除后：\");\n    PrintList(L);\n\n    //按位查找\n    printf(\"\\n第1个：%d\",GetElement(L,0));\n    printf(\"\\n第2个：%d\",GetElement(L,1));\n\n    //按值查找\n    printf(\"\\n2的位序：%d\",LocateElement(L,2));\n    printf(\"\\n4的位序：：%d\",LocateElement(L,4));\n    printf(\"\\n5的位序：：%d\",LocateElement(L,5));\n    return 0;\n}\n\n//位序插入\nint ListInsert(SqList *L,int i, int e){\n    //满否\n    if (L->length>=MaxSize || i<1 || i>L->length+1) return 0;\n    int j=0;\n    for(j=L->length;j>=i;j--)//后移\n        L->data[j]=L->data[j-1];\n    L->data[j]=e;//插入\n    return ++(L->length);\n}\n\n//删除\nint ListDelete(SqList *L,int i, int *e){\n    if(i<1|| i>L->length) return 0;\n    *e=L->data[i-1];\n    int j=i;\n    for(;j<L->length;j++) L->data[j-1]=L->data[j];\n    L->length--;\n    return 1;\n}\n\n//按位查找\nint GetElement(SqList L,int i){\n    if(i<1 || i>L.length) return 0;\n    return L.data[i-1];\n}\n\n//按值查找\nint LocateElement(SqList L,int e){\n    int i=0;\n    for(;i<L.length;i++)\n        if(L.data[i]==e)\n            return i+1;\n    return -1;\n}\n```\n\n\n||按位插入/删除||\n|:---:|:---:|:---:|\n|最好|尾插|O(1)|\n|最坏|头插|O(n)|\n|平均|$$p=\\frac{1}{n+1}$$ $$1p+2p+...+np$$|O(n)|\n\n||按位查找|按值查找|\n|:---:|:---:|:---:|\n|最好|O(1)|O(1)|\n|最坏|O(1)|O(n)|\n|平均|O(1)|O(n)|","slug":"datastruct/2_linearlist/1_linearlist","published":1,"updated":"2023-10-23T12:02:34.209Z","comments":1,"layout":"post","photos":[],"_id":"clvulgd5q000q5ovw711kdnsv","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/2_linearlist/1.png\"><br><img src=\"/img/datastruct/2_linearlist/1.png\" alt=\"数据结构\"></p>\n<h2 id=\"2-1-定义\"><a href=\"#2-1-定义\" class=\"headerlink\" title=\"2.1 定义\"></a>2.1 定义</h2><ul>\n<li><p>L&#x3D;(a<sub>1</sub>,a<sub>2</sub>,a<sub>3</sub>,…,a<sub>n</sub>)</p>\n</li>\n<li><p>有限个相同数据类型的数据元素的有序序列</p>\n</li>\n<li><p>一些概念</p>\n<p> ·位序：数据元素在线性表中的位置</p>\n<p> ·表头：第一个元素a<sub>1</sub></p>\n<p> ·表尾：最后一个元素a<sub>n</sub></p>\n</li>\n<li><p>一些性质</p>\n<p> ·除第一个元素外，每个元素有且仅有一个直接前驱<br> ·除最后一个元素外，每个元素有且仅有一个直接后继</p>\n</li>\n</ul>\n<h2 id=\"2-2-基本操作\"><a href=\"#2-2-基本操作\" class=\"headerlink\" title=\"2.2 基本操作\"></a>2.2 基本操作</h2><table>\n<thead>\n<tr>\n<th align=\"center\">函数</th>\n<th align=\"center\">功能</th>\n<th align=\"center\">说明</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">InitList(&amp;L)</td>\n<td align=\"center\">初始化</td>\n<td align=\"center\">构造一个空的线性表L，分配内存空间</td>\n</tr>\n<tr>\n<td align=\"center\">DestroyList(&amp;L)</td>\n<td align=\"center\">销毁</td>\n<td align=\"center\">销毁线性表，并释放内存空间</td>\n</tr>\n<tr>\n<td align=\"center\">ClearList(&amp;L)</td>\n<td align=\"center\">清空</td>\n<td align=\"center\">清空线性表，保留内存空间</td>\n</tr>\n<tr>\n<td align=\"center\">Empty(L)</td>\n<td align=\"center\">判空</td>\n<td align=\"center\">判断线性表是否为空</td>\n</tr>\n<tr>\n<td align=\"center\">Length(L)</td>\n<td align=\"center\">求长</td>\n<td align=\"center\">返回线性表的长度</td>\n</tr>\n<tr>\n<td align=\"center\">GetElem(L,i,&amp;e)</td>\n<td align=\"center\">取值</td>\n<td align=\"center\">返回线性表中第i个元素的值</td>\n</tr>\n<tr>\n<td align=\"center\">LocateElem(L,e,compare())</td>\n<td align=\"center\">查找</td>\n<td align=\"center\">返回线性表中第一个与e满足compare()的元素的位序</td>\n</tr>\n<tr>\n<td align=\"center\">PriorElem(L,cur_e,&amp;pre_e)</td>\n<td align=\"center\">前驱</td>\n<td align=\"center\">返回线性表中元素cur_e的前驱元素的值</td>\n</tr>\n<tr>\n<td align=\"center\">NextElem(L,cur_e,&amp;next_e)</td>\n<td align=\"center\">后继</td>\n<td align=\"center\">返回线性表中元素cur_e的后继元素的值</td>\n</tr>\n<tr>\n<td align=\"center\">ListInsert(&amp;L,i,e)</td>\n<td align=\"center\">插入</td>\n<td align=\"center\">在线性表的第i个位置插入元素e</td>\n</tr>\n<tr>\n<td align=\"center\">ListDelete(&amp;L,i,&amp;e)</td>\n<td align=\"center\">删除</td>\n<td align=\"center\">删除线性表中第i个位置的元素，并返回其值</td>\n</tr>\n<tr>\n<td align=\"center\">ListTraverse(L,visit())</td>\n<td align=\"center\">遍历</td>\n<td align=\"center\">依次对线性表中每个元素调用visit()函数</td>\n</tr>\n</tbody></table>\n<h2 id=\"2-3-顺序表\"><a href=\"#2-3-顺序表\" class=\"headerlink\" title=\"2.3 顺序表\"></a>2.3 顺序表</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/2_linearlist/2.png\"><br><img src=\"/img/datastruct/2_linearlist/2.png\" alt=\"数据结构\"></p>\n<h3 id=\"2-3-1-顺序存储实现线性表\"><a href=\"#2-3-1-顺序存储实现线性表\" class=\"headerlink\" title=\"2.3.1 顺序存储实现线性表\"></a>2.3.1 顺序存储实现线性表</h3><ul>\n<li>静态分配<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">实现顺序表</span></span><br><span class=\"line\"><span class=\"comment\">静态分配</span></span><br><span class=\"line\"><span class=\"comment\">*/</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> MaxSize 10</span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    <span class=\"type\">int</span> data[MaxSize];</span><br><span class=\"line\">    <span class=\"type\">int</span> length;</span><br><span class=\"line\">&#125;SqList;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//初始化</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">InitList</span><span class=\"params\">(SqList* L)</span>&#123;</span><br><span class=\"line\">    L-&gt;length=<span class=\"number\">0</span>;<span class=\"comment\">//逻辑归零</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">    SqList L;</span><br><span class=\"line\">    InitList(&amp;L);</span><br><span class=\"line\">    <span class=\"comment\">//违规访问</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">0</span>;i&lt;MaxSize;i++)&#123;</span><br><span class=\"line\">        <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;ta[%d]=%d\\n&quot;</span>,i,L.data[i]);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n<li>动态分配<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">顺序表</span></span><br><span class=\"line\"><span class=\"comment\">动态分配</span></span><br><span class=\"line\"><span class=\"comment\">*/</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;stdlib.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> InitSize 10 <span class=\"comment\">//默认最大长度</span></span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    <span class=\"type\">int</span> *data ;<span class=\"comment\">//指针，用于动态分配</span></span><br><span class=\"line\">    <span class=\"type\">int</span> MaxSize; </span><br><span class=\"line\">    <span class=\"type\">int</span> length;</span><br><span class=\"line\">&#125;SeqList;</span><br><span class=\"line\"><span class=\"comment\">//初始化</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">InitList</span><span class=\"params\">(SeqList *L)</span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">//申请空间</span></span><br><span class=\"line\">    L-&gt;data=(<span class=\"type\">int</span>*)<span class=\"built_in\">malloc</span>(InitSize*<span class=\"keyword\">sizeof</span>(<span class=\"type\">int</span>));</span><br><span class=\"line\">    L-&gt;length=<span class=\"number\">0</span>;</span><br><span class=\"line\">    L-&gt;MaxSize=InitSize;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//动态增长</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">IncreaseSize</span><span class=\"params\">(SeqList *L, <span class=\"type\">int</span> len)</span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span>* p=L-&gt;data;</span><br><span class=\"line\">    L-&gt;data=(<span class=\"type\">int</span> *)<span class=\"built_in\">malloc</span>((L-&gt;MaxSize+len)*<span class=\"keyword\">sizeof</span>(<span class=\"type\">int</span>));</span><br><span class=\"line\">    <span class=\"comment\">//将数据复制到新的区域</span></span><br><span class=\"line\">    <span class=\"type\">int</span> i=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(i=<span class=\"number\">0</span>;i&lt;L-&gt;length;i++)&#123;</span><br><span class=\"line\">        L-&gt;data[i]=p[i];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    L-&gt;MaxSize=L-&gt;MaxSize+len;</span><br><span class=\"line\">    <span class=\"built_in\">free</span>(p);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">    SeqList L;</span><br><span class=\"line\">    <span class=\"comment\">//初始化</span></span><br><span class=\"line\">    InitList(&amp;L);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;最大长度：%d\\n&quot;</span>,L.MaxSize);</span><br><span class=\"line\">    <span class=\"comment\">//增长</span></span><br><span class=\"line\">    IncreaseSize(&amp;L,<span class=\"number\">5</span>);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;+5\\n&quot;</span>);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;最大长度：%d&quot;</span>,L.MaxSize);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n<li>基本操作<br><img src=\"/../../../../themes/yilia/source/img/datastruct/2_linearlist/3.png\"><br><img src=\"/img/datastruct/2_linearlist/3.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/2_linearlist/4.png\"><br><img src=\"/img/datastruct/2_linearlist/4.png\" alt=\"数据结构\"><figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">顺序表</span></span><br><span class=\"line\"><span class=\"comment\">静态分配</span></span><br><span class=\"line\"><span class=\"comment\">*/</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> MaxSize 10</span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    <span class=\"type\">int</span> data[MaxSize];</span><br><span class=\"line\">    <span class=\"type\">int</span> length;</span><br><span class=\"line\">&#125;SqList;</span><br><span class=\"line\"><span class=\"comment\">//初始化</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">InitList</span><span class=\"params\">(SqList* L)</span>&#123;</span><br><span class=\"line\">    L-&gt;length=<span class=\"number\">0</span>;<span class=\"comment\">//逻辑上置零</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">PrintList</span><span class=\"params\">(SqList L)</span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">//判空</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span>(L.data==<span class=\"literal\">NULL</span> || L.length==<span class=\"number\">0</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span>;</span><br><span class=\"line\">    <span class=\"type\">int</span> i=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(i=<span class=\"number\">0</span>;i&lt;L.length;i++)</span><br><span class=\"line\">        <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;%d &quot;</span>,L.data[i]);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">    SqList L;</span><br><span class=\"line\">    InitList(&amp;L);</span><br><span class=\"line\">    ListInsert(&amp;L,<span class=\"number\">1</span>,<span class=\"number\">1</span>);</span><br><span class=\"line\">    ListInsert(&amp;L,<span class=\"number\">2</span>,<span class=\"number\">2</span>);</span><br><span class=\"line\">    ListInsert(&amp;L,<span class=\"number\">3</span>,<span class=\"number\">4</span>);</span><br><span class=\"line\">    ListInsert(&amp;L,<span class=\"number\">4</span>,<span class=\"number\">5</span>);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;前：&quot;</span>);</span><br><span class=\"line\">    PrintList(L);</span><br><span class=\"line\">    ListInsert(&amp;L,<span class=\"number\">3</span>,<span class=\"number\">3</span>);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n位序：3，插入：3&quot;</span>);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n后：&quot;</span>);</span><br><span class=\"line\">    PrintList(L);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//删除</span></span><br><span class=\"line\">    <span class=\"type\">int</span> x=<span class=\"number\">0</span>;</span><br><span class=\"line\">    ListDelete(&amp;L,<span class=\"number\">1</span>,&amp;x);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n删除第%d个元素：%d&quot;</span>,<span class=\"number\">1</span>,x);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n删除后：&quot;</span>);</span><br><span class=\"line\">    PrintList(L);</span><br><span class=\"line\"></span><br><span class=\"line\">    ListDelete(&amp;L,<span class=\"number\">2</span>,&amp;x);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n删除第%d个元素：%d&quot;</span>,<span class=\"number\">2</span>,x);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n删除后：&quot;</span>);</span><br><span class=\"line\">    PrintList(L);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n删除第%d个元素：%d&quot;</span>,L.length,x);</span><br><span class=\"line\">    ListDelete(&amp;L,L.length,&amp;x);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n删除后：&quot;</span>);</span><br><span class=\"line\">    PrintList(L);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//按位查找</span></span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n第1个：%d&quot;</span>,GetElement(L,<span class=\"number\">0</span>));</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n第2个：%d&quot;</span>,GetElement(L,<span class=\"number\">1</span>));</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//按值查找</span></span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n2的位序：%d&quot;</span>,LocateElement(L,<span class=\"number\">2</span>));</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n4的位序：：%d&quot;</span>,LocateElement(L,<span class=\"number\">4</span>));</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n5的位序：：%d&quot;</span>,LocateElement(L,<span class=\"number\">5</span>));</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//位序插入</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">ListInsert</span><span class=\"params\">(SqList *L,<span class=\"type\">int</span> i, <span class=\"type\">int</span> e)</span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">//满否</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (L-&gt;length&gt;=MaxSize || i&lt;<span class=\"number\">1</span> || i&gt;L-&gt;length+<span class=\"number\">1</span>) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"type\">int</span> j=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(j=L-&gt;length;j&gt;=i;j--)<span class=\"comment\">//后移</span></span><br><span class=\"line\">        L-&gt;data[j]=L-&gt;data[j<span class=\"number\">-1</span>];</span><br><span class=\"line\">    L-&gt;data[j]=e;<span class=\"comment\">//插入</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> ++(L-&gt;length);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//删除</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">ListDelete</span><span class=\"params\">(SqList *L,<span class=\"type\">int</span> i, <span class=\"type\">int</span> *e)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(i&lt;<span class=\"number\">1</span>|| i&gt;L-&gt;length) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    *e=L-&gt;data[i<span class=\"number\">-1</span>];</span><br><span class=\"line\">    <span class=\"type\">int</span> j=i;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(;j&lt;L-&gt;length;j++) L-&gt;data[j<span class=\"number\">-1</span>]=L-&gt;data[j];</span><br><span class=\"line\">    L-&gt;length--;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//按位查找</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">GetElement</span><span class=\"params\">(SqList L,<span class=\"type\">int</span> i)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(i&lt;<span class=\"number\">1</span> || i&gt;L.length) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> L.data[i<span class=\"number\">-1</span>];</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//按值查找</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">LocateElement</span><span class=\"params\">(SqList L,<span class=\"type\">int</span> e)</span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> i=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(;i&lt;L.length;i++)</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(L.data[i]==e)</span><br><span class=\"line\">            <span class=\"keyword\">return</span> i+<span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">-1</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n</ul>\n<table>\n<thead>\n<tr>\n<th align=\"center\"></th>\n<th align=\"center\">按位插入&#x2F;删除</th>\n<th align=\"center\"></th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">最好</td>\n<td align=\"center\">尾插</td>\n<td align=\"center\">O(1)</td>\n</tr>\n<tr>\n<td align=\"center\">最坏</td>\n<td align=\"center\">头插</td>\n<td align=\"center\">O(n)</td>\n</tr>\n<tr>\n<td align=\"center\">平均</td>\n<td align=\"center\">$$p&#x3D;\\frac{1}{n+1}$$ $$1p+2p+…+np$$</td>\n<td align=\"center\">O(n)</td>\n</tr>\n</tbody></table>\n<table>\n<thead>\n<tr>\n<th align=\"center\"></th>\n<th align=\"center\">按位查找</th>\n<th align=\"center\">按值查找</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">最好</td>\n<td align=\"center\">O(1)</td>\n<td align=\"center\">O(1)</td>\n</tr>\n<tr>\n<td align=\"center\">最坏</td>\n<td align=\"center\">O(1)</td>\n<td align=\"center\">O(n)</td>\n</tr>\n<tr>\n<td align=\"center\">平均</td>\n<td align=\"center\">O(1)</td>\n<td align=\"center\">O(n)</td>\n</tr>\n</tbody></table>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<p><img src=\"/../../../../themes/yilia/source/img/datastruct/2_linearlist/1.png\"><br><img src=\"/img/datastruct/2_linearlist/1.png\" alt=\"数据结构\"></p>\n<h2 id=\"2-1-定义\"><a href=\"#2-1-定义\" class=\"headerlink\" title=\"2.1 定义\"></a>2.1 定义</h2><ul>\n<li><p>L&#x3D;(a<sub>1</sub>,a<sub>2</sub>,a<sub>3</sub>,…,a<sub>n</sub>)</p>\n</li>\n<li><p>有限个相同数据类型的数据元素的有序序列</p>\n</li>\n<li><p>一些概念</p>\n<p> ·位序：数据元素在线性表中的位置</p>\n<p> ·表头：第一个元素a<sub>1</sub></p>\n<p> ·表尾：最后一个元素a<sub>n</sub></p>\n</li>\n<li><p>一些性质</p>\n<p> ·除第一个元素外，每个元素有且仅有一个直接前驱<br> ·除最后一个元素外，每个元素有且仅有一个直接后继</p>\n</li>\n</ul>\n<h2 id=\"2-2-基本操作\"><a href=\"#2-2-基本操作\" class=\"headerlink\" title=\"2.2 基本操作\"></a>2.2 基本操作</h2><table>\n<thead>\n<tr>\n<th align=\"center\">函数</th>\n<th align=\"center\">功能</th>\n<th align=\"center\">说明</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">InitList(&amp;L)</td>\n<td align=\"center\">初始化</td>\n<td align=\"center\">构造一个空的线性表L，分配内存空间</td>\n</tr>\n<tr>\n<td align=\"center\">DestroyList(&amp;L)</td>\n<td align=\"center\">销毁</td>\n<td align=\"center\">销毁线性表，并释放内存空间</td>\n</tr>\n<tr>\n<td align=\"center\">ClearList(&amp;L)</td>\n<td align=\"center\">清空</td>\n<td align=\"center\">清空线性表，保留内存空间</td>\n</tr>\n<tr>\n<td align=\"center\">Empty(L)</td>\n<td align=\"center\">判空</td>\n<td align=\"center\">判断线性表是否为空</td>\n</tr>\n<tr>\n<td align=\"center\">Length(L)</td>\n<td align=\"center\">求长</td>\n<td align=\"center\">返回线性表的长度</td>\n</tr>\n<tr>\n<td align=\"center\">GetElem(L,i,&amp;e)</td>\n<td align=\"center\">取值</td>\n<td align=\"center\">返回线性表中第i个元素的值</td>\n</tr>\n<tr>\n<td align=\"center\">LocateElem(L,e,compare())</td>\n<td align=\"center\">查找</td>\n<td align=\"center\">返回线性表中第一个与e满足compare()的元素的位序</td>\n</tr>\n<tr>\n<td align=\"center\">PriorElem(L,cur_e,&amp;pre_e)</td>\n<td align=\"center\">前驱</td>\n<td align=\"center\">返回线性表中元素cur_e的前驱元素的值</td>\n</tr>\n<tr>\n<td align=\"center\">NextElem(L,cur_e,&amp;next_e)</td>\n<td align=\"center\">后继</td>\n<td align=\"center\">返回线性表中元素cur_e的后继元素的值</td>\n</tr>\n<tr>\n<td align=\"center\">ListInsert(&amp;L,i,e)</td>\n<td align=\"center\">插入</td>\n<td align=\"center\">在线性表的第i个位置插入元素e</td>\n</tr>\n<tr>\n<td align=\"center\">ListDelete(&amp;L,i,&amp;e)</td>\n<td align=\"center\">删除</td>\n<td align=\"center\">删除线性表中第i个位置的元素，并返回其值</td>\n</tr>\n<tr>\n<td align=\"center\">ListTraverse(L,visit())</td>\n<td align=\"center\">遍历</td>\n<td align=\"center\">依次对线性表中每个元素调用visit()函数</td>\n</tr>\n</tbody></table>\n<h2 id=\"2-3-顺序表\"><a href=\"#2-3-顺序表\" class=\"headerlink\" title=\"2.3 顺序表\"></a>2.3 顺序表</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/2_linearlist/2.png\"><br><img src=\"/img/datastruct/2_linearlist/2.png\" alt=\"数据结构\"></p>\n<h3 id=\"2-3-1-顺序存储实现线性表\"><a href=\"#2-3-1-顺序存储实现线性表\" class=\"headerlink\" title=\"2.3.1 顺序存储实现线性表\"></a>2.3.1 顺序存储实现线性表</h3><ul>\n<li>静态分配<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">实现顺序表</span></span><br><span class=\"line\"><span class=\"comment\">静态分配</span></span><br><span class=\"line\"><span class=\"comment\">*/</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> MaxSize 10</span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    <span class=\"type\">int</span> data[MaxSize];</span><br><span class=\"line\">    <span class=\"type\">int</span> length;</span><br><span class=\"line\">&#125;SqList;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//初始化</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">InitList</span><span class=\"params\">(SqList* L)</span>&#123;</span><br><span class=\"line\">    L-&gt;length=<span class=\"number\">0</span>;<span class=\"comment\">//逻辑归零</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">    SqList L;</span><br><span class=\"line\">    InitList(&amp;L);</span><br><span class=\"line\">    <span class=\"comment\">//违规访问</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">0</span>;i&lt;MaxSize;i++)&#123;</span><br><span class=\"line\">        <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;ta[%d]=%d\\n&quot;</span>,i,L.data[i]);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n<li>动态分配<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">顺序表</span></span><br><span class=\"line\"><span class=\"comment\">动态分配</span></span><br><span class=\"line\"><span class=\"comment\">*/</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;stdlib.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> InitSize 10 <span class=\"comment\">//默认最大长度</span></span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    <span class=\"type\">int</span> *data ;<span class=\"comment\">//指针，用于动态分配</span></span><br><span class=\"line\">    <span class=\"type\">int</span> MaxSize; </span><br><span class=\"line\">    <span class=\"type\">int</span> length;</span><br><span class=\"line\">&#125;SeqList;</span><br><span class=\"line\"><span class=\"comment\">//初始化</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">InitList</span><span class=\"params\">(SeqList *L)</span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">//申请空间</span></span><br><span class=\"line\">    L-&gt;data=(<span class=\"type\">int</span>*)<span class=\"built_in\">malloc</span>(InitSize*<span class=\"keyword\">sizeof</span>(<span class=\"type\">int</span>));</span><br><span class=\"line\">    L-&gt;length=<span class=\"number\">0</span>;</span><br><span class=\"line\">    L-&gt;MaxSize=InitSize;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//动态增长</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">IncreaseSize</span><span class=\"params\">(SeqList *L, <span class=\"type\">int</span> len)</span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span>* p=L-&gt;data;</span><br><span class=\"line\">    L-&gt;data=(<span class=\"type\">int</span> *)<span class=\"built_in\">malloc</span>((L-&gt;MaxSize+len)*<span class=\"keyword\">sizeof</span>(<span class=\"type\">int</span>));</span><br><span class=\"line\">    <span class=\"comment\">//将数据复制到新的区域</span></span><br><span class=\"line\">    <span class=\"type\">int</span> i=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(i=<span class=\"number\">0</span>;i&lt;L-&gt;length;i++)&#123;</span><br><span class=\"line\">        L-&gt;data[i]=p[i];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    L-&gt;MaxSize=L-&gt;MaxSize+len;</span><br><span class=\"line\">    <span class=\"built_in\">free</span>(p);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">    SeqList L;</span><br><span class=\"line\">    <span class=\"comment\">//初始化</span></span><br><span class=\"line\">    InitList(&amp;L);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;最大长度：%d\\n&quot;</span>,L.MaxSize);</span><br><span class=\"line\">    <span class=\"comment\">//增长</span></span><br><span class=\"line\">    IncreaseSize(&amp;L,<span class=\"number\">5</span>);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;+5\\n&quot;</span>);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;最大长度：%d&quot;</span>,L.MaxSize);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n<li>基本操作<br><img src=\"/../../../../themes/yilia/source/img/datastruct/2_linearlist/3.png\"><br><img src=\"/img/datastruct/2_linearlist/3.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/2_linearlist/4.png\"><br><img src=\"/img/datastruct/2_linearlist/4.png\" alt=\"数据结构\"><figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">顺序表</span></span><br><span class=\"line\"><span class=\"comment\">静态分配</span></span><br><span class=\"line\"><span class=\"comment\">*/</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> MaxSize 10</span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    <span class=\"type\">int</span> data[MaxSize];</span><br><span class=\"line\">    <span class=\"type\">int</span> length;</span><br><span class=\"line\">&#125;SqList;</span><br><span class=\"line\"><span class=\"comment\">//初始化</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">InitList</span><span class=\"params\">(SqList* L)</span>&#123;</span><br><span class=\"line\">    L-&gt;length=<span class=\"number\">0</span>;<span class=\"comment\">//逻辑上置零</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">PrintList</span><span class=\"params\">(SqList L)</span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">//判空</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span>(L.data==<span class=\"literal\">NULL</span> || L.length==<span class=\"number\">0</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span>;</span><br><span class=\"line\">    <span class=\"type\">int</span> i=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(i=<span class=\"number\">0</span>;i&lt;L.length;i++)</span><br><span class=\"line\">        <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;%d &quot;</span>,L.data[i]);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">    SqList L;</span><br><span class=\"line\">    InitList(&amp;L);</span><br><span class=\"line\">    ListInsert(&amp;L,<span class=\"number\">1</span>,<span class=\"number\">1</span>);</span><br><span class=\"line\">    ListInsert(&amp;L,<span class=\"number\">2</span>,<span class=\"number\">2</span>);</span><br><span class=\"line\">    ListInsert(&amp;L,<span class=\"number\">3</span>,<span class=\"number\">4</span>);</span><br><span class=\"line\">    ListInsert(&amp;L,<span class=\"number\">4</span>,<span class=\"number\">5</span>);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;前：&quot;</span>);</span><br><span class=\"line\">    PrintList(L);</span><br><span class=\"line\">    ListInsert(&amp;L,<span class=\"number\">3</span>,<span class=\"number\">3</span>);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n位序：3，插入：3&quot;</span>);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n后：&quot;</span>);</span><br><span class=\"line\">    PrintList(L);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//删除</span></span><br><span class=\"line\">    <span class=\"type\">int</span> x=<span class=\"number\">0</span>;</span><br><span class=\"line\">    ListDelete(&amp;L,<span class=\"number\">1</span>,&amp;x);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n删除第%d个元素：%d&quot;</span>,<span class=\"number\">1</span>,x);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n删除后：&quot;</span>);</span><br><span class=\"line\">    PrintList(L);</span><br><span class=\"line\"></span><br><span class=\"line\">    ListDelete(&amp;L,<span class=\"number\">2</span>,&amp;x);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n删除第%d个元素：%d&quot;</span>,<span class=\"number\">2</span>,x);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n删除后：&quot;</span>);</span><br><span class=\"line\">    PrintList(L);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n删除第%d个元素：%d&quot;</span>,L.length,x);</span><br><span class=\"line\">    ListDelete(&amp;L,L.length,&amp;x);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n删除后：&quot;</span>);</span><br><span class=\"line\">    PrintList(L);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//按位查找</span></span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n第1个：%d&quot;</span>,GetElement(L,<span class=\"number\">0</span>));</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n第2个：%d&quot;</span>,GetElement(L,<span class=\"number\">1</span>));</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//按值查找</span></span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n2的位序：%d&quot;</span>,LocateElement(L,<span class=\"number\">2</span>));</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n4的位序：：%d&quot;</span>,LocateElement(L,<span class=\"number\">4</span>));</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;\\n5的位序：：%d&quot;</span>,LocateElement(L,<span class=\"number\">5</span>));</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//位序插入</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">ListInsert</span><span class=\"params\">(SqList *L,<span class=\"type\">int</span> i, <span class=\"type\">int</span> e)</span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">//满否</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (L-&gt;length&gt;=MaxSize || i&lt;<span class=\"number\">1</span> || i&gt;L-&gt;length+<span class=\"number\">1</span>) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"type\">int</span> j=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(j=L-&gt;length;j&gt;=i;j--)<span class=\"comment\">//后移</span></span><br><span class=\"line\">        L-&gt;data[j]=L-&gt;data[j<span class=\"number\">-1</span>];</span><br><span class=\"line\">    L-&gt;data[j]=e;<span class=\"comment\">//插入</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> ++(L-&gt;length);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//删除</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">ListDelete</span><span class=\"params\">(SqList *L,<span class=\"type\">int</span> i, <span class=\"type\">int</span> *e)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(i&lt;<span class=\"number\">1</span>|| i&gt;L-&gt;length) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    *e=L-&gt;data[i<span class=\"number\">-1</span>];</span><br><span class=\"line\">    <span class=\"type\">int</span> j=i;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(;j&lt;L-&gt;length;j++) L-&gt;data[j<span class=\"number\">-1</span>]=L-&gt;data[j];</span><br><span class=\"line\">    L-&gt;length--;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//按位查找</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">GetElement</span><span class=\"params\">(SqList L,<span class=\"type\">int</span> i)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(i&lt;<span class=\"number\">1</span> || i&gt;L.length) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> L.data[i<span class=\"number\">-1</span>];</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//按值查找</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">LocateElement</span><span class=\"params\">(SqList L,<span class=\"type\">int</span> e)</span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> i=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(;i&lt;L.length;i++)</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(L.data[i]==e)</span><br><span class=\"line\">            <span class=\"keyword\">return</span> i+<span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">-1</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n</ul>\n<table>\n<thead>\n<tr>\n<th align=\"center\"></th>\n<th align=\"center\">按位插入&#x2F;删除</th>\n<th align=\"center\"></th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">最好</td>\n<td align=\"center\">尾插</td>\n<td align=\"center\">O(1)</td>\n</tr>\n<tr>\n<td align=\"center\">最坏</td>\n<td align=\"center\">头插</td>\n<td align=\"center\">O(n)</td>\n</tr>\n<tr>\n<td align=\"center\">平均</td>\n<td align=\"center\">$$p&#x3D;\\frac{1}{n+1}$$ $$1p+2p+…+np$$</td>\n<td align=\"center\">O(n)</td>\n</tr>\n</tbody></table>\n<table>\n<thead>\n<tr>\n<th align=\"center\"></th>\n<th align=\"center\">按位查找</th>\n<th align=\"center\">按值查找</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">最好</td>\n<td align=\"center\">O(1)</td>\n<td align=\"center\">O(1)</td>\n</tr>\n<tr>\n<td align=\"center\">最坏</td>\n<td align=\"center\">O(1)</td>\n<td align=\"center\">O(n)</td>\n</tr>\n<tr>\n<td align=\"center\">平均</td>\n<td align=\"center\">O(1)</td>\n<td align=\"center\">O(n)</td>\n</tr>\n</tbody></table>"},{"title":"4. 串","date":"2023-08-06T16:00:00.000Z","comment":false,"toc":true,"_content":"#\n<!--more-->\n\n### \n\n## 4.1 定义\n![](../../../../themes/yilia/source/img/datastruct/4_string/1.png)\n![数据结构](/img/datastruct/4_string/1.png)\n- 一种特殊的线性表\n\n## 4.2 串的存储结构\n![](../../../../themes/yilia/source/img/datastruct/4_string/2.png)\n![数据结构](/img/datastruct/4_string/2.png)\n\n- 顺序存储\n![](../../../../themes/yilia/source/img/datastruct/4_string/3.png)\n![数据结构](/img/datastruct/4_string/3.png)\n```c\n#define MAXLEN 255\ntypedef struct{\n    char ch[MAXLEN]; //静态数组\n    int length;\n} SString;\n\ntypedef struct{\n    char *ch; //动态数组\n    int length;\n} HString;\n```\n\n- 链式存储\n\n![](../../../../themes/yilia/source/img/datastruct/4_string/5.png)\n![数据结构](/img/datastruct/4_string/5.png)\n```c\n//2. 链式存储\ntypedef struct StringNode{\n    char ch; //1：1\n    struct StringNode *next;\n} StringNode, *String;\n\ntypedef struct StringNode{\n    char ch[4]; //1：4\n    struct StringNode *next;\n} StringNode, *String;\n```\n\n![](../../../../themes/yilia/source/img/datastruct/4_string/4.png)\n![数据结构](/img/datastruct/4_string/4.png)\n\n## 4.3 基本操作\n\n|函数名|功能|时间复杂度|\n|:---|:---|:---|\n|StrAssign(SString &T, char *chars)|生成一个其值等于chars的串T|O(n)|\n|StrCopy(SString &T, SString S)|由串S复制得串T|O(n)|\n|StrEmpty(SString S)|若S为空串，则返回true，否则返回false|O(1)|\n|StrLength(SString S)|返回串S的元素个数，即串的长度|O(1)|\n|ClearString(SString &S)|将串S清为空串|O(1)|\n|DestroyString(SString &S)|串S存在，则销毁它|O(1)|\n|Concat(SString &T, SString S1, SString S2)|用T返回由S1和S2联接而成的新串|O(n)|\n\n```c\n//求子串\nint SubString(SString *Sub, SString S, int pos, int len){\n    //1. 参数合法性判断\n    if(pos<1 || pos>S.length || len<0 || len>S.length-pos+1)\n        return 0;\n    //2. 子串赋值\n    for(int i=pos; i<pos+len; i++)\n        Sub->ch[i-pos+1]=S.ch[i];\n    Sub->length=len;\n    return 1;\n}\n\n//比较\nint StrCompare(SString S, SString T){\n    for(int i=1; i<=S.length && i<=T.length; i++){\n        if(S.ch[i]!=T.ch[i])\n            return S.ch[i]-T.ch[i];\n    }\n    return S.length-T.length;\n}\n\n//定位\nint Index(SString S, SString T){\n    int i=1, n=S.length, m=T.length;\n    SString sub;\n    while(i<=n-m+1){\n        SubString(&sub, S, i, m);\n        if(StrCompare(sub, T)!=0)\n            ++i;\n        else\n            return i;\n    }\n    return 0;\n}\n```\n\n## 4.4 模式匹配\n![](../../../../themes/yilia/source/img/datastruct/4_string/6.png)\n![数据结构](/img/datastruct/4_string/6.png)\n\n### 4.4.1 朴素模式匹配算法\n\n```c\n//朴素模式匹配算法\nint Index(SString S, SString T){\n    int i=1, n=S.length, m=T.length;\n    while(i<=n-m+1){\n        int j=1;\n        while(j<=m){\n            if(S.ch[i+j-1]==T.ch[j])\n                ++j;\n            else\n                break;\n        }\n        if(j>m)\n            return i;\n        else\n            ++i;\n    }\n    return 0;\n}\n//定位算法中，子串的长度是固定的，而模式匹配算法中，子串的长度是可变的\n```\n### 4.4.2 KMP模式匹配算法\n![](../../../../themes/yilia/source/img/datastruct/4_string/7.png)\n![数据结构](/img/datastruct/4_string/7.png)\n![](../../../../themes/yilia/source/img/datastruct/4_string/8.png)\n![数据结构](/img/datastruct/4_string/8.png)\n\n- 1）根据模式求next数组\n![](../../../../themes/yilia/source/img/datastruct/4_string/9.png)\n![数据结构](/img/datastruct/4_string/9.png)\n![](../../../../themes/yilia/source/img/datastruct/4_string/11.png)\n![数据结构](/img/datastruct/4_string/11.png)\n\n![](../../../../themes/yilia/source/img/datastruct/4_string/22.png)\n![数据结构](/img/datastruct/4_string/22.png)\n\n- 算法优化\n![](../../../../themes/yilia/source/img/datastruct/4_string/33.png)\n![数据结构](/img/datastruct/4_string/33.png)\n\nnext[3]=1 , next[3]与next[1]相等，所以转到next[1]后也会失配，所以next[3]可以改为next[1]=0\n\n```c\n//KMP算法\n//求next\nvoid get_next(SString T, int next[]){\n    //1. 初始化\n    int i=1, j=0;\n    next[1]=0;\n    while(i<T.length){\n        if(j==0 || T.ch[i]==T.ch[j]){\n            ++i;\n            ++j;\n            next[i]=j;\n        }\n        else\n            j=next[j];\n    }\n}\n\n//求nextval\nvoid get_nextval(SString T, int nextval[]){\n    //1. 初始化\n    int i=1, j=0;\n    nextval[1]=0;\n    while(i<T.length){\n        if(j==0 || T.ch[i]==T.ch[j]){\n            ++i;\n            ++j;\n            if(T.ch[i]!=T.ch[j])\n                nextval[i]=j;\n            else\n                nextval[i]=nextval[j];\n        }\n        else\n            j=nextval[j];\n    }\n}\n\n\nint Index_KMP(SString S, SString T, int next[]){\n    int i=1, j=1;\n    while(i<=S.length && j<=T.length){\n        if(j==0 || S.ch[i]==T.ch[j]){\n            ++i;\n            ++j;\n        }\n        else\n            j=next[j];//i不回溯\n    }\n    if(j>T.length)\n        return i-T.length;\n    else\n        return 0;\n}\n```","source":"_posts/datastruct/4_string/1_string.md","raw":"---\ntitle: 4. 串\ndate: 2023-08-07 00:00:00\ntags: [数据结构,串]\ncategories: [数据结构]\ncomment: false\ntoc: true\n---\n#\n<!--more-->\n\n### \n\n## 4.1 定义\n![](../../../../themes/yilia/source/img/datastruct/4_string/1.png)\n![数据结构](/img/datastruct/4_string/1.png)\n- 一种特殊的线性表\n\n## 4.2 串的存储结构\n![](../../../../themes/yilia/source/img/datastruct/4_string/2.png)\n![数据结构](/img/datastruct/4_string/2.png)\n\n- 顺序存储\n![](../../../../themes/yilia/source/img/datastruct/4_string/3.png)\n![数据结构](/img/datastruct/4_string/3.png)\n```c\n#define MAXLEN 255\ntypedef struct{\n    char ch[MAXLEN]; //静态数组\n    int length;\n} SString;\n\ntypedef struct{\n    char *ch; //动态数组\n    int length;\n} HString;\n```\n\n- 链式存储\n\n![](../../../../themes/yilia/source/img/datastruct/4_string/5.png)\n![数据结构](/img/datastruct/4_string/5.png)\n```c\n//2. 链式存储\ntypedef struct StringNode{\n    char ch; //1：1\n    struct StringNode *next;\n} StringNode, *String;\n\ntypedef struct StringNode{\n    char ch[4]; //1：4\n    struct StringNode *next;\n} StringNode, *String;\n```\n\n![](../../../../themes/yilia/source/img/datastruct/4_string/4.png)\n![数据结构](/img/datastruct/4_string/4.png)\n\n## 4.3 基本操作\n\n|函数名|功能|时间复杂度|\n|:---|:---|:---|\n|StrAssign(SString &T, char *chars)|生成一个其值等于chars的串T|O(n)|\n|StrCopy(SString &T, SString S)|由串S复制得串T|O(n)|\n|StrEmpty(SString S)|若S为空串，则返回true，否则返回false|O(1)|\n|StrLength(SString S)|返回串S的元素个数，即串的长度|O(1)|\n|ClearString(SString &S)|将串S清为空串|O(1)|\n|DestroyString(SString &S)|串S存在，则销毁它|O(1)|\n|Concat(SString &T, SString S1, SString S2)|用T返回由S1和S2联接而成的新串|O(n)|\n\n```c\n//求子串\nint SubString(SString *Sub, SString S, int pos, int len){\n    //1. 参数合法性判断\n    if(pos<1 || pos>S.length || len<0 || len>S.length-pos+1)\n        return 0;\n    //2. 子串赋值\n    for(int i=pos; i<pos+len; i++)\n        Sub->ch[i-pos+1]=S.ch[i];\n    Sub->length=len;\n    return 1;\n}\n\n//比较\nint StrCompare(SString S, SString T){\n    for(int i=1; i<=S.length && i<=T.length; i++){\n        if(S.ch[i]!=T.ch[i])\n            return S.ch[i]-T.ch[i];\n    }\n    return S.length-T.length;\n}\n\n//定位\nint Index(SString S, SString T){\n    int i=1, n=S.length, m=T.length;\n    SString sub;\n    while(i<=n-m+1){\n        SubString(&sub, S, i, m);\n        if(StrCompare(sub, T)!=0)\n            ++i;\n        else\n            return i;\n    }\n    return 0;\n}\n```\n\n## 4.4 模式匹配\n![](../../../../themes/yilia/source/img/datastruct/4_string/6.png)\n![数据结构](/img/datastruct/4_string/6.png)\n\n### 4.4.1 朴素模式匹配算法\n\n```c\n//朴素模式匹配算法\nint Index(SString S, SString T){\n    int i=1, n=S.length, m=T.length;\n    while(i<=n-m+1){\n        int j=1;\n        while(j<=m){\n            if(S.ch[i+j-1]==T.ch[j])\n                ++j;\n            else\n                break;\n        }\n        if(j>m)\n            return i;\n        else\n            ++i;\n    }\n    return 0;\n}\n//定位算法中，子串的长度是固定的，而模式匹配算法中，子串的长度是可变的\n```\n### 4.4.2 KMP模式匹配算法\n![](../../../../themes/yilia/source/img/datastruct/4_string/7.png)\n![数据结构](/img/datastruct/4_string/7.png)\n![](../../../../themes/yilia/source/img/datastruct/4_string/8.png)\n![数据结构](/img/datastruct/4_string/8.png)\n\n- 1）根据模式求next数组\n![](../../../../themes/yilia/source/img/datastruct/4_string/9.png)\n![数据结构](/img/datastruct/4_string/9.png)\n![](../../../../themes/yilia/source/img/datastruct/4_string/11.png)\n![数据结构](/img/datastruct/4_string/11.png)\n\n![](../../../../themes/yilia/source/img/datastruct/4_string/22.png)\n![数据结构](/img/datastruct/4_string/22.png)\n\n- 算法优化\n![](../../../../themes/yilia/source/img/datastruct/4_string/33.png)\n![数据结构](/img/datastruct/4_string/33.png)\n\nnext[3]=1 , next[3]与next[1]相等，所以转到next[1]后也会失配，所以next[3]可以改为next[1]=0\n\n```c\n//KMP算法\n//求next\nvoid get_next(SString T, int next[]){\n    //1. 初始化\n    int i=1, j=0;\n    next[1]=0;\n    while(i<T.length){\n        if(j==0 || T.ch[i]==T.ch[j]){\n            ++i;\n            ++j;\n            next[i]=j;\n        }\n        else\n            j=next[j];\n    }\n}\n\n//求nextval\nvoid get_nextval(SString T, int nextval[]){\n    //1. 初始化\n    int i=1, j=0;\n    nextval[1]=0;\n    while(i<T.length){\n        if(j==0 || T.ch[i]==T.ch[j]){\n            ++i;\n            ++j;\n            if(T.ch[i]!=T.ch[j])\n                nextval[i]=j;\n            else\n                nextval[i]=nextval[j];\n        }\n        else\n            j=nextval[j];\n    }\n}\n\n\nint Index_KMP(SString S, SString T, int next[]){\n    int i=1, j=1;\n    while(i<=S.length && j<=T.length){\n        if(j==0 || S.ch[i]==T.ch[j]){\n            ++i;\n            ++j;\n        }\n        else\n            j=next[j];//i不回溯\n    }\n    if(j>T.length)\n        return i-T.length;\n    else\n        return 0;\n}\n```","slug":"datastruct/4_string/1_string","published":1,"updated":"2023-10-23T12:15:39.435Z","comments":1,"layout":"post","photos":[],"_id":"clvulgd5q000r5ovw922r30zr","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h3 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a></h3><h2 id=\"4-1-定义\"><a href=\"#4-1-定义\" class=\"headerlink\" title=\"4.1 定义\"></a>4.1 定义</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/4_string/1.png\"><br><img src=\"/img/datastruct/4_string/1.png\" alt=\"数据结构\"></p>\n<ul>\n<li>一种特殊的线性表</li>\n</ul>\n<h2 id=\"4-2-串的存储结构\"><a href=\"#4-2-串的存储结构\" class=\"headerlink\" title=\"4.2 串的存储结构\"></a>4.2 串的存储结构</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/4_string/2.png\"><br><img src=\"/img/datastruct/4_string/2.png\" alt=\"数据结构\"></p>\n<ul>\n<li><p>顺序存储<br><img src=\"/../../../../themes/yilia/source/img/datastruct/4_string/3.png\"><br><img src=\"/img/datastruct/4_string/3.png\" alt=\"数据结构\"></p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> MAXLEN 255</span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span>&#123;</span></span><br><span class=\"line\">    <span class=\"type\">char</span> ch[MAXLEN]; <span class=\"comment\">//静态数组</span></span><br><span class=\"line\">    <span class=\"type\">int</span> length;</span><br><span class=\"line\">&#125; SString;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span>&#123;</span></span><br><span class=\"line\">    <span class=\"type\">char</span> *ch; <span class=\"comment\">//动态数组</span></span><br><span class=\"line\">    <span class=\"type\">int</span> length;</span><br><span class=\"line\">&#125; HString;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>链式存储</p>\n</li>\n</ul>\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/4_string/5.png\"><br><img src=\"/img/datastruct/4_string/5.png\" alt=\"数据结构\"></p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//2. 链式存储</span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">StringNode</span>&#123;</span></span><br><span class=\"line\">    <span class=\"type\">char</span> ch; <span class=\"comment\">//1：1</span></span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">StringNode</span> *<span class=\"title\">next</span>;</span></span><br><span class=\"line\">&#125; StringNode, *String;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">StringNode</span>&#123;</span></span><br><span class=\"line\">    <span class=\"type\">char</span> ch[<span class=\"number\">4</span>]; <span class=\"comment\">//1：4</span></span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">StringNode</span> *<span class=\"title\">next</span>;</span></span><br><span class=\"line\">&#125; StringNode, *String;</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/4_string/4.png\"><br><img src=\"/img/datastruct/4_string/4.png\" alt=\"数据结构\"></p>\n<h2 id=\"4-3-基本操作\"><a href=\"#4-3-基本操作\" class=\"headerlink\" title=\"4.3 基本操作\"></a>4.3 基本操作</h2><table>\n<thead>\n<tr>\n<th align=\"left\">函数名</th>\n<th align=\"left\">功能</th>\n<th align=\"left\">时间复杂度</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">StrAssign(SString &amp;T, char *chars)</td>\n<td align=\"left\">生成一个其值等于chars的串T</td>\n<td align=\"left\">O(n)</td>\n</tr>\n<tr>\n<td align=\"left\">StrCopy(SString &amp;T, SString S)</td>\n<td align=\"left\">由串S复制得串T</td>\n<td align=\"left\">O(n)</td>\n</tr>\n<tr>\n<td align=\"left\">StrEmpty(SString S)</td>\n<td align=\"left\">若S为空串，则返回true，否则返回false</td>\n<td align=\"left\">O(1)</td>\n</tr>\n<tr>\n<td align=\"left\">StrLength(SString S)</td>\n<td align=\"left\">返回串S的元素个数，即串的长度</td>\n<td align=\"left\">O(1)</td>\n</tr>\n<tr>\n<td align=\"left\">ClearString(SString &amp;S)</td>\n<td align=\"left\">将串S清为空串</td>\n<td align=\"left\">O(1)</td>\n</tr>\n<tr>\n<td align=\"left\">DestroyString(SString &amp;S)</td>\n<td align=\"left\">串S存在，则销毁它</td>\n<td align=\"left\">O(1)</td>\n</tr>\n<tr>\n<td align=\"left\">Concat(SString &amp;T, SString S1, SString S2)</td>\n<td align=\"left\">用T返回由S1和S2联接而成的新串</td>\n<td align=\"left\">O(n)</td>\n</tr>\n</tbody></table>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//求子串</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">SubString</span><span class=\"params\">(SString *Sub, SString S, <span class=\"type\">int</span> pos, <span class=\"type\">int</span> len)</span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">//1. 参数合法性判断</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span>(pos&lt;<span class=\"number\">1</span> || pos&gt;S.length || len&lt;<span class=\"number\">0</span> || len&gt;S.length-pos+<span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"comment\">//2. 子串赋值</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=pos; i&lt;pos+len; i++)</span><br><span class=\"line\">        Sub-&gt;ch[i-pos+<span class=\"number\">1</span>]=S.ch[i];</span><br><span class=\"line\">    Sub-&gt;length=len;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//比较</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">StrCompare</span><span class=\"params\">(SString S, SString T)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">1</span>; i&lt;=S.length &amp;&amp; i&lt;=T.length; i++)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(S.ch[i]!=T.ch[i])</span><br><span class=\"line\">            <span class=\"keyword\">return</span> S.ch[i]-T.ch[i];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> S.length-T.length;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//定位</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">Index</span><span class=\"params\">(SString S, SString T)</span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> i=<span class=\"number\">1</span>, n=S.length, m=T.length;</span><br><span class=\"line\">    SString sub;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(i&lt;=n-m+<span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        SubString(&amp;sub, S, i, m);</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(StrCompare(sub, T)!=<span class=\"number\">0</span>)</span><br><span class=\"line\">            ++i;</span><br><span class=\"line\">        <span class=\"keyword\">else</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span> i;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"4-4-模式匹配\"><a href=\"#4-4-模式匹配\" class=\"headerlink\" title=\"4.4 模式匹配\"></a>4.4 模式匹配</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/4_string/6.png\"><br><img src=\"/img/datastruct/4_string/6.png\" alt=\"数据结构\"></p>\n<h3 id=\"4-4-1-朴素模式匹配算法\"><a href=\"#4-4-1-朴素模式匹配算法\" class=\"headerlink\" title=\"4.4.1 朴素模式匹配算法\"></a>4.4.1 朴素模式匹配算法</h3><figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//朴素模式匹配算法</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">Index</span><span class=\"params\">(SString S, SString T)</span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> i=<span class=\"number\">1</span>, n=S.length, m=T.length;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(i&lt;=n-m+<span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> j=<span class=\"number\">1</span>;</span><br><span class=\"line\">        <span class=\"keyword\">while</span>(j&lt;=m)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(S.ch[i+j<span class=\"number\">-1</span>]==T.ch[j])</span><br><span class=\"line\">                ++j;</span><br><span class=\"line\">            <span class=\"keyword\">else</span></span><br><span class=\"line\">                <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(j&gt;m)</span><br><span class=\"line\">            <span class=\"keyword\">return</span> i;</span><br><span class=\"line\">        <span class=\"keyword\">else</span></span><br><span class=\"line\">            ++i;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">//定位算法中，子串的长度是固定的，而模式匹配算法中，子串的长度是可变的</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"4-4-2-KMP模式匹配算法\"><a href=\"#4-4-2-KMP模式匹配算法\" class=\"headerlink\" title=\"4.4.2 KMP模式匹配算法\"></a>4.4.2 KMP模式匹配算法</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/4_string/7.png\"><br><img src=\"/img/datastruct/4_string/7.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/4_string/8.png\"><br><img src=\"/img/datastruct/4_string/8.png\" alt=\"数据结构\"></p>\n<ul>\n<li>1）根据模式求next数组<br><img src=\"/../../../../themes/yilia/source/img/datastruct/4_string/9.png\"><br><img src=\"/img/datastruct/4_string/9.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/4_string/11.png\"><br><img src=\"/img/datastruct/4_string/11.png\" alt=\"数据结构\"></li>\n</ul>\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/4_string/22.png\"><br><img src=\"/img/datastruct/4_string/22.png\" alt=\"数据结构\"></p>\n<ul>\n<li>算法优化<br><img src=\"/../../../../themes/yilia/source/img/datastruct/4_string/33.png\"><br><img src=\"/img/datastruct/4_string/33.png\" alt=\"数据结构\"></li>\n</ul>\n<p>next[3]&#x3D;1 , next[3]与next[1]相等，所以转到next[1]后也会失配，所以next[3]可以改为next[1]&#x3D;0</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//KMP算法</span></span><br><span class=\"line\"><span class=\"comment\">//求next</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">get_next</span><span class=\"params\">(SString T, <span class=\"type\">int</span> next[])</span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">//1. 初始化</span></span><br><span class=\"line\">    <span class=\"type\">int</span> i=<span class=\"number\">1</span>, j=<span class=\"number\">0</span>;</span><br><span class=\"line\">    next[<span class=\"number\">1</span>]=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(i&lt;T.length)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(j==<span class=\"number\">0</span> || T.ch[i]==T.ch[j])&#123;</span><br><span class=\"line\">            ++i;</span><br><span class=\"line\">            ++j;</span><br><span class=\"line\">            next[i]=j;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">else</span></span><br><span class=\"line\">            j=next[j];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//求nextval</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">get_nextval</span><span class=\"params\">(SString T, <span class=\"type\">int</span> nextval[])</span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">//1. 初始化</span></span><br><span class=\"line\">    <span class=\"type\">int</span> i=<span class=\"number\">1</span>, j=<span class=\"number\">0</span>;</span><br><span class=\"line\">    nextval[<span class=\"number\">1</span>]=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(i&lt;T.length)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(j==<span class=\"number\">0</span> || T.ch[i]==T.ch[j])&#123;</span><br><span class=\"line\">            ++i;</span><br><span class=\"line\">            ++j;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(T.ch[i]!=T.ch[j])</span><br><span class=\"line\">                nextval[i]=j;</span><br><span class=\"line\">            <span class=\"keyword\">else</span></span><br><span class=\"line\">                nextval[i]=nextval[j];</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">else</span></span><br><span class=\"line\">            j=nextval[j];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">Index_KMP</span><span class=\"params\">(SString S, SString T, <span class=\"type\">int</span> next[])</span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> i=<span class=\"number\">1</span>, j=<span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(i&lt;=S.length &amp;&amp; j&lt;=T.length)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(j==<span class=\"number\">0</span> || S.ch[i]==T.ch[j])&#123;</span><br><span class=\"line\">            ++i;</span><br><span class=\"line\">            ++j;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">else</span></span><br><span class=\"line\">            j=next[j];<span class=\"comment\">//i不回溯</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(j&gt;T.length)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> i-T.length;</span><br><span class=\"line\">    <span class=\"keyword\">else</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h3 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a></h3><h2 id=\"4-1-定义\"><a href=\"#4-1-定义\" class=\"headerlink\" title=\"4.1 定义\"></a>4.1 定义</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/4_string/1.png\"><br><img src=\"/img/datastruct/4_string/1.png\" alt=\"数据结构\"></p>\n<ul>\n<li>一种特殊的线性表</li>\n</ul>\n<h2 id=\"4-2-串的存储结构\"><a href=\"#4-2-串的存储结构\" class=\"headerlink\" title=\"4.2 串的存储结构\"></a>4.2 串的存储结构</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/4_string/2.png\"><br><img src=\"/img/datastruct/4_string/2.png\" alt=\"数据结构\"></p>\n<ul>\n<li><p>顺序存储<br><img src=\"/../../../../themes/yilia/source/img/datastruct/4_string/3.png\"><br><img src=\"/img/datastruct/4_string/3.png\" alt=\"数据结构\"></p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> MAXLEN 255</span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span>&#123;</span></span><br><span class=\"line\">    <span class=\"type\">char</span> ch[MAXLEN]; <span class=\"comment\">//静态数组</span></span><br><span class=\"line\">    <span class=\"type\">int</span> length;</span><br><span class=\"line\">&#125; SString;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span>&#123;</span></span><br><span class=\"line\">    <span class=\"type\">char</span> *ch; <span class=\"comment\">//动态数组</span></span><br><span class=\"line\">    <span class=\"type\">int</span> length;</span><br><span class=\"line\">&#125; HString;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>链式存储</p>\n</li>\n</ul>\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/4_string/5.png\"><br><img src=\"/img/datastruct/4_string/5.png\" alt=\"数据结构\"></p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//2. 链式存储</span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">StringNode</span>&#123;</span></span><br><span class=\"line\">    <span class=\"type\">char</span> ch; <span class=\"comment\">//1：1</span></span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">StringNode</span> *<span class=\"title\">next</span>;</span></span><br><span class=\"line\">&#125; StringNode, *String;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">StringNode</span>&#123;</span></span><br><span class=\"line\">    <span class=\"type\">char</span> ch[<span class=\"number\">4</span>]; <span class=\"comment\">//1：4</span></span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">StringNode</span> *<span class=\"title\">next</span>;</span></span><br><span class=\"line\">&#125; StringNode, *String;</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/4_string/4.png\"><br><img src=\"/img/datastruct/4_string/4.png\" alt=\"数据结构\"></p>\n<h2 id=\"4-3-基本操作\"><a href=\"#4-3-基本操作\" class=\"headerlink\" title=\"4.3 基本操作\"></a>4.3 基本操作</h2><table>\n<thead>\n<tr>\n<th align=\"left\">函数名</th>\n<th align=\"left\">功能</th>\n<th align=\"left\">时间复杂度</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">StrAssign(SString &amp;T, char *chars)</td>\n<td align=\"left\">生成一个其值等于chars的串T</td>\n<td align=\"left\">O(n)</td>\n</tr>\n<tr>\n<td align=\"left\">StrCopy(SString &amp;T, SString S)</td>\n<td align=\"left\">由串S复制得串T</td>\n<td align=\"left\">O(n)</td>\n</tr>\n<tr>\n<td align=\"left\">StrEmpty(SString S)</td>\n<td align=\"left\">若S为空串，则返回true，否则返回false</td>\n<td align=\"left\">O(1)</td>\n</tr>\n<tr>\n<td align=\"left\">StrLength(SString S)</td>\n<td align=\"left\">返回串S的元素个数，即串的长度</td>\n<td align=\"left\">O(1)</td>\n</tr>\n<tr>\n<td align=\"left\">ClearString(SString &amp;S)</td>\n<td align=\"left\">将串S清为空串</td>\n<td align=\"left\">O(1)</td>\n</tr>\n<tr>\n<td align=\"left\">DestroyString(SString &amp;S)</td>\n<td align=\"left\">串S存在，则销毁它</td>\n<td align=\"left\">O(1)</td>\n</tr>\n<tr>\n<td align=\"left\">Concat(SString &amp;T, SString S1, SString S2)</td>\n<td align=\"left\">用T返回由S1和S2联接而成的新串</td>\n<td align=\"left\">O(n)</td>\n</tr>\n</tbody></table>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//求子串</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">SubString</span><span class=\"params\">(SString *Sub, SString S, <span class=\"type\">int</span> pos, <span class=\"type\">int</span> len)</span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">//1. 参数合法性判断</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span>(pos&lt;<span class=\"number\">1</span> || pos&gt;S.length || len&lt;<span class=\"number\">0</span> || len&gt;S.length-pos+<span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"comment\">//2. 子串赋值</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=pos; i&lt;pos+len; i++)</span><br><span class=\"line\">        Sub-&gt;ch[i-pos+<span class=\"number\">1</span>]=S.ch[i];</span><br><span class=\"line\">    Sub-&gt;length=len;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//比较</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">StrCompare</span><span class=\"params\">(SString S, SString T)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">1</span>; i&lt;=S.length &amp;&amp; i&lt;=T.length; i++)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(S.ch[i]!=T.ch[i])</span><br><span class=\"line\">            <span class=\"keyword\">return</span> S.ch[i]-T.ch[i];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> S.length-T.length;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//定位</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">Index</span><span class=\"params\">(SString S, SString T)</span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> i=<span class=\"number\">1</span>, n=S.length, m=T.length;</span><br><span class=\"line\">    SString sub;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(i&lt;=n-m+<span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        SubString(&amp;sub, S, i, m);</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(StrCompare(sub, T)!=<span class=\"number\">0</span>)</span><br><span class=\"line\">            ++i;</span><br><span class=\"line\">        <span class=\"keyword\">else</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span> i;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"4-4-模式匹配\"><a href=\"#4-4-模式匹配\" class=\"headerlink\" title=\"4.4 模式匹配\"></a>4.4 模式匹配</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/4_string/6.png\"><br><img src=\"/img/datastruct/4_string/6.png\" alt=\"数据结构\"></p>\n<h3 id=\"4-4-1-朴素模式匹配算法\"><a href=\"#4-4-1-朴素模式匹配算法\" class=\"headerlink\" title=\"4.4.1 朴素模式匹配算法\"></a>4.4.1 朴素模式匹配算法</h3><figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//朴素模式匹配算法</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">Index</span><span class=\"params\">(SString S, SString T)</span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> i=<span class=\"number\">1</span>, n=S.length, m=T.length;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(i&lt;=n-m+<span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> j=<span class=\"number\">1</span>;</span><br><span class=\"line\">        <span class=\"keyword\">while</span>(j&lt;=m)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(S.ch[i+j<span class=\"number\">-1</span>]==T.ch[j])</span><br><span class=\"line\">                ++j;</span><br><span class=\"line\">            <span class=\"keyword\">else</span></span><br><span class=\"line\">                <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(j&gt;m)</span><br><span class=\"line\">            <span class=\"keyword\">return</span> i;</span><br><span class=\"line\">        <span class=\"keyword\">else</span></span><br><span class=\"line\">            ++i;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">//定位算法中，子串的长度是固定的，而模式匹配算法中，子串的长度是可变的</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"4-4-2-KMP模式匹配算法\"><a href=\"#4-4-2-KMP模式匹配算法\" class=\"headerlink\" title=\"4.4.2 KMP模式匹配算法\"></a>4.4.2 KMP模式匹配算法</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/4_string/7.png\"><br><img src=\"/img/datastruct/4_string/7.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/4_string/8.png\"><br><img src=\"/img/datastruct/4_string/8.png\" alt=\"数据结构\"></p>\n<ul>\n<li>1）根据模式求next数组<br><img src=\"/../../../../themes/yilia/source/img/datastruct/4_string/9.png\"><br><img src=\"/img/datastruct/4_string/9.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/4_string/11.png\"><br><img src=\"/img/datastruct/4_string/11.png\" alt=\"数据结构\"></li>\n</ul>\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/4_string/22.png\"><br><img src=\"/img/datastruct/4_string/22.png\" alt=\"数据结构\"></p>\n<ul>\n<li>算法优化<br><img src=\"/../../../../themes/yilia/source/img/datastruct/4_string/33.png\"><br><img src=\"/img/datastruct/4_string/33.png\" alt=\"数据结构\"></li>\n</ul>\n<p>next[3]&#x3D;1 , next[3]与next[1]相等，所以转到next[1]后也会失配，所以next[3]可以改为next[1]&#x3D;0</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//KMP算法</span></span><br><span class=\"line\"><span class=\"comment\">//求next</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">get_next</span><span class=\"params\">(SString T, <span class=\"type\">int</span> next[])</span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">//1. 初始化</span></span><br><span class=\"line\">    <span class=\"type\">int</span> i=<span class=\"number\">1</span>, j=<span class=\"number\">0</span>;</span><br><span class=\"line\">    next[<span class=\"number\">1</span>]=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(i&lt;T.length)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(j==<span class=\"number\">0</span> || T.ch[i]==T.ch[j])&#123;</span><br><span class=\"line\">            ++i;</span><br><span class=\"line\">            ++j;</span><br><span class=\"line\">            next[i]=j;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">else</span></span><br><span class=\"line\">            j=next[j];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//求nextval</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">get_nextval</span><span class=\"params\">(SString T, <span class=\"type\">int</span> nextval[])</span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">//1. 初始化</span></span><br><span class=\"line\">    <span class=\"type\">int</span> i=<span class=\"number\">1</span>, j=<span class=\"number\">0</span>;</span><br><span class=\"line\">    nextval[<span class=\"number\">1</span>]=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(i&lt;T.length)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(j==<span class=\"number\">0</span> || T.ch[i]==T.ch[j])&#123;</span><br><span class=\"line\">            ++i;</span><br><span class=\"line\">            ++j;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(T.ch[i]!=T.ch[j])</span><br><span class=\"line\">                nextval[i]=j;</span><br><span class=\"line\">            <span class=\"keyword\">else</span></span><br><span class=\"line\">                nextval[i]=nextval[j];</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">else</span></span><br><span class=\"line\">            j=nextval[j];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">Index_KMP</span><span class=\"params\">(SString S, SString T, <span class=\"type\">int</span> next[])</span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> i=<span class=\"number\">1</span>, j=<span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(i&lt;=S.length &amp;&amp; j&lt;=T.length)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(j==<span class=\"number\">0</span> || S.ch[i]==T.ch[j])&#123;</span><br><span class=\"line\">            ++i;</span><br><span class=\"line\">            ++j;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">else</span></span><br><span class=\"line\">            j=next[j];<span class=\"comment\">//i不回溯</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(j&gt;T.length)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> i-T.length;</span><br><span class=\"line\">    <span class=\"keyword\">else</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>"},{"title":"3.1 栈的定义","date":"2023-08-06T16:00:00.000Z","comment":false,"toc":true,"_content":"#\n<!--more-->\n\n![](../../../../themes/yilia/source/img/datastruct/3_stack/stack/1.png)\n![数据结构](/img/datastruct/3_stack/stack/1.png)\n\n\n## 3.1 定义\n- 栈是限定仅在表尾进行插入和删除操作的线性表\n\n## 3.2 基本操作\n|操作|描述|\n|:---:|:---:|\n|InitStack(&S)|初始化栈S，构造一个空栈|\n|DestroyStack(&S)|若栈存在，则销毁它|\n|Push(&S,x)|若栈S存在，插入新元素x到栈S中并成为栈顶元素|\n|Pop(&S,&x)|删除栈S中栈顶元素，并用x返回其值|\n|GetTop(S,&x)|返回栈S中栈顶元素，不修改栈顶指针|\n|StackEmpty(S)|若栈为空，返回true，否则返回false|\n\n- （常考：给你一个出栈序列，问你能不能通过入栈操作得到这个出栈序列）共有卡特兰数种出栈序列：$C_n=\\frac{1}{n+1}\\binom{2n}{n}$\n\n## 3.3 顺序栈\n![](../../../../themes/yilia/source/img/datastruct/3_stack/stack/2.png)\n![数据结构](/img/datastruct/3_stack/stack/2.png)\n\n- 顺序栈的实现\n```c\n#include<stdio.h>\n#include<stdlib.h>\n#define MAXSIZE 10\ntypedef struct{\n    int data[MAXSIZE];\n    int top;\n} SqStack;\n\n//初始化\nvoid InitStack(SqStack *S){\n    S->top=-1;\n}\n\n//判空\nint StackEmpty(SqStack S){\n    if(S.top==-1)\n        return 1;\n    else\n        return 0;\n}\n\n//入栈\nint Push(SqStack *S, int x){\n    if(S->top==MAXSIZE-1)\n        return 0;\n    S->top++;\n    S->data[S->top]=x;\n    return 1;\n}\n\n//出栈\nint Pop(SqStack *S, int *x){\n    if(S->top==-1)\n        return 0;\n    *x=S->data[S->top];\n    S->top--;\n    return 1;\n}\n\n//取栈顶元素\nint GetTop(SqStack S, int *x){\n    if(S.top==-1)\n        return 0;\n    *x=S.data[S.top];\n    return 1;\n}\n```\n\n- 共享栈\n![](../../../../themes/yilia/source/img/datastruct/3_stack/stack/3.png)\n![数据结构](/img/datastruct/3_stack/stack/3.png)\n\n## 3.4 链栈\n![](../../../../themes/yilia/source/img/datastruct/3_stack/stack/4.png)\n![数据结构](/img/datastruct/3_stack/stack/4.png)\n- 链栈的实现\n```c\n\n```","source":"_posts/datastruct/3_stack/1_stack.md","raw":"---\ntitle: 3.1 栈的定义\ndate: 2023-08-07 00:00:00\ntags: [数据结构,栈]\ncategories: [数据结构]\ncomment: false\ntoc: true\n---\n#\n<!--more-->\n\n![](../../../../themes/yilia/source/img/datastruct/3_stack/stack/1.png)\n![数据结构](/img/datastruct/3_stack/stack/1.png)\n\n\n## 3.1 定义\n- 栈是限定仅在表尾进行插入和删除操作的线性表\n\n## 3.2 基本操作\n|操作|描述|\n|:---:|:---:|\n|InitStack(&S)|初始化栈S，构造一个空栈|\n|DestroyStack(&S)|若栈存在，则销毁它|\n|Push(&S,x)|若栈S存在，插入新元素x到栈S中并成为栈顶元素|\n|Pop(&S,&x)|删除栈S中栈顶元素，并用x返回其值|\n|GetTop(S,&x)|返回栈S中栈顶元素，不修改栈顶指针|\n|StackEmpty(S)|若栈为空，返回true，否则返回false|\n\n- （常考：给你一个出栈序列，问你能不能通过入栈操作得到这个出栈序列）共有卡特兰数种出栈序列：$C_n=\\frac{1}{n+1}\\binom{2n}{n}$\n\n## 3.3 顺序栈\n![](../../../../themes/yilia/source/img/datastruct/3_stack/stack/2.png)\n![数据结构](/img/datastruct/3_stack/stack/2.png)\n\n- 顺序栈的实现\n```c\n#include<stdio.h>\n#include<stdlib.h>\n#define MAXSIZE 10\ntypedef struct{\n    int data[MAXSIZE];\n    int top;\n} SqStack;\n\n//初始化\nvoid InitStack(SqStack *S){\n    S->top=-1;\n}\n\n//判空\nint StackEmpty(SqStack S){\n    if(S.top==-1)\n        return 1;\n    else\n        return 0;\n}\n\n//入栈\nint Push(SqStack *S, int x){\n    if(S->top==MAXSIZE-1)\n        return 0;\n    S->top++;\n    S->data[S->top]=x;\n    return 1;\n}\n\n//出栈\nint Pop(SqStack *S, int *x){\n    if(S->top==-1)\n        return 0;\n    *x=S->data[S->top];\n    S->top--;\n    return 1;\n}\n\n//取栈顶元素\nint GetTop(SqStack S, int *x){\n    if(S.top==-1)\n        return 0;\n    *x=S.data[S.top];\n    return 1;\n}\n```\n\n- 共享栈\n![](../../../../themes/yilia/source/img/datastruct/3_stack/stack/3.png)\n![数据结构](/img/datastruct/3_stack/stack/3.png)\n\n## 3.4 链栈\n![](../../../../themes/yilia/source/img/datastruct/3_stack/stack/4.png)\n![数据结构](/img/datastruct/3_stack/stack/4.png)\n- 链栈的实现\n```c\n\n```","slug":"datastruct/3_stack/1_stack","published":1,"updated":"2023-10-23T13:20:15.453Z","comments":1,"layout":"post","photos":[],"_id":"clvulgd5r000v5ovwell8gto0","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/stack/1.png\"><br><img src=\"/img/datastruct/3_stack/stack/1.png\" alt=\"数据结构\"></p>\n<h2 id=\"3-1-定义\"><a href=\"#3-1-定义\" class=\"headerlink\" title=\"3.1 定义\"></a>3.1 定义</h2><ul>\n<li>栈是限定仅在表尾进行插入和删除操作的线性表</li>\n</ul>\n<h2 id=\"3-2-基本操作\"><a href=\"#3-2-基本操作\" class=\"headerlink\" title=\"3.2 基本操作\"></a>3.2 基本操作</h2><table>\n<thead>\n<tr>\n<th align=\"center\">操作</th>\n<th align=\"center\">描述</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">InitStack(&amp;S)</td>\n<td align=\"center\">初始化栈S，构造一个空栈</td>\n</tr>\n<tr>\n<td align=\"center\">DestroyStack(&amp;S)</td>\n<td align=\"center\">若栈存在，则销毁它</td>\n</tr>\n<tr>\n<td align=\"center\">Push(&amp;S,x)</td>\n<td align=\"center\">若栈S存在，插入新元素x到栈S中并成为栈顶元素</td>\n</tr>\n<tr>\n<td align=\"center\">Pop(&amp;S,&amp;x)</td>\n<td align=\"center\">删除栈S中栈顶元素，并用x返回其值</td>\n</tr>\n<tr>\n<td align=\"center\">GetTop(S,&amp;x)</td>\n<td align=\"center\">返回栈S中栈顶元素，不修改栈顶指针</td>\n</tr>\n<tr>\n<td align=\"center\">StackEmpty(S)</td>\n<td align=\"center\">若栈为空，返回true，否则返回false</td>\n</tr>\n</tbody></table>\n<ul>\n<li>（常考：给你一个出栈序列，问你能不能通过入栈操作得到这个出栈序列）共有卡特兰数种出栈序列：$C_n&#x3D;\\frac{1}{n+1}\\binom{2n}{n}$</li>\n</ul>\n<h2 id=\"3-3-顺序栈\"><a href=\"#3-3-顺序栈\" class=\"headerlink\" title=\"3.3 顺序栈\"></a>3.3 顺序栈</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/stack/2.png\"><br><img src=\"/img/datastruct/3_stack/stack/2.png\" alt=\"数据结构\"></p>\n<ul>\n<li><p>顺序栈的实现</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;stdlib.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> MAXSIZE 10</span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span>&#123;</span></span><br><span class=\"line\">    <span class=\"type\">int</span> data[MAXSIZE];</span><br><span class=\"line\">    <span class=\"type\">int</span> top;</span><br><span class=\"line\">&#125; SqStack;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//初始化</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">InitStack</span><span class=\"params\">(SqStack *S)</span>&#123;</span><br><span class=\"line\">    S-&gt;top=<span class=\"number\">-1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//判空</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">StackEmpty</span><span class=\"params\">(SqStack S)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(S.top==<span class=\"number\">-1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">else</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//入栈</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">Push</span><span class=\"params\">(SqStack *S, <span class=\"type\">int</span> x)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(S-&gt;top==MAXSIZE<span class=\"number\">-1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    S-&gt;top++;</span><br><span class=\"line\">    S-&gt;data[S-&gt;top]=x;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//出栈</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">Pop</span><span class=\"params\">(SqStack *S, <span class=\"type\">int</span> *x)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(S-&gt;top==<span class=\"number\">-1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    *x=S-&gt;data[S-&gt;top];</span><br><span class=\"line\">    S-&gt;top--;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//取栈顶元素</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">GetTop</span><span class=\"params\">(SqStack S, <span class=\"type\">int</span> *x)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(S.top==<span class=\"number\">-1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    *x=S.data[S.top];</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>共享栈<br><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/stack/3.png\"><br><img src=\"/img/datastruct/3_stack/stack/3.png\" alt=\"数据结构\"></p>\n</li>\n</ul>\n<h2 id=\"3-4-链栈\"><a href=\"#3-4-链栈\" class=\"headerlink\" title=\"3.4 链栈\"></a>3.4 链栈</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/stack/4.png\"><br><img src=\"/img/datastruct/3_stack/stack/4.png\" alt=\"数据结构\"></p>\n<ul>\n<li>链栈的实现<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br></pre></td></tr></table></figure></li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<p><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/stack/1.png\"><br><img src=\"/img/datastruct/3_stack/stack/1.png\" alt=\"数据结构\"></p>\n<h2 id=\"3-1-定义\"><a href=\"#3-1-定义\" class=\"headerlink\" title=\"3.1 定义\"></a>3.1 定义</h2><ul>\n<li>栈是限定仅在表尾进行插入和删除操作的线性表</li>\n</ul>\n<h2 id=\"3-2-基本操作\"><a href=\"#3-2-基本操作\" class=\"headerlink\" title=\"3.2 基本操作\"></a>3.2 基本操作</h2><table>\n<thead>\n<tr>\n<th align=\"center\">操作</th>\n<th align=\"center\">描述</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">InitStack(&amp;S)</td>\n<td align=\"center\">初始化栈S，构造一个空栈</td>\n</tr>\n<tr>\n<td align=\"center\">DestroyStack(&amp;S)</td>\n<td align=\"center\">若栈存在，则销毁它</td>\n</tr>\n<tr>\n<td align=\"center\">Push(&amp;S,x)</td>\n<td align=\"center\">若栈S存在，插入新元素x到栈S中并成为栈顶元素</td>\n</tr>\n<tr>\n<td align=\"center\">Pop(&amp;S,&amp;x)</td>\n<td align=\"center\">删除栈S中栈顶元素，并用x返回其值</td>\n</tr>\n<tr>\n<td align=\"center\">GetTop(S,&amp;x)</td>\n<td align=\"center\">返回栈S中栈顶元素，不修改栈顶指针</td>\n</tr>\n<tr>\n<td align=\"center\">StackEmpty(S)</td>\n<td align=\"center\">若栈为空，返回true，否则返回false</td>\n</tr>\n</tbody></table>\n<ul>\n<li>（常考：给你一个出栈序列，问你能不能通过入栈操作得到这个出栈序列）共有卡特兰数种出栈序列：$C_n&#x3D;\\frac{1}{n+1}\\binom{2n}{n}$</li>\n</ul>\n<h2 id=\"3-3-顺序栈\"><a href=\"#3-3-顺序栈\" class=\"headerlink\" title=\"3.3 顺序栈\"></a>3.3 顺序栈</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/stack/2.png\"><br><img src=\"/img/datastruct/3_stack/stack/2.png\" alt=\"数据结构\"></p>\n<ul>\n<li><p>顺序栈的实现</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;stdlib.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> MAXSIZE 10</span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span>&#123;</span></span><br><span class=\"line\">    <span class=\"type\">int</span> data[MAXSIZE];</span><br><span class=\"line\">    <span class=\"type\">int</span> top;</span><br><span class=\"line\">&#125; SqStack;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//初始化</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">InitStack</span><span class=\"params\">(SqStack *S)</span>&#123;</span><br><span class=\"line\">    S-&gt;top=<span class=\"number\">-1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//判空</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">StackEmpty</span><span class=\"params\">(SqStack S)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(S.top==<span class=\"number\">-1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">else</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//入栈</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">Push</span><span class=\"params\">(SqStack *S, <span class=\"type\">int</span> x)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(S-&gt;top==MAXSIZE<span class=\"number\">-1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    S-&gt;top++;</span><br><span class=\"line\">    S-&gt;data[S-&gt;top]=x;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//出栈</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">Pop</span><span class=\"params\">(SqStack *S, <span class=\"type\">int</span> *x)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(S-&gt;top==<span class=\"number\">-1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    *x=S-&gt;data[S-&gt;top];</span><br><span class=\"line\">    S-&gt;top--;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//取栈顶元素</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">GetTop</span><span class=\"params\">(SqStack S, <span class=\"type\">int</span> *x)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(S.top==<span class=\"number\">-1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    *x=S.data[S.top];</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>共享栈<br><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/stack/3.png\"><br><img src=\"/img/datastruct/3_stack/stack/3.png\" alt=\"数据结构\"></p>\n</li>\n</ul>\n<h2 id=\"3-4-链栈\"><a href=\"#3-4-链栈\" class=\"headerlink\" title=\"3.4 链栈\"></a>3.4 链栈</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/stack/4.png\"><br><img src=\"/img/datastruct/3_stack/stack/4.png\" alt=\"数据结构\"></p>\n<ul>\n<li>链栈的实现<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br></pre></td></tr></table></figure></li>\n</ul>"},{"title":"1. 数据结构基本概念","date":"2023-01-31T16:00:00.000Z","comment":false,"toc":true,"_content":"#\n<!--more-->\n![数据结构]<!--more-->\n(/img/datastruct/1_extract/1.png)\n![](../../../../themes/yilia/source/img/datastruct/1_extract/1.png)\n\n\n## 1.1 数据\n- 信息的载体，被计算机程序识别和处理的符号的集合\n## 1.2 数据元素\n- 数据的基本单位，常作为一个整体进行考虑和处理。一个数据元素可由若干数据项组成。\n- 例如一个人的信息\n## 1.3 数据项\n- 数据不可分割的最小单位。\n- 例如姓名字段\n## 1.4 数据对象\n- 性质相同的数据元素的集合，是数据的一个子集。\n- 例如所有人的信息\n\n|概念|定义|举例|\n|:---:|:---:|:---:|\n|数据|信息的载体，被计算机程序识别和处理的符号的集合|一个人的信息|\n|数据元素|数据的基本单位，常作为一个整体进行考虑和处理。一个数据元素可由若干数据项组成。|一个人的信息|\n|数据项|数据不可分割的最小单位。|姓名字段|\n|数据对象|性质相同的数据元素的集合，是数据的一个子集。|所有人的信息|\n\n\n## 1.5 数据结构\n- 相互之间存在一种或多种特定关系的数据元素的集合。\n- 例如所有人的信息按照年龄排序\n## 1.6 逻辑结构\n- 数据对象中数据元素之间的相互关系\n- 例如所有人的信息按照年龄排序\n## 1.7 物理结构\n- 数据的逻辑结构在计算机中的存储形式\n- 例如所有人的信息按照年龄排序，存储在数组中\n## 1.8 数据类型\n- 一组性质相同的值的集合及定义在此集合上的一些操作的总称\n- 例如整数类型，浮点数类型\n## 1.9 抽象数据类型\n- 一个数学模型及定义在该模型上的一组操作\n- 例如整数类型，浮点数类型\n## 1.10 算法\n- 为解决特定问题而规定的一个有限长的操作序列\n- 例如排序算法\n## 1.11 数据类型\n- 一个值的集合和定义在此集合上的一组操作的总称\n\n|原子类型|其值不可再分的数据类型|\n|:---:|:---:|\n|结构类型|其值可再分为若干成分的数据类型|\n## 1.12 抽象数据类型\n- 一个数学模型及定义在该模型上的一组操作\n---\n\n# 2 数据结构三要素\n## 2.1 逻辑结构\n- 数据元素之间的逻辑关系\n- 集合、线性、树形、图形\n\n## 2.2 数据运算\n- 增删查改\n\n## 2.3 物理结构（存储结构）\n- 用计算机表示数据元素的逻辑关系\n插入表格：\n\n|顺序存储|链式存储|索引存储|散列存储|\n|:---:|:---:|:---:|:---:|\n|数组|链表|索引表|散列表|\n|逻辑相邻则物理相邻|逻辑相邻则物理不一定相邻|附加索引表|根据关键字得到存储地址|\n|![](../../../../themes/yilia/source/img/datastruct/1_extract/2.png)![数据结构](/img/datastruct/1_extract/2.png)|![](../../../../themes/yilia/source/img/datastruct/1_extract/3.png)![数据结构](/img/datastruct/1_extract/3.png)|![](../../../../themes/yilia/source/img/datastruct/1_extract/4.png)![数据结构](/img/datastruct/1_extract/4.png)||\n---\n# 3 算法的基本概念\n![](../../../../themes/yilia/source/img/datastruct/1_extract/5.png)![数据结构](/img/datastruct/1_extract/5.png)\n## 3.1 啥是算法\n- 对特定问题求解步骤的一种描述\n- 指令的有限序列\n- 每条指令表示一个或多个操作\n## 3.2 算法特性\n- 有穷性\n\n   ·执行有限步骤后结束\n   \n   ·每一步都可在有限时间内完成\n\n- 确定性\n \n   ·每一步都有确定的含义\n   \n   ·对于相同的输入只能得出相同的输出\n\n- 可行性\n\n   ·可以通过已经实现的基本运算执行有限次得出结果\n\n- 输入\n\n   ·有0个或多个输入\n\n- 输出\n\n   ·至少有1个或多个输出\n\n## 3.3 算法设计的要求\n- 正确性\n\n   ·算法能够得出正确的结果\n\n- 可读性\n\n   ·算法要便于阅读、理解和交流\n\n- 健壮性\n\n   ·算法对不合理数据输入有适当的处理能力\n\n- 时间效率高和存储量低\n\n   ·算法执行时间短，占用存储空间少\n\n## 3.4 算法效率的度量方法\n### 3.4.1 事后统计方法\n\n- 运行时间取决于计算机的硬件、软件环境\n\n### 3.4.2 事前分析估算方法\n- O(1)<O(log<sub>n</sub>)<O(n)<O(nlog<sub>n</sub>)<O(n<sup>2</sup>)<O(n<sup>3</sup>)<O(2<sup>n</sup>)<O(n!)<O(n<sup>n</sup>)\n- 时间复杂度\n![](../../../../themes/yilia/source/img/datastruct/1_extract/8.png)![数据结构](/img/datastruct/1_extract/8.png)\n- 空间复杂度\n![](../../../../themes/yilia/source/img/datastruct/1_extract/9.png)![数据结构](/img/datastruct/1_extract/9.png)\n- 例子：\n![](../../../../themes/yilia/source/img/datastruct/1_extract/6.png)![数据结构](/img/datastruct/1_extract/6.png)\n\n![](../../../../themes/yilia/source/img/datastruct/1_extract/7.png)![数据结构](/img/datastruct/1_extract/7.png)","source":"_posts/datastruct/1_extract/1_基础.md","raw":"---\ntitle: 1. 数据结构基本概念\ndate: 2023-02-01 00:00:00\ntags: [数据结构]\ncategories: [数据结构]\ncomment: false\ntoc: true\n---\n#\n<!--more-->\n![数据结构]<!--more-->\n(/img/datastruct/1_extract/1.png)\n![](../../../../themes/yilia/source/img/datastruct/1_extract/1.png)\n\n\n## 1.1 数据\n- 信息的载体，被计算机程序识别和处理的符号的集合\n## 1.2 数据元素\n- 数据的基本单位，常作为一个整体进行考虑和处理。一个数据元素可由若干数据项组成。\n- 例如一个人的信息\n## 1.3 数据项\n- 数据不可分割的最小单位。\n- 例如姓名字段\n## 1.4 数据对象\n- 性质相同的数据元素的集合，是数据的一个子集。\n- 例如所有人的信息\n\n|概念|定义|举例|\n|:---:|:---:|:---:|\n|数据|信息的载体，被计算机程序识别和处理的符号的集合|一个人的信息|\n|数据元素|数据的基本单位，常作为一个整体进行考虑和处理。一个数据元素可由若干数据项组成。|一个人的信息|\n|数据项|数据不可分割的最小单位。|姓名字段|\n|数据对象|性质相同的数据元素的集合，是数据的一个子集。|所有人的信息|\n\n\n## 1.5 数据结构\n- 相互之间存在一种或多种特定关系的数据元素的集合。\n- 例如所有人的信息按照年龄排序\n## 1.6 逻辑结构\n- 数据对象中数据元素之间的相互关系\n- 例如所有人的信息按照年龄排序\n## 1.7 物理结构\n- 数据的逻辑结构在计算机中的存储形式\n- 例如所有人的信息按照年龄排序，存储在数组中\n## 1.8 数据类型\n- 一组性质相同的值的集合及定义在此集合上的一些操作的总称\n- 例如整数类型，浮点数类型\n## 1.9 抽象数据类型\n- 一个数学模型及定义在该模型上的一组操作\n- 例如整数类型，浮点数类型\n## 1.10 算法\n- 为解决特定问题而规定的一个有限长的操作序列\n- 例如排序算法\n## 1.11 数据类型\n- 一个值的集合和定义在此集合上的一组操作的总称\n\n|原子类型|其值不可再分的数据类型|\n|:---:|:---:|\n|结构类型|其值可再分为若干成分的数据类型|\n## 1.12 抽象数据类型\n- 一个数学模型及定义在该模型上的一组操作\n---\n\n# 2 数据结构三要素\n## 2.1 逻辑结构\n- 数据元素之间的逻辑关系\n- 集合、线性、树形、图形\n\n## 2.2 数据运算\n- 增删查改\n\n## 2.3 物理结构（存储结构）\n- 用计算机表示数据元素的逻辑关系\n插入表格：\n\n|顺序存储|链式存储|索引存储|散列存储|\n|:---:|:---:|:---:|:---:|\n|数组|链表|索引表|散列表|\n|逻辑相邻则物理相邻|逻辑相邻则物理不一定相邻|附加索引表|根据关键字得到存储地址|\n|![](../../../../themes/yilia/source/img/datastruct/1_extract/2.png)![数据结构](/img/datastruct/1_extract/2.png)|![](../../../../themes/yilia/source/img/datastruct/1_extract/3.png)![数据结构](/img/datastruct/1_extract/3.png)|![](../../../../themes/yilia/source/img/datastruct/1_extract/4.png)![数据结构](/img/datastruct/1_extract/4.png)||\n---\n# 3 算法的基本概念\n![](../../../../themes/yilia/source/img/datastruct/1_extract/5.png)![数据结构](/img/datastruct/1_extract/5.png)\n## 3.1 啥是算法\n- 对特定问题求解步骤的一种描述\n- 指令的有限序列\n- 每条指令表示一个或多个操作\n## 3.2 算法特性\n- 有穷性\n\n   ·执行有限步骤后结束\n   \n   ·每一步都可在有限时间内完成\n\n- 确定性\n \n   ·每一步都有确定的含义\n   \n   ·对于相同的输入只能得出相同的输出\n\n- 可行性\n\n   ·可以通过已经实现的基本运算执行有限次得出结果\n\n- 输入\n\n   ·有0个或多个输入\n\n- 输出\n\n   ·至少有1个或多个输出\n\n## 3.3 算法设计的要求\n- 正确性\n\n   ·算法能够得出正确的结果\n\n- 可读性\n\n   ·算法要便于阅读、理解和交流\n\n- 健壮性\n\n   ·算法对不合理数据输入有适当的处理能力\n\n- 时间效率高和存储量低\n\n   ·算法执行时间短，占用存储空间少\n\n## 3.4 算法效率的度量方法\n### 3.4.1 事后统计方法\n\n- 运行时间取决于计算机的硬件、软件环境\n\n### 3.4.2 事前分析估算方法\n- O(1)<O(log<sub>n</sub>)<O(n)<O(nlog<sub>n</sub>)<O(n<sup>2</sup>)<O(n<sup>3</sup>)<O(2<sup>n</sup>)<O(n!)<O(n<sup>n</sup>)\n- 时间复杂度\n![](../../../../themes/yilia/source/img/datastruct/1_extract/8.png)![数据结构](/img/datastruct/1_extract/8.png)\n- 空间复杂度\n![](../../../../themes/yilia/source/img/datastruct/1_extract/9.png)![数据结构](/img/datastruct/1_extract/9.png)\n- 例子：\n![](../../../../themes/yilia/source/img/datastruct/1_extract/6.png)![数据结构](/img/datastruct/1_extract/6.png)\n\n![](../../../../themes/yilia/source/img/datastruct/1_extract/7.png)![数据结构](/img/datastruct/1_extract/7.png)","slug":"datastruct/1_extract/1_基础","published":1,"updated":"2023-10-23T11:59:15.750Z","comments":1,"layout":"post","photos":[],"_id":"clvulgd5s000x5ovw2vjt9c6j","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<p>![数据结构]<!--more--><br>(&#x2F;img&#x2F;datastruct&#x2F;1_extract&#x2F;1.png)<br><img src=\"/../../../../themes/yilia/source/img/datastruct/1_extract/1.png\"></p>\n<h2 id=\"1-1-数据\"><a href=\"#1-1-数据\" class=\"headerlink\" title=\"1.1 数据\"></a>1.1 数据</h2><ul>\n<li>信息的载体，被计算机程序识别和处理的符号的集合</li>\n</ul>\n<h2 id=\"1-2-数据元素\"><a href=\"#1-2-数据元素\" class=\"headerlink\" title=\"1.2 数据元素\"></a>1.2 数据元素</h2><ul>\n<li>数据的基本单位，常作为一个整体进行考虑和处理。一个数据元素可由若干数据项组成。</li>\n<li>例如一个人的信息</li>\n</ul>\n<h2 id=\"1-3-数据项\"><a href=\"#1-3-数据项\" class=\"headerlink\" title=\"1.3 数据项\"></a>1.3 数据项</h2><ul>\n<li>数据不可分割的最小单位。</li>\n<li>例如姓名字段</li>\n</ul>\n<h2 id=\"1-4-数据对象\"><a href=\"#1-4-数据对象\" class=\"headerlink\" title=\"1.4 数据对象\"></a>1.4 数据对象</h2><ul>\n<li>性质相同的数据元素的集合，是数据的一个子集。</li>\n<li>例如所有人的信息</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th align=\"center\">概念</th>\n<th align=\"center\">定义</th>\n<th align=\"center\">举例</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">数据</td>\n<td align=\"center\">信息的载体，被计算机程序识别和处理的符号的集合</td>\n<td align=\"center\">一个人的信息</td>\n</tr>\n<tr>\n<td align=\"center\">数据元素</td>\n<td align=\"center\">数据的基本单位，常作为一个整体进行考虑和处理。一个数据元素可由若干数据项组成。</td>\n<td align=\"center\">一个人的信息</td>\n</tr>\n<tr>\n<td align=\"center\">数据项</td>\n<td align=\"center\">数据不可分割的最小单位。</td>\n<td align=\"center\">姓名字段</td>\n</tr>\n<tr>\n<td align=\"center\">数据对象</td>\n<td align=\"center\">性质相同的数据元素的集合，是数据的一个子集。</td>\n<td align=\"center\">所有人的信息</td>\n</tr>\n</tbody></table>\n<h2 id=\"1-5-数据结构\"><a href=\"#1-5-数据结构\" class=\"headerlink\" title=\"1.5 数据结构\"></a>1.5 数据结构</h2><ul>\n<li>相互之间存在一种或多种特定关系的数据元素的集合。</li>\n<li>例如所有人的信息按照年龄排序</li>\n</ul>\n<h2 id=\"1-6-逻辑结构\"><a href=\"#1-6-逻辑结构\" class=\"headerlink\" title=\"1.6 逻辑结构\"></a>1.6 逻辑结构</h2><ul>\n<li>数据对象中数据元素之间的相互关系</li>\n<li>例如所有人的信息按照年龄排序</li>\n</ul>\n<h2 id=\"1-7-物理结构\"><a href=\"#1-7-物理结构\" class=\"headerlink\" title=\"1.7 物理结构\"></a>1.7 物理结构</h2><ul>\n<li>数据的逻辑结构在计算机中的存储形式</li>\n<li>例如所有人的信息按照年龄排序，存储在数组中</li>\n</ul>\n<h2 id=\"1-8-数据类型\"><a href=\"#1-8-数据类型\" class=\"headerlink\" title=\"1.8 数据类型\"></a>1.8 数据类型</h2><ul>\n<li>一组性质相同的值的集合及定义在此集合上的一些操作的总称</li>\n<li>例如整数类型，浮点数类型</li>\n</ul>\n<h2 id=\"1-9-抽象数据类型\"><a href=\"#1-9-抽象数据类型\" class=\"headerlink\" title=\"1.9 抽象数据类型\"></a>1.9 抽象数据类型</h2><ul>\n<li>一个数学模型及定义在该模型上的一组操作</li>\n<li>例如整数类型，浮点数类型</li>\n</ul>\n<h2 id=\"1-10-算法\"><a href=\"#1-10-算法\" class=\"headerlink\" title=\"1.10 算法\"></a>1.10 算法</h2><ul>\n<li>为解决特定问题而规定的一个有限长的操作序列</li>\n<li>例如排序算法</li>\n</ul>\n<h2 id=\"1-11-数据类型\"><a href=\"#1-11-数据类型\" class=\"headerlink\" title=\"1.11 数据类型\"></a>1.11 数据类型</h2><ul>\n<li>一个值的集合和定义在此集合上的一组操作的总称</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th align=\"center\">原子类型</th>\n<th align=\"center\">其值不可再分的数据类型</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">结构类型</td>\n<td align=\"center\">其值可再分为若干成分的数据类型</td>\n</tr>\n</tbody></table>\n<h2 id=\"1-12-抽象数据类型\"><a href=\"#1-12-抽象数据类型\" class=\"headerlink\" title=\"1.12 抽象数据类型\"></a>1.12 抽象数据类型</h2><ul>\n<li>一个数学模型及定义在该模型上的一组操作</li>\n</ul>\n<hr>\n<h1 id=\"2-数据结构三要素\"><a href=\"#2-数据结构三要素\" class=\"headerlink\" title=\"2 数据结构三要素\"></a>2 数据结构三要素</h1><h2 id=\"2-1-逻辑结构\"><a href=\"#2-1-逻辑结构\" class=\"headerlink\" title=\"2.1 逻辑结构\"></a>2.1 逻辑结构</h2><ul>\n<li>数据元素之间的逻辑关系</li>\n<li>集合、线性、树形、图形</li>\n</ul>\n<h2 id=\"2-2-数据运算\"><a href=\"#2-2-数据运算\" class=\"headerlink\" title=\"2.2 数据运算\"></a>2.2 数据运算</h2><ul>\n<li>增删查改</li>\n</ul>\n<h2 id=\"2-3-物理结构（存储结构）\"><a href=\"#2-3-物理结构（存储结构）\" class=\"headerlink\" title=\"2.3 物理结构（存储结构）\"></a>2.3 物理结构（存储结构）</h2><ul>\n<li>用计算机表示数据元素的逻辑关系<br>插入表格：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th align=\"center\">顺序存储</th>\n<th align=\"center\">链式存储</th>\n<th align=\"center\">索引存储</th>\n<th align=\"center\">散列存储</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">数组</td>\n<td align=\"center\">链表</td>\n<td align=\"center\">索引表</td>\n<td align=\"center\">散列表</td>\n</tr>\n<tr>\n<td align=\"center\">逻辑相邻则物理相邻</td>\n<td align=\"center\">逻辑相邻则物理不一定相邻</td>\n<td align=\"center\">附加索引表</td>\n<td align=\"center\">根据关键字得到存储地址</td>\n</tr>\n<tr>\n<td align=\"center\"><img src=\"/../../../../themes/yilia/source/img/datastruct/1_extract/2.png\"><img src=\"/img/datastruct/1_extract/2.png\" alt=\"数据结构\"></td>\n<td align=\"center\"><img src=\"/../../../../themes/yilia/source/img/datastruct/1_extract/3.png\"><img src=\"/img/datastruct/1_extract/3.png\" alt=\"数据结构\"></td>\n<td align=\"center\"><img src=\"/../../../../themes/yilia/source/img/datastruct/1_extract/4.png\"><img src=\"/img/datastruct/1_extract/4.png\" alt=\"数据结构\"></td>\n<td align=\"center\"></td>\n</tr>\n</tbody></table>\n<hr>\n<h1 id=\"3-算法的基本概念\"><a href=\"#3-算法的基本概念\" class=\"headerlink\" title=\"3 算法的基本概念\"></a>3 算法的基本概念</h1><p><img src=\"/../../../../themes/yilia/source/img/datastruct/1_extract/5.png\"><img src=\"/img/datastruct/1_extract/5.png\" alt=\"数据结构\"></p>\n<h2 id=\"3-1-啥是算法\"><a href=\"#3-1-啥是算法\" class=\"headerlink\" title=\"3.1 啥是算法\"></a>3.1 啥是算法</h2><ul>\n<li>对特定问题求解步骤的一种描述</li>\n<li>指令的有限序列</li>\n<li>每条指令表示一个或多个操作</li>\n</ul>\n<h2 id=\"3-2-算法特性\"><a href=\"#3-2-算法特性\" class=\"headerlink\" title=\"3.2 算法特性\"></a>3.2 算法特性</h2><ul>\n<li><p>有穷性</p>\n<p> ·执行有限步骤后结束</p>\n<p> ·每一步都可在有限时间内完成</p>\n</li>\n<li><p>确定性</p>\n<p> ·每一步都有确定的含义</p>\n<p> ·对于相同的输入只能得出相同的输出</p>\n</li>\n<li><p>可行性</p>\n<p> ·可以通过已经实现的基本运算执行有限次得出结果</p>\n</li>\n<li><p>输入</p>\n<p> ·有0个或多个输入</p>\n</li>\n<li><p>输出</p>\n<p> ·至少有1个或多个输出</p>\n</li>\n</ul>\n<h2 id=\"3-3-算法设计的要求\"><a href=\"#3-3-算法设计的要求\" class=\"headerlink\" title=\"3.3 算法设计的要求\"></a>3.3 算法设计的要求</h2><ul>\n<li><p>正确性</p>\n<p> ·算法能够得出正确的结果</p>\n</li>\n<li><p>可读性</p>\n<p> ·算法要便于阅读、理解和交流</p>\n</li>\n<li><p>健壮性</p>\n<p> ·算法对不合理数据输入有适当的处理能力</p>\n</li>\n<li><p>时间效率高和存储量低</p>\n<p> ·算法执行时间短，占用存储空间少</p>\n</li>\n</ul>\n<h2 id=\"3-4-算法效率的度量方法\"><a href=\"#3-4-算法效率的度量方法\" class=\"headerlink\" title=\"3.4 算法效率的度量方法\"></a>3.4 算法效率的度量方法</h2><h3 id=\"3-4-1-事后统计方法\"><a href=\"#3-4-1-事后统计方法\" class=\"headerlink\" title=\"3.4.1 事后统计方法\"></a>3.4.1 事后统计方法</h3><ul>\n<li>运行时间取决于计算机的硬件、软件环境</li>\n</ul>\n<h3 id=\"3-4-2-事前分析估算方法\"><a href=\"#3-4-2-事前分析估算方法\" class=\"headerlink\" title=\"3.4.2 事前分析估算方法\"></a>3.4.2 事前分析估算方法</h3><ul>\n<li>O(1)&lt;O(log<sub>n</sub>)&lt;O(n)&lt;O(nlog<sub>n</sub>)&lt;O(n<sup>2</sup>)&lt;O(n<sup>3</sup>)&lt;O(2<sup>n</sup>)&lt;O(n!)&lt;O(n<sup>n</sup>)</li>\n<li>时间复杂度<br><img src=\"/../../../../themes/yilia/source/img/datastruct/1_extract/8.png\"><img src=\"/img/datastruct/1_extract/8.png\" alt=\"数据结构\"></li>\n<li>空间复杂度<br><img src=\"/../../../../themes/yilia/source/img/datastruct/1_extract/9.png\"><img src=\"/img/datastruct/1_extract/9.png\" alt=\"数据结构\"></li>\n<li>例子：<br><img src=\"/../../../../themes/yilia/source/img/datastruct/1_extract/6.png\"><img src=\"/img/datastruct/1_extract/6.png\" alt=\"数据结构\"></li>\n</ul>\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/1_extract/7.png\"><img src=\"/img/datastruct/1_extract/7.png\" alt=\"数据结构\"></p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<p>![数据结构]<!--more--><br>(&#x2F;img&#x2F;datastruct&#x2F;1_extract&#x2F;1.png)<br><img src=\"/../../../../themes/yilia/source/img/datastruct/1_extract/1.png\"></p>\n<h2 id=\"1-1-数据\"><a href=\"#1-1-数据\" class=\"headerlink\" title=\"1.1 数据\"></a>1.1 数据</h2><ul>\n<li>信息的载体，被计算机程序识别和处理的符号的集合</li>\n</ul>\n<h2 id=\"1-2-数据元素\"><a href=\"#1-2-数据元素\" class=\"headerlink\" title=\"1.2 数据元素\"></a>1.2 数据元素</h2><ul>\n<li>数据的基本单位，常作为一个整体进行考虑和处理。一个数据元素可由若干数据项组成。</li>\n<li>例如一个人的信息</li>\n</ul>\n<h2 id=\"1-3-数据项\"><a href=\"#1-3-数据项\" class=\"headerlink\" title=\"1.3 数据项\"></a>1.3 数据项</h2><ul>\n<li>数据不可分割的最小单位。</li>\n<li>例如姓名字段</li>\n</ul>\n<h2 id=\"1-4-数据对象\"><a href=\"#1-4-数据对象\" class=\"headerlink\" title=\"1.4 数据对象\"></a>1.4 数据对象</h2><ul>\n<li>性质相同的数据元素的集合，是数据的一个子集。</li>\n<li>例如所有人的信息</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th align=\"center\">概念</th>\n<th align=\"center\">定义</th>\n<th align=\"center\">举例</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">数据</td>\n<td align=\"center\">信息的载体，被计算机程序识别和处理的符号的集合</td>\n<td align=\"center\">一个人的信息</td>\n</tr>\n<tr>\n<td align=\"center\">数据元素</td>\n<td align=\"center\">数据的基本单位，常作为一个整体进行考虑和处理。一个数据元素可由若干数据项组成。</td>\n<td align=\"center\">一个人的信息</td>\n</tr>\n<tr>\n<td align=\"center\">数据项</td>\n<td align=\"center\">数据不可分割的最小单位。</td>\n<td align=\"center\">姓名字段</td>\n</tr>\n<tr>\n<td align=\"center\">数据对象</td>\n<td align=\"center\">性质相同的数据元素的集合，是数据的一个子集。</td>\n<td align=\"center\">所有人的信息</td>\n</tr>\n</tbody></table>\n<h2 id=\"1-5-数据结构\"><a href=\"#1-5-数据结构\" class=\"headerlink\" title=\"1.5 数据结构\"></a>1.5 数据结构</h2><ul>\n<li>相互之间存在一种或多种特定关系的数据元素的集合。</li>\n<li>例如所有人的信息按照年龄排序</li>\n</ul>\n<h2 id=\"1-6-逻辑结构\"><a href=\"#1-6-逻辑结构\" class=\"headerlink\" title=\"1.6 逻辑结构\"></a>1.6 逻辑结构</h2><ul>\n<li>数据对象中数据元素之间的相互关系</li>\n<li>例如所有人的信息按照年龄排序</li>\n</ul>\n<h2 id=\"1-7-物理结构\"><a href=\"#1-7-物理结构\" class=\"headerlink\" title=\"1.7 物理结构\"></a>1.7 物理结构</h2><ul>\n<li>数据的逻辑结构在计算机中的存储形式</li>\n<li>例如所有人的信息按照年龄排序，存储在数组中</li>\n</ul>\n<h2 id=\"1-8-数据类型\"><a href=\"#1-8-数据类型\" class=\"headerlink\" title=\"1.8 数据类型\"></a>1.8 数据类型</h2><ul>\n<li>一组性质相同的值的集合及定义在此集合上的一些操作的总称</li>\n<li>例如整数类型，浮点数类型</li>\n</ul>\n<h2 id=\"1-9-抽象数据类型\"><a href=\"#1-9-抽象数据类型\" class=\"headerlink\" title=\"1.9 抽象数据类型\"></a>1.9 抽象数据类型</h2><ul>\n<li>一个数学模型及定义在该模型上的一组操作</li>\n<li>例如整数类型，浮点数类型</li>\n</ul>\n<h2 id=\"1-10-算法\"><a href=\"#1-10-算法\" class=\"headerlink\" title=\"1.10 算法\"></a>1.10 算法</h2><ul>\n<li>为解决特定问题而规定的一个有限长的操作序列</li>\n<li>例如排序算法</li>\n</ul>\n<h2 id=\"1-11-数据类型\"><a href=\"#1-11-数据类型\" class=\"headerlink\" title=\"1.11 数据类型\"></a>1.11 数据类型</h2><ul>\n<li>一个值的集合和定义在此集合上的一组操作的总称</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th align=\"center\">原子类型</th>\n<th align=\"center\">其值不可再分的数据类型</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">结构类型</td>\n<td align=\"center\">其值可再分为若干成分的数据类型</td>\n</tr>\n</tbody></table>\n<h2 id=\"1-12-抽象数据类型\"><a href=\"#1-12-抽象数据类型\" class=\"headerlink\" title=\"1.12 抽象数据类型\"></a>1.12 抽象数据类型</h2><ul>\n<li>一个数学模型及定义在该模型上的一组操作</li>\n</ul>\n<hr>\n<h1 id=\"2-数据结构三要素\"><a href=\"#2-数据结构三要素\" class=\"headerlink\" title=\"2 数据结构三要素\"></a>2 数据结构三要素</h1><h2 id=\"2-1-逻辑结构\"><a href=\"#2-1-逻辑结构\" class=\"headerlink\" title=\"2.1 逻辑结构\"></a>2.1 逻辑结构</h2><ul>\n<li>数据元素之间的逻辑关系</li>\n<li>集合、线性、树形、图形</li>\n</ul>\n<h2 id=\"2-2-数据运算\"><a href=\"#2-2-数据运算\" class=\"headerlink\" title=\"2.2 数据运算\"></a>2.2 数据运算</h2><ul>\n<li>增删查改</li>\n</ul>\n<h2 id=\"2-3-物理结构（存储结构）\"><a href=\"#2-3-物理结构（存储结构）\" class=\"headerlink\" title=\"2.3 物理结构（存储结构）\"></a>2.3 物理结构（存储结构）</h2><ul>\n<li>用计算机表示数据元素的逻辑关系<br>插入表格：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th align=\"center\">顺序存储</th>\n<th align=\"center\">链式存储</th>\n<th align=\"center\">索引存储</th>\n<th align=\"center\">散列存储</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">数组</td>\n<td align=\"center\">链表</td>\n<td align=\"center\">索引表</td>\n<td align=\"center\">散列表</td>\n</tr>\n<tr>\n<td align=\"center\">逻辑相邻则物理相邻</td>\n<td align=\"center\">逻辑相邻则物理不一定相邻</td>\n<td align=\"center\">附加索引表</td>\n<td align=\"center\">根据关键字得到存储地址</td>\n</tr>\n<tr>\n<td align=\"center\"><img src=\"/../../../../themes/yilia/source/img/datastruct/1_extract/2.png\"><img src=\"/img/datastruct/1_extract/2.png\" alt=\"数据结构\"></td>\n<td align=\"center\"><img src=\"/../../../../themes/yilia/source/img/datastruct/1_extract/3.png\"><img src=\"/img/datastruct/1_extract/3.png\" alt=\"数据结构\"></td>\n<td align=\"center\"><img src=\"/../../../../themes/yilia/source/img/datastruct/1_extract/4.png\"><img src=\"/img/datastruct/1_extract/4.png\" alt=\"数据结构\"></td>\n<td align=\"center\"></td>\n</tr>\n</tbody></table>\n<hr>\n<h1 id=\"3-算法的基本概念\"><a href=\"#3-算法的基本概念\" class=\"headerlink\" title=\"3 算法的基本概念\"></a>3 算法的基本概念</h1><p><img src=\"/../../../../themes/yilia/source/img/datastruct/1_extract/5.png\"><img src=\"/img/datastruct/1_extract/5.png\" alt=\"数据结构\"></p>\n<h2 id=\"3-1-啥是算法\"><a href=\"#3-1-啥是算法\" class=\"headerlink\" title=\"3.1 啥是算法\"></a>3.1 啥是算法</h2><ul>\n<li>对特定问题求解步骤的一种描述</li>\n<li>指令的有限序列</li>\n<li>每条指令表示一个或多个操作</li>\n</ul>\n<h2 id=\"3-2-算法特性\"><a href=\"#3-2-算法特性\" class=\"headerlink\" title=\"3.2 算法特性\"></a>3.2 算法特性</h2><ul>\n<li><p>有穷性</p>\n<p> ·执行有限步骤后结束</p>\n<p> ·每一步都可在有限时间内完成</p>\n</li>\n<li><p>确定性</p>\n<p> ·每一步都有确定的含义</p>\n<p> ·对于相同的输入只能得出相同的输出</p>\n</li>\n<li><p>可行性</p>\n<p> ·可以通过已经实现的基本运算执行有限次得出结果</p>\n</li>\n<li><p>输入</p>\n<p> ·有0个或多个输入</p>\n</li>\n<li><p>输出</p>\n<p> ·至少有1个或多个输出</p>\n</li>\n</ul>\n<h2 id=\"3-3-算法设计的要求\"><a href=\"#3-3-算法设计的要求\" class=\"headerlink\" title=\"3.3 算法设计的要求\"></a>3.3 算法设计的要求</h2><ul>\n<li><p>正确性</p>\n<p> ·算法能够得出正确的结果</p>\n</li>\n<li><p>可读性</p>\n<p> ·算法要便于阅读、理解和交流</p>\n</li>\n<li><p>健壮性</p>\n<p> ·算法对不合理数据输入有适当的处理能力</p>\n</li>\n<li><p>时间效率高和存储量低</p>\n<p> ·算法执行时间短，占用存储空间少</p>\n</li>\n</ul>\n<h2 id=\"3-4-算法效率的度量方法\"><a href=\"#3-4-算法效率的度量方法\" class=\"headerlink\" title=\"3.4 算法效率的度量方法\"></a>3.4 算法效率的度量方法</h2><h3 id=\"3-4-1-事后统计方法\"><a href=\"#3-4-1-事后统计方法\" class=\"headerlink\" title=\"3.4.1 事后统计方法\"></a>3.4.1 事后统计方法</h3><ul>\n<li>运行时间取决于计算机的硬件、软件环境</li>\n</ul>\n<h3 id=\"3-4-2-事前分析估算方法\"><a href=\"#3-4-2-事前分析估算方法\" class=\"headerlink\" title=\"3.4.2 事前分析估算方法\"></a>3.4.2 事前分析估算方法</h3><ul>\n<li>O(1)&lt;O(log<sub>n</sub>)&lt;O(n)&lt;O(nlog<sub>n</sub>)&lt;O(n<sup>2</sup>)&lt;O(n<sup>3</sup>)&lt;O(2<sup>n</sup>)&lt;O(n!)&lt;O(n<sup>n</sup>)</li>\n<li>时间复杂度<br><img src=\"/../../../../themes/yilia/source/img/datastruct/1_extract/8.png\"><img src=\"/img/datastruct/1_extract/8.png\" alt=\"数据结构\"></li>\n<li>空间复杂度<br><img src=\"/../../../../themes/yilia/source/img/datastruct/1_extract/9.png\"><img src=\"/img/datastruct/1_extract/9.png\" alt=\"数据结构\"></li>\n<li>例子：<br><img src=\"/../../../../themes/yilia/source/img/datastruct/1_extract/6.png\"><img src=\"/img/datastruct/1_extract/6.png\" alt=\"数据结构\"></li>\n</ul>\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/1_extract/7.png\"><img src=\"/img/datastruct/1_extract/7.png\" alt=\"数据结构\"></p>"},{"title":"3.2 队列","date":"2023-08-06T16:00:00.000Z","comment":false,"toc":true,"_content":"#\n<!--more-->\n\n![](../../../../themes/yilia/source/img/datastruct/3_stack/queue/1.png)\n![数据结构](/img/datastruct/3_stack/queue/1.png)\n\n\n## 3.2.1 定义\n- 队列是只允许在一端进行插入操作，而在另一端进行删除操作的线性表\n\n## 3.2.2 基本操作\n|操作|描述|\n|:---:|:---:|\n|InitQueue(&Q)|初始化队列Q，构造一个空队列|\n|DestroyQueue(&Q)|若队列Q存在，则销毁它|\n|EnQueue(&Q,x)|若队列Q存在，插入新元素x到队列Q中并成为队尾元素|\n|DeQueue(&Q,&x)|删除队列Q中队头元素，并用x返回其值|\n|GetHead(Q,&x)|返回队列Q中队头元素，不修改队头指针|\n|QueueEmpty(Q)|若队列为空，返回true，否则返回false|\n\n## 3.2.3 顺序队列\n![](../../../../themes/yilia/source/img/datastruct/3_stack/queue/2.png)\n![数据结构](/img/datastruct/3_stack/queue/2.png)\n\n- 顺序队列的实现\n```c\n#include<stdio.h>\n#include<stdlib.h>\n#define MAXSIZE 10\ntypedef struct{\n    int data[MAXSIZE];\n    int front,rear;\n} SqQueue;\n\n//初始化\nvoid InitQueue(SqQueue *Q){\n    int i;\n    for(i=0;i<MAXSIZE;i++)\n        Q->data[i]=0;\n}\n\n//判空\nint QueueEmpty(SqQueue Q){\n    if(Q.front==Q.rear)\n        return 1;\n    else\n        return 0;\n}\n\n//入队\nint EnQueue(SqQueue *Q, int x){\n    if((Q->rear+1)%MAXSIZE==Q->front)\n        return 0;\n    Q->data[Q->rear]=x;\n    Q->rear=(Q->rear+1)%MAXSIZE;\n    return 1;\n}\n\n//出队\nint DeQueue(SqQueue *Q, int *x){\n    if(Q->front==Q->rear)\n        return 0;\n    *x=Q->data[Q->front];\n    Q->front=(Q->front+1)%MAXSIZE;\n    return 1;\n}\n\n```\n-  循环队列\n    - 入队：\n    \n            Q.data[Q.rear]=x\n            Q.rear=(Q.rear+1)%MaxSize\n\n    - 出队：\n\n            x=Q.data[Q.front]\n            Q.front=(Q.front+1)%MaxSize \n\n\n    - 元素个数：\n            (rear+MaxSize-front)%MaxSize\n\n||队满|队空|\n|:---:|:---:|:---:|\n|1|(Q.rear+1)%MaxSize==Q.front|Q.rear==Q.front\n|2|size==MaxSize|size=0\n(删除成功时flag=0,插入成功时flag=1)|front==rear && flag==1|front==rear && flag==0\n\n## 3.2.4 链式队列\n![](../../../../themes/yilia/source/img/datastruct/3_stack/queue/3.png)\n![数据结构](/img/datastruct/3_stack/queue/3.png)\n\n```c\n#include<stdio.h>\n#include<stdlib.h>\ntypedef struct LinkNode\n{\n    int data;\n    struct LinkNode *next;\n}LinkNode;\n\ntypedef struct\n{\n    LinkNode *front,*rear;\n}LinkQueue;\n\n//初始化\nvoid InitQueue(LinkQueue *Q){\n    Q->front=Q->rear=(LinkNode*)malloc(sizeof(LinkNode));\n    Q->front->next=NULL;\n}\n\n//判空\nint QueueEmpty(LinkQueue Q){\n    if(Q.front==Q.rear)\n        return 1;\n    else\n        return 0;\n}\n\n//入队\nint EnQueue(LinkQueue *Q, int x){\n    LinkNode *s=(LinkNode*)malloc(sizeof(LinkNode));\n    s->data=x;\n    s->next=NULL;\n    Q->rear->next=s;\n    Q->rear=s;\n    return 1;\n}\n\n//出队\nint DeQueue(LinkQueue *Q, int *x){\n    if(Q->front==Q->rear)\n        return 0;\n    LinkNode *p=Q->front->next;\n    *x=p->data;\n    Q->front->next=p->next;\n    if(Q->rear==p)\n        Q->rear=Q->front;\n    free(p);\n    return 1;\n}\n```\n\n- 双端队列\n![](../../../../themes/yilia/source/img/datastruct/3_stack/queue/4.png)\n![数据结构](/img/datastruct/3_stack/queue/4.png)\n\n![](../../../../themes/yilia/source/img/datastruct/3_stack/queue/5.png)\n![数据结构](/img/datastruct/3_stack/queue/5.png)","source":"_posts/datastruct/3_stack/2_queue.md","raw":"---\ntitle: 3.2 队列\ndate: 2023-08-07 00:00:00\ntags: [数据结构,队列]\ncategories: [数据结构]\ncomment: false\ntoc: true\n---\n#\n<!--more-->\n\n![](../../../../themes/yilia/source/img/datastruct/3_stack/queue/1.png)\n![数据结构](/img/datastruct/3_stack/queue/1.png)\n\n\n## 3.2.1 定义\n- 队列是只允许在一端进行插入操作，而在另一端进行删除操作的线性表\n\n## 3.2.2 基本操作\n|操作|描述|\n|:---:|:---:|\n|InitQueue(&Q)|初始化队列Q，构造一个空队列|\n|DestroyQueue(&Q)|若队列Q存在，则销毁它|\n|EnQueue(&Q,x)|若队列Q存在，插入新元素x到队列Q中并成为队尾元素|\n|DeQueue(&Q,&x)|删除队列Q中队头元素，并用x返回其值|\n|GetHead(Q,&x)|返回队列Q中队头元素，不修改队头指针|\n|QueueEmpty(Q)|若队列为空，返回true，否则返回false|\n\n## 3.2.3 顺序队列\n![](../../../../themes/yilia/source/img/datastruct/3_stack/queue/2.png)\n![数据结构](/img/datastruct/3_stack/queue/2.png)\n\n- 顺序队列的实现\n```c\n#include<stdio.h>\n#include<stdlib.h>\n#define MAXSIZE 10\ntypedef struct{\n    int data[MAXSIZE];\n    int front,rear;\n} SqQueue;\n\n//初始化\nvoid InitQueue(SqQueue *Q){\n    int i;\n    for(i=0;i<MAXSIZE;i++)\n        Q->data[i]=0;\n}\n\n//判空\nint QueueEmpty(SqQueue Q){\n    if(Q.front==Q.rear)\n        return 1;\n    else\n        return 0;\n}\n\n//入队\nint EnQueue(SqQueue *Q, int x){\n    if((Q->rear+1)%MAXSIZE==Q->front)\n        return 0;\n    Q->data[Q->rear]=x;\n    Q->rear=(Q->rear+1)%MAXSIZE;\n    return 1;\n}\n\n//出队\nint DeQueue(SqQueue *Q, int *x){\n    if(Q->front==Q->rear)\n        return 0;\n    *x=Q->data[Q->front];\n    Q->front=(Q->front+1)%MAXSIZE;\n    return 1;\n}\n\n```\n-  循环队列\n    - 入队：\n    \n            Q.data[Q.rear]=x\n            Q.rear=(Q.rear+1)%MaxSize\n\n    - 出队：\n\n            x=Q.data[Q.front]\n            Q.front=(Q.front+1)%MaxSize \n\n\n    - 元素个数：\n            (rear+MaxSize-front)%MaxSize\n\n||队满|队空|\n|:---:|:---:|:---:|\n|1|(Q.rear+1)%MaxSize==Q.front|Q.rear==Q.front\n|2|size==MaxSize|size=0\n(删除成功时flag=0,插入成功时flag=1)|front==rear && flag==1|front==rear && flag==0\n\n## 3.2.4 链式队列\n![](../../../../themes/yilia/source/img/datastruct/3_stack/queue/3.png)\n![数据结构](/img/datastruct/3_stack/queue/3.png)\n\n```c\n#include<stdio.h>\n#include<stdlib.h>\ntypedef struct LinkNode\n{\n    int data;\n    struct LinkNode *next;\n}LinkNode;\n\ntypedef struct\n{\n    LinkNode *front,*rear;\n}LinkQueue;\n\n//初始化\nvoid InitQueue(LinkQueue *Q){\n    Q->front=Q->rear=(LinkNode*)malloc(sizeof(LinkNode));\n    Q->front->next=NULL;\n}\n\n//判空\nint QueueEmpty(LinkQueue Q){\n    if(Q.front==Q.rear)\n        return 1;\n    else\n        return 0;\n}\n\n//入队\nint EnQueue(LinkQueue *Q, int x){\n    LinkNode *s=(LinkNode*)malloc(sizeof(LinkNode));\n    s->data=x;\n    s->next=NULL;\n    Q->rear->next=s;\n    Q->rear=s;\n    return 1;\n}\n\n//出队\nint DeQueue(LinkQueue *Q, int *x){\n    if(Q->front==Q->rear)\n        return 0;\n    LinkNode *p=Q->front->next;\n    *x=p->data;\n    Q->front->next=p->next;\n    if(Q->rear==p)\n        Q->rear=Q->front;\n    free(p);\n    return 1;\n}\n```\n\n- 双端队列\n![](../../../../themes/yilia/source/img/datastruct/3_stack/queue/4.png)\n![数据结构](/img/datastruct/3_stack/queue/4.png)\n\n![](../../../../themes/yilia/source/img/datastruct/3_stack/queue/5.png)\n![数据结构](/img/datastruct/3_stack/queue/5.png)","slug":"datastruct/3_stack/2_queue","published":1,"updated":"2023-10-23T12:08:20.396Z","comments":1,"layout":"post","photos":[],"_id":"clvulgd5t00125ovwd7qf9rm1","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/queue/1.png\"><br><img src=\"/img/datastruct/3_stack/queue/1.png\" alt=\"数据结构\"></p>\n<h2 id=\"3-2-1-定义\"><a href=\"#3-2-1-定义\" class=\"headerlink\" title=\"3.2.1 定义\"></a>3.2.1 定义</h2><ul>\n<li>队列是只允许在一端进行插入操作，而在另一端进行删除操作的线性表</li>\n</ul>\n<h2 id=\"3-2-2-基本操作\"><a href=\"#3-2-2-基本操作\" class=\"headerlink\" title=\"3.2.2 基本操作\"></a>3.2.2 基本操作</h2><table>\n<thead>\n<tr>\n<th align=\"center\">操作</th>\n<th align=\"center\">描述</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">InitQueue(&amp;Q)</td>\n<td align=\"center\">初始化队列Q，构造一个空队列</td>\n</tr>\n<tr>\n<td align=\"center\">DestroyQueue(&amp;Q)</td>\n<td align=\"center\">若队列Q存在，则销毁它</td>\n</tr>\n<tr>\n<td align=\"center\">EnQueue(&amp;Q,x)</td>\n<td align=\"center\">若队列Q存在，插入新元素x到队列Q中并成为队尾元素</td>\n</tr>\n<tr>\n<td align=\"center\">DeQueue(&amp;Q,&amp;x)</td>\n<td align=\"center\">删除队列Q中队头元素，并用x返回其值</td>\n</tr>\n<tr>\n<td align=\"center\">GetHead(Q,&amp;x)</td>\n<td align=\"center\">返回队列Q中队头元素，不修改队头指针</td>\n</tr>\n<tr>\n<td align=\"center\">QueueEmpty(Q)</td>\n<td align=\"center\">若队列为空，返回true，否则返回false</td>\n</tr>\n</tbody></table>\n<h2 id=\"3-2-3-顺序队列\"><a href=\"#3-2-3-顺序队列\" class=\"headerlink\" title=\"3.2.3 顺序队列\"></a>3.2.3 顺序队列</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/queue/2.png\"><br><img src=\"/img/datastruct/3_stack/queue/2.png\" alt=\"数据结构\"></p>\n<ul>\n<li>顺序队列的实现<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;stdlib.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> MAXSIZE 10</span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span>&#123;</span></span><br><span class=\"line\">    <span class=\"type\">int</span> data[MAXSIZE];</span><br><span class=\"line\">    <span class=\"type\">int</span> front,rear;</span><br><span class=\"line\">&#125; SqQueue;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//初始化</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">InitQueue</span><span class=\"params\">(SqQueue *Q)</span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> i;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(i=<span class=\"number\">0</span>;i&lt;MAXSIZE;i++)</span><br><span class=\"line\">        Q-&gt;data[i]=<span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//判空</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">QueueEmpty</span><span class=\"params\">(SqQueue Q)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(Q.front==Q.rear)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">else</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//入队</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">EnQueue</span><span class=\"params\">(SqQueue *Q, <span class=\"type\">int</span> x)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>((Q-&gt;rear+<span class=\"number\">1</span>)%MAXSIZE==Q-&gt;front)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    Q-&gt;data[Q-&gt;rear]=x;</span><br><span class=\"line\">    Q-&gt;rear=(Q-&gt;rear+<span class=\"number\">1</span>)%MAXSIZE;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//出队</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">DeQueue</span><span class=\"params\">(SqQueue *Q, <span class=\"type\">int</span> *x)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(Q-&gt;front==Q-&gt;rear)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    *x=Q-&gt;data[Q-&gt;front];</span><br><span class=\"line\">    Q-&gt;front=(Q-&gt;front+<span class=\"number\">1</span>)%MAXSIZE;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></li>\n<li>循环队列<ul>\n<li><p>入队：</p>\n<pre><code>  Q.data[Q.rear]=x\n  Q.rear=(Q.rear+1)%MaxSize\n</code></pre>\n</li>\n<li><p>出队：</p>\n<pre><code>  x=Q.data[Q.front]\n  Q.front=(Q.front+1)%MaxSize \n</code></pre>\n</li>\n<li><p>元素个数：<br>  (rear+MaxSize-front)%MaxSize</p>\n</li>\n</ul>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th align=\"center\"></th>\n<th align=\"center\">队满</th>\n<th align=\"center\">队空</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">1</td>\n<td align=\"center\">(Q.rear+1)%MaxSize&#x3D;&#x3D;Q.front</td>\n<td align=\"center\">Q.rear&#x3D;&#x3D;Q.front</td>\n</tr>\n<tr>\n<td align=\"center\">2</td>\n<td align=\"center\">size&#x3D;&#x3D;MaxSize</td>\n<td align=\"center\">size&#x3D;0</td>\n</tr>\n<tr>\n<td align=\"center\">(删除成功时flag&#x3D;0,插入成功时flag&#x3D;1)</td>\n<td align=\"center\">front&#x3D;&#x3D;rear &amp;&amp; flag&#x3D;&#x3D;1</td>\n<td align=\"center\">front&#x3D;&#x3D;rear &amp;&amp; flag&#x3D;&#x3D;0</td>\n</tr>\n</tbody></table>\n<h2 id=\"3-2-4-链式队列\"><a href=\"#3-2-4-链式队列\" class=\"headerlink\" title=\"3.2.4 链式队列\"></a>3.2.4 链式队列</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/queue/3.png\"><br><img src=\"/img/datastruct/3_stack/queue/3.png\" alt=\"数据结构\"></p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;stdlib.h&gt;</span></span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">LinkNode</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    <span class=\"type\">int</span> data;</span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">LinkNode</span> *<span class=\"title\">next</span>;</span></span><br><span class=\"line\">&#125;LinkNode;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    LinkNode *front,*rear;</span><br><span class=\"line\">&#125;LinkQueue;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//初始化</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">InitQueue</span><span class=\"params\">(LinkQueue *Q)</span>&#123;</span><br><span class=\"line\">    Q-&gt;front=Q-&gt;rear=(LinkNode*)<span class=\"built_in\">malloc</span>(<span class=\"keyword\">sizeof</span>(LinkNode));</span><br><span class=\"line\">    Q-&gt;front-&gt;next=<span class=\"literal\">NULL</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//判空</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">QueueEmpty</span><span class=\"params\">(LinkQueue Q)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(Q.front==Q.rear)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">else</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//入队</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">EnQueue</span><span class=\"params\">(LinkQueue *Q, <span class=\"type\">int</span> x)</span>&#123;</span><br><span class=\"line\">    LinkNode *s=(LinkNode*)<span class=\"built_in\">malloc</span>(<span class=\"keyword\">sizeof</span>(LinkNode));</span><br><span class=\"line\">    s-&gt;data=x;</span><br><span class=\"line\">    s-&gt;next=<span class=\"literal\">NULL</span>;</span><br><span class=\"line\">    Q-&gt;rear-&gt;next=s;</span><br><span class=\"line\">    Q-&gt;rear=s;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//出队</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">DeQueue</span><span class=\"params\">(LinkQueue *Q, <span class=\"type\">int</span> *x)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(Q-&gt;front==Q-&gt;rear)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    LinkNode *p=Q-&gt;front-&gt;next;</span><br><span class=\"line\">    *x=p-&gt;data;</span><br><span class=\"line\">    Q-&gt;front-&gt;next=p-&gt;next;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(Q-&gt;rear==p)</span><br><span class=\"line\">        Q-&gt;rear=Q-&gt;front;</span><br><span class=\"line\">    <span class=\"built_in\">free</span>(p);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>双端队列<br><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/queue/4.png\"><br><img src=\"/img/datastruct/3_stack/queue/4.png\" alt=\"数据结构\"></li>\n</ul>\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/queue/5.png\"><br><img src=\"/img/datastruct/3_stack/queue/5.png\" alt=\"数据结构\"></p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<p><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/queue/1.png\"><br><img src=\"/img/datastruct/3_stack/queue/1.png\" alt=\"数据结构\"></p>\n<h2 id=\"3-2-1-定义\"><a href=\"#3-2-1-定义\" class=\"headerlink\" title=\"3.2.1 定义\"></a>3.2.1 定义</h2><ul>\n<li>队列是只允许在一端进行插入操作，而在另一端进行删除操作的线性表</li>\n</ul>\n<h2 id=\"3-2-2-基本操作\"><a href=\"#3-2-2-基本操作\" class=\"headerlink\" title=\"3.2.2 基本操作\"></a>3.2.2 基本操作</h2><table>\n<thead>\n<tr>\n<th align=\"center\">操作</th>\n<th align=\"center\">描述</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">InitQueue(&amp;Q)</td>\n<td align=\"center\">初始化队列Q，构造一个空队列</td>\n</tr>\n<tr>\n<td align=\"center\">DestroyQueue(&amp;Q)</td>\n<td align=\"center\">若队列Q存在，则销毁它</td>\n</tr>\n<tr>\n<td align=\"center\">EnQueue(&amp;Q,x)</td>\n<td align=\"center\">若队列Q存在，插入新元素x到队列Q中并成为队尾元素</td>\n</tr>\n<tr>\n<td align=\"center\">DeQueue(&amp;Q,&amp;x)</td>\n<td align=\"center\">删除队列Q中队头元素，并用x返回其值</td>\n</tr>\n<tr>\n<td align=\"center\">GetHead(Q,&amp;x)</td>\n<td align=\"center\">返回队列Q中队头元素，不修改队头指针</td>\n</tr>\n<tr>\n<td align=\"center\">QueueEmpty(Q)</td>\n<td align=\"center\">若队列为空，返回true，否则返回false</td>\n</tr>\n</tbody></table>\n<h2 id=\"3-2-3-顺序队列\"><a href=\"#3-2-3-顺序队列\" class=\"headerlink\" title=\"3.2.3 顺序队列\"></a>3.2.3 顺序队列</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/queue/2.png\"><br><img src=\"/img/datastruct/3_stack/queue/2.png\" alt=\"数据结构\"></p>\n<ul>\n<li>顺序队列的实现<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;stdlib.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> MAXSIZE 10</span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span>&#123;</span></span><br><span class=\"line\">    <span class=\"type\">int</span> data[MAXSIZE];</span><br><span class=\"line\">    <span class=\"type\">int</span> front,rear;</span><br><span class=\"line\">&#125; SqQueue;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//初始化</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">InitQueue</span><span class=\"params\">(SqQueue *Q)</span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> i;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(i=<span class=\"number\">0</span>;i&lt;MAXSIZE;i++)</span><br><span class=\"line\">        Q-&gt;data[i]=<span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//判空</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">QueueEmpty</span><span class=\"params\">(SqQueue Q)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(Q.front==Q.rear)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">else</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//入队</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">EnQueue</span><span class=\"params\">(SqQueue *Q, <span class=\"type\">int</span> x)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>((Q-&gt;rear+<span class=\"number\">1</span>)%MAXSIZE==Q-&gt;front)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    Q-&gt;data[Q-&gt;rear]=x;</span><br><span class=\"line\">    Q-&gt;rear=(Q-&gt;rear+<span class=\"number\">1</span>)%MAXSIZE;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//出队</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">DeQueue</span><span class=\"params\">(SqQueue *Q, <span class=\"type\">int</span> *x)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(Q-&gt;front==Q-&gt;rear)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    *x=Q-&gt;data[Q-&gt;front];</span><br><span class=\"line\">    Q-&gt;front=(Q-&gt;front+<span class=\"number\">1</span>)%MAXSIZE;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></li>\n<li>循环队列<ul>\n<li><p>入队：</p>\n<pre><code>  Q.data[Q.rear]=x\n  Q.rear=(Q.rear+1)%MaxSize\n</code></pre>\n</li>\n<li><p>出队：</p>\n<pre><code>  x=Q.data[Q.front]\n  Q.front=(Q.front+1)%MaxSize \n</code></pre>\n</li>\n<li><p>元素个数：<br>  (rear+MaxSize-front)%MaxSize</p>\n</li>\n</ul>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th align=\"center\"></th>\n<th align=\"center\">队满</th>\n<th align=\"center\">队空</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">1</td>\n<td align=\"center\">(Q.rear+1)%MaxSize&#x3D;&#x3D;Q.front</td>\n<td align=\"center\">Q.rear&#x3D;&#x3D;Q.front</td>\n</tr>\n<tr>\n<td align=\"center\">2</td>\n<td align=\"center\">size&#x3D;&#x3D;MaxSize</td>\n<td align=\"center\">size&#x3D;0</td>\n</tr>\n<tr>\n<td align=\"center\">(删除成功时flag&#x3D;0,插入成功时flag&#x3D;1)</td>\n<td align=\"center\">front&#x3D;&#x3D;rear &amp;&amp; flag&#x3D;&#x3D;1</td>\n<td align=\"center\">front&#x3D;&#x3D;rear &amp;&amp; flag&#x3D;&#x3D;0</td>\n</tr>\n</tbody></table>\n<h2 id=\"3-2-4-链式队列\"><a href=\"#3-2-4-链式队列\" class=\"headerlink\" title=\"3.2.4 链式队列\"></a>3.2.4 链式队列</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/queue/3.png\"><br><img src=\"/img/datastruct/3_stack/queue/3.png\" alt=\"数据结构\"></p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;stdlib.h&gt;</span></span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">LinkNode</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    <span class=\"type\">int</span> data;</span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">LinkNode</span> *<span class=\"title\">next</span>;</span></span><br><span class=\"line\">&#125;LinkNode;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">    LinkNode *front,*rear;</span><br><span class=\"line\">&#125;LinkQueue;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//初始化</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">InitQueue</span><span class=\"params\">(LinkQueue *Q)</span>&#123;</span><br><span class=\"line\">    Q-&gt;front=Q-&gt;rear=(LinkNode*)<span class=\"built_in\">malloc</span>(<span class=\"keyword\">sizeof</span>(LinkNode));</span><br><span class=\"line\">    Q-&gt;front-&gt;next=<span class=\"literal\">NULL</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//判空</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">QueueEmpty</span><span class=\"params\">(LinkQueue Q)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(Q.front==Q.rear)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">else</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//入队</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">EnQueue</span><span class=\"params\">(LinkQueue *Q, <span class=\"type\">int</span> x)</span>&#123;</span><br><span class=\"line\">    LinkNode *s=(LinkNode*)<span class=\"built_in\">malloc</span>(<span class=\"keyword\">sizeof</span>(LinkNode));</span><br><span class=\"line\">    s-&gt;data=x;</span><br><span class=\"line\">    s-&gt;next=<span class=\"literal\">NULL</span>;</span><br><span class=\"line\">    Q-&gt;rear-&gt;next=s;</span><br><span class=\"line\">    Q-&gt;rear=s;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//出队</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">DeQueue</span><span class=\"params\">(LinkQueue *Q, <span class=\"type\">int</span> *x)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(Q-&gt;front==Q-&gt;rear)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    LinkNode *p=Q-&gt;front-&gt;next;</span><br><span class=\"line\">    *x=p-&gt;data;</span><br><span class=\"line\">    Q-&gt;front-&gt;next=p-&gt;next;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(Q-&gt;rear==p)</span><br><span class=\"line\">        Q-&gt;rear=Q-&gt;front;</span><br><span class=\"line\">    <span class=\"built_in\">free</span>(p);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>双端队列<br><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/queue/4.png\"><br><img src=\"/img/datastruct/3_stack/queue/4.png\" alt=\"数据结构\"></li>\n</ul>\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/queue/5.png\"><br><img src=\"/img/datastruct/3_stack/queue/5.png\" alt=\"数据结构\"></p>"},{"title":"3.3 栈与队列的应用","date":"2023-08-06T16:00:00.000Z","comment":false,"toc":true,"_content":"#\n<!--more-->\n\n## 3.3.1 括号匹配问题\n![](../../../../themes/yilia/source/img/datastruct/3_stack/exam/1.png)\n![数据结构](/img/datastruct/3_stack/exam/1.png)\n\n```c\n#include<stdio.h>\n#define MAXSIZE 10\ntypedef struct{\n    char data[MAXSIZE];\n    int top;\n} SqStack;\n\n//初始化\nvoid InitStack(SqStack *S){\n    S->top=-1;\n}\n\n//判空\nint StackEmpty(SqStack S){\n    if(S.top==-1)\n        return 1;\n    else\n        return 0;\n}\n\n//入栈\nint Push(SqStack *S, char x){\n    if(S->top==MAXSIZE-1)\n        return 0;\n    S->top++;\n    S->data[S->top]=x;\n    return 1;\n}\n\n//出栈\nint Pop(SqStack *S, char *x){\n    if(S->top==-1)\n        return 0;\n    *x=S->data[S->top];\n    S->top--;\n    return 1;\n}\n\n//取栈顶元素\nint GetTop(SqStack S, char *x){\n    if(S.top==-1)\n        return 0;\n    *x=S.data[S.top];\n    return 1;\n}\n\nint main(){\n    //1. 括号匹配\n    char str1[100] = \"((())){[]}\"; //合法\n    char str2[100] = \"((()){[]})\"; //合法\n    char str3[100] = \"((()){[]}\"; //不合法\n    printf(\"%d\",match(str1));\n    printf(\"%d\",match(str2));\n    printf(\"%d\",match(str3));\n}\n\nint match(char str[]){\n    SqStack stack;\n    InitStack(&stack);\n    int i=0;\n    char x;\n    for(;str[i]!='\\0';i++){\n        switch (str[i])\n        {\n        case '(':\n        case '[':\n        case '{':\n            Push(&stack, str[i]);\n            break;\n        case ')':\n            if(!Pop(&stack, &x) || x!='(')\n                return 0;\n            break;\n        case ']':\n            if(!Pop(&stack, &x) || x!='[')\n                return 0;\n            break;\n        case '}':\n            if(!Pop(&stack, &x) || x!='{')\n                return 0;\n            break;\n        default:\n            break;\n        }\n    }\n    if(!StackEmpty(stack))\n        return 0;\n    return 1;\n}\n```\n\n## 3.3.2 表达式求值\n![](../../../../themes/yilia/source/img/datastruct/3_stack/exam/2.png)\n![数据结构](/img/datastruct/3_stack/exam/2.png)\n![](../../../../themes/yilia/source/img/datastruct/3_stack/exam/6.png)\n![数据结构](/img/datastruct/3_stack/exam/6.png)\n- 中缀->后缀\n    - 手算\n\n            1）确定各个运算符的运算顺序\n            2）选择下一个运算符(左右符)\n            3）还有符号则返回2）\n\n    - 代码\n        ![](../../../../themes/yilia/source/img/datastruct/3_stack/exam/4.png)\n        ![数据结构](/img/datastruct/3_stack/exam/4.png)\n\n- 中缀表达式的计算\n![](../../../../themes/yilia/source/img/datastruct/3_stack/exam/5.png)\n![数据结构](/img/datastruct/3_stack/exam/5.png)\n\n\n- 用栈实现后缀表达式的计算\n\n        1）从左往右扫描下一个元素，直到处理完所有元素\n        2）若扫描到操作数则入栈，返回1）\n        3）若扫描到运算符，弹出两个栈顶，执行运算后入栈(先出栈的是右操作数)\n\n- 中缀->前缀\n\n        1）确定各个运算符的运算顺序\n        2）选择下一个运算符(符左右)\n        3）还有符号则返回2）\n![](../../../../themes/yilia/source/img/datastruct/3_stack/exam/3.png)\n![数据结构](/img/datastruct/3_stack/exam/3.png)","source":"_posts/datastruct/3_stack/3_exam.md","raw":"---\ntitle: 3.3 栈与队列的应用\ndate: 2023-08-07 00:00:00\ntags: [数据结构,栈,队列]\ncategories: [数据结构]\ncomment: false\ntoc: true\n---\n#\n<!--more-->\n\n## 3.3.1 括号匹配问题\n![](../../../../themes/yilia/source/img/datastruct/3_stack/exam/1.png)\n![数据结构](/img/datastruct/3_stack/exam/1.png)\n\n```c\n#include<stdio.h>\n#define MAXSIZE 10\ntypedef struct{\n    char data[MAXSIZE];\n    int top;\n} SqStack;\n\n//初始化\nvoid InitStack(SqStack *S){\n    S->top=-1;\n}\n\n//判空\nint StackEmpty(SqStack S){\n    if(S.top==-1)\n        return 1;\n    else\n        return 0;\n}\n\n//入栈\nint Push(SqStack *S, char x){\n    if(S->top==MAXSIZE-1)\n        return 0;\n    S->top++;\n    S->data[S->top]=x;\n    return 1;\n}\n\n//出栈\nint Pop(SqStack *S, char *x){\n    if(S->top==-1)\n        return 0;\n    *x=S->data[S->top];\n    S->top--;\n    return 1;\n}\n\n//取栈顶元素\nint GetTop(SqStack S, char *x){\n    if(S.top==-1)\n        return 0;\n    *x=S.data[S.top];\n    return 1;\n}\n\nint main(){\n    //1. 括号匹配\n    char str1[100] = \"((())){[]}\"; //合法\n    char str2[100] = \"((()){[]})\"; //合法\n    char str3[100] = \"((()){[]}\"; //不合法\n    printf(\"%d\",match(str1));\n    printf(\"%d\",match(str2));\n    printf(\"%d\",match(str3));\n}\n\nint match(char str[]){\n    SqStack stack;\n    InitStack(&stack);\n    int i=0;\n    char x;\n    for(;str[i]!='\\0';i++){\n        switch (str[i])\n        {\n        case '(':\n        case '[':\n        case '{':\n            Push(&stack, str[i]);\n            break;\n        case ')':\n            if(!Pop(&stack, &x) || x!='(')\n                return 0;\n            break;\n        case ']':\n            if(!Pop(&stack, &x) || x!='[')\n                return 0;\n            break;\n        case '}':\n            if(!Pop(&stack, &x) || x!='{')\n                return 0;\n            break;\n        default:\n            break;\n        }\n    }\n    if(!StackEmpty(stack))\n        return 0;\n    return 1;\n}\n```\n\n## 3.3.2 表达式求值\n![](../../../../themes/yilia/source/img/datastruct/3_stack/exam/2.png)\n![数据结构](/img/datastruct/3_stack/exam/2.png)\n![](../../../../themes/yilia/source/img/datastruct/3_stack/exam/6.png)\n![数据结构](/img/datastruct/3_stack/exam/6.png)\n- 中缀->后缀\n    - 手算\n\n            1）确定各个运算符的运算顺序\n            2）选择下一个运算符(左右符)\n            3）还有符号则返回2）\n\n    - 代码\n        ![](../../../../themes/yilia/source/img/datastruct/3_stack/exam/4.png)\n        ![数据结构](/img/datastruct/3_stack/exam/4.png)\n\n- 中缀表达式的计算\n![](../../../../themes/yilia/source/img/datastruct/3_stack/exam/5.png)\n![数据结构](/img/datastruct/3_stack/exam/5.png)\n\n\n- 用栈实现后缀表达式的计算\n\n        1）从左往右扫描下一个元素，直到处理完所有元素\n        2）若扫描到操作数则入栈，返回1）\n        3）若扫描到运算符，弹出两个栈顶，执行运算后入栈(先出栈的是右操作数)\n\n- 中缀->前缀\n\n        1）确定各个运算符的运算顺序\n        2）选择下一个运算符(符左右)\n        3）还有符号则返回2）\n![](../../../../themes/yilia/source/img/datastruct/3_stack/exam/3.png)\n![数据结构](/img/datastruct/3_stack/exam/3.png)","slug":"datastruct/3_stack/3_exam","published":1,"updated":"2023-10-23T12:08:38.400Z","comments":1,"layout":"post","photos":[],"_id":"clvulgd5t00145ovw4k4d9z5y","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h2 id=\"3-3-1-括号匹配问题\"><a href=\"#3-3-1-括号匹配问题\" class=\"headerlink\" title=\"3.3.1 括号匹配问题\"></a>3.3.1 括号匹配问题</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/exam/1.png\"><br><img src=\"/img/datastruct/3_stack/exam/1.png\" alt=\"数据结构\"></p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> MAXSIZE 10</span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span>&#123;</span></span><br><span class=\"line\">    <span class=\"type\">char</span> data[MAXSIZE];</span><br><span class=\"line\">    <span class=\"type\">int</span> top;</span><br><span class=\"line\">&#125; SqStack;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//初始化</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">InitStack</span><span class=\"params\">(SqStack *S)</span>&#123;</span><br><span class=\"line\">    S-&gt;top=<span class=\"number\">-1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//判空</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">StackEmpty</span><span class=\"params\">(SqStack S)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(S.top==<span class=\"number\">-1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">else</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//入栈</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">Push</span><span class=\"params\">(SqStack *S, <span class=\"type\">char</span> x)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(S-&gt;top==MAXSIZE<span class=\"number\">-1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    S-&gt;top++;</span><br><span class=\"line\">    S-&gt;data[S-&gt;top]=x;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//出栈</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">Pop</span><span class=\"params\">(SqStack *S, <span class=\"type\">char</span> *x)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(S-&gt;top==<span class=\"number\">-1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    *x=S-&gt;data[S-&gt;top];</span><br><span class=\"line\">    S-&gt;top--;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//取栈顶元素</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">GetTop</span><span class=\"params\">(SqStack S, <span class=\"type\">char</span> *x)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(S.top==<span class=\"number\">-1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    *x=S.data[S.top];</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">//1. 括号匹配</span></span><br><span class=\"line\">    <span class=\"type\">char</span> str1[<span class=\"number\">100</span>] = <span class=\"string\">&quot;((()))&#123;[]&#125;&quot;</span>; <span class=\"comment\">//合法</span></span><br><span class=\"line\">    <span class=\"type\">char</span> str2[<span class=\"number\">100</span>] = <span class=\"string\">&quot;((())&#123;[]&#125;)&quot;</span>; <span class=\"comment\">//合法</span></span><br><span class=\"line\">    <span class=\"type\">char</span> str3[<span class=\"number\">100</span>] = <span class=\"string\">&quot;((())&#123;[]&#125;&quot;</span>; <span class=\"comment\">//不合法</span></span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;%d&quot;</span>,match(str1));</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;%d&quot;</span>,match(str2));</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;%d&quot;</span>,match(str3));</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">match</span><span class=\"params\">(<span class=\"type\">char</span> str[])</span>&#123;</span><br><span class=\"line\">    SqStack <span class=\"built_in\">stack</span>;</span><br><span class=\"line\">    InitStack(&amp;<span class=\"built_in\">stack</span>);</span><br><span class=\"line\">    <span class=\"type\">int</span> i=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"type\">char</span> x;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(;str[i]!=<span class=\"string\">&#x27;\\0&#x27;</span>;i++)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">switch</span> (str[i])</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"string\">&#x27;(&#x27;</span>:</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"string\">&#x27;[&#x27;</span>:</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"string\">&#x27;&#123;&#x27;</span>:</span><br><span class=\"line\">            Push(&amp;<span class=\"built_in\">stack</span>, str[i]);</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"string\">&#x27;)&#x27;</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(!Pop(&amp;<span class=\"built_in\">stack</span>, &amp;x) || x!=<span class=\"string\">&#x27;(&#x27;</span>)</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"string\">&#x27;]&#x27;</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(!Pop(&amp;<span class=\"built_in\">stack</span>, &amp;x) || x!=<span class=\"string\">&#x27;[&#x27;</span>)</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"string\">&#x27;&#125;&#x27;</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(!Pop(&amp;<span class=\"built_in\">stack</span>, &amp;x) || x!=<span class=\"string\">&#x27;&#123;&#x27;</span>)</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        <span class=\"keyword\">default</span>:</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(!StackEmpty(<span class=\"built_in\">stack</span>))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"3-3-2-表达式求值\"><a href=\"#3-3-2-表达式求值\" class=\"headerlink\" title=\"3.3.2 表达式求值\"></a>3.3.2 表达式求值</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/exam/2.png\"><br><img src=\"/img/datastruct/3_stack/exam/2.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/exam/6.png\"><br><img src=\"/img/datastruct/3_stack/exam/6.png\" alt=\"数据结构\"></p>\n<ul>\n<li><p>中缀-&gt;后缀</p>\n<ul>\n<li><p>手算</p>\n<pre><code>  1）确定各个运算符的运算顺序\n  2）选择下一个运算符(左右符)\n  3）还有符号则返回2）\n</code></pre>\n</li>\n<li><p>代码<br>  <img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/exam/4.png\"><br>  <img src=\"/img/datastruct/3_stack/exam/4.png\" alt=\"数据结构\"></p>\n</li>\n</ul>\n</li>\n<li><p>中缀表达式的计算<br><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/exam/5.png\"><br><img src=\"/img/datastruct/3_stack/exam/5.png\" alt=\"数据结构\"></p>\n</li>\n<li><p>用栈实现后缀表达式的计算</p>\n<pre><code>  1）从左往右扫描下一个元素，直到处理完所有元素\n  2）若扫描到操作数则入栈，返回1）\n  3）若扫描到运算符，弹出两个栈顶，执行运算后入栈(先出栈的是右操作数)\n</code></pre>\n</li>\n<li><p>中缀-&gt;前缀</p>\n<pre><code>  1）确定各个运算符的运算顺序\n  2）选择下一个运算符(符左右)\n  3）还有符号则返回2）\n</code></pre>\n</li>\n</ul>\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/exam/3.png\"><br><img src=\"/img/datastruct/3_stack/exam/3.png\" alt=\"数据结构\"></p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h2 id=\"3-3-1-括号匹配问题\"><a href=\"#3-3-1-括号匹配问题\" class=\"headerlink\" title=\"3.3.1 括号匹配问题\"></a>3.3.1 括号匹配问题</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/exam/1.png\"><br><img src=\"/img/datastruct/3_stack/exam/1.png\" alt=\"数据结构\"></p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> MAXSIZE 10</span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span>&#123;</span></span><br><span class=\"line\">    <span class=\"type\">char</span> data[MAXSIZE];</span><br><span class=\"line\">    <span class=\"type\">int</span> top;</span><br><span class=\"line\">&#125; SqStack;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//初始化</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">InitStack</span><span class=\"params\">(SqStack *S)</span>&#123;</span><br><span class=\"line\">    S-&gt;top=<span class=\"number\">-1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//判空</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">StackEmpty</span><span class=\"params\">(SqStack S)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(S.top==<span class=\"number\">-1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">else</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//入栈</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">Push</span><span class=\"params\">(SqStack *S, <span class=\"type\">char</span> x)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(S-&gt;top==MAXSIZE<span class=\"number\">-1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    S-&gt;top++;</span><br><span class=\"line\">    S-&gt;data[S-&gt;top]=x;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//出栈</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">Pop</span><span class=\"params\">(SqStack *S, <span class=\"type\">char</span> *x)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(S-&gt;top==<span class=\"number\">-1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    *x=S-&gt;data[S-&gt;top];</span><br><span class=\"line\">    S-&gt;top--;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//取栈顶元素</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">GetTop</span><span class=\"params\">(SqStack S, <span class=\"type\">char</span> *x)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(S.top==<span class=\"number\">-1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    *x=S.data[S.top];</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">//1. 括号匹配</span></span><br><span class=\"line\">    <span class=\"type\">char</span> str1[<span class=\"number\">100</span>] = <span class=\"string\">&quot;((()))&#123;[]&#125;&quot;</span>; <span class=\"comment\">//合法</span></span><br><span class=\"line\">    <span class=\"type\">char</span> str2[<span class=\"number\">100</span>] = <span class=\"string\">&quot;((())&#123;[]&#125;)&quot;</span>; <span class=\"comment\">//合法</span></span><br><span class=\"line\">    <span class=\"type\">char</span> str3[<span class=\"number\">100</span>] = <span class=\"string\">&quot;((())&#123;[]&#125;&quot;</span>; <span class=\"comment\">//不合法</span></span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;%d&quot;</span>,match(str1));</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;%d&quot;</span>,match(str2));</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;%d&quot;</span>,match(str3));</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">match</span><span class=\"params\">(<span class=\"type\">char</span> str[])</span>&#123;</span><br><span class=\"line\">    SqStack <span class=\"built_in\">stack</span>;</span><br><span class=\"line\">    InitStack(&amp;<span class=\"built_in\">stack</span>);</span><br><span class=\"line\">    <span class=\"type\">int</span> i=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"type\">char</span> x;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(;str[i]!=<span class=\"string\">&#x27;\\0&#x27;</span>;i++)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">switch</span> (str[i])</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"string\">&#x27;(&#x27;</span>:</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"string\">&#x27;[&#x27;</span>:</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"string\">&#x27;&#123;&#x27;</span>:</span><br><span class=\"line\">            Push(&amp;<span class=\"built_in\">stack</span>, str[i]);</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"string\">&#x27;)&#x27;</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(!Pop(&amp;<span class=\"built_in\">stack</span>, &amp;x) || x!=<span class=\"string\">&#x27;(&#x27;</span>)</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"string\">&#x27;]&#x27;</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(!Pop(&amp;<span class=\"built_in\">stack</span>, &amp;x) || x!=<span class=\"string\">&#x27;[&#x27;</span>)</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"string\">&#x27;&#125;&#x27;</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(!Pop(&amp;<span class=\"built_in\">stack</span>, &amp;x) || x!=<span class=\"string\">&#x27;&#123;&#x27;</span>)</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        <span class=\"keyword\">default</span>:</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(!StackEmpty(<span class=\"built_in\">stack</span>))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"3-3-2-表达式求值\"><a href=\"#3-3-2-表达式求值\" class=\"headerlink\" title=\"3.3.2 表达式求值\"></a>3.3.2 表达式求值</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/exam/2.png\"><br><img src=\"/img/datastruct/3_stack/exam/2.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/exam/6.png\"><br><img src=\"/img/datastruct/3_stack/exam/6.png\" alt=\"数据结构\"></p>\n<ul>\n<li><p>中缀-&gt;后缀</p>\n<ul>\n<li><p>手算</p>\n<pre><code>  1）确定各个运算符的运算顺序\n  2）选择下一个运算符(左右符)\n  3）还有符号则返回2）\n</code></pre>\n</li>\n<li><p>代码<br>  <img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/exam/4.png\"><br>  <img src=\"/img/datastruct/3_stack/exam/4.png\" alt=\"数据结构\"></p>\n</li>\n</ul>\n</li>\n<li><p>中缀表达式的计算<br><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/exam/5.png\"><br><img src=\"/img/datastruct/3_stack/exam/5.png\" alt=\"数据结构\"></p>\n</li>\n<li><p>用栈实现后缀表达式的计算</p>\n<pre><code>  1）从左往右扫描下一个元素，直到处理完所有元素\n  2）若扫描到操作数则入栈，返回1）\n  3）若扫描到运算符，弹出两个栈顶，执行运算后入栈(先出栈的是右操作数)\n</code></pre>\n</li>\n<li><p>中缀-&gt;前缀</p>\n<pre><code>  1）确定各个运算符的运算顺序\n  2）选择下一个运算符(符左右)\n  3）还有符号则返回2）\n</code></pre>\n</li>\n</ul>\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/exam/3.png\"><br><img src=\"/img/datastruct/3_stack/exam/3.png\" alt=\"数据结构\"></p>"},{"title":"7.1 顺序查找","date":"2023-08-06T16:00:00.000Z","comment":false,"toc":true,"_content":"#\n<!--more-->\n\n### \n\n## 7.1-1 概念\n![](../../../../themes/yilia/source/img/datastruct/7_search/search/1.png)\n![数据结构](/img/datastruct/7_search/search/1.png)\n<!--more-->\n- 平均查找长度：$$ASL=\\sum_{i=1}^n p_i c_i$$\n\n## 7.1-2 顺序查找\n![](../../../../themes/yilia/source/img/datastruct/7_search/search/2.png)\n![数据结构](/img/datastruct/7_search/search/2.png)\n\n## 7.1-3 折半查找\n![](../../../../themes/yilia/source/img/datastruct/7_search/search/3.png)\n![数据结构](/img/datastruct/7_search/search/3.png)\n\n![](../../../../themes/yilia/source/img/datastruct/7_search/search/4.png)\n![数据结构](/img/datastruct/7_search/search/4.png)\n\n![](../../../../themes/yilia/source/img/datastruct/7_search/search/5.png)\n![数据结构](/img/datastruct/7_search/search/5.png)\n\n## 7.1-4 分块查找\n![](../../../../themes/yilia/source/img/datastruct/7_search/search/6.png)\n![数据结构](/img/datastruct/7_search/search/6.png)\n\n![](../../../../themes/yilia/source/img/datastruct/7_search/search/7.png)\n![数据结构](/img/datastruct/7_search/search/7.png)\n\n![](../../../../themes/yilia/source/img/datastruct/7_search/search/8.png)\n![数据结构](/img/datastruct/7_search/search/8.png)","source":"_posts/datastruct/7_search/1_search.md","raw":"---\ntitle: 7.1 顺序查找\ndate: 2023-08-07 00:00:00\ntags: [数据结构,查找]\ncategories: [数据结构]\ncomment: false\ntoc: true\n---\n#\n<!--more-->\n\n### \n\n## 7.1-1 概念\n![](../../../../themes/yilia/source/img/datastruct/7_search/search/1.png)\n![数据结构](/img/datastruct/7_search/search/1.png)\n<!--more-->\n- 平均查找长度：$$ASL=\\sum_{i=1}^n p_i c_i$$\n\n## 7.1-2 顺序查找\n![](../../../../themes/yilia/source/img/datastruct/7_search/search/2.png)\n![数据结构](/img/datastruct/7_search/search/2.png)\n\n## 7.1-3 折半查找\n![](../../../../themes/yilia/source/img/datastruct/7_search/search/3.png)\n![数据结构](/img/datastruct/7_search/search/3.png)\n\n![](../../../../themes/yilia/source/img/datastruct/7_search/search/4.png)\n![数据结构](/img/datastruct/7_search/search/4.png)\n\n![](../../../../themes/yilia/source/img/datastruct/7_search/search/5.png)\n![数据结构](/img/datastruct/7_search/search/5.png)\n\n## 7.1-4 分块查找\n![](../../../../themes/yilia/source/img/datastruct/7_search/search/6.png)\n![数据结构](/img/datastruct/7_search/search/6.png)\n\n![](../../../../themes/yilia/source/img/datastruct/7_search/search/7.png)\n![数据结构](/img/datastruct/7_search/search/7.png)\n\n![](../../../../themes/yilia/source/img/datastruct/7_search/search/8.png)\n![数据结构](/img/datastruct/7_search/search/8.png)","slug":"datastruct/7_search/1_search","published":1,"updated":"2023-10-23T12:50:18.625Z","comments":1,"layout":"post","photos":[],"_id":"clvulgd5u00185ovwdgnf79h0","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h3 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a></h3><h2 id=\"7-1-1-概念\"><a href=\"#7-1-1-概念\" class=\"headerlink\" title=\"7.1-1 概念\"></a>7.1-1 概念</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/search/1.png\"><br><img src=\"/img/datastruct/7_search/search/1.png\" alt=\"数据结构\"></p>\n<!--more-->\n<ul>\n<li>平均查找长度：$$ASL&#x3D;\\sum_{i&#x3D;1}^n p_i c_i$$</li>\n</ul>\n<h2 id=\"7-1-2-顺序查找\"><a href=\"#7-1-2-顺序查找\" class=\"headerlink\" title=\"7.1-2 顺序查找\"></a>7.1-2 顺序查找</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/search/2.png\"><br><img src=\"/img/datastruct/7_search/search/2.png\" alt=\"数据结构\"></p>\n<h2 id=\"7-1-3-折半查找\"><a href=\"#7-1-3-折半查找\" class=\"headerlink\" title=\"7.1-3 折半查找\"></a>7.1-3 折半查找</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/search/3.png\"><br><img src=\"/img/datastruct/7_search/search/3.png\" alt=\"数据结构\"></p>\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/search/4.png\"><br><img src=\"/img/datastruct/7_search/search/4.png\" alt=\"数据结构\"></p>\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/search/5.png\"><br><img src=\"/img/datastruct/7_search/search/5.png\" alt=\"数据结构\"></p>\n<h2 id=\"7-1-4-分块查找\"><a href=\"#7-1-4-分块查找\" class=\"headerlink\" title=\"7.1-4 分块查找\"></a>7.1-4 分块查找</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/search/6.png\"><br><img src=\"/img/datastruct/7_search/search/6.png\" alt=\"数据结构\"></p>\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/search/7.png\"><br><img src=\"/img/datastruct/7_search/search/7.png\" alt=\"数据结构\"></p>\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/search/8.png\"><br><img src=\"/img/datastruct/7_search/search/8.png\" alt=\"数据结构\"></p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h3 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a></h3><h2 id=\"7-1-1-概念\"><a href=\"#7-1-1-概念\" class=\"headerlink\" title=\"7.1-1 概念\"></a>7.1-1 概念</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/search/1.png\"><br><img src=\"/img/datastruct/7_search/search/1.png\" alt=\"数据结构\"></p>\n<!--more-->\n<ul>\n<li>平均查找长度：$$ASL&#x3D;\\sum_{i&#x3D;1}^n p_i c_i$$</li>\n</ul>\n<h2 id=\"7-1-2-顺序查找\"><a href=\"#7-1-2-顺序查找\" class=\"headerlink\" title=\"7.1-2 顺序查找\"></a>7.1-2 顺序查找</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/search/2.png\"><br><img src=\"/img/datastruct/7_search/search/2.png\" alt=\"数据结构\"></p>\n<h2 id=\"7-1-3-折半查找\"><a href=\"#7-1-3-折半查找\" class=\"headerlink\" title=\"7.1-3 折半查找\"></a>7.1-3 折半查找</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/search/3.png\"><br><img src=\"/img/datastruct/7_search/search/3.png\" alt=\"数据结构\"></p>\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/search/4.png\"><br><img src=\"/img/datastruct/7_search/search/4.png\" alt=\"数据结构\"></p>\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/search/5.png\"><br><img src=\"/img/datastruct/7_search/search/5.png\" alt=\"数据结构\"></p>\n<h2 id=\"7-1-4-分块查找\"><a href=\"#7-1-4-分块查找\" class=\"headerlink\" title=\"7.1-4 分块查找\"></a>7.1-4 分块查找</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/search/6.png\"><br><img src=\"/img/datastruct/7_search/search/6.png\" alt=\"数据结构\"></p>\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/search/7.png\"><br><img src=\"/img/datastruct/7_search/search/7.png\" alt=\"数据结构\"></p>\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/search/8.png\"><br><img src=\"/img/datastruct/7_search/search/8.png\" alt=\"数据结构\"></p>"},{"title":"7.2 二叉排序树","date":"2023-08-06T16:00:00.000Z","comment":false,"toc":true,"_content":"#\n<!--more-->\n\n### \n\n![](../../../../themes/yilia/source/img/datastruct/7_search/BST/1.png)\n![数据结构](/img/datastruct/7_search/BST/1.png)\n<!--more-->\n- 左<根<右\n- 中序遍历：升序排列\n- 操作\n    - 查找\n![](../../../../themes/yilia/source/img/datastruct/7_search/BST/2.png)\n![数据结构](/img/datastruct/7_search/BST/2.png)\n    - 插入\n![](../../../../themes/yilia/source/img/datastruct/7_search/BST/3.png)\n![数据结构](/img/datastruct/7_search/BST/3.png)\n    - 构造\n![](../../../../themes/yilia/source/img/datastruct/7_search/BST/4.png)\n![数据结构](/img/datastruct/7_search/BST/4.png)\n    - 删除z\n        - z是叶子\n\n            删\n        - z只有一颗左子树/右子树\n\n            删，子树代替\n        - z有两棵子树\n\n            用前驱（左子树最右下）/后继（右子树最左下）代替，删除前驱/后继\n- 查找效率分析\n\n    - 最好：O(log<sub>2</sub>n)\n    - 最坏：O(n)\n","source":"_posts/datastruct/7_search/2_BST.md","raw":"---\ntitle: 7.2 二叉排序树\ndate: 2023-08-07 00:00:00\ntags: [数据结构,查找,二叉排序树]\ncategories: [数据结构]\ncomment: false\ntoc: true\n---\n#\n<!--more-->\n\n### \n\n![](../../../../themes/yilia/source/img/datastruct/7_search/BST/1.png)\n![数据结构](/img/datastruct/7_search/BST/1.png)\n<!--more-->\n- 左<根<右\n- 中序遍历：升序排列\n- 操作\n    - 查找\n![](../../../../themes/yilia/source/img/datastruct/7_search/BST/2.png)\n![数据结构](/img/datastruct/7_search/BST/2.png)\n    - 插入\n![](../../../../themes/yilia/source/img/datastruct/7_search/BST/3.png)\n![数据结构](/img/datastruct/7_search/BST/3.png)\n    - 构造\n![](../../../../themes/yilia/source/img/datastruct/7_search/BST/4.png)\n![数据结构](/img/datastruct/7_search/BST/4.png)\n    - 删除z\n        - z是叶子\n\n            删\n        - z只有一颗左子树/右子树\n\n            删，子树代替\n        - z有两棵子树\n\n            用前驱（左子树最右下）/后继（右子树最左下）代替，删除前驱/后继\n- 查找效率分析\n\n    - 最好：O(log<sub>2</sub>n)\n    - 最坏：O(n)\n","slug":"datastruct/7_search/2_BST","published":1,"updated":"2023-10-23T12:53:49.690Z","comments":1,"layout":"post","photos":[],"_id":"clvulgd5v001a5ovw8qtq7tw0","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h3 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a></h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/BST/1.png\"><br><img src=\"/img/datastruct/7_search/BST/1.png\" alt=\"数据结构\"></p>\n<!--more-->\n<ul>\n<li><p>左&lt;根&lt;右</p>\n</li>\n<li><p>中序遍历：升序排列</p>\n</li>\n<li><p>操作</p>\n<ul>\n<li>查找<br><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/BST/2.png\"><br><img src=\"/img/datastruct/7_search/BST/2.png\" alt=\"数据结构\"></li>\n<li>插入<br><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/BST/3.png\"><br><img src=\"/img/datastruct/7_search/BST/3.png\" alt=\"数据结构\"></li>\n<li>构造<br><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/BST/4.png\"><br><img src=\"/img/datastruct/7_search/BST/4.png\" alt=\"数据结构\"></li>\n<li>删除z<ul>\n<li><p>z是叶子</p>\n<p>  删</p>\n</li>\n<li><p>z只有一颗左子树&#x2F;右子树</p>\n<p>  删，子树代替</p>\n</li>\n<li><p>z有两棵子树</p>\n<p>  用前驱（左子树最右下）&#x2F;后继（右子树最左下）代替，删除前驱&#x2F;后继</p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>查找效率分析</p>\n<ul>\n<li>最好：O(log<sub>2</sub>n)</li>\n<li>最坏：O(n)</li>\n</ul>\n</li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h3 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a></h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/BST/1.png\"><br><img src=\"/img/datastruct/7_search/BST/1.png\" alt=\"数据结构\"></p>\n<!--more-->\n<ul>\n<li><p>左&lt;根&lt;右</p>\n</li>\n<li><p>中序遍历：升序排列</p>\n</li>\n<li><p>操作</p>\n<ul>\n<li>查找<br><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/BST/2.png\"><br><img src=\"/img/datastruct/7_search/BST/2.png\" alt=\"数据结构\"></li>\n<li>插入<br><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/BST/3.png\"><br><img src=\"/img/datastruct/7_search/BST/3.png\" alt=\"数据结构\"></li>\n<li>构造<br><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/BST/4.png\"><br><img src=\"/img/datastruct/7_search/BST/4.png\" alt=\"数据结构\"></li>\n<li>删除z<ul>\n<li><p>z是叶子</p>\n<p>  删</p>\n</li>\n<li><p>z只有一颗左子树&#x2F;右子树</p>\n<p>  删，子树代替</p>\n</li>\n<li><p>z有两棵子树</p>\n<p>  用前驱（左子树最右下）&#x2F;后继（右子树最左下）代替，删除前驱&#x2F;后继</p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>查找效率分析</p>\n<ul>\n<li>最好：O(log<sub>2</sub>n)</li>\n<li>最坏：O(n)</li>\n</ul>\n</li>\n</ul>"},{"title":"3.4 矩阵压缩","date":"2023-08-06T16:00:00.000Z","comment":false,"toc":true,"_content":"#\n<!--more-->\n\n![](../../../../themes/yilia/source/img/datastruct/3_stack/matrix/1.png)\n![数据结构](/img/datastruct/3_stack/matrix/1.png)\n\n## 3.4.1 对称矩阵\n![](../../../../themes/yilia/source/img/datastruct/3_stack/matrix/2.png)\n![数据结构](/img/datastruct/3_stack/matrix/2.png)\n- 存主对角线+下三角\n\n    $$a_{k}=a_{ij}=a_{ji} (i>=j)$$\n\n    - 列优先\n\n        $$k=\\frac{i(i-1)}{2}+j-1$$\n    - 行优先\n\n        $$k=\\frac{j(j-1)}{2}+i-1$$\n\n## 3.4.2 三角矩阵\n![](../../../../themes/yilia/source/img/datastruct/3_stack/matrix/3.png)\n![数据结构](/img/datastruct/3_stack/matrix/3.png)\n![](../../../../themes/yilia/source/img/datastruct/3_stack/matrix/4.png)\n![数据结构](/img/datastruct/3_stack/matrix/4.png)\n- 存主对角线+下三角/上三角+c\n\n    $$a_{k}=a_{ij} (i>=j)$$\n\n- 下三角，行优先\n\n    $$k=\\left\\{\\begin{array}{l}{\\frac{i(i-1)}{2}+j-1} & {i \\geq j} \\\\ {\\frac{n(n-1)}{2}} & {i<j}\\end{array}\\right.$$\n\n- 上三角，行优先\n    \n    $$k=\\left\\{\\begin{array}{l}{\\frac{(i-1)(2n-i+2)}{2}+j-i} & {i \\leq j} \\\\ {\\frac{n(n-1)}{2}} & {i>j}\\end{array}\\right.$$\n\n## 3.4.3 三对角矩阵\n![](../../../../themes/yilia/source/img/datastruct/3_stack/matrix/5.png)\n![数据结构](/img/datastruct/3_stack/matrix/5.png)\n![](../../../../themes/yilia/source/img/datastruct/3_stack/matrix/6.png)\n![数据结构](/img/datastruct/3_stack/matrix/6.png)\n- $$k=2i+j-3$$\n\n## 3.4.4 稀疏矩阵\n- 顺序存储\n![](../../../../themes/yilia/source/img/datastruct/3_stack/matrix/7.png)\n![数据结构](/img/datastruct/3_stack/matrix/7.png)\n\n- 链式存储\n![](../../../../themes/yilia/source/img/datastruct/3_stack/matrix/8.png)\n![数据结构](/img/datastruct/3_stack/matrix/8.png)\n\n![](../../../../themes/yilia/source/img/datastruct/3_stack/matrix/9.png)\n![数据结构](/img/datastruct/3_stack/matrix/9.png)","source":"_posts/datastruct/3_stack/4_matrix.md","raw":"---\ntitle: 3.4 矩阵压缩\ndate: 2023-08-07 00:00:00\ntags: [数据结构]\ncategories: [数据结构]\ncomment: false\ntoc: true\n---\n#\n<!--more-->\n\n![](../../../../themes/yilia/source/img/datastruct/3_stack/matrix/1.png)\n![数据结构](/img/datastruct/3_stack/matrix/1.png)\n\n## 3.4.1 对称矩阵\n![](../../../../themes/yilia/source/img/datastruct/3_stack/matrix/2.png)\n![数据结构](/img/datastruct/3_stack/matrix/2.png)\n- 存主对角线+下三角\n\n    $$a_{k}=a_{ij}=a_{ji} (i>=j)$$\n\n    - 列优先\n\n        $$k=\\frac{i(i-1)}{2}+j-1$$\n    - 行优先\n\n        $$k=\\frac{j(j-1)}{2}+i-1$$\n\n## 3.4.2 三角矩阵\n![](../../../../themes/yilia/source/img/datastruct/3_stack/matrix/3.png)\n![数据结构](/img/datastruct/3_stack/matrix/3.png)\n![](../../../../themes/yilia/source/img/datastruct/3_stack/matrix/4.png)\n![数据结构](/img/datastruct/3_stack/matrix/4.png)\n- 存主对角线+下三角/上三角+c\n\n    $$a_{k}=a_{ij} (i>=j)$$\n\n- 下三角，行优先\n\n    $$k=\\left\\{\\begin{array}{l}{\\frac{i(i-1)}{2}+j-1} & {i \\geq j} \\\\ {\\frac{n(n-1)}{2}} & {i<j}\\end{array}\\right.$$\n\n- 上三角，行优先\n    \n    $$k=\\left\\{\\begin{array}{l}{\\frac{(i-1)(2n-i+2)}{2}+j-i} & {i \\leq j} \\\\ {\\frac{n(n-1)}{2}} & {i>j}\\end{array}\\right.$$\n\n## 3.4.3 三对角矩阵\n![](../../../../themes/yilia/source/img/datastruct/3_stack/matrix/5.png)\n![数据结构](/img/datastruct/3_stack/matrix/5.png)\n![](../../../../themes/yilia/source/img/datastruct/3_stack/matrix/6.png)\n![数据结构](/img/datastruct/3_stack/matrix/6.png)\n- $$k=2i+j-3$$\n\n## 3.4.4 稀疏矩阵\n- 顺序存储\n![](../../../../themes/yilia/source/img/datastruct/3_stack/matrix/7.png)\n![数据结构](/img/datastruct/3_stack/matrix/7.png)\n\n- 链式存储\n![](../../../../themes/yilia/source/img/datastruct/3_stack/matrix/8.png)\n![数据结构](/img/datastruct/3_stack/matrix/8.png)\n\n![](../../../../themes/yilia/source/img/datastruct/3_stack/matrix/9.png)\n![数据结构](/img/datastruct/3_stack/matrix/9.png)","slug":"datastruct/3_stack/4_matrix","published":1,"updated":"2023-10-23T12:09:13.828Z","comments":1,"layout":"post","photos":[],"_id":"clvulgd5v001e5ovw4y6h7sly","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/matrix/1.png\"><br><img src=\"/img/datastruct/3_stack/matrix/1.png\" alt=\"数据结构\"></p>\n<h2 id=\"3-4-1-对称矩阵\"><a href=\"#3-4-1-对称矩阵\" class=\"headerlink\" title=\"3.4.1 对称矩阵\"></a>3.4.1 对称矩阵</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/matrix/2.png\"><br><img src=\"/img/datastruct/3_stack/matrix/2.png\" alt=\"数据结构\"></p>\n<ul>\n<li><p>存主对角线+下三角</p>\n<p>  $$a_{k}&#x3D;a_{ij}&#x3D;a_{ji} (i&gt;&#x3D;j)$$</p>\n<ul>\n<li><p>列优先</p>\n<p>  $$k&#x3D;\\frac{i(i-1)}{2}+j-1$$</p>\n</li>\n<li><p>行优先</p>\n<p>  $$k&#x3D;\\frac{j(j-1)}{2}+i-1$$</p>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"3-4-2-三角矩阵\"><a href=\"#3-4-2-三角矩阵\" class=\"headerlink\" title=\"3.4.2 三角矩阵\"></a>3.4.2 三角矩阵</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/matrix/3.png\"><br><img src=\"/img/datastruct/3_stack/matrix/3.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/matrix/4.png\"><br><img src=\"/img/datastruct/3_stack/matrix/4.png\" alt=\"数据结构\"></p>\n<ul>\n<li><p>存主对角线+下三角&#x2F;上三角+c</p>\n<p>  $$a_{k}&#x3D;a_{ij} (i&gt;&#x3D;j)$$</p>\n</li>\n<li><p>下三角，行优先</p>\n<p>  $$k&#x3D;\\left{\\begin{array}{l}{\\frac{i(i-1)}{2}+j-1} &amp; {i \\geq j} \\ {\\frac{n(n-1)}{2}} &amp; {i&lt;j}\\end{array}\\right.$$</p>\n</li>\n<li><p>上三角，行优先</p>\n<p>  $$k&#x3D;\\left{\\begin{array}{l}{\\frac{(i-1)(2n-i+2)}{2}+j-i} &amp; {i \\leq j} \\ {\\frac{n(n-1)}{2}} &amp; {i&gt;j}\\end{array}\\right.$$</p>\n</li>\n</ul>\n<h2 id=\"3-4-3-三对角矩阵\"><a href=\"#3-4-3-三对角矩阵\" class=\"headerlink\" title=\"3.4.3 三对角矩阵\"></a>3.4.3 三对角矩阵</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/matrix/5.png\"><br><img src=\"/img/datastruct/3_stack/matrix/5.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/matrix/6.png\"><br><img src=\"/img/datastruct/3_stack/matrix/6.png\" alt=\"数据结构\"></p>\n<ul>\n<li>$$k&#x3D;2i+j-3$$</li>\n</ul>\n<h2 id=\"3-4-4-稀疏矩阵\"><a href=\"#3-4-4-稀疏矩阵\" class=\"headerlink\" title=\"3.4.4 稀疏矩阵\"></a>3.4.4 稀疏矩阵</h2><ul>\n<li><p>顺序存储<br><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/matrix/7.png\"><br><img src=\"/img/datastruct/3_stack/matrix/7.png\" alt=\"数据结构\"></p>\n</li>\n<li><p>链式存储<br><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/matrix/8.png\"><br><img src=\"/img/datastruct/3_stack/matrix/8.png\" alt=\"数据结构\"></p>\n</li>\n</ul>\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/matrix/9.png\"><br><img src=\"/img/datastruct/3_stack/matrix/9.png\" alt=\"数据结构\"></p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<p><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/matrix/1.png\"><br><img src=\"/img/datastruct/3_stack/matrix/1.png\" alt=\"数据结构\"></p>\n<h2 id=\"3-4-1-对称矩阵\"><a href=\"#3-4-1-对称矩阵\" class=\"headerlink\" title=\"3.4.1 对称矩阵\"></a>3.4.1 对称矩阵</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/matrix/2.png\"><br><img src=\"/img/datastruct/3_stack/matrix/2.png\" alt=\"数据结构\"></p>\n<ul>\n<li><p>存主对角线+下三角</p>\n<p>  $$a_{k}&#x3D;a_{ij}&#x3D;a_{ji} (i&gt;&#x3D;j)$$</p>\n<ul>\n<li><p>列优先</p>\n<p>  $$k&#x3D;\\frac{i(i-1)}{2}+j-1$$</p>\n</li>\n<li><p>行优先</p>\n<p>  $$k&#x3D;\\frac{j(j-1)}{2}+i-1$$</p>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"3-4-2-三角矩阵\"><a href=\"#3-4-2-三角矩阵\" class=\"headerlink\" title=\"3.4.2 三角矩阵\"></a>3.4.2 三角矩阵</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/matrix/3.png\"><br><img src=\"/img/datastruct/3_stack/matrix/3.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/matrix/4.png\"><br><img src=\"/img/datastruct/3_stack/matrix/4.png\" alt=\"数据结构\"></p>\n<ul>\n<li><p>存主对角线+下三角&#x2F;上三角+c</p>\n<p>  $$a_{k}&#x3D;a_{ij} (i&gt;&#x3D;j)$$</p>\n</li>\n<li><p>下三角，行优先</p>\n<p>  $$k&#x3D;\\left{\\begin{array}{l}{\\frac{i(i-1)}{2}+j-1} &amp; {i \\geq j} \\ {\\frac{n(n-1)}{2}} &amp; {i&lt;j}\\end{array}\\right.$$</p>\n</li>\n<li><p>上三角，行优先</p>\n<p>  $$k&#x3D;\\left{\\begin{array}{l}{\\frac{(i-1)(2n-i+2)}{2}+j-i} &amp; {i \\leq j} \\ {\\frac{n(n-1)}{2}} &amp; {i&gt;j}\\end{array}\\right.$$</p>\n</li>\n</ul>\n<h2 id=\"3-4-3-三对角矩阵\"><a href=\"#3-4-3-三对角矩阵\" class=\"headerlink\" title=\"3.4.3 三对角矩阵\"></a>3.4.3 三对角矩阵</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/matrix/5.png\"><br><img src=\"/img/datastruct/3_stack/matrix/5.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/matrix/6.png\"><br><img src=\"/img/datastruct/3_stack/matrix/6.png\" alt=\"数据结构\"></p>\n<ul>\n<li>$$k&#x3D;2i+j-3$$</li>\n</ul>\n<h2 id=\"3-4-4-稀疏矩阵\"><a href=\"#3-4-4-稀疏矩阵\" class=\"headerlink\" title=\"3.4.4 稀疏矩阵\"></a>3.4.4 稀疏矩阵</h2><ul>\n<li><p>顺序存储<br><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/matrix/7.png\"><br><img src=\"/img/datastruct/3_stack/matrix/7.png\" alt=\"数据结构\"></p>\n</li>\n<li><p>链式存储<br><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/matrix/8.png\"><br><img src=\"/img/datastruct/3_stack/matrix/8.png\" alt=\"数据结构\"></p>\n</li>\n</ul>\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/3_stack/matrix/9.png\"><br><img src=\"/img/datastruct/3_stack/matrix/9.png\" alt=\"数据结构\"></p>"},{"title":"7.3 平衡二叉树","comment":false,"date":"2023-12-11T16:00:00.000Z","toc":true,"_content":"#\n<!--more-->\n\n\n\n![](../../../../themes/yilia/source/img/datastruct/7_search/AVL/1.png)\n![数据结构](/img/datastruct/7_search/AVL/1.png)\n<!--more-->\n\n### 7.3-1 定义\n- 左右子树高度差不超过1的二叉排序树，简称AVL树\n\n### 7.3-2 操作\n- 插入\n    - 1） 插入二叉排序树\n    - 2）调整最小不平衡子树A\n\n        ![](../../../../themes/yilia/source/img/datastruct/7_search/AVL/6.png)\n        ![数据结构](/img/datastruct/7_search/AVL/6.png)\n\n        - LL型：在A的左孩子的左子树插入\n\n        ![](../../../../themes/yilia/source/img/datastruct/7_search/AVL/2.png)\n        ![数据结构](/img/datastruct/7_search/AVL/2.png)\n\n        - RR型：在A的右孩子的右子树插入\n\n        ![](../../../../themes/yilia/source/img/datastruct/7_search/AVL/3.png)\n        ![数据结构](/img/datastruct/7_search/AVL/3.png)\n\n        - LR型：在A的左孩子的右子树插入\n        \n    \n        ![](../../../../themes/yilia/source/img/datastruct/7_search/AVL/4.png)\n        ![数据结构](/img/datastruct/7_search/AVL/4.png)\n\n        - RL型：在A的右孩子的左子树插入\n        \n        ![](../../../../themes/yilia/source/img/datastruct/7_search/AVL/5.png)\n        ![数据结构](/img/datastruct/7_search/AVL/5.png)\n\n- 删除\n![](../../../../themes/yilia/source/img/datastruct/7_search/AVL/7.png)\n![数据结构](/img/datastruct/7_search/AVL/7.png)","source":"_posts/datastruct/7_search/3_AVL.md","raw":"---\ntitle: 7.3 平衡二叉树\ntags: [数据结构,查找,平衡二叉树]\ncategories: [数据结构]\ncomment: false\ndate: 2023-12-12\ntoc: true\n---\n#\n<!--more-->\n\n\n\n![](../../../../themes/yilia/source/img/datastruct/7_search/AVL/1.png)\n![数据结构](/img/datastruct/7_search/AVL/1.png)\n<!--more-->\n\n### 7.3-1 定义\n- 左右子树高度差不超过1的二叉排序树，简称AVL树\n\n### 7.3-2 操作\n- 插入\n    - 1） 插入二叉排序树\n    - 2）调整最小不平衡子树A\n\n        ![](../../../../themes/yilia/source/img/datastruct/7_search/AVL/6.png)\n        ![数据结构](/img/datastruct/7_search/AVL/6.png)\n\n        - LL型：在A的左孩子的左子树插入\n\n        ![](../../../../themes/yilia/source/img/datastruct/7_search/AVL/2.png)\n        ![数据结构](/img/datastruct/7_search/AVL/2.png)\n\n        - RR型：在A的右孩子的右子树插入\n\n        ![](../../../../themes/yilia/source/img/datastruct/7_search/AVL/3.png)\n        ![数据结构](/img/datastruct/7_search/AVL/3.png)\n\n        - LR型：在A的左孩子的右子树插入\n        \n    \n        ![](../../../../themes/yilia/source/img/datastruct/7_search/AVL/4.png)\n        ![数据结构](/img/datastruct/7_search/AVL/4.png)\n\n        - RL型：在A的右孩子的左子树插入\n        \n        ![](../../../../themes/yilia/source/img/datastruct/7_search/AVL/5.png)\n        ![数据结构](/img/datastruct/7_search/AVL/5.png)\n\n- 删除\n![](../../../../themes/yilia/source/img/datastruct/7_search/AVL/7.png)\n![数据结构](/img/datastruct/7_search/AVL/7.png)","slug":"datastruct/7_search/3_AVL","published":1,"updated":"2024-05-06T06:46:53.793Z","_id":"clvulgd5w001g5ovwfi7zd0sc","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n\n\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/AVL/1.png\"><br><img src=\"/img/datastruct/7_search/AVL/1.png\" alt=\"数据结构\"></p>\n<!--more-->\n\n<h3 id=\"7-3-1-定义\"><a href=\"#7-3-1-定义\" class=\"headerlink\" title=\"7.3-1 定义\"></a>7.3-1 定义</h3><ul>\n<li>左右子树高度差不超过1的二叉排序树，简称AVL树</li>\n</ul>\n<h3 id=\"7-3-2-操作\"><a href=\"#7-3-2-操作\" class=\"headerlink\" title=\"7.3-2 操作\"></a>7.3-2 操作</h3><ul>\n<li><p>插入</p>\n<ul>\n<li><p>1） 插入二叉排序树</p>\n</li>\n<li><p>2）调整最小不平衡子树A</p>\n<p>  <img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/AVL/6.png\"><br>  <img src=\"/img/datastruct/7_search/AVL/6.png\" alt=\"数据结构\"></p>\n<ul>\n<li>LL型：在A的左孩子的左子树插入</li>\n</ul>\n<p>  <img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/AVL/2.png\"><br>  <img src=\"/img/datastruct/7_search/AVL/2.png\" alt=\"数据结构\"></p>\n<ul>\n<li>RR型：在A的右孩子的右子树插入</li>\n</ul>\n<p>  <img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/AVL/3.png\"><br>  <img src=\"/img/datastruct/7_search/AVL/3.png\" alt=\"数据结构\"></p>\n<ul>\n<li>LR型：在A的左孩子的右子树插入</li>\n</ul>\n<p>  <img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/AVL/4.png\"><br>  <img src=\"/img/datastruct/7_search/AVL/4.png\" alt=\"数据结构\"></p>\n<ul>\n<li>RL型：在A的右孩子的左子树插入</li>\n</ul>\n<p>  <img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/AVL/5.png\"><br>  <img src=\"/img/datastruct/7_search/AVL/5.png\" alt=\"数据结构\"></p>\n</li>\n</ul>\n</li>\n<li><p>删除<br><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/AVL/7.png\"><br><img src=\"/img/datastruct/7_search/AVL/7.png\" alt=\"数据结构\"></p>\n</li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<p><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/AVL/1.png\"><br><img src=\"/img/datastruct/7_search/AVL/1.png\" alt=\"数据结构\"></p>\n<!--more-->\n\n<h3 id=\"7-3-1-定义\"><a href=\"#7-3-1-定义\" class=\"headerlink\" title=\"7.3-1 定义\"></a>7.3-1 定义</h3><ul>\n<li>左右子树高度差不超过1的二叉排序树，简称AVL树</li>\n</ul>\n<h3 id=\"7-3-2-操作\"><a href=\"#7-3-2-操作\" class=\"headerlink\" title=\"7.3-2 操作\"></a>7.3-2 操作</h3><ul>\n<li><p>插入</p>\n<ul>\n<li><p>1） 插入二叉排序树</p>\n</li>\n<li><p>2）调整最小不平衡子树A</p>\n<p>  <img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/AVL/6.png\"><br>  <img src=\"/img/datastruct/7_search/AVL/6.png\" alt=\"数据结构\"></p>\n<ul>\n<li>LL型：在A的左孩子的左子树插入</li>\n</ul>\n<p>  <img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/AVL/2.png\"><br>  <img src=\"/img/datastruct/7_search/AVL/2.png\" alt=\"数据结构\"></p>\n<ul>\n<li>RR型：在A的右孩子的右子树插入</li>\n</ul>\n<p>  <img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/AVL/3.png\"><br>  <img src=\"/img/datastruct/7_search/AVL/3.png\" alt=\"数据结构\"></p>\n<ul>\n<li>LR型：在A的左孩子的右子树插入</li>\n</ul>\n<p>  <img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/AVL/4.png\"><br>  <img src=\"/img/datastruct/7_search/AVL/4.png\" alt=\"数据结构\"></p>\n<ul>\n<li>RL型：在A的右孩子的左子树插入</li>\n</ul>\n<p>  <img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/AVL/5.png\"><br>  <img src=\"/img/datastruct/7_search/AVL/5.png\" alt=\"数据结构\"></p>\n</li>\n</ul>\n</li>\n<li><p>删除<br><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/AVL/7.png\"><br><img src=\"/img/datastruct/7_search/AVL/7.png\" alt=\"数据结构\"></p>\n</li>\n</ul>"},{"title":"7.4 红黑树","date":"2023-12-11T16:00:00.000Z","comment":false,"toc":true,"_content":"#\n<!--more-->\n\n## 9.7 红黑树\n![](../../../../themes/yilia/source/img/datastruct/7_search/red/1.png)\n![数据结构](/img/datastruct/7_search/red/1.png)\n<!--more-->\n\n### 7.4-1 为什么要有红黑树\n- 平衡二叉树插入/删除操作效率低，因为调整次数多\n![](../../../../themes/yilia/source/img/datastruct/7_search/red/2.png)\n![数据结构](/img/datastruct/7_search/red/2.png)\n\n### 7.4-2 定义\n- 二叉排序树\n- 左根右\n- 根叶黑\n- 不红红\n- 黑路同\n![](../../../../themes/yilia/source/img/datastruct/7_search/red/3.png)\n![数据结构](/img/datastruct/7_search/red/3.png)\n\n### 7.4-3 性质\n- 从根到叶子的最长的可能路径不多于最短的可能路径的两倍长\n- 有n个结点的红黑树的高度至多为2log<sub>2</sub>(n+1)\n- 若根节点黑高为h，则红黑树的高度至多为2h\n\n### 7.4-4 操作\n- 查找\n- 插入\n![](../../../../themes/yilia/source/img/datastruct/7_search/red/4.png)\n![数据结构](/img/datastruct/7_search/red/4.png)\n![](../../../../themes/yilia/source/img/datastruct/7_search/red/5.png)\n![数据结构](/img/datastruct/7_search/red/5.png)\n- 删除\n","source":"_posts/datastruct/7_search/4_redblacktree.md","raw":"---\ntitle: 7.4 红黑树\ntags: [数据结构]\ncategories: [数据结构]\ndate: 2023-12-12\ncomment: false\ntoc: true\n---\n#\n<!--more-->\n\n## 9.7 红黑树\n![](../../../../themes/yilia/source/img/datastruct/7_search/red/1.png)\n![数据结构](/img/datastruct/7_search/red/1.png)\n<!--more-->\n\n### 7.4-1 为什么要有红黑树\n- 平衡二叉树插入/删除操作效率低，因为调整次数多\n![](../../../../themes/yilia/source/img/datastruct/7_search/red/2.png)\n![数据结构](/img/datastruct/7_search/red/2.png)\n\n### 7.4-2 定义\n- 二叉排序树\n- 左根右\n- 根叶黑\n- 不红红\n- 黑路同\n![](../../../../themes/yilia/source/img/datastruct/7_search/red/3.png)\n![数据结构](/img/datastruct/7_search/red/3.png)\n\n### 7.4-3 性质\n- 从根到叶子的最长的可能路径不多于最短的可能路径的两倍长\n- 有n个结点的红黑树的高度至多为2log<sub>2</sub>(n+1)\n- 若根节点黑高为h，则红黑树的高度至多为2h\n\n### 7.4-4 操作\n- 查找\n- 插入\n![](../../../../themes/yilia/source/img/datastruct/7_search/red/4.png)\n![数据结构](/img/datastruct/7_search/red/4.png)\n![](../../../../themes/yilia/source/img/datastruct/7_search/red/5.png)\n![数据结构](/img/datastruct/7_search/red/5.png)\n- 删除\n","slug":"datastruct/7_search/4_redblacktree","published":1,"updated":"2024-05-06T06:46:41.822Z","_id":"clvulgd5x001k5ovwd7192w0p","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h2 id=\"9-7-红黑树\"><a href=\"#9-7-红黑树\" class=\"headerlink\" title=\"9.7 红黑树\"></a>9.7 红黑树</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/red/1.png\"><br><img src=\"/img/datastruct/7_search/red/1.png\" alt=\"数据结构\"></p>\n<!--more-->\n\n<h3 id=\"7-4-1-为什么要有红黑树\"><a href=\"#7-4-1-为什么要有红黑树\" class=\"headerlink\" title=\"7.4-1 为什么要有红黑树\"></a>7.4-1 为什么要有红黑树</h3><ul>\n<li>平衡二叉树插入&#x2F;删除操作效率低，因为调整次数多<br><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/red/2.png\"><br><img src=\"/img/datastruct/7_search/red/2.png\" alt=\"数据结构\"></li>\n</ul>\n<h3 id=\"7-4-2-定义\"><a href=\"#7-4-2-定义\" class=\"headerlink\" title=\"7.4-2 定义\"></a>7.4-2 定义</h3><ul>\n<li>二叉排序树</li>\n<li>左根右</li>\n<li>根叶黑</li>\n<li>不红红</li>\n<li>黑路同<br><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/red/3.png\"><br><img src=\"/img/datastruct/7_search/red/3.png\" alt=\"数据结构\"></li>\n</ul>\n<h3 id=\"7-4-3-性质\"><a href=\"#7-4-3-性质\" class=\"headerlink\" title=\"7.4-3 性质\"></a>7.4-3 性质</h3><ul>\n<li>从根到叶子的最长的可能路径不多于最短的可能路径的两倍长</li>\n<li>有n个结点的红黑树的高度至多为2log<sub>2</sub>(n+1)</li>\n<li>若根节点黑高为h，则红黑树的高度至多为2h</li>\n</ul>\n<h3 id=\"7-4-4-操作\"><a href=\"#7-4-4-操作\" class=\"headerlink\" title=\"7.4-4 操作\"></a>7.4-4 操作</h3><ul>\n<li>查找</li>\n<li>插入<br><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/red/4.png\"><br><img src=\"/img/datastruct/7_search/red/4.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/red/5.png\"><br><img src=\"/img/datastruct/7_search/red/5.png\" alt=\"数据结构\"></li>\n<li>删除</li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h2 id=\"9-7-红黑树\"><a href=\"#9-7-红黑树\" class=\"headerlink\" title=\"9.7 红黑树\"></a>9.7 红黑树</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/red/1.png\"><br><img src=\"/img/datastruct/7_search/red/1.png\" alt=\"数据结构\"></p>\n<!--more-->\n\n<h3 id=\"7-4-1-为什么要有红黑树\"><a href=\"#7-4-1-为什么要有红黑树\" class=\"headerlink\" title=\"7.4-1 为什么要有红黑树\"></a>7.4-1 为什么要有红黑树</h3><ul>\n<li>平衡二叉树插入&#x2F;删除操作效率低，因为调整次数多<br><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/red/2.png\"><br><img src=\"/img/datastruct/7_search/red/2.png\" alt=\"数据结构\"></li>\n</ul>\n<h3 id=\"7-4-2-定义\"><a href=\"#7-4-2-定义\" class=\"headerlink\" title=\"7.4-2 定义\"></a>7.4-2 定义</h3><ul>\n<li>二叉排序树</li>\n<li>左根右</li>\n<li>根叶黑</li>\n<li>不红红</li>\n<li>黑路同<br><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/red/3.png\"><br><img src=\"/img/datastruct/7_search/red/3.png\" alt=\"数据结构\"></li>\n</ul>\n<h3 id=\"7-4-3-性质\"><a href=\"#7-4-3-性质\" class=\"headerlink\" title=\"7.4-3 性质\"></a>7.4-3 性质</h3><ul>\n<li>从根到叶子的最长的可能路径不多于最短的可能路径的两倍长</li>\n<li>有n个结点的红黑树的高度至多为2log<sub>2</sub>(n+1)</li>\n<li>若根节点黑高为h，则红黑树的高度至多为2h</li>\n</ul>\n<h3 id=\"7-4-4-操作\"><a href=\"#7-4-4-操作\" class=\"headerlink\" title=\"7.4-4 操作\"></a>7.4-4 操作</h3><ul>\n<li>查找</li>\n<li>插入<br><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/red/4.png\"><br><img src=\"/img/datastruct/7_search/red/4.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/red/5.png\"><br><img src=\"/img/datastruct/7_search/red/5.png\" alt=\"数据结构\"></li>\n<li>删除</li>\n</ul>"},{"title":"7.5 B树","date":"2023-12-11T16:00:00.000Z","comment":false,"toc":true,"_content":"#\n<!--more-->\n\n![](../../../../themes/yilia/source/img/datastruct/7_search/B/1.png)\n![数据结构](/img/datastruct/7_search/B/1.png)\n\n\n","source":"_posts/datastruct/7_search/5_Btree.md","raw":"---\ntitle: 7.5 B树\ntags: [数据结构,查找,B树]\ndate: 2023-12-12\ncategories: [数据结构]\ncomment: false\ntoc: true\n---\n#\n<!--more-->\n\n![](../../../../themes/yilia/source/img/datastruct/7_search/B/1.png)\n![数据结构](/img/datastruct/7_search/B/1.png)\n\n\n","slug":"datastruct/7_search/5_Btree","published":1,"updated":"2024-05-06T06:46:33.873Z","_id":"clvulgd5y001n5ovw0o8lcnb5","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/B/1.png\"><br><img src=\"/img/datastruct/7_search/B/1.png\" alt=\"数据结构\"></p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<p><img src=\"/../../../../themes/yilia/source/img/datastruct/7_search/B/1.png\"><br><img src=\"/img/datastruct/7_search/B/1.png\" alt=\"数据结构\"></p>"},{"title":"5.1 树","date":"2023-08-06T16:00:00.000Z","comment":false,"toc":true,"_content":"#\n<!--more-->\n\n\n![](../../../../themes/yilia/source/img/datastruct/5_tree/tree/1.png)\n![数据结构](/img/datastruct/5_tree/tree/1.png)\n<!--more-->\n\n## 5.1.1 一些性质\n![](../../../../themes/yilia/source/img/datastruct/5_tree/tree/3.png)\n![数据结构](/img/datastruct/5_tree/tree/3.png)\n- 根在第0层\n- 节点数=总度数+1\n- 结点的度：结点拥有的子树的个数\n- m叉树中可以没有m个子树，但是不能超过m个子树\n- 度为m的树，第i层最多有$$m^{i-1}$$个结点\n- 高度为h的m叉树，最多有$$\\frac{m^h-1}{m-1}$$个结点\n- 高度为h的m叉树，最少有h个结点\n- 高度为h，度为m的树，最多有$$\\frac{m^{h+1}-1}{m-1}$$个结点，最少有h+m-1个结点\n- n个结点的m叉树的最小高度为：$$\\lceil log_m(n(m-1)+1)\\rceil$$\n\n![](../../../../themes/yilia/source/img/datastruct/5_tree/tree/1.png)\n![数据结构](/img/datastruct/5_tree/tree/1.png)","source":"_posts/datastruct/5_tree/1_tree.md","raw":"---\ntitle: 5.1 树\ndate: 2023-08-07 00:00:00\ntags: [数据结构,树]\ncategories: [数据结构]\ncomment: false\ntoc: true\n---\n#\n<!--more-->\n\n\n![](../../../../themes/yilia/source/img/datastruct/5_tree/tree/1.png)\n![数据结构](/img/datastruct/5_tree/tree/1.png)\n<!--more-->\n\n## 5.1.1 一些性质\n![](../../../../themes/yilia/source/img/datastruct/5_tree/tree/3.png)\n![数据结构](/img/datastruct/5_tree/tree/3.png)\n- 根在第0层\n- 节点数=总度数+1\n- 结点的度：结点拥有的子树的个数\n- m叉树中可以没有m个子树，但是不能超过m个子树\n- 度为m的树，第i层最多有$$m^{i-1}$$个结点\n- 高度为h的m叉树，最多有$$\\frac{m^h-1}{m-1}$$个结点\n- 高度为h的m叉树，最少有h个结点\n- 高度为h，度为m的树，最多有$$\\frac{m^{h+1}-1}{m-1}$$个结点，最少有h+m-1个结点\n- n个结点的m叉树的最小高度为：$$\\lceil log_m(n(m-1)+1)\\rceil$$\n\n![](../../../../themes/yilia/source/img/datastruct/5_tree/tree/1.png)\n![数据结构](/img/datastruct/5_tree/tree/1.png)","slug":"datastruct/5_tree/1_tree","published":1,"updated":"2023-10-23T12:15:55.713Z","comments":1,"layout":"post","photos":[],"_id":"clvulgd5z001q5ovwbsozai6g","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/tree/1.png\"><br><img src=\"/img/datastruct/5_tree/tree/1.png\" alt=\"数据结构\"></p>\n<!--more-->\n\n<h2 id=\"5-1-1-一些性质\"><a href=\"#5-1-1-一些性质\" class=\"headerlink\" title=\"5.1.1 一些性质\"></a>5.1.1 一些性质</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/tree/3.png\"><br><img src=\"/img/datastruct/5_tree/tree/3.png\" alt=\"数据结构\"></p>\n<ul>\n<li>根在第0层</li>\n<li>节点数&#x3D;总度数+1</li>\n<li>结点的度：结点拥有的子树的个数</li>\n<li>m叉树中可以没有m个子树，但是不能超过m个子树</li>\n<li>度为m的树，第i层最多有$$m^{i-1}$$个结点</li>\n<li>高度为h的m叉树，最多有$$\\frac{m^h-1}{m-1}$$个结点</li>\n<li>高度为h的m叉树，最少有h个结点</li>\n<li>高度为h，度为m的树，最多有$$\\frac{m^{h+1}-1}{m-1}$$个结点，最少有h+m-1个结点</li>\n<li>n个结点的m叉树的最小高度为：$$\\lceil log_m(n(m-1)+1)\\rceil$$</li>\n</ul>\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/tree/1.png\"><br><img src=\"/img/datastruct/5_tree/tree/1.png\" alt=\"数据结构\"></p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/tree/1.png\"><br><img src=\"/img/datastruct/5_tree/tree/1.png\" alt=\"数据结构\"></p>\n<!--more-->\n\n<h2 id=\"5-1-1-一些性质\"><a href=\"#5-1-1-一些性质\" class=\"headerlink\" title=\"5.1.1 一些性质\"></a>5.1.1 一些性质</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/tree/3.png\"><br><img src=\"/img/datastruct/5_tree/tree/3.png\" alt=\"数据结构\"></p>\n<ul>\n<li>根在第0层</li>\n<li>节点数&#x3D;总度数+1</li>\n<li>结点的度：结点拥有的子树的个数</li>\n<li>m叉树中可以没有m个子树，但是不能超过m个子树</li>\n<li>度为m的树，第i层最多有$$m^{i-1}$$个结点</li>\n<li>高度为h的m叉树，最多有$$\\frac{m^h-1}{m-1}$$个结点</li>\n<li>高度为h的m叉树，最少有h个结点</li>\n<li>高度为h，度为m的树，最多有$$\\frac{m^{h+1}-1}{m-1}$$个结点，最少有h+m-1个结点</li>\n<li>n个结点的m叉树的最小高度为：$$\\lceil log_m(n(m-1)+1)\\rceil$$</li>\n</ul>\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/tree/1.png\"><br><img src=\"/img/datastruct/5_tree/tree/1.png\" alt=\"数据结构\"></p>"},{"title":"5.3 树的存储结构","date":"2023-08-06T16:00:00.000Z","comment":false,"toc":true,"_content":"#\n<!--more-->\n\n![](../../../../themes/yilia/source/img/datastruct/5_tree/treesave/1.png)\n![数据结构](/img/datastruct/5_tree/treesave/1.png)\n<!--more-->\n\n### 5.3.1 双亲表示法\n![](../../../../themes/yilia/source/img/datastruct/5_tree/treesave/2.png)\n![数据结构](/img/datastruct/5_tree/treesave/2.png)\n- 查双亲方便\n- 空数据导致遍历慢\n- 查孩子只能从头遍历\n\n### 5.3.2 孩子表示法\n![](../../../../themes/yilia/source/img/datastruct/5_tree/treesave/3.png)\n![数据结构](/img/datastruct/5_tree/treesave/3.png)\n\n### 5.3.3 孩子兄弟表示法\n![](../../../../themes/yilia/source/img/datastruct/5_tree/treesave/4.png)\n![数据结构](/img/datastruct/5_tree/treesave/4.png)\n\n## 5.3.4 树、森林的遍历\n![](../../../../themes/yilia/source/img/datastruct/5_tree/treesave/5.png)\n![数据结构](/img/datastruct/5_tree/treesave/5.png)\n\n### 5.3.5 树的先根遍历\n![](../../../../themes/yilia/source/img/datastruct/5_tree/treesave/6.png)\n![数据结构](/img/datastruct/5_tree/treesave/6.png)\n\n### 5.3.6 树的后根遍历\n![](../../../../themes/yilia/source/img/datastruct/5_tree/treesave/7.png)\n![数据结构](/img/datastruct/5_tree/treesave/7.png)\n\n### 5.3.7 树的层次遍历\n![](../../../../themes/yilia/source/img/datastruct/5_tree/treesave/8.png)\n![数据结构](/img/datastruct/5_tree/treesave/8.png)\n\n|树|森林|二叉树\n|:---|:---|:---|\n|先根遍历|先根遍历|先序遍历|\n|后根遍历|中序遍历|中序遍历|","source":"_posts/datastruct/5_tree/3_treesave.md","raw":"---\ntitle: 5.3 树的存储结构\ndate: 2023-08-07 00:00:00\ntags: [数据结构,树]\ncategories: [数据结构]\ncomment: false\ntoc: true\n---\n#\n<!--more-->\n\n![](../../../../themes/yilia/source/img/datastruct/5_tree/treesave/1.png)\n![数据结构](/img/datastruct/5_tree/treesave/1.png)\n<!--more-->\n\n### 5.3.1 双亲表示法\n![](../../../../themes/yilia/source/img/datastruct/5_tree/treesave/2.png)\n![数据结构](/img/datastruct/5_tree/treesave/2.png)\n- 查双亲方便\n- 空数据导致遍历慢\n- 查孩子只能从头遍历\n\n### 5.3.2 孩子表示法\n![](../../../../themes/yilia/source/img/datastruct/5_tree/treesave/3.png)\n![数据结构](/img/datastruct/5_tree/treesave/3.png)\n\n### 5.3.3 孩子兄弟表示法\n![](../../../../themes/yilia/source/img/datastruct/5_tree/treesave/4.png)\n![数据结构](/img/datastruct/5_tree/treesave/4.png)\n\n## 5.3.4 树、森林的遍历\n![](../../../../themes/yilia/source/img/datastruct/5_tree/treesave/5.png)\n![数据结构](/img/datastruct/5_tree/treesave/5.png)\n\n### 5.3.5 树的先根遍历\n![](../../../../themes/yilia/source/img/datastruct/5_tree/treesave/6.png)\n![数据结构](/img/datastruct/5_tree/treesave/6.png)\n\n### 5.3.6 树的后根遍历\n![](../../../../themes/yilia/source/img/datastruct/5_tree/treesave/7.png)\n![数据结构](/img/datastruct/5_tree/treesave/7.png)\n\n### 5.3.7 树的层次遍历\n![](../../../../themes/yilia/source/img/datastruct/5_tree/treesave/8.png)\n![数据结构](/img/datastruct/5_tree/treesave/8.png)\n\n|树|森林|二叉树\n|:---|:---|:---|\n|先根遍历|先根遍历|先序遍历|\n|后根遍历|中序遍历|中序遍历|","slug":"datastruct/5_tree/3_treesave","published":1,"updated":"2023-10-23T12:20:08.035Z","comments":1,"layout":"post","photos":[],"_id":"clvulgd5z001t5ovwasrobibp","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/treesave/1.png\"><br><img src=\"/img/datastruct/5_tree/treesave/1.png\" alt=\"数据结构\"></p>\n<!--more-->\n\n<h3 id=\"5-3-1-双亲表示法\"><a href=\"#5-3-1-双亲表示法\" class=\"headerlink\" title=\"5.3.1 双亲表示法\"></a>5.3.1 双亲表示法</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/treesave/2.png\"><br><img src=\"/img/datastruct/5_tree/treesave/2.png\" alt=\"数据结构\"></p>\n<ul>\n<li>查双亲方便</li>\n<li>空数据导致遍历慢</li>\n<li>查孩子只能从头遍历</li>\n</ul>\n<h3 id=\"5-3-2-孩子表示法\"><a href=\"#5-3-2-孩子表示法\" class=\"headerlink\" title=\"5.3.2 孩子表示法\"></a>5.3.2 孩子表示法</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/treesave/3.png\"><br><img src=\"/img/datastruct/5_tree/treesave/3.png\" alt=\"数据结构\"></p>\n<h3 id=\"5-3-3-孩子兄弟表示法\"><a href=\"#5-3-3-孩子兄弟表示法\" class=\"headerlink\" title=\"5.3.3 孩子兄弟表示法\"></a>5.3.3 孩子兄弟表示法</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/treesave/4.png\"><br><img src=\"/img/datastruct/5_tree/treesave/4.png\" alt=\"数据结构\"></p>\n<h2 id=\"5-3-4-树、森林的遍历\"><a href=\"#5-3-4-树、森林的遍历\" class=\"headerlink\" title=\"5.3.4 树、森林的遍历\"></a>5.3.4 树、森林的遍历</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/treesave/5.png\"><br><img src=\"/img/datastruct/5_tree/treesave/5.png\" alt=\"数据结构\"></p>\n<h3 id=\"5-3-5-树的先根遍历\"><a href=\"#5-3-5-树的先根遍历\" class=\"headerlink\" title=\"5.3.5 树的先根遍历\"></a>5.3.5 树的先根遍历</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/treesave/6.png\"><br><img src=\"/img/datastruct/5_tree/treesave/6.png\" alt=\"数据结构\"></p>\n<h3 id=\"5-3-6-树的后根遍历\"><a href=\"#5-3-6-树的后根遍历\" class=\"headerlink\" title=\"5.3.6 树的后根遍历\"></a>5.3.6 树的后根遍历</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/treesave/7.png\"><br><img src=\"/img/datastruct/5_tree/treesave/7.png\" alt=\"数据结构\"></p>\n<h3 id=\"5-3-7-树的层次遍历\"><a href=\"#5-3-7-树的层次遍历\" class=\"headerlink\" title=\"5.3.7 树的层次遍历\"></a>5.3.7 树的层次遍历</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/treesave/8.png\"><br><img src=\"/img/datastruct/5_tree/treesave/8.png\" alt=\"数据结构\"></p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">树</th>\n<th align=\"left\">森林</th>\n<th align=\"left\">二叉树</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">先根遍历</td>\n<td align=\"left\">先根遍历</td>\n<td align=\"left\">先序遍历</td>\n</tr>\n<tr>\n<td align=\"left\">后根遍历</td>\n<td align=\"left\">中序遍历</td>\n<td align=\"left\">中序遍历</td>\n</tr>\n</tbody></table>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/treesave/1.png\"><br><img src=\"/img/datastruct/5_tree/treesave/1.png\" alt=\"数据结构\"></p>\n<!--more-->\n\n<h3 id=\"5-3-1-双亲表示法\"><a href=\"#5-3-1-双亲表示法\" class=\"headerlink\" title=\"5.3.1 双亲表示法\"></a>5.3.1 双亲表示法</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/treesave/2.png\"><br><img src=\"/img/datastruct/5_tree/treesave/2.png\" alt=\"数据结构\"></p>\n<ul>\n<li>查双亲方便</li>\n<li>空数据导致遍历慢</li>\n<li>查孩子只能从头遍历</li>\n</ul>\n<h3 id=\"5-3-2-孩子表示法\"><a href=\"#5-3-2-孩子表示法\" class=\"headerlink\" title=\"5.3.2 孩子表示法\"></a>5.3.2 孩子表示法</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/treesave/3.png\"><br><img src=\"/img/datastruct/5_tree/treesave/3.png\" alt=\"数据结构\"></p>\n<h3 id=\"5-3-3-孩子兄弟表示法\"><a href=\"#5-3-3-孩子兄弟表示法\" class=\"headerlink\" title=\"5.3.3 孩子兄弟表示法\"></a>5.3.3 孩子兄弟表示法</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/treesave/4.png\"><br><img src=\"/img/datastruct/5_tree/treesave/4.png\" alt=\"数据结构\"></p>\n<h2 id=\"5-3-4-树、森林的遍历\"><a href=\"#5-3-4-树、森林的遍历\" class=\"headerlink\" title=\"5.3.4 树、森林的遍历\"></a>5.3.4 树、森林的遍历</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/treesave/5.png\"><br><img src=\"/img/datastruct/5_tree/treesave/5.png\" alt=\"数据结构\"></p>\n<h3 id=\"5-3-5-树的先根遍历\"><a href=\"#5-3-5-树的先根遍历\" class=\"headerlink\" title=\"5.3.5 树的先根遍历\"></a>5.3.5 树的先根遍历</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/treesave/6.png\"><br><img src=\"/img/datastruct/5_tree/treesave/6.png\" alt=\"数据结构\"></p>\n<h3 id=\"5-3-6-树的后根遍历\"><a href=\"#5-3-6-树的后根遍历\" class=\"headerlink\" title=\"5.3.6 树的后根遍历\"></a>5.3.6 树的后根遍历</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/treesave/7.png\"><br><img src=\"/img/datastruct/5_tree/treesave/7.png\" alt=\"数据结构\"></p>\n<h3 id=\"5-3-7-树的层次遍历\"><a href=\"#5-3-7-树的层次遍历\" class=\"headerlink\" title=\"5.3.7 树的层次遍历\"></a>5.3.7 树的层次遍历</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/treesave/8.png\"><br><img src=\"/img/datastruct/5_tree/treesave/8.png\" alt=\"数据结构\"></p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">树</th>\n<th align=\"left\">森林</th>\n<th align=\"left\">二叉树</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">先根遍历</td>\n<td align=\"left\">先根遍历</td>\n<td align=\"left\">先序遍历</td>\n</tr>\n<tr>\n<td align=\"left\">后根遍历</td>\n<td align=\"left\">中序遍历</td>\n<td align=\"left\">中序遍历</td>\n</tr>\n</tbody></table>"},{"title":"5.4 哈夫曼树","date":"2023-08-06T16:00:00.000Z","comment":false,"toc":true,"_content":"#\n<!--more-->\n\n\n![](../../../../themes/yilia/source/img/datastruct/5_tree/hafmantree/1.png)\n![数据结构](/img/datastruct/5_tree/hafmantree/1.png)\n<!--more-->\n\n### 5.4.1 带权路径长度\n![](../../../../themes/yilia/source/img/datastruct/5_tree/hafmantree/2.png)\n![数据结构](/img/datastruct/5_tree/hafmantree/2.png)\n\n### 5.4.2 定义\n- n个带权叶子结点构成的带权路径最小的二叉树，也称为最优二叉树\n\n### 5.4.3 构造算法\n![](../../../../themes/yilia/source/img/datastruct/5_tree/hafmantree/3.png)\n![数据结构](/img/datastruct/5_tree/hafmantree/3.png)\n\n### 5.4.4 哈夫曼编码\n![](../../../../themes/yilia/source/img/datastruct/5_tree/hafmantree/4.png)\n![数据结构](/img/datastruct/5_tree/hafmantree/4.png)\n\n- 前缀码","source":"_posts/datastruct/5_tree/4_Hafmantree.md","raw":"---\ntitle: 5.4 哈夫曼树\ndate: 2023-08-07 00:00:00\ntags: [数据结构,树,哈夫曼树]\ncategories: [数据结构]\ncomment: false\ntoc: true\n---\n#\n<!--more-->\n\n\n![](../../../../themes/yilia/source/img/datastruct/5_tree/hafmantree/1.png)\n![数据结构](/img/datastruct/5_tree/hafmantree/1.png)\n<!--more-->\n\n### 5.4.1 带权路径长度\n![](../../../../themes/yilia/source/img/datastruct/5_tree/hafmantree/2.png)\n![数据结构](/img/datastruct/5_tree/hafmantree/2.png)\n\n### 5.4.2 定义\n- n个带权叶子结点构成的带权路径最小的二叉树，也称为最优二叉树\n\n### 5.4.3 构造算法\n![](../../../../themes/yilia/source/img/datastruct/5_tree/hafmantree/3.png)\n![数据结构](/img/datastruct/5_tree/hafmantree/3.png)\n\n### 5.4.4 哈夫曼编码\n![](../../../../themes/yilia/source/img/datastruct/5_tree/hafmantree/4.png)\n![数据结构](/img/datastruct/5_tree/hafmantree/4.png)\n\n- 前缀码","slug":"datastruct/5_tree/4_Hafmantree","published":1,"updated":"2023-10-23T12:21:19.991Z","comments":1,"layout":"post","photos":[],"_id":"clvulgd60001w5ovw9psv8bnw","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/hafmantree/1.png\"><br><img src=\"/img/datastruct/5_tree/hafmantree/1.png\" alt=\"数据结构\"></p>\n<!--more-->\n\n<h3 id=\"5-4-1-带权路径长度\"><a href=\"#5-4-1-带权路径长度\" class=\"headerlink\" title=\"5.4.1 带权路径长度\"></a>5.4.1 带权路径长度</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/hafmantree/2.png\"><br><img src=\"/img/datastruct/5_tree/hafmantree/2.png\" alt=\"数据结构\"></p>\n<h3 id=\"5-4-2-定义\"><a href=\"#5-4-2-定义\" class=\"headerlink\" title=\"5.4.2 定义\"></a>5.4.2 定义</h3><ul>\n<li>n个带权叶子结点构成的带权路径最小的二叉树，也称为最优二叉树</li>\n</ul>\n<h3 id=\"5-4-3-构造算法\"><a href=\"#5-4-3-构造算法\" class=\"headerlink\" title=\"5.4.3 构造算法\"></a>5.4.3 构造算法</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/hafmantree/3.png\"><br><img src=\"/img/datastruct/5_tree/hafmantree/3.png\" alt=\"数据结构\"></p>\n<h3 id=\"5-4-4-哈夫曼编码\"><a href=\"#5-4-4-哈夫曼编码\" class=\"headerlink\" title=\"5.4.4 哈夫曼编码\"></a>5.4.4 哈夫曼编码</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/hafmantree/4.png\"><br><img src=\"/img/datastruct/5_tree/hafmantree/4.png\" alt=\"数据结构\"></p>\n<ul>\n<li>前缀码</li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/hafmantree/1.png\"><br><img src=\"/img/datastruct/5_tree/hafmantree/1.png\" alt=\"数据结构\"></p>\n<!--more-->\n\n<h3 id=\"5-4-1-带权路径长度\"><a href=\"#5-4-1-带权路径长度\" class=\"headerlink\" title=\"5.4.1 带权路径长度\"></a>5.4.1 带权路径长度</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/hafmantree/2.png\"><br><img src=\"/img/datastruct/5_tree/hafmantree/2.png\" alt=\"数据结构\"></p>\n<h3 id=\"5-4-2-定义\"><a href=\"#5-4-2-定义\" class=\"headerlink\" title=\"5.4.2 定义\"></a>5.4.2 定义</h3><ul>\n<li>n个带权叶子结点构成的带权路径最小的二叉树，也称为最优二叉树</li>\n</ul>\n<h3 id=\"5-4-3-构造算法\"><a href=\"#5-4-3-构造算法\" class=\"headerlink\" title=\"5.4.3 构造算法\"></a>5.4.3 构造算法</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/hafmantree/3.png\"><br><img src=\"/img/datastruct/5_tree/hafmantree/3.png\" alt=\"数据结构\"></p>\n<h3 id=\"5-4-4-哈夫曼编码\"><a href=\"#5-4-4-哈夫曼编码\" class=\"headerlink\" title=\"5.4.4 哈夫曼编码\"></a>5.4.4 哈夫曼编码</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/hafmantree/4.png\"><br><img src=\"/img/datastruct/5_tree/hafmantree/4.png\" alt=\"数据结构\"></p>\n<ul>\n<li>前缀码</li>\n</ul>"},{"title":"5.5 并查集","date":"2023-08-06T16:00:00.000Z","comment":false,"toc":true,"_content":"#\n<!--more-->\n\n![](../../../../themes/yilia/source/img/datastruct/5_tree/set/1.png)\n![数据结构](/img/datastruct/5_tree/set/1.png)\n### 5.5.1 存储结构\n\n- 双亲表示法\n![](../../../../themes/yilia/source/img/datastruct/5_tree/set/2.png)\n![数据结构](/img/datastruct/5_tree/set/2.png)\n\n\n### 5.5.2 操作\n![](../../../../themes/yilia/source/img/datastruct/5_tree/set/3.png)\n![数据结构](/img/datastruct/5_tree/set/3.png)\n- 时间复杂度\n\n|操作|最坏时间复杂度|\n|:---|:---|\n|Find(x)|O(n)|\n|Union(x,y)|O(1)|\n- 优化union操作\n    ![](../../../../themes/yilia/source/img/datastruct/5_tree/set/4.png)\n    ![数据结构](/img/datastruct/5_tree/set/4.png)\n\n    - 树高不超过$$\\lfloor log_2n \\rfloor+1$$\n    - find时间复杂度为$$O( log_2n)$$\n- 优化find操作\n\n    ![](../../../../themes/yilia/source/img/datastruct/5_tree/set/5.png)\n    ![数据结构](/img/datastruct/5_tree/set/5.png)\n\n    - 每次查找时，将路径上的结点都放到根下面\n\n\n​        ","source":"_posts/datastruct/5_tree/5_set.md","raw":"---\ntitle: 5.5 并查集\ndate: 2023-08-07 00:00:00\ntags: [数据结构,并查集]\ncategories: [数据结构]\ncomment: false\ntoc: true\n---\n#\n<!--more-->\n\n![](../../../../themes/yilia/source/img/datastruct/5_tree/set/1.png)\n![数据结构](/img/datastruct/5_tree/set/1.png)\n### 5.5.1 存储结构\n\n- 双亲表示法\n![](../../../../themes/yilia/source/img/datastruct/5_tree/set/2.png)\n![数据结构](/img/datastruct/5_tree/set/2.png)\n\n\n### 5.5.2 操作\n![](../../../../themes/yilia/source/img/datastruct/5_tree/set/3.png)\n![数据结构](/img/datastruct/5_tree/set/3.png)\n- 时间复杂度\n\n|操作|最坏时间复杂度|\n|:---|:---|\n|Find(x)|O(n)|\n|Union(x,y)|O(1)|\n- 优化union操作\n    ![](../../../../themes/yilia/source/img/datastruct/5_tree/set/4.png)\n    ![数据结构](/img/datastruct/5_tree/set/4.png)\n\n    - 树高不超过$$\\lfloor log_2n \\rfloor+1$$\n    - find时间复杂度为$$O( log_2n)$$\n- 优化find操作\n\n    ![](../../../../themes/yilia/source/img/datastruct/5_tree/set/5.png)\n    ![数据结构](/img/datastruct/5_tree/set/5.png)\n\n    - 每次查找时，将路径上的结点都放到根下面\n\n\n​        ","slug":"datastruct/5_tree/5_set","published":1,"updated":"2023-10-23T12:23:20.125Z","comments":1,"layout":"post","photos":[],"_id":"clvulgd61001z5ovwf8v0bsfj","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/set/1.png\"><br><img src=\"/img/datastruct/5_tree/set/1.png\" alt=\"数据结构\"></p>\n<h3 id=\"5-5-1-存储结构\"><a href=\"#5-5-1-存储结构\" class=\"headerlink\" title=\"5.5.1 存储结构\"></a>5.5.1 存储结构</h3><ul>\n<li>双亲表示法<br><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/set/2.png\"><br><img src=\"/img/datastruct/5_tree/set/2.png\" alt=\"数据结构\"></li>\n</ul>\n<h3 id=\"5-5-2-操作\"><a href=\"#5-5-2-操作\" class=\"headerlink\" title=\"5.5.2 操作\"></a>5.5.2 操作</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/set/3.png\"><br><img src=\"/img/datastruct/5_tree/set/3.png\" alt=\"数据结构\"></p>\n<ul>\n<li>时间复杂度</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th align=\"left\">操作</th>\n<th align=\"left\">最坏时间复杂度</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">Find(x)</td>\n<td align=\"left\">O(n)</td>\n</tr>\n<tr>\n<td align=\"left\">Union(x,y)</td>\n<td align=\"left\">O(1)</td>\n</tr>\n</tbody></table>\n<ul>\n<li><p>优化union操作<br>  <img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/set/4.png\"><br>  <img src=\"/img/datastruct/5_tree/set/4.png\" alt=\"数据结构\"></p>\n<ul>\n<li>树高不超过$$\\lfloor log_2n \\rfloor+1$$</li>\n<li>find时间复杂度为$$O( log_2n)$$</li>\n</ul>\n</li>\n<li><p>优化find操作</p>\n<p>  <img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/set/5.png\"><br>  <img src=\"/img/datastruct/5_tree/set/5.png\" alt=\"数据结构\"></p>\n<ul>\n<li>每次查找时，将路径上的结点都放到根下面</li>\n</ul>\n</li>\n</ul>\n<p>​        </p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/set/1.png\"><br><img src=\"/img/datastruct/5_tree/set/1.png\" alt=\"数据结构\"></p>\n<h3 id=\"5-5-1-存储结构\"><a href=\"#5-5-1-存储结构\" class=\"headerlink\" title=\"5.5.1 存储结构\"></a>5.5.1 存储结构</h3><ul>\n<li>双亲表示法<br><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/set/2.png\"><br><img src=\"/img/datastruct/5_tree/set/2.png\" alt=\"数据结构\"></li>\n</ul>\n<h3 id=\"5-5-2-操作\"><a href=\"#5-5-2-操作\" class=\"headerlink\" title=\"5.5.2 操作\"></a>5.5.2 操作</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/set/3.png\"><br><img src=\"/img/datastruct/5_tree/set/3.png\" alt=\"数据结构\"></p>\n<ul>\n<li>时间复杂度</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th align=\"left\">操作</th>\n<th align=\"left\">最坏时间复杂度</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">Find(x)</td>\n<td align=\"left\">O(n)</td>\n</tr>\n<tr>\n<td align=\"left\">Union(x,y)</td>\n<td align=\"left\">O(1)</td>\n</tr>\n</tbody></table>\n<ul>\n<li><p>优化union操作<br>  <img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/set/4.png\"><br>  <img src=\"/img/datastruct/5_tree/set/4.png\" alt=\"数据结构\"></p>\n<ul>\n<li>树高不超过$$\\lfloor log_2n \\rfloor+1$$</li>\n<li>find时间复杂度为$$O( log_2n)$$</li>\n</ul>\n</li>\n<li><p>优化find操作</p>\n<p>  <img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/set/5.png\"><br>  <img src=\"/img/datastruct/5_tree/set/5.png\" alt=\"数据结构\"></p>\n<ul>\n<li>每次查找时，将路径上的结点都放到根下面</li>\n</ul>\n</li>\n</ul>\n<p>​        </p>"},{"title":"5.2 二叉树","date":"2023-08-06T16:00:00.000Z","comment":false,"toc":true,"_content":"#\n<!--more-->\n\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/1.png)\n![数据结构](/img/datastruct/5_tree/bintree/1.png)\n<!--more-->\n- 二叉树是有序树，有左右之分\n- 二叉树有五种基本形态：空二叉树、只有根结点、只有左子树、只有右子树、左右子树都有\n\n### 5.2.1 特殊二叉树\n|名称|说明|特点|\n|:---:|:---:|:---:|\n|满二叉树|所有分支结点都存在左子树和右子树，并且所有叶子都在同一层上|![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/2.png)|\n|完全二叉树|叶子结点只能出现在最下层和次下层，并且最下面一层的结点都集中在该层最左边的若干位置的二叉树|![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/3.png)|\n|二叉排序树|空树 / 左子树上所有结点的值均小于它的根结点的值；右子树上所有结点的值均大于它的根结点的值；左右子树也分别为二叉排序树||\n|平衡二叉树|空树 / 左子树和右子树的深度之差的绝对值不超过1，且它的左子树和右子树都是一棵平衡二叉树||\n\n### 5.2.2 一些性质\n- 叶子结点比度为2的结点多1个\n- 第i层最多有2<sup>i-1</sup>个结点\n- 高度为h的二叉树至多有2<sup>h</sup>-1个结点\n- 有n个结点的完全二叉树高度为$$\\lceil log_2(n+1)\\rceil 或 \\lfloor log_2n\\rfloor+1$$\n- 若完全二叉树有2k个结点，n<sub>0</sub>=k,n<sub>1</sub>=1,n<sub>2</sub>=k-1\n\n- 若完全二叉树有2k-1个结点，n<sub>0</sub>=k,n<sub>1</sub>=0,n<sub>2</sub>=k-1\n\n### 5.2.3 存储结构\n- 顺序存储\n```c\n#define MaxSize 100\ntypedef struct{\n    int value;\n    int isEmpty;\n}TreeNode;\n\nTreeNode t[MaxSize];\n```\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/4.png)\n![数据结构](/img/datastruct/5_tree/bintree/4.png)\n\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/5.png)\n![数据结构](/img/datastruct/5_tree/bintree/5.png)\n\n- 链式存储\n  \n```c\ntypedef struct BiTnode{\n    int data;\n    struct BiTnode *lchild, *rchild;\n}BiTNode,BiTree;\n```\n- n个结点的二叉链表共有n+1个空链域\n\n### 5.2.4 遍历\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/9.png)\n![数据结构](/img/datastruct/5_tree/bintree/9.png)\n\n- 先序遍历\n```c\n//先序遍历\nvoid PreOrder(BiTree T){\n    if(T){\n        visit(T);//操作\n        PreOrder(T->lchild);\n        PreOrder(T->rchild);\n    }\n}\n```\n- 中序遍历\n```c\n//中序遍历\nvoid InOrder(BiTree T){\n    if(T){\n        InOrder(T->lchild);\n        visit(T);//操作\n        InOrder(T->rchild);\n    }\n}\n```\n- 后序遍历\n```c\n//后序遍历\nvoid PostOrder(BiTree T){\n    if(T){\n        PostOrder(T->lchild);\n        PostOrder(T->rchild);\n        vist(T);//操作\n    }\n}\n```\n\n- 求树的深度\n```c\n//求树的深度\nint treeDepth(BiTree T){\n    if(!T)\n        return 0;\n    else{\n        int l=treeDepth(T->lchild);\n        int r=treeDepth(T->rchild);\n        return (l>r)?(l+1):(r+1);\n    }\n}\n```\n\n- 层次遍历\n```c\n//层次遍历\nvoid LevelOrder(BiTree T){\n    BiTree p;\n    BiTree q[MaxSize];\n    int front=0, rear=0;\n    if(T){\n        rear=(rear+1)%MaxSize;\n        q[rear]=T;\n        while(front!=rear){\n            front=(front+1)%MaxSize;\n            p=q[front];\n            visit(p);\n            if(p->lchild){\n                rear=(rear+1)%MaxSize;\n                q[rear]=p->lchild;\n            }\n            if(p->rchild){\n                rear=(rear+1)%MaxSize;\n                q[rear]=p->rchild;\n            }\n        }\n    }\n}\n```\n\n### 5.2.5 练习\n- 求遍历序列\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/6.png)\n![数据结构](/img/datastruct/5_tree/bintree/6.png)\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/7.png)\n![数据结构](/img/datastruct/5_tree/bintree/7.png)\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/8.png)\n![数据结构](/img/datastruct/5_tree/bintree/8.png)\n\n- 根据序列求树\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/10.png)\n![数据结构](/img/datastruct/5_tree/bintree/10.png)\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/11.png)\n![数据结构](/img/datastruct/5_tree/bintree/11.png)\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/12.png)\n![数据结构](/img/datastruct/5_tree/bintree/12.png)\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/13.png)\n![数据结构](/img/datastruct/5_tree/bintree/13.png)\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/14.png)\n![数据结构](/img/datastruct/5_tree/bintree/14.png)\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/15.png)\n![数据结构](/img/datastruct/5_tree/bintree/15.png)\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/16.png)\n![数据结构](/img/datastruct/5_tree/bintree/16.png)\n\n### 5.2.6 线索二叉树\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/17.png)\n![数据结构](/img/datastruct/5_tree/bintree/17.png)\n- 存储结构\n```c\n//线索二叉树\ntypedef struct ThreadNode{\n    int data;\n    struct ThreadNode *lchild, *rchild;\n    int ltag, rtag;//左右线索标志\n}ThreadNode, *ThreadTree;\n```\n- 中序线索二叉树\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/18.png)\n![数据结构](/img/datastruct/5_tree/bintree/18.png)\n- 先序线索二叉树\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/19.png)\n![数据结构](/img/datastruct/5_tree/bintree/19.png)\n- 后序线索二叉树\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/20.png)\n![数据结构](/img/datastruct/5_tree/bintree/20.png)\n- 线索化\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/21.png)\n![数据结构](/img/datastruct/5_tree/bintree/21.png)\n```c\n//土办法找到中序前驱\nBiTNode *p=NULL, *pre=NULL,*final=NULL;\nvoid findPre(BiTree T){\n    if(T){\n        findPre(T->lchild);\n        visit(T);\n        findPre(T->rchild);\n    }\n}\nvoid visit(BiTNode* q){\n    if(q==p)\n        final=pre;\n    else\n        pre=q;\n}\n```\n\n```c\n//中序线索化\nThreadNode *pre=NULL;\nvoid InThread(ThreadTree p){\n    if(p){\n        InThread(p->lchild);\n        if(!p->lchild){\n            p->lchild=pre;\n            p->ltag=1;\n        }\n        if(!pre->rchild){\n            pre->rchild=p;\n            pre->rtag=1;\n        }\n        pre=p;\n        InThread(p->rchild);\n    }\n}\n//先序线索化\nvoid PreThread(ThreadTree p){\n    if(p){\n        if(!p->lchild){\n            p->lchild=pre;\n            p->ltag=1;\n        }\n        if(!pre->rchild){\n            pre->rchild=p;\n            pre->rtag=1;\n        }\n        pre=p;\n        if(p->ltag==0)\n            PreThread(p->lchild);\n        PreThread(p->rchild);\n    }\n}\n//后序线索化\nvoid PostThread(ThreadTree p){\n    if(p){\n        if(!p->lchild){\n            p->lchild=pre;\n            p->ltag=1;\n        }\n        if(!pre->rchild){\n            pre->rchild=p;\n            pre->rtag=1;\n        }\n        pre=p;\n        PreThread(p->lchild);\n        PreThread(p->rchild);\n    }\n}\nvoid CreateInThread(ThreadTree T){\n    pre=NULL;\n    if(T){\n        InThread(T);\n        if(!pre->rchild){\n            pre->rtag=1;\n        }\n    }\n}\n```\n\n- 线索二叉树找前驱/后继\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/22.png)\n![数据结构](/img/datastruct/5_tree/bintree/22.png)\n    - 中序，后继\n\n        1）p->rtag==1，则next=p->rchild\n\n        2）p->rtag==0，则\n        ```c\n        //中序线索二叉树找后继\n        ThreadNode *Firstnode(ThreadNode *p){\n            while(p->ltag==0)\n                p=p->lchild;\n            return p;\n        }\n        ThreadNode *Nextnode(ThreadNode *p){\n            if(p->rtag==0)\n                return Firstnode(p->rchild);\n            else\n                return p->rchild;\n        }\n        ```\n    - 中序，前驱\n\n        1）p->ltag==1，则pre=p->lchild\n\n        2）p->ltag==0,pre=左子树的最右下\n\n        ```c\n        //中序线索二叉树找到前驱\n        ThreadNode *Lastnode(ThreadNode *p){\n            while(p->rtag==0)\n                p=p->rchild;\n            return p;\n        }\n        ThreadNode *Prenode(ThreadNode *p){\n            if(p->ltag==0)\n                return Lastnode(p->lchild);\n            else\n                return p->lchild;\n        }\n        //对中序线索二叉树逆向遍历\n        void InOrder(ThreadNode *T){\n            for(ThreadNode *p=Lastnode(T); p!=NULL; p=Prenode(p))\n                visit(p);\n        }\n        ```\n    - 先序，后继\n\n        1）p->rtag==1，则next=p->rchild\n\n        2）p->rtag==0，则next=左孩子（无则右孩子）\n\n    - 先序，前驱\n\n\n        1）p->ltag==1，则prep->lchild\n    \n        2）p->ltag==0\n    \n        - 若能找到p的父，p为左孩子，则pre=父\n    \n        - 若能找到p的父，p为右孩子，左兄弟空，则pre=父\n    \n        - 若能找到p的父，p为右孩子，左兄弟不空，则pre=左子树中最后一个被中序遍历的结点\n    \n    - 后序，前驱\n    \n        1）p->ltag==1，则pre=p->lchild\n    \n        2）p->ltag==0，则pre=右孩子（无则左孩子）\n    \n    - 后序，后继\n\n\n        1）p->rtag==1，则next->rchild\n    \n        2）p->rtag==0\n    \n        - 若能找到p的父，p为右孩子，则next=父\n    \n        - 若能找到p的父，p为左孩子，右兄弟空，则next=父\n    \n        - 若能找到p的父，p为左孩子，右兄弟不空，则next=右子树中第一个被后序遍历的结点\n\n\n\n","source":"_posts/datastruct/5_tree/2_bintree.md","raw":"---\ntitle: 5.2 二叉树\ndate: 2023-08-07 00:00:00\ntags: [数据结构,树,二叉树]\ncategories: [数据结构]\ncomment: false\ntoc: true\n---\n#\n<!--more-->\n\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/1.png)\n![数据结构](/img/datastruct/5_tree/bintree/1.png)\n<!--more-->\n- 二叉树是有序树，有左右之分\n- 二叉树有五种基本形态：空二叉树、只有根结点、只有左子树、只有右子树、左右子树都有\n\n### 5.2.1 特殊二叉树\n|名称|说明|特点|\n|:---:|:---:|:---:|\n|满二叉树|所有分支结点都存在左子树和右子树，并且所有叶子都在同一层上|![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/2.png)|\n|完全二叉树|叶子结点只能出现在最下层和次下层，并且最下面一层的结点都集中在该层最左边的若干位置的二叉树|![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/3.png)|\n|二叉排序树|空树 / 左子树上所有结点的值均小于它的根结点的值；右子树上所有结点的值均大于它的根结点的值；左右子树也分别为二叉排序树||\n|平衡二叉树|空树 / 左子树和右子树的深度之差的绝对值不超过1，且它的左子树和右子树都是一棵平衡二叉树||\n\n### 5.2.2 一些性质\n- 叶子结点比度为2的结点多1个\n- 第i层最多有2<sup>i-1</sup>个结点\n- 高度为h的二叉树至多有2<sup>h</sup>-1个结点\n- 有n个结点的完全二叉树高度为$$\\lceil log_2(n+1)\\rceil 或 \\lfloor log_2n\\rfloor+1$$\n- 若完全二叉树有2k个结点，n<sub>0</sub>=k,n<sub>1</sub>=1,n<sub>2</sub>=k-1\n\n- 若完全二叉树有2k-1个结点，n<sub>0</sub>=k,n<sub>1</sub>=0,n<sub>2</sub>=k-1\n\n### 5.2.3 存储结构\n- 顺序存储\n```c\n#define MaxSize 100\ntypedef struct{\n    int value;\n    int isEmpty;\n}TreeNode;\n\nTreeNode t[MaxSize];\n```\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/4.png)\n![数据结构](/img/datastruct/5_tree/bintree/4.png)\n\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/5.png)\n![数据结构](/img/datastruct/5_tree/bintree/5.png)\n\n- 链式存储\n  \n```c\ntypedef struct BiTnode{\n    int data;\n    struct BiTnode *lchild, *rchild;\n}BiTNode,BiTree;\n```\n- n个结点的二叉链表共有n+1个空链域\n\n### 5.2.4 遍历\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/9.png)\n![数据结构](/img/datastruct/5_tree/bintree/9.png)\n\n- 先序遍历\n```c\n//先序遍历\nvoid PreOrder(BiTree T){\n    if(T){\n        visit(T);//操作\n        PreOrder(T->lchild);\n        PreOrder(T->rchild);\n    }\n}\n```\n- 中序遍历\n```c\n//中序遍历\nvoid InOrder(BiTree T){\n    if(T){\n        InOrder(T->lchild);\n        visit(T);//操作\n        InOrder(T->rchild);\n    }\n}\n```\n- 后序遍历\n```c\n//后序遍历\nvoid PostOrder(BiTree T){\n    if(T){\n        PostOrder(T->lchild);\n        PostOrder(T->rchild);\n        vist(T);//操作\n    }\n}\n```\n\n- 求树的深度\n```c\n//求树的深度\nint treeDepth(BiTree T){\n    if(!T)\n        return 0;\n    else{\n        int l=treeDepth(T->lchild);\n        int r=treeDepth(T->rchild);\n        return (l>r)?(l+1):(r+1);\n    }\n}\n```\n\n- 层次遍历\n```c\n//层次遍历\nvoid LevelOrder(BiTree T){\n    BiTree p;\n    BiTree q[MaxSize];\n    int front=0, rear=0;\n    if(T){\n        rear=(rear+1)%MaxSize;\n        q[rear]=T;\n        while(front!=rear){\n            front=(front+1)%MaxSize;\n            p=q[front];\n            visit(p);\n            if(p->lchild){\n                rear=(rear+1)%MaxSize;\n                q[rear]=p->lchild;\n            }\n            if(p->rchild){\n                rear=(rear+1)%MaxSize;\n                q[rear]=p->rchild;\n            }\n        }\n    }\n}\n```\n\n### 5.2.5 练习\n- 求遍历序列\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/6.png)\n![数据结构](/img/datastruct/5_tree/bintree/6.png)\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/7.png)\n![数据结构](/img/datastruct/5_tree/bintree/7.png)\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/8.png)\n![数据结构](/img/datastruct/5_tree/bintree/8.png)\n\n- 根据序列求树\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/10.png)\n![数据结构](/img/datastruct/5_tree/bintree/10.png)\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/11.png)\n![数据结构](/img/datastruct/5_tree/bintree/11.png)\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/12.png)\n![数据结构](/img/datastruct/5_tree/bintree/12.png)\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/13.png)\n![数据结构](/img/datastruct/5_tree/bintree/13.png)\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/14.png)\n![数据结构](/img/datastruct/5_tree/bintree/14.png)\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/15.png)\n![数据结构](/img/datastruct/5_tree/bintree/15.png)\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/16.png)\n![数据结构](/img/datastruct/5_tree/bintree/16.png)\n\n### 5.2.6 线索二叉树\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/17.png)\n![数据结构](/img/datastruct/5_tree/bintree/17.png)\n- 存储结构\n```c\n//线索二叉树\ntypedef struct ThreadNode{\n    int data;\n    struct ThreadNode *lchild, *rchild;\n    int ltag, rtag;//左右线索标志\n}ThreadNode, *ThreadTree;\n```\n- 中序线索二叉树\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/18.png)\n![数据结构](/img/datastruct/5_tree/bintree/18.png)\n- 先序线索二叉树\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/19.png)\n![数据结构](/img/datastruct/5_tree/bintree/19.png)\n- 后序线索二叉树\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/20.png)\n![数据结构](/img/datastruct/5_tree/bintree/20.png)\n- 线索化\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/21.png)\n![数据结构](/img/datastruct/5_tree/bintree/21.png)\n```c\n//土办法找到中序前驱\nBiTNode *p=NULL, *pre=NULL,*final=NULL;\nvoid findPre(BiTree T){\n    if(T){\n        findPre(T->lchild);\n        visit(T);\n        findPre(T->rchild);\n    }\n}\nvoid visit(BiTNode* q){\n    if(q==p)\n        final=pre;\n    else\n        pre=q;\n}\n```\n\n```c\n//中序线索化\nThreadNode *pre=NULL;\nvoid InThread(ThreadTree p){\n    if(p){\n        InThread(p->lchild);\n        if(!p->lchild){\n            p->lchild=pre;\n            p->ltag=1;\n        }\n        if(!pre->rchild){\n            pre->rchild=p;\n            pre->rtag=1;\n        }\n        pre=p;\n        InThread(p->rchild);\n    }\n}\n//先序线索化\nvoid PreThread(ThreadTree p){\n    if(p){\n        if(!p->lchild){\n            p->lchild=pre;\n            p->ltag=1;\n        }\n        if(!pre->rchild){\n            pre->rchild=p;\n            pre->rtag=1;\n        }\n        pre=p;\n        if(p->ltag==0)\n            PreThread(p->lchild);\n        PreThread(p->rchild);\n    }\n}\n//后序线索化\nvoid PostThread(ThreadTree p){\n    if(p){\n        if(!p->lchild){\n            p->lchild=pre;\n            p->ltag=1;\n        }\n        if(!pre->rchild){\n            pre->rchild=p;\n            pre->rtag=1;\n        }\n        pre=p;\n        PreThread(p->lchild);\n        PreThread(p->rchild);\n    }\n}\nvoid CreateInThread(ThreadTree T){\n    pre=NULL;\n    if(T){\n        InThread(T);\n        if(!pre->rchild){\n            pre->rtag=1;\n        }\n    }\n}\n```\n\n- 线索二叉树找前驱/后继\n![](../../../../themes/yilia/source/img/datastruct/5_tree/bintree/22.png)\n![数据结构](/img/datastruct/5_tree/bintree/22.png)\n    - 中序，后继\n\n        1）p->rtag==1，则next=p->rchild\n\n        2）p->rtag==0，则\n        ```c\n        //中序线索二叉树找后继\n        ThreadNode *Firstnode(ThreadNode *p){\n            while(p->ltag==0)\n                p=p->lchild;\n            return p;\n        }\n        ThreadNode *Nextnode(ThreadNode *p){\n            if(p->rtag==0)\n                return Firstnode(p->rchild);\n            else\n                return p->rchild;\n        }\n        ```\n    - 中序，前驱\n\n        1）p->ltag==1，则pre=p->lchild\n\n        2）p->ltag==0,pre=左子树的最右下\n\n        ```c\n        //中序线索二叉树找到前驱\n        ThreadNode *Lastnode(ThreadNode *p){\n            while(p->rtag==0)\n                p=p->rchild;\n            return p;\n        }\n        ThreadNode *Prenode(ThreadNode *p){\n            if(p->ltag==0)\n                return Lastnode(p->lchild);\n            else\n                return p->lchild;\n        }\n        //对中序线索二叉树逆向遍历\n        void InOrder(ThreadNode *T){\n            for(ThreadNode *p=Lastnode(T); p!=NULL; p=Prenode(p))\n                visit(p);\n        }\n        ```\n    - 先序，后继\n\n        1）p->rtag==1，则next=p->rchild\n\n        2）p->rtag==0，则next=左孩子（无则右孩子）\n\n    - 先序，前驱\n\n\n        1）p->ltag==1，则prep->lchild\n    \n        2）p->ltag==0\n    \n        - 若能找到p的父，p为左孩子，则pre=父\n    \n        - 若能找到p的父，p为右孩子，左兄弟空，则pre=父\n    \n        - 若能找到p的父，p为右孩子，左兄弟不空，则pre=左子树中最后一个被中序遍历的结点\n    \n    - 后序，前驱\n    \n        1）p->ltag==1，则pre=p->lchild\n    \n        2）p->ltag==0，则pre=右孩子（无则左孩子）\n    \n    - 后序，后继\n\n\n        1）p->rtag==1，则next->rchild\n    \n        2）p->rtag==0\n    \n        - 若能找到p的父，p为右孩子，则next=父\n    \n        - 若能找到p的父，p为左孩子，右兄弟空，则next=父\n    \n        - 若能找到p的父，p为左孩子，右兄弟不空，则next=右子树中第一个被后序遍历的结点\n\n\n\n","slug":"datastruct/5_tree/2_bintree","published":1,"updated":"2023-10-23T12:16:20.694Z","comments":1,"layout":"post","photos":[],"_id":"clvulgd6200225ovwg1jbc0l5","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/1.png\"><br><img src=\"/img/datastruct/5_tree/bintree/1.png\" alt=\"数据结构\"></p>\n<!--more-->\n<ul>\n<li>二叉树是有序树，有左右之分</li>\n<li>二叉树有五种基本形态：空二叉树、只有根结点、只有左子树、只有右子树、左右子树都有</li>\n</ul>\n<h3 id=\"5-2-1-特殊二叉树\"><a href=\"#5-2-1-特殊二叉树\" class=\"headerlink\" title=\"5.2.1 特殊二叉树\"></a>5.2.1 特殊二叉树</h3><table>\n<thead>\n<tr>\n<th align=\"center\">名称</th>\n<th align=\"center\">说明</th>\n<th align=\"center\">特点</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">满二叉树</td>\n<td align=\"center\">所有分支结点都存在左子树和右子树，并且所有叶子都在同一层上</td>\n<td align=\"center\"><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/2.png\"></td>\n</tr>\n<tr>\n<td align=\"center\">完全二叉树</td>\n<td align=\"center\">叶子结点只能出现在最下层和次下层，并且最下面一层的结点都集中在该层最左边的若干位置的二叉树</td>\n<td align=\"center\"><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/3.png\"></td>\n</tr>\n<tr>\n<td align=\"center\">二叉排序树</td>\n<td align=\"center\">空树 &#x2F; 左子树上所有结点的值均小于它的根结点的值；右子树上所有结点的值均大于它的根结点的值；左右子树也分别为二叉排序树</td>\n<td align=\"center\"></td>\n</tr>\n<tr>\n<td align=\"center\">平衡二叉树</td>\n<td align=\"center\">空树 &#x2F; 左子树和右子树的深度之差的绝对值不超过1，且它的左子树和右子树都是一棵平衡二叉树</td>\n<td align=\"center\"></td>\n</tr>\n</tbody></table>\n<h3 id=\"5-2-2-一些性质\"><a href=\"#5-2-2-一些性质\" class=\"headerlink\" title=\"5.2.2 一些性质\"></a>5.2.2 一些性质</h3><ul>\n<li><p>叶子结点比度为2的结点多1个</p>\n</li>\n<li><p>第i层最多有2<sup>i-1</sup>个结点</p>\n</li>\n<li><p>高度为h的二叉树至多有2<sup>h</sup>-1个结点</p>\n</li>\n<li><p>有n个结点的完全二叉树高度为$$\\lceil log_2(n+1)\\rceil 或 \\lfloor log_2n\\rfloor+1$$</p>\n</li>\n<li><p>若完全二叉树有2k个结点，n<sub>0</sub>&#x3D;k,n<sub>1</sub>&#x3D;1,n<sub>2</sub>&#x3D;k-1</p>\n</li>\n<li><p>若完全二叉树有2k-1个结点，n<sub>0</sub>&#x3D;k,n<sub>1</sub>&#x3D;0,n<sub>2</sub>&#x3D;k-1</p>\n</li>\n</ul>\n<h3 id=\"5-2-3-存储结构\"><a href=\"#5-2-3-存储结构\" class=\"headerlink\" title=\"5.2.3 存储结构\"></a>5.2.3 存储结构</h3><ul>\n<li>顺序存储<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> MaxSize 100</span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span>&#123;</span></span><br><span class=\"line\">    <span class=\"type\">int</span> value;</span><br><span class=\"line\">    <span class=\"type\">int</span> isEmpty;</span><br><span class=\"line\">&#125;TreeNode;</span><br><span class=\"line\"></span><br><span class=\"line\">TreeNode t[MaxSize];</span><br></pre></td></tr></table></figure>\n<img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/4.png\"><br><img src=\"/img/datastruct/5_tree/bintree/4.png\" alt=\"数据结构\"></li>\n</ul>\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/5.png\"><br><img src=\"/img/datastruct/5_tree/bintree/5.png\" alt=\"数据结构\"></p>\n<ul>\n<li>链式存储</li>\n</ul>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">BiTnode</span>&#123;</span></span><br><span class=\"line\">    <span class=\"type\">int</span> data;</span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">BiTnode</span> *<span class=\"title\">lchild</span>, *<span class=\"title\">rchild</span>;</span></span><br><span class=\"line\">&#125;BiTNode,BiTree;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>n个结点的二叉链表共有n+1个空链域</li>\n</ul>\n<h3 id=\"5-2-4-遍历\"><a href=\"#5-2-4-遍历\" class=\"headerlink\" title=\"5.2.4 遍历\"></a>5.2.4 遍历</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/9.png\"><br><img src=\"/img/datastruct/5_tree/bintree/9.png\" alt=\"数据结构\"></p>\n<ul>\n<li><p>先序遍历</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//先序遍历</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">PreOrder</span><span class=\"params\">(BiTree T)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(T)&#123;</span><br><span class=\"line\">        visit(T);<span class=\"comment\">//操作</span></span><br><span class=\"line\">        PreOrder(T-&gt;lchild);</span><br><span class=\"line\">        PreOrder(T-&gt;rchild);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n<li><p>中序遍历</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//中序遍历</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">InOrder</span><span class=\"params\">(BiTree T)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(T)&#123;</span><br><span class=\"line\">        InOrder(T-&gt;lchild);</span><br><span class=\"line\">        visit(T);<span class=\"comment\">//操作</span></span><br><span class=\"line\">        InOrder(T-&gt;rchild);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n<li><p>后序遍历</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//后序遍历</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">PostOrder</span><span class=\"params\">(BiTree T)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(T)&#123;</span><br><span class=\"line\">        PostOrder(T-&gt;lchild);</span><br><span class=\"line\">        PostOrder(T-&gt;rchild);</span><br><span class=\"line\">        vist(T);<span class=\"comment\">//操作</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>求树的深度</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//求树的深度</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">treeDepth</span><span class=\"params\">(BiTree T)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(!T)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> l=treeDepth(T-&gt;lchild);</span><br><span class=\"line\">        <span class=\"type\">int</span> r=treeDepth(T-&gt;rchild);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> (l&gt;r)?(l+<span class=\"number\">1</span>):(r+<span class=\"number\">1</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>层次遍历</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//层次遍历</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">LevelOrder</span><span class=\"params\">(BiTree T)</span>&#123;</span><br><span class=\"line\">    BiTree p;</span><br><span class=\"line\">    BiTree q[MaxSize];</span><br><span class=\"line\">    <span class=\"type\">int</span> front=<span class=\"number\">0</span>, rear=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(T)&#123;</span><br><span class=\"line\">        rear=(rear+<span class=\"number\">1</span>)%MaxSize;</span><br><span class=\"line\">        q[rear]=T;</span><br><span class=\"line\">        <span class=\"keyword\">while</span>(front!=rear)&#123;</span><br><span class=\"line\">            front=(front+<span class=\"number\">1</span>)%MaxSize;</span><br><span class=\"line\">            p=q[front];</span><br><span class=\"line\">            visit(p);</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(p-&gt;lchild)&#123;</span><br><span class=\"line\">                rear=(rear+<span class=\"number\">1</span>)%MaxSize;</span><br><span class=\"line\">                q[rear]=p-&gt;lchild;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(p-&gt;rchild)&#123;</span><br><span class=\"line\">                rear=(rear+<span class=\"number\">1</span>)%MaxSize;</span><br><span class=\"line\">                q[rear]=p-&gt;rchild;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h3 id=\"5-2-5-练习\"><a href=\"#5-2-5-练习\" class=\"headerlink\" title=\"5.2.5 练习\"></a>5.2.5 练习</h3><ul>\n<li><p>求遍历序列<br><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/6.png\"><br><img src=\"/img/datastruct/5_tree/bintree/6.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/7.png\"><br><img src=\"/img/datastruct/5_tree/bintree/7.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/8.png\"><br><img src=\"/img/datastruct/5_tree/bintree/8.png\" alt=\"数据结构\"></p>\n</li>\n<li><p>根据序列求树<br><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/10.png\"><br><img src=\"/img/datastruct/5_tree/bintree/10.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/11.png\"><br><img src=\"/img/datastruct/5_tree/bintree/11.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/12.png\"><br><img src=\"/img/datastruct/5_tree/bintree/12.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/13.png\"><br><img src=\"/img/datastruct/5_tree/bintree/13.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/14.png\"><br><img src=\"/img/datastruct/5_tree/bintree/14.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/15.png\"><br><img src=\"/img/datastruct/5_tree/bintree/15.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/16.png\"><br><img src=\"/img/datastruct/5_tree/bintree/16.png\" alt=\"数据结构\"></p>\n</li>\n</ul>\n<h3 id=\"5-2-6-线索二叉树\"><a href=\"#5-2-6-线索二叉树\" class=\"headerlink\" title=\"5.2.6 线索二叉树\"></a>5.2.6 线索二叉树</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/17.png\"><br><img src=\"/img/datastruct/5_tree/bintree/17.png\" alt=\"数据结构\"></p>\n<ul>\n<li>存储结构<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//线索二叉树</span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">ThreadNode</span>&#123;</span></span><br><span class=\"line\">    <span class=\"type\">int</span> data;</span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">ThreadNode</span> *<span class=\"title\">lchild</span>, *<span class=\"title\">rchild</span>;</span></span><br><span class=\"line\">    <span class=\"type\">int</span> ltag, rtag;<span class=\"comment\">//左右线索标志</span></span><br><span class=\"line\">&#125;ThreadNode, *ThreadTree;</span><br></pre></td></tr></table></figure></li>\n<li>中序线索二叉树<br><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/18.png\"><br><img src=\"/img/datastruct/5_tree/bintree/18.png\" alt=\"数据结构\"></li>\n<li>先序线索二叉树<br><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/19.png\"><br><img src=\"/img/datastruct/5_tree/bintree/19.png\" alt=\"数据结构\"></li>\n<li>后序线索二叉树<br><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/20.png\"><br><img src=\"/img/datastruct/5_tree/bintree/20.png\" alt=\"数据结构\"></li>\n<li>线索化<br><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/21.png\"><br><img src=\"/img/datastruct/5_tree/bintree/21.png\" alt=\"数据结构\"><figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//土办法找到中序前驱</span></span><br><span class=\"line\">BiTNode *p=<span class=\"literal\">NULL</span>, *pre=<span class=\"literal\">NULL</span>,*final=<span class=\"literal\">NULL</span>;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">findPre</span><span class=\"params\">(BiTree T)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(T)&#123;</span><br><span class=\"line\">        findPre(T-&gt;lchild);</span><br><span class=\"line\">        visit(T);</span><br><span class=\"line\">        findPre(T-&gt;rchild);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">visit</span><span class=\"params\">(BiTNode* q)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(q==p)</span><br><span class=\"line\">        final=pre;</span><br><span class=\"line\">    <span class=\"keyword\">else</span></span><br><span class=\"line\">        pre=q;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n</ul>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//中序线索化</span></span><br><span class=\"line\">ThreadNode *pre=<span class=\"literal\">NULL</span>;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">InThread</span><span class=\"params\">(ThreadTree p)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(p)&#123;</span><br><span class=\"line\">        InThread(p-&gt;lchild);</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(!p-&gt;lchild)&#123;</span><br><span class=\"line\">            p-&gt;lchild=pre;</span><br><span class=\"line\">            p-&gt;ltag=<span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(!pre-&gt;rchild)&#123;</span><br><span class=\"line\">            pre-&gt;rchild=p;</span><br><span class=\"line\">            pre-&gt;rtag=<span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        pre=p;</span><br><span class=\"line\">        InThread(p-&gt;rchild);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">//先序线索化</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">PreThread</span><span class=\"params\">(ThreadTree p)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(p)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(!p-&gt;lchild)&#123;</span><br><span class=\"line\">            p-&gt;lchild=pre;</span><br><span class=\"line\">            p-&gt;ltag=<span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(!pre-&gt;rchild)&#123;</span><br><span class=\"line\">            pre-&gt;rchild=p;</span><br><span class=\"line\">            pre-&gt;rtag=<span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        pre=p;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(p-&gt;ltag==<span class=\"number\">0</span>)</span><br><span class=\"line\">            PreThread(p-&gt;lchild);</span><br><span class=\"line\">        PreThread(p-&gt;rchild);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">//后序线索化</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">PostThread</span><span class=\"params\">(ThreadTree p)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(p)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(!p-&gt;lchild)&#123;</span><br><span class=\"line\">            p-&gt;lchild=pre;</span><br><span class=\"line\">            p-&gt;ltag=<span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(!pre-&gt;rchild)&#123;</span><br><span class=\"line\">            pre-&gt;rchild=p;</span><br><span class=\"line\">            pre-&gt;rtag=<span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        pre=p;</span><br><span class=\"line\">        PreThread(p-&gt;lchild);</span><br><span class=\"line\">        PreThread(p-&gt;rchild);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">CreateInThread</span><span class=\"params\">(ThreadTree T)</span>&#123;</span><br><span class=\"line\">    pre=<span class=\"literal\">NULL</span>;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(T)&#123;</span><br><span class=\"line\">        InThread(T);</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(!pre-&gt;rchild)&#123;</span><br><span class=\"line\">            pre-&gt;rtag=<span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>线索二叉树找前驱&#x2F;后继<br><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/22.png\"><br><img src=\"/img/datastruct/5_tree/bintree/22.png\" alt=\"数据结构\"><ul>\n<li><p>中序，后继</p>\n<p>  1）p-&gt;rtag&#x3D;&#x3D;1，则next&#x3D;p-&gt;rchild</p>\n<p>  2）p-&gt;rtag&#x3D;&#x3D;0，则</p>\n  <figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//中序线索二叉树找后继</span></span><br><span class=\"line\">ThreadNode *<span class=\"title function_\">Firstnode</span><span class=\"params\">(ThreadNode *p)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(p-&gt;ltag==<span class=\"number\">0</span>)</span><br><span class=\"line\">        p=p-&gt;lchild;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> p;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">ThreadNode *<span class=\"title function_\">Nextnode</span><span class=\"params\">(ThreadNode *p)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(p-&gt;rtag==<span class=\"number\">0</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> Firstnode(p-&gt;rchild);</span><br><span class=\"line\">    <span class=\"keyword\">else</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> p-&gt;rchild;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n<li><p>中序，前驱</p>\n<p>  1）p-&gt;ltag&#x3D;&#x3D;1，则pre&#x3D;p-&gt;lchild</p>\n<p>  2）p-&gt;ltag&#x3D;&#x3D;0,pre&#x3D;左子树的最右下</p>\n  <figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//中序线索二叉树找到前驱</span></span><br><span class=\"line\">ThreadNode *<span class=\"title function_\">Lastnode</span><span class=\"params\">(ThreadNode *p)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(p-&gt;rtag==<span class=\"number\">0</span>)</span><br><span class=\"line\">        p=p-&gt;rchild;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> p;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">ThreadNode *<span class=\"title function_\">Prenode</span><span class=\"params\">(ThreadNode *p)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(p-&gt;ltag==<span class=\"number\">0</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> Lastnode(p-&gt;lchild);</span><br><span class=\"line\">    <span class=\"keyword\">else</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> p-&gt;lchild;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">//对中序线索二叉树逆向遍历</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">InOrder</span><span class=\"params\">(ThreadNode *T)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(ThreadNode *p=Lastnode(T); p!=<span class=\"literal\">NULL</span>; p=Prenode(p))</span><br><span class=\"line\">        visit(p);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n<li><p>先序，后继</p>\n<p>  1）p-&gt;rtag&#x3D;&#x3D;1，则next&#x3D;p-&gt;rchild</p>\n<p>  2）p-&gt;rtag&#x3D;&#x3D;0，则next&#x3D;左孩子（无则右孩子）</p>\n</li>\n<li><p>先序，前驱</p>\n<p>  1）p-&gt;ltag&#x3D;&#x3D;1，则prep-&gt;lchild</p>\n<p>  2）p-&gt;ltag&#x3D;&#x3D;0</p>\n<ul>\n<li><p>若能找到p的父，p为左孩子，则pre&#x3D;父</p>\n</li>\n<li><p>若能找到p的父，p为右孩子，左兄弟空，则pre&#x3D;父</p>\n</li>\n<li><p>若能找到p的父，p为右孩子，左兄弟不空，则pre&#x3D;左子树中最后一个被中序遍历的结点</p>\n</li>\n</ul>\n</li>\n<li><p>后序，前驱</p>\n<p>  1）p-&gt;ltag&#x3D;&#x3D;1，则pre&#x3D;p-&gt;lchild</p>\n<p>  2）p-&gt;ltag&#x3D;&#x3D;0，则pre&#x3D;右孩子（无则左孩子）</p>\n</li>\n<li><p>后序，后继</p>\n<p>  1）p-&gt;rtag&#x3D;&#x3D;1，则next-&gt;rchild</p>\n<p>  2）p-&gt;rtag&#x3D;&#x3D;0</p>\n<ul>\n<li><p>若能找到p的父，p为右孩子，则next&#x3D;父</p>\n</li>\n<li><p>若能找到p的父，p为左孩子，右兄弟空，则next&#x3D;父</p>\n</li>\n<li><p>若能找到p的父，p为左孩子，右兄弟不空，则next&#x3D;右子树中第一个被后序遍历的结点</p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/1.png\"><br><img src=\"/img/datastruct/5_tree/bintree/1.png\" alt=\"数据结构\"></p>\n<!--more-->\n<ul>\n<li>二叉树是有序树，有左右之分</li>\n<li>二叉树有五种基本形态：空二叉树、只有根结点、只有左子树、只有右子树、左右子树都有</li>\n</ul>\n<h3 id=\"5-2-1-特殊二叉树\"><a href=\"#5-2-1-特殊二叉树\" class=\"headerlink\" title=\"5.2.1 特殊二叉树\"></a>5.2.1 特殊二叉树</h3><table>\n<thead>\n<tr>\n<th align=\"center\">名称</th>\n<th align=\"center\">说明</th>\n<th align=\"center\">特点</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">满二叉树</td>\n<td align=\"center\">所有分支结点都存在左子树和右子树，并且所有叶子都在同一层上</td>\n<td align=\"center\"><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/2.png\"></td>\n</tr>\n<tr>\n<td align=\"center\">完全二叉树</td>\n<td align=\"center\">叶子结点只能出现在最下层和次下层，并且最下面一层的结点都集中在该层最左边的若干位置的二叉树</td>\n<td align=\"center\"><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/3.png\"></td>\n</tr>\n<tr>\n<td align=\"center\">二叉排序树</td>\n<td align=\"center\">空树 &#x2F; 左子树上所有结点的值均小于它的根结点的值；右子树上所有结点的值均大于它的根结点的值；左右子树也分别为二叉排序树</td>\n<td align=\"center\"></td>\n</tr>\n<tr>\n<td align=\"center\">平衡二叉树</td>\n<td align=\"center\">空树 &#x2F; 左子树和右子树的深度之差的绝对值不超过1，且它的左子树和右子树都是一棵平衡二叉树</td>\n<td align=\"center\"></td>\n</tr>\n</tbody></table>\n<h3 id=\"5-2-2-一些性质\"><a href=\"#5-2-2-一些性质\" class=\"headerlink\" title=\"5.2.2 一些性质\"></a>5.2.2 一些性质</h3><ul>\n<li><p>叶子结点比度为2的结点多1个</p>\n</li>\n<li><p>第i层最多有2<sup>i-1</sup>个结点</p>\n</li>\n<li><p>高度为h的二叉树至多有2<sup>h</sup>-1个结点</p>\n</li>\n<li><p>有n个结点的完全二叉树高度为$$\\lceil log_2(n+1)\\rceil 或 \\lfloor log_2n\\rfloor+1$$</p>\n</li>\n<li><p>若完全二叉树有2k个结点，n<sub>0</sub>&#x3D;k,n<sub>1</sub>&#x3D;1,n<sub>2</sub>&#x3D;k-1</p>\n</li>\n<li><p>若完全二叉树有2k-1个结点，n<sub>0</sub>&#x3D;k,n<sub>1</sub>&#x3D;0,n<sub>2</sub>&#x3D;k-1</p>\n</li>\n</ul>\n<h3 id=\"5-2-3-存储结构\"><a href=\"#5-2-3-存储结构\" class=\"headerlink\" title=\"5.2.3 存储结构\"></a>5.2.3 存储结构</h3><ul>\n<li>顺序存储<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> MaxSize 100</span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span>&#123;</span></span><br><span class=\"line\">    <span class=\"type\">int</span> value;</span><br><span class=\"line\">    <span class=\"type\">int</span> isEmpty;</span><br><span class=\"line\">&#125;TreeNode;</span><br><span class=\"line\"></span><br><span class=\"line\">TreeNode t[MaxSize];</span><br></pre></td></tr></table></figure>\n<img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/4.png\"><br><img src=\"/img/datastruct/5_tree/bintree/4.png\" alt=\"数据结构\"></li>\n</ul>\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/5.png\"><br><img src=\"/img/datastruct/5_tree/bintree/5.png\" alt=\"数据结构\"></p>\n<ul>\n<li>链式存储</li>\n</ul>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">BiTnode</span>&#123;</span></span><br><span class=\"line\">    <span class=\"type\">int</span> data;</span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">BiTnode</span> *<span class=\"title\">lchild</span>, *<span class=\"title\">rchild</span>;</span></span><br><span class=\"line\">&#125;BiTNode,BiTree;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>n个结点的二叉链表共有n+1个空链域</li>\n</ul>\n<h3 id=\"5-2-4-遍历\"><a href=\"#5-2-4-遍历\" class=\"headerlink\" title=\"5.2.4 遍历\"></a>5.2.4 遍历</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/9.png\"><br><img src=\"/img/datastruct/5_tree/bintree/9.png\" alt=\"数据结构\"></p>\n<ul>\n<li><p>先序遍历</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//先序遍历</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">PreOrder</span><span class=\"params\">(BiTree T)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(T)&#123;</span><br><span class=\"line\">        visit(T);<span class=\"comment\">//操作</span></span><br><span class=\"line\">        PreOrder(T-&gt;lchild);</span><br><span class=\"line\">        PreOrder(T-&gt;rchild);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n<li><p>中序遍历</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//中序遍历</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">InOrder</span><span class=\"params\">(BiTree T)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(T)&#123;</span><br><span class=\"line\">        InOrder(T-&gt;lchild);</span><br><span class=\"line\">        visit(T);<span class=\"comment\">//操作</span></span><br><span class=\"line\">        InOrder(T-&gt;rchild);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n<li><p>后序遍历</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//后序遍历</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">PostOrder</span><span class=\"params\">(BiTree T)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(T)&#123;</span><br><span class=\"line\">        PostOrder(T-&gt;lchild);</span><br><span class=\"line\">        PostOrder(T-&gt;rchild);</span><br><span class=\"line\">        vist(T);<span class=\"comment\">//操作</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>求树的深度</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//求树的深度</span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">treeDepth</span><span class=\"params\">(BiTree T)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(!T)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> l=treeDepth(T-&gt;lchild);</span><br><span class=\"line\">        <span class=\"type\">int</span> r=treeDepth(T-&gt;rchild);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> (l&gt;r)?(l+<span class=\"number\">1</span>):(r+<span class=\"number\">1</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>层次遍历</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//层次遍历</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">LevelOrder</span><span class=\"params\">(BiTree T)</span>&#123;</span><br><span class=\"line\">    BiTree p;</span><br><span class=\"line\">    BiTree q[MaxSize];</span><br><span class=\"line\">    <span class=\"type\">int</span> front=<span class=\"number\">0</span>, rear=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(T)&#123;</span><br><span class=\"line\">        rear=(rear+<span class=\"number\">1</span>)%MaxSize;</span><br><span class=\"line\">        q[rear]=T;</span><br><span class=\"line\">        <span class=\"keyword\">while</span>(front!=rear)&#123;</span><br><span class=\"line\">            front=(front+<span class=\"number\">1</span>)%MaxSize;</span><br><span class=\"line\">            p=q[front];</span><br><span class=\"line\">            visit(p);</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(p-&gt;lchild)&#123;</span><br><span class=\"line\">                rear=(rear+<span class=\"number\">1</span>)%MaxSize;</span><br><span class=\"line\">                q[rear]=p-&gt;lchild;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(p-&gt;rchild)&#123;</span><br><span class=\"line\">                rear=(rear+<span class=\"number\">1</span>)%MaxSize;</span><br><span class=\"line\">                q[rear]=p-&gt;rchild;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h3 id=\"5-2-5-练习\"><a href=\"#5-2-5-练习\" class=\"headerlink\" title=\"5.2.5 练习\"></a>5.2.5 练习</h3><ul>\n<li><p>求遍历序列<br><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/6.png\"><br><img src=\"/img/datastruct/5_tree/bintree/6.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/7.png\"><br><img src=\"/img/datastruct/5_tree/bintree/7.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/8.png\"><br><img src=\"/img/datastruct/5_tree/bintree/8.png\" alt=\"数据结构\"></p>\n</li>\n<li><p>根据序列求树<br><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/10.png\"><br><img src=\"/img/datastruct/5_tree/bintree/10.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/11.png\"><br><img src=\"/img/datastruct/5_tree/bintree/11.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/12.png\"><br><img src=\"/img/datastruct/5_tree/bintree/12.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/13.png\"><br><img src=\"/img/datastruct/5_tree/bintree/13.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/14.png\"><br><img src=\"/img/datastruct/5_tree/bintree/14.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/15.png\"><br><img src=\"/img/datastruct/5_tree/bintree/15.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/16.png\"><br><img src=\"/img/datastruct/5_tree/bintree/16.png\" alt=\"数据结构\"></p>\n</li>\n</ul>\n<h3 id=\"5-2-6-线索二叉树\"><a href=\"#5-2-6-线索二叉树\" class=\"headerlink\" title=\"5.2.6 线索二叉树\"></a>5.2.6 线索二叉树</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/17.png\"><br><img src=\"/img/datastruct/5_tree/bintree/17.png\" alt=\"数据结构\"></p>\n<ul>\n<li>存储结构<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//线索二叉树</span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">ThreadNode</span>&#123;</span></span><br><span class=\"line\">    <span class=\"type\">int</span> data;</span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">ThreadNode</span> *<span class=\"title\">lchild</span>, *<span class=\"title\">rchild</span>;</span></span><br><span class=\"line\">    <span class=\"type\">int</span> ltag, rtag;<span class=\"comment\">//左右线索标志</span></span><br><span class=\"line\">&#125;ThreadNode, *ThreadTree;</span><br></pre></td></tr></table></figure></li>\n<li>中序线索二叉树<br><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/18.png\"><br><img src=\"/img/datastruct/5_tree/bintree/18.png\" alt=\"数据结构\"></li>\n<li>先序线索二叉树<br><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/19.png\"><br><img src=\"/img/datastruct/5_tree/bintree/19.png\" alt=\"数据结构\"></li>\n<li>后序线索二叉树<br><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/20.png\"><br><img src=\"/img/datastruct/5_tree/bintree/20.png\" alt=\"数据结构\"></li>\n<li>线索化<br><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/21.png\"><br><img src=\"/img/datastruct/5_tree/bintree/21.png\" alt=\"数据结构\"><figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//土办法找到中序前驱</span></span><br><span class=\"line\">BiTNode *p=<span class=\"literal\">NULL</span>, *pre=<span class=\"literal\">NULL</span>,*final=<span class=\"literal\">NULL</span>;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">findPre</span><span class=\"params\">(BiTree T)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(T)&#123;</span><br><span class=\"line\">        findPre(T-&gt;lchild);</span><br><span class=\"line\">        visit(T);</span><br><span class=\"line\">        findPre(T-&gt;rchild);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">visit</span><span class=\"params\">(BiTNode* q)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(q==p)</span><br><span class=\"line\">        final=pre;</span><br><span class=\"line\">    <span class=\"keyword\">else</span></span><br><span class=\"line\">        pre=q;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n</ul>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//中序线索化</span></span><br><span class=\"line\">ThreadNode *pre=<span class=\"literal\">NULL</span>;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">InThread</span><span class=\"params\">(ThreadTree p)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(p)&#123;</span><br><span class=\"line\">        InThread(p-&gt;lchild);</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(!p-&gt;lchild)&#123;</span><br><span class=\"line\">            p-&gt;lchild=pre;</span><br><span class=\"line\">            p-&gt;ltag=<span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(!pre-&gt;rchild)&#123;</span><br><span class=\"line\">            pre-&gt;rchild=p;</span><br><span class=\"line\">            pre-&gt;rtag=<span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        pre=p;</span><br><span class=\"line\">        InThread(p-&gt;rchild);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">//先序线索化</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">PreThread</span><span class=\"params\">(ThreadTree p)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(p)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(!p-&gt;lchild)&#123;</span><br><span class=\"line\">            p-&gt;lchild=pre;</span><br><span class=\"line\">            p-&gt;ltag=<span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(!pre-&gt;rchild)&#123;</span><br><span class=\"line\">            pre-&gt;rchild=p;</span><br><span class=\"line\">            pre-&gt;rtag=<span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        pre=p;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(p-&gt;ltag==<span class=\"number\">0</span>)</span><br><span class=\"line\">            PreThread(p-&gt;lchild);</span><br><span class=\"line\">        PreThread(p-&gt;rchild);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">//后序线索化</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">PostThread</span><span class=\"params\">(ThreadTree p)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(p)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(!p-&gt;lchild)&#123;</span><br><span class=\"line\">            p-&gt;lchild=pre;</span><br><span class=\"line\">            p-&gt;ltag=<span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(!pre-&gt;rchild)&#123;</span><br><span class=\"line\">            pre-&gt;rchild=p;</span><br><span class=\"line\">            pre-&gt;rtag=<span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        pre=p;</span><br><span class=\"line\">        PreThread(p-&gt;lchild);</span><br><span class=\"line\">        PreThread(p-&gt;rchild);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">CreateInThread</span><span class=\"params\">(ThreadTree T)</span>&#123;</span><br><span class=\"line\">    pre=<span class=\"literal\">NULL</span>;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(T)&#123;</span><br><span class=\"line\">        InThread(T);</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(!pre-&gt;rchild)&#123;</span><br><span class=\"line\">            pre-&gt;rtag=<span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>线索二叉树找前驱&#x2F;后继<br><img src=\"/../../../../themes/yilia/source/img/datastruct/5_tree/bintree/22.png\"><br><img src=\"/img/datastruct/5_tree/bintree/22.png\" alt=\"数据结构\"><ul>\n<li><p>中序，后继</p>\n<p>  1）p-&gt;rtag&#x3D;&#x3D;1，则next&#x3D;p-&gt;rchild</p>\n<p>  2）p-&gt;rtag&#x3D;&#x3D;0，则</p>\n  <figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//中序线索二叉树找后继</span></span><br><span class=\"line\">ThreadNode *<span class=\"title function_\">Firstnode</span><span class=\"params\">(ThreadNode *p)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(p-&gt;ltag==<span class=\"number\">0</span>)</span><br><span class=\"line\">        p=p-&gt;lchild;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> p;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">ThreadNode *<span class=\"title function_\">Nextnode</span><span class=\"params\">(ThreadNode *p)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(p-&gt;rtag==<span class=\"number\">0</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> Firstnode(p-&gt;rchild);</span><br><span class=\"line\">    <span class=\"keyword\">else</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> p-&gt;rchild;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n<li><p>中序，前驱</p>\n<p>  1）p-&gt;ltag&#x3D;&#x3D;1，则pre&#x3D;p-&gt;lchild</p>\n<p>  2）p-&gt;ltag&#x3D;&#x3D;0,pre&#x3D;左子树的最右下</p>\n  <figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//中序线索二叉树找到前驱</span></span><br><span class=\"line\">ThreadNode *<span class=\"title function_\">Lastnode</span><span class=\"params\">(ThreadNode *p)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(p-&gt;rtag==<span class=\"number\">0</span>)</span><br><span class=\"line\">        p=p-&gt;rchild;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> p;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">ThreadNode *<span class=\"title function_\">Prenode</span><span class=\"params\">(ThreadNode *p)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(p-&gt;ltag==<span class=\"number\">0</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> Lastnode(p-&gt;lchild);</span><br><span class=\"line\">    <span class=\"keyword\">else</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> p-&gt;lchild;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">//对中序线索二叉树逆向遍历</span></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">InOrder</span><span class=\"params\">(ThreadNode *T)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(ThreadNode *p=Lastnode(T); p!=<span class=\"literal\">NULL</span>; p=Prenode(p))</span><br><span class=\"line\">        visit(p);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n<li><p>先序，后继</p>\n<p>  1）p-&gt;rtag&#x3D;&#x3D;1，则next&#x3D;p-&gt;rchild</p>\n<p>  2）p-&gt;rtag&#x3D;&#x3D;0，则next&#x3D;左孩子（无则右孩子）</p>\n</li>\n<li><p>先序，前驱</p>\n<p>  1）p-&gt;ltag&#x3D;&#x3D;1，则prep-&gt;lchild</p>\n<p>  2）p-&gt;ltag&#x3D;&#x3D;0</p>\n<ul>\n<li><p>若能找到p的父，p为左孩子，则pre&#x3D;父</p>\n</li>\n<li><p>若能找到p的父，p为右孩子，左兄弟空，则pre&#x3D;父</p>\n</li>\n<li><p>若能找到p的父，p为右孩子，左兄弟不空，则pre&#x3D;左子树中最后一个被中序遍历的结点</p>\n</li>\n</ul>\n</li>\n<li><p>后序，前驱</p>\n<p>  1）p-&gt;ltag&#x3D;&#x3D;1，则pre&#x3D;p-&gt;lchild</p>\n<p>  2）p-&gt;ltag&#x3D;&#x3D;0，则pre&#x3D;右孩子（无则左孩子）</p>\n</li>\n<li><p>后序，后继</p>\n<p>  1）p-&gt;rtag&#x3D;&#x3D;1，则next-&gt;rchild</p>\n<p>  2）p-&gt;rtag&#x3D;&#x3D;0</p>\n<ul>\n<li><p>若能找到p的父，p为右孩子，则next&#x3D;父</p>\n</li>\n<li><p>若能找到p的父，p为左孩子，右兄弟空，则next&#x3D;父</p>\n</li>\n<li><p>若能找到p的父，p为左孩子，右兄弟不空，则next&#x3D;右子树中第一个被后序遍历的结点</p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>"},{"title":"6.2 最小生成树","date":"2023-08-06T16:00:00.000Z","comment":false,"toc":true,"_content":"#\n<!--more-->\n\n![](../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/1.png)\n![数据结构](/img/datastruct/6_graph/generatetree/1.png)\n\n\n### 6.2-1 定义\n- T是连通网的生成树，T中所有边的代价之和称为生成树的代价，代价最小的生成树称为最小生成树\n- 最小生成树不唯一\n- 生成树的边数=顶点数-1\n- 只有连通网才有最小生成树\n\n### 6.2-2 Prim算法\n- 从某个顶点出发，选择代价最小的边，然后再选择与之相连的代价最小的边，直到所有顶点都被选中（看顶点）\n![](../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/2.png)\n![数据结构](/img/datastruct/6_graph/generatetree/2.png)\n    - 过程\n![](../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/4.png)\n![数据结构](/img/datastruct/6_graph/generatetree/4.png)\n![](../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/5.png)\n![数据结构](/img/datastruct/6_graph/generatetree/5.png)\n![](../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/6.png)\n![数据结构](/img/datastruct/6_graph/generatetree/6.png)\n\n### 6.2-3 Kruskal算法\n- 从代价最小的边开始，依次选择代价更小的边，直到所有顶点都被选中（看边）\n![](../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/3.png)\n![数据结构](/img/datastruct/6_graph/generatetree/3.png)\n    - 过程\n    ![](../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/10.png)\n![数据结构](/img/datastruct/6_graph/generatetree/10.png)\n![](../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/7.png)\n![数据结构](/img/datastruct/6_graph/generatetree/7.png)\n![](../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/8.png)\n![数据结构](/img/datastruct/6_graph/generatetree/8.png)\n![](../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/9.png)\n![数据结构](/img/datastruct/6_graph/generatetree/9.png)\n","source":"_posts/datastruct/6_graph/2_generatetree.md","raw":"---\ntitle: 6.2 最小生成树\ndate: 2023-08-07 00:00:00\ntags: [数据结构,图,最小生成树]\ncategories: [数据结构]\ncomment: false\ntoc: true\n---\n#\n<!--more-->\n\n![](../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/1.png)\n![数据结构](/img/datastruct/6_graph/generatetree/1.png)\n\n\n### 6.2-1 定义\n- T是连通网的生成树，T中所有边的代价之和称为生成树的代价，代价最小的生成树称为最小生成树\n- 最小生成树不唯一\n- 生成树的边数=顶点数-1\n- 只有连通网才有最小生成树\n\n### 6.2-2 Prim算法\n- 从某个顶点出发，选择代价最小的边，然后再选择与之相连的代价最小的边，直到所有顶点都被选中（看顶点）\n![](../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/2.png)\n![数据结构](/img/datastruct/6_graph/generatetree/2.png)\n    - 过程\n![](../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/4.png)\n![数据结构](/img/datastruct/6_graph/generatetree/4.png)\n![](../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/5.png)\n![数据结构](/img/datastruct/6_graph/generatetree/5.png)\n![](../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/6.png)\n![数据结构](/img/datastruct/6_graph/generatetree/6.png)\n\n### 6.2-3 Kruskal算法\n- 从代价最小的边开始，依次选择代价更小的边，直到所有顶点都被选中（看边）\n![](../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/3.png)\n![数据结构](/img/datastruct/6_graph/generatetree/3.png)\n    - 过程\n    ![](../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/10.png)\n![数据结构](/img/datastruct/6_graph/generatetree/10.png)\n![](../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/7.png)\n![数据结构](/img/datastruct/6_graph/generatetree/7.png)\n![](../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/8.png)\n![数据结构](/img/datastruct/6_graph/generatetree/8.png)\n![](../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/9.png)\n![数据结构](/img/datastruct/6_graph/generatetree/9.png)\n","slug":"datastruct/6_graph/2_generatetree","published":1,"updated":"2023-10-23T12:30:35.357Z","comments":1,"layout":"post","photos":[],"_id":"clvulgd6300255ovw2lu89hru","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/1.png\"><br><img src=\"/img/datastruct/6_graph/generatetree/1.png\" alt=\"数据结构\"></p>\n<h3 id=\"6-2-1-定义\"><a href=\"#6-2-1-定义\" class=\"headerlink\" title=\"6.2-1 定义\"></a>6.2-1 定义</h3><ul>\n<li>T是连通网的生成树，T中所有边的代价之和称为生成树的代价，代价最小的生成树称为最小生成树</li>\n<li>最小生成树不唯一</li>\n<li>生成树的边数&#x3D;顶点数-1</li>\n<li>只有连通网才有最小生成树</li>\n</ul>\n<h3 id=\"6-2-2-Prim算法\"><a href=\"#6-2-2-Prim算法\" class=\"headerlink\" title=\"6.2-2 Prim算法\"></a>6.2-2 Prim算法</h3><ul>\n<li>从某个顶点出发，选择代价最小的边，然后再选择与之相连的代价最小的边，直到所有顶点都被选中（看顶点）<br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/2.png\"><br><img src=\"/img/datastruct/6_graph/generatetree/2.png\" alt=\"数据结构\"><ul>\n<li>过程<br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/4.png\"><br><img src=\"/img/datastruct/6_graph/generatetree/4.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/5.png\"><br><img src=\"/img/datastruct/6_graph/generatetree/5.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/6.png\"><br><img src=\"/img/datastruct/6_graph/generatetree/6.png\" alt=\"数据结构\"></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"6-2-3-Kruskal算法\"><a href=\"#6-2-3-Kruskal算法\" class=\"headerlink\" title=\"6.2-3 Kruskal算法\"></a>6.2-3 Kruskal算法</h3><ul>\n<li>从代价最小的边开始，依次选择代价更小的边，直到所有顶点都被选中（看边）<br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/3.png\"><br><img src=\"/img/datastruct/6_graph/generatetree/3.png\" alt=\"数据结构\"><ul>\n<li>过程<br>  <img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/10.png\"><br><img src=\"/img/datastruct/6_graph/generatetree/10.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/7.png\"><br><img src=\"/img/datastruct/6_graph/generatetree/7.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/8.png\"><br><img src=\"/img/datastruct/6_graph/generatetree/8.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/9.png\"><br><img src=\"/img/datastruct/6_graph/generatetree/9.png\" alt=\"数据结构\"></li>\n</ul>\n</li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<p><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/1.png\"><br><img src=\"/img/datastruct/6_graph/generatetree/1.png\" alt=\"数据结构\"></p>\n<h3 id=\"6-2-1-定义\"><a href=\"#6-2-1-定义\" class=\"headerlink\" title=\"6.2-1 定义\"></a>6.2-1 定义</h3><ul>\n<li>T是连通网的生成树，T中所有边的代价之和称为生成树的代价，代价最小的生成树称为最小生成树</li>\n<li>最小生成树不唯一</li>\n<li>生成树的边数&#x3D;顶点数-1</li>\n<li>只有连通网才有最小生成树</li>\n</ul>\n<h3 id=\"6-2-2-Prim算法\"><a href=\"#6-2-2-Prim算法\" class=\"headerlink\" title=\"6.2-2 Prim算法\"></a>6.2-2 Prim算法</h3><ul>\n<li>从某个顶点出发，选择代价最小的边，然后再选择与之相连的代价最小的边，直到所有顶点都被选中（看顶点）<br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/2.png\"><br><img src=\"/img/datastruct/6_graph/generatetree/2.png\" alt=\"数据结构\"><ul>\n<li>过程<br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/4.png\"><br><img src=\"/img/datastruct/6_graph/generatetree/4.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/5.png\"><br><img src=\"/img/datastruct/6_graph/generatetree/5.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/6.png\"><br><img src=\"/img/datastruct/6_graph/generatetree/6.png\" alt=\"数据结构\"></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"6-2-3-Kruskal算法\"><a href=\"#6-2-3-Kruskal算法\" class=\"headerlink\" title=\"6.2-3 Kruskal算法\"></a>6.2-3 Kruskal算法</h3><ul>\n<li>从代价最小的边开始，依次选择代价更小的边，直到所有顶点都被选中（看边）<br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/3.png\"><br><img src=\"/img/datastruct/6_graph/generatetree/3.png\" alt=\"数据结构\"><ul>\n<li>过程<br>  <img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/10.png\"><br><img src=\"/img/datastruct/6_graph/generatetree/10.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/7.png\"><br><img src=\"/img/datastruct/6_graph/generatetree/7.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/8.png\"><br><img src=\"/img/datastruct/6_graph/generatetree/8.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/generatetree/9.png\"><br><img src=\"/img/datastruct/6_graph/generatetree/9.png\" alt=\"数据结构\"></li>\n</ul>\n</li>\n</ul>"},{"title":"6.3 最短路径","date":"2023-08-06T16:00:00.000Z","comment":false,"toc":true,"_content":"#\n<!--more-->\n\n### \n\n![](../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/1.png)\n![数据结构](/img/datastruct/6_graph/shortestpath/1.png)\n<!--more-->\n### 6.3-1 单源最短路径\n- ##### 1） BFS算法(无权)\n    ![](../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/2.png)\n    ![数据结构](/img/datastruct/6_graph/shortestpath/2.png)\n    ![](../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/3.png)\n    ![数据结构](/img/datastruct/6_graph/shortestpath/3.png)\n\n- ##### 2）Dijkstra(带权)\n    ![](../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/4.png)\n    ![数据结构](/img/datastruct/6_graph/shortestpath/4.png)\n    ![](../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/5.png)\n    ![数据结构](/img/datastruct/6_graph/shortestpath/5.png)\n    ![](../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/6.png)\n    ![数据结构](/img/datastruct/6_graph/shortestpath/6.png)\n    ![](../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/7.png)\n    ![数据结构](/img/datastruct/6_graph/shortestpath/7.png)\n    ![](../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/8.png)\n    ![数据结构](/img/datastruct/6_graph/shortestpath/8.png)\n    ![](../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/9.png)\n    ![数据结构](/img/datastruct/6_graph/shortestpath/9.png)\n    - 时间复杂度：O(|V|<sup>2</sup>)\n    - 不可用于有负权边的图\n\n### 6.3-2 各顶点间最短路径\n- ##### 1）Floyd算法\n    - 动态规划思想：\n\n        - 1）允许在v0中转，求最短路径\n        - 2）允许在v0、v1中转，求最短路径\n        - 3）允许在v0、v1、v2中转，求最短路径\n        - ...\n    \n\n    ![](../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/10.png)\n    ![数据结构](/img/datastruct/6_graph/shortestpath/10.png)\n    ![](../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/11.png)\n    ![数据结构](/img/datastruct/6_graph/shortestpath/11.png)\n    ![](../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/12.png)\n    ![数据结构](/img/datastruct/6_graph/shortestpath/12.png)\n    ![](../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/13.png)\n    ![数据结构](/img/datastruct/6_graph/shortestpath/13.png)\n    ![](../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/14.png)\n    ![数据结构](/img/datastruct/6_graph/shortestpath/14.png)\n    - 时间复杂度：O(|V|<sup>3</sup>)\n    - 不能解决负权回路的问题","source":"_posts/datastruct/6_graph/3_shortestpath.md","raw":"---\ntitle: 6.3 最短路径\ndate: 2023-08-07 00:00:00\ntags: [数据结构,图,最短路径]\ncategories: [数据结构]\ncomment: false\ntoc: true\n---\n#\n<!--more-->\n\n### \n\n![](../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/1.png)\n![数据结构](/img/datastruct/6_graph/shortestpath/1.png)\n<!--more-->\n### 6.3-1 单源最短路径\n- ##### 1） BFS算法(无权)\n    ![](../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/2.png)\n    ![数据结构](/img/datastruct/6_graph/shortestpath/2.png)\n    ![](../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/3.png)\n    ![数据结构](/img/datastruct/6_graph/shortestpath/3.png)\n\n- ##### 2）Dijkstra(带权)\n    ![](../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/4.png)\n    ![数据结构](/img/datastruct/6_graph/shortestpath/4.png)\n    ![](../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/5.png)\n    ![数据结构](/img/datastruct/6_graph/shortestpath/5.png)\n    ![](../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/6.png)\n    ![数据结构](/img/datastruct/6_graph/shortestpath/6.png)\n    ![](../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/7.png)\n    ![数据结构](/img/datastruct/6_graph/shortestpath/7.png)\n    ![](../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/8.png)\n    ![数据结构](/img/datastruct/6_graph/shortestpath/8.png)\n    ![](../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/9.png)\n    ![数据结构](/img/datastruct/6_graph/shortestpath/9.png)\n    - 时间复杂度：O(|V|<sup>2</sup>)\n    - 不可用于有负权边的图\n\n### 6.3-2 各顶点间最短路径\n- ##### 1）Floyd算法\n    - 动态规划思想：\n\n        - 1）允许在v0中转，求最短路径\n        - 2）允许在v0、v1中转，求最短路径\n        - 3）允许在v0、v1、v2中转，求最短路径\n        - ...\n    \n\n    ![](../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/10.png)\n    ![数据结构](/img/datastruct/6_graph/shortestpath/10.png)\n    ![](../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/11.png)\n    ![数据结构](/img/datastruct/6_graph/shortestpath/11.png)\n    ![](../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/12.png)\n    ![数据结构](/img/datastruct/6_graph/shortestpath/12.png)\n    ![](../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/13.png)\n    ![数据结构](/img/datastruct/6_graph/shortestpath/13.png)\n    ![](../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/14.png)\n    ![数据结构](/img/datastruct/6_graph/shortestpath/14.png)\n    - 时间复杂度：O(|V|<sup>3</sup>)\n    - 不能解决负权回路的问题","slug":"datastruct/6_graph/3_shortestpath","published":1,"updated":"2023-10-23T12:30:27.797Z","comments":1,"layout":"post","photos":[],"_id":"clvulgd6400295ovwf5ym0vcu","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h3 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a></h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/1.png\"><br><img src=\"/img/datastruct/6_graph/shortestpath/1.png\" alt=\"数据结构\"></p>\n<!--more-->\n<h3 id=\"6-3-1-单源最短路径\"><a href=\"#6-3-1-单源最短路径\" class=\"headerlink\" title=\"6.3-1 单源最短路径\"></a>6.3-1 单源最短路径</h3><ul>\n<li><h5 id=\"1）-BFS算法-无权\"><a href=\"#1）-BFS算法-无权\" class=\"headerlink\" title=\"1） BFS算法(无权)\"></a>1） BFS算法(无权)</h5><p>  <img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/2.png\"><br>  <img src=\"/img/datastruct/6_graph/shortestpath/2.png\" alt=\"数据结构\"><br>  <img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/3.png\"><br>  <img src=\"/img/datastruct/6_graph/shortestpath/3.png\" alt=\"数据结构\"></p>\n</li>\n<li><h5 id=\"2）Dijkstra-带权\"><a href=\"#2）Dijkstra-带权\" class=\"headerlink\" title=\"2）Dijkstra(带权)\"></a>2）Dijkstra(带权)</h5><p>  <img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/4.png\"><br>  <img src=\"/img/datastruct/6_graph/shortestpath/4.png\" alt=\"数据结构\"><br>  <img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/5.png\"><br>  <img src=\"/img/datastruct/6_graph/shortestpath/5.png\" alt=\"数据结构\"><br>  <img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/6.png\"><br>  <img src=\"/img/datastruct/6_graph/shortestpath/6.png\" alt=\"数据结构\"><br>  <img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/7.png\"><br>  <img src=\"/img/datastruct/6_graph/shortestpath/7.png\" alt=\"数据结构\"><br>  <img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/8.png\"><br>  <img src=\"/img/datastruct/6_graph/shortestpath/8.png\" alt=\"数据结构\"><br>  <img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/9.png\"><br>  <img src=\"/img/datastruct/6_graph/shortestpath/9.png\" alt=\"数据结构\"></p>\n<ul>\n<li>时间复杂度：O(|V|<sup>2</sup>)</li>\n<li>不可用于有负权边的图</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"6-3-2-各顶点间最短路径\"><a href=\"#6-3-2-各顶点间最短路径\" class=\"headerlink\" title=\"6.3-2 各顶点间最短路径\"></a>6.3-2 各顶点间最短路径</h3><ul>\n<li><h5 id=\"1）Floyd算法\"><a href=\"#1）Floyd算法\" class=\"headerlink\" title=\"1）Floyd算法\"></a>1）Floyd算法</h5><ul>\n<li><p>动态规划思想：</p>\n<ul>\n<li>1）允许在v0中转，求最短路径</li>\n<li>2）允许在v0、v1中转，求最短路径</li>\n<li>3）允许在v0、v1、v2中转，求最短路径</li>\n<li>…</li>\n</ul>\n</li>\n</ul>\n<p>  <img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/10.png\"><br>  <img src=\"/img/datastruct/6_graph/shortestpath/10.png\" alt=\"数据结构\"><br>  <img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/11.png\"><br>  <img src=\"/img/datastruct/6_graph/shortestpath/11.png\" alt=\"数据结构\"><br>  <img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/12.png\"><br>  <img src=\"/img/datastruct/6_graph/shortestpath/12.png\" alt=\"数据结构\"><br>  <img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/13.png\"><br>  <img src=\"/img/datastruct/6_graph/shortestpath/13.png\" alt=\"数据结构\"><br>  <img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/14.png\"><br>  <img src=\"/img/datastruct/6_graph/shortestpath/14.png\" alt=\"数据结构\"></p>\n<ul>\n<li>时间复杂度：O(|V|<sup>3</sup>)</li>\n<li>不能解决负权回路的问题</li>\n</ul>\n</li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h3 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a></h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/1.png\"><br><img src=\"/img/datastruct/6_graph/shortestpath/1.png\" alt=\"数据结构\"></p>\n<!--more-->\n<h3 id=\"6-3-1-单源最短路径\"><a href=\"#6-3-1-单源最短路径\" class=\"headerlink\" title=\"6.3-1 单源最短路径\"></a>6.3-1 单源最短路径</h3><ul>\n<li><h5 id=\"1）-BFS算法-无权\"><a href=\"#1）-BFS算法-无权\" class=\"headerlink\" title=\"1） BFS算法(无权)\"></a>1） BFS算法(无权)</h5><p>  <img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/2.png\"><br>  <img src=\"/img/datastruct/6_graph/shortestpath/2.png\" alt=\"数据结构\"><br>  <img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/3.png\"><br>  <img src=\"/img/datastruct/6_graph/shortestpath/3.png\" alt=\"数据结构\"></p>\n</li>\n<li><h5 id=\"2）Dijkstra-带权\"><a href=\"#2）Dijkstra-带权\" class=\"headerlink\" title=\"2）Dijkstra(带权)\"></a>2）Dijkstra(带权)</h5><p>  <img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/4.png\"><br>  <img src=\"/img/datastruct/6_graph/shortestpath/4.png\" alt=\"数据结构\"><br>  <img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/5.png\"><br>  <img src=\"/img/datastruct/6_graph/shortestpath/5.png\" alt=\"数据结构\"><br>  <img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/6.png\"><br>  <img src=\"/img/datastruct/6_graph/shortestpath/6.png\" alt=\"数据结构\"><br>  <img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/7.png\"><br>  <img src=\"/img/datastruct/6_graph/shortestpath/7.png\" alt=\"数据结构\"><br>  <img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/8.png\"><br>  <img src=\"/img/datastruct/6_graph/shortestpath/8.png\" alt=\"数据结构\"><br>  <img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/9.png\"><br>  <img src=\"/img/datastruct/6_graph/shortestpath/9.png\" alt=\"数据结构\"></p>\n<ul>\n<li>时间复杂度：O(|V|<sup>2</sup>)</li>\n<li>不可用于有负权边的图</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"6-3-2-各顶点间最短路径\"><a href=\"#6-3-2-各顶点间最短路径\" class=\"headerlink\" title=\"6.3-2 各顶点间最短路径\"></a>6.3-2 各顶点间最短路径</h3><ul>\n<li><h5 id=\"1）Floyd算法\"><a href=\"#1）Floyd算法\" class=\"headerlink\" title=\"1）Floyd算法\"></a>1）Floyd算法</h5><ul>\n<li><p>动态规划思想：</p>\n<ul>\n<li>1）允许在v0中转，求最短路径</li>\n<li>2）允许在v0、v1中转，求最短路径</li>\n<li>3）允许在v0、v1、v2中转，求最短路径</li>\n<li>…</li>\n</ul>\n</li>\n</ul>\n<p>  <img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/10.png\"><br>  <img src=\"/img/datastruct/6_graph/shortestpath/10.png\" alt=\"数据结构\"><br>  <img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/11.png\"><br>  <img src=\"/img/datastruct/6_graph/shortestpath/11.png\" alt=\"数据结构\"><br>  <img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/12.png\"><br>  <img src=\"/img/datastruct/6_graph/shortestpath/12.png\" alt=\"数据结构\"><br>  <img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/13.png\"><br>  <img src=\"/img/datastruct/6_graph/shortestpath/13.png\" alt=\"数据结构\"><br>  <img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/shortestpath/14.png\"><br>  <img src=\"/img/datastruct/6_graph/shortestpath/14.png\" alt=\"数据结构\"></p>\n<ul>\n<li>时间复杂度：O(|V|<sup>3</sup>)</li>\n<li>不能解决负权回路的问题</li>\n</ul>\n</li>\n</ul>"},{"title":"6.1 图","date":"2023-08-06T16:00:00.000Z","comment":false,"toc":true,"_content":"#\n<!--more-->\n\n![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/1.png)\n![数据结构](/img/datastruct/6_graph/graph/1.png)\n\n\n## 6.1-1 定义\n- 由顶点的有穷非空集合和顶点之间边的集合组成\n- 通常表示为：G(V,E)，其中，G表示一个图，V是图G中顶点的集合，E是图G中边的集合。\n- 线性表可以空，树可以空，图不可以\n\n\n## 6.1-2 一些概念\n![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/2.png)\n![数据结构](/img/datastruct/6_graph/graph/2.png)\n![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/3.png)\n![数据结构](/img/datastruct/6_graph/graph/3.png)\n![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/4.png)\n![数据结构](/img/datastruct/6_graph/graph/4.png)\n- 生成子图要包含所有顶点\n![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/5.png)\n![数据结构](/img/datastruct/6_graph/graph/5.png)\n- 强连通分量：有向图中\n\n- 连通图的生成树：连通图的极小连通子图\n\n## 6.1-3 图的存储\n![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/6.png)\n![数据结构](/img/datastruct/6_graph/graph/6.png)\n### 6.1-3.1 邻接矩阵\n![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/7.png)\n![数据结构](/img/datastruct/6_graph/graph/7.png)\n- 出度：一行中非0元素的个数\n- 入度：一列中非0元素的个数\n- 空间复杂度：O(|v|<sup>2</sup>)\n- 无向图->对称->矩阵压缩\n- 设邻接矩阵A只含0、1，则A<sup>k</sup>中非零元素表示从i到j的长度为k的路径数\n\n### 6.1-3.2 邻接表\n![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/8.png)\n![数据结构](/img/datastruct/6_graph/graph/8.png)\n- 空间复杂度：O(|v|+|e|)\n\n### 6.1-3.3 十字链表（有向图）\n![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/9.png)\n![数据结构](/img/datastruct/6_graph/graph/9.png)\n- 空间复杂度：O(|v|+|e|)\n- 如何找到指定顶点的所有出边：沿着绿色的箭头找\n- 如何找到指定顶点的所有入边：沿着橙色的箭头找\n\n### 6.1-3.4 邻接多重表（无向图）\n![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/10.png)\n![数据结构](/img/datastruct/6_graph/graph/10.png)\n- 空间复杂度：O(|v|+|e|)\n- 删除边、顶点很方便\n\n\n## 6.1-4 基本操作\n|函数|功能|\n|:---|:---|\n|Adjacent(G,x,y)|判断是否有从x到y的边，无向图只需判断一次|\n|Neighbors(G,x)|返回与x邻接的顶点|\n|InsertVertex(G,x)|插入顶点|\n|DeleteVertex(G,x)|删除顶点|\n|AddEdge(G,x,y)|插入边|\n|RemoveEdge(G,x,y)|删除边|\n|FirstNeighbor(G,x)|返回x的第一个邻接点|\n|NextNeighbor(G,x,y)|返回x相对于y的下一个邻接点|\n|Get_edge_value(G,x,y)|返回边(x,y)的权值|\n|Set_edge_value(G,x,y,v)|设置边(x,y)的权值为v|\n\n|有向图|无向图|\n|:---|:---|\n|![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/11.png)![数据结构](/img/datastruct/6_graph/graph/11.png)|![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/12.png)![数据结构](/img/datastruct/6_graph/graph/12.png)|\n|![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/13.png)![数据结构](/img/datastruct/6_graph/graph/13.png)|![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/14.png)![数据结构](/img/datastruct/6_graph/graph/14.png)|\n|![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/15.png)![数据结构](/img/datastruct/6_graph/graph/15.png)|![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/15.png)![数据结构](/img/datastruct/6_graph/graph/15.png)|\n|![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/16.png)![数据结构](/img/datastruct/6_graph/graph/16.png)|![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/17.png)![数据结构](/img/datastruct/6_graph/graph/17.png)|\n|![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/18.png)![数据结构](/img/datastruct/6_graph/graph/18.png)|![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/18.png)![数据结构](/img/datastruct/6_graph/graph/18.png)|\n|![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/19.png)![数据结构](/img/datastruct/6_graph/graph/19.png)|![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/19.png)![数据结构](/img/datastruct/6_graph/graph/19.png)|\n|![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/20.png)![数据结构](/img/datastruct/6_graph/graph/20.png)|![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/20.png)![数据结构](/img/datastruct/6_graph/graph/20.png)|\n\n### 6.1-4.1 图的遍历\n- 广度优先遍历（BFS）\n![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/21.png)\n![数据结构](/img/datastruct/6_graph/graph/21.png)\n\n    - 找到所有与v相邻的顶点：FirstNeighbor(G,v)，NextNeighbor(G,v,w)\n    - 标记哪个顶点已经访问过：visited[]\n    - 用队列保存已经访问过的顶点\n![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/22.png)\n![数据结构](/img/datastruct/6_graph/graph/22.png)\n    - 如果图是非连通的，需要对每个连通分量进行BFS\n![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/23.png)\n![数据结构](/img/datastruct/6_graph/graph/23.png)\n    - 广度优先生成树\n![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/24.png)\n![数据结构](/img/datastruct/6_graph/graph/24.png)\n    - 广度优先生成森林\n- 深度优先遍历（DFS）\n![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/25.png)\n![数据结构](/img/datastruct/6_graph/graph/25.png)\n![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/26.png)\n![数据结构](/img/datastruct/6_graph/graph/26.png)\n    - 深度优先生成树\n    - 深度优先生成森林\n","source":"_posts/datastruct/6_graph/1_graph.md","raw":"---\ntitle: 6.1 图\ndate: 2023-08-07 00:00:00\ntags: [数据结构,图]\ncategories: [数据结构]\ncomment: false\ntoc: true\n---\n#\n<!--more-->\n\n![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/1.png)\n![数据结构](/img/datastruct/6_graph/graph/1.png)\n\n\n## 6.1-1 定义\n- 由顶点的有穷非空集合和顶点之间边的集合组成\n- 通常表示为：G(V,E)，其中，G表示一个图，V是图G中顶点的集合，E是图G中边的集合。\n- 线性表可以空，树可以空，图不可以\n\n\n## 6.1-2 一些概念\n![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/2.png)\n![数据结构](/img/datastruct/6_graph/graph/2.png)\n![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/3.png)\n![数据结构](/img/datastruct/6_graph/graph/3.png)\n![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/4.png)\n![数据结构](/img/datastruct/6_graph/graph/4.png)\n- 生成子图要包含所有顶点\n![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/5.png)\n![数据结构](/img/datastruct/6_graph/graph/5.png)\n- 强连通分量：有向图中\n\n- 连通图的生成树：连通图的极小连通子图\n\n## 6.1-3 图的存储\n![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/6.png)\n![数据结构](/img/datastruct/6_graph/graph/6.png)\n### 6.1-3.1 邻接矩阵\n![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/7.png)\n![数据结构](/img/datastruct/6_graph/graph/7.png)\n- 出度：一行中非0元素的个数\n- 入度：一列中非0元素的个数\n- 空间复杂度：O(|v|<sup>2</sup>)\n- 无向图->对称->矩阵压缩\n- 设邻接矩阵A只含0、1，则A<sup>k</sup>中非零元素表示从i到j的长度为k的路径数\n\n### 6.1-3.2 邻接表\n![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/8.png)\n![数据结构](/img/datastruct/6_graph/graph/8.png)\n- 空间复杂度：O(|v|+|e|)\n\n### 6.1-3.3 十字链表（有向图）\n![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/9.png)\n![数据结构](/img/datastruct/6_graph/graph/9.png)\n- 空间复杂度：O(|v|+|e|)\n- 如何找到指定顶点的所有出边：沿着绿色的箭头找\n- 如何找到指定顶点的所有入边：沿着橙色的箭头找\n\n### 6.1-3.4 邻接多重表（无向图）\n![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/10.png)\n![数据结构](/img/datastruct/6_graph/graph/10.png)\n- 空间复杂度：O(|v|+|e|)\n- 删除边、顶点很方便\n\n\n## 6.1-4 基本操作\n|函数|功能|\n|:---|:---|\n|Adjacent(G,x,y)|判断是否有从x到y的边，无向图只需判断一次|\n|Neighbors(G,x)|返回与x邻接的顶点|\n|InsertVertex(G,x)|插入顶点|\n|DeleteVertex(G,x)|删除顶点|\n|AddEdge(G,x,y)|插入边|\n|RemoveEdge(G,x,y)|删除边|\n|FirstNeighbor(G,x)|返回x的第一个邻接点|\n|NextNeighbor(G,x,y)|返回x相对于y的下一个邻接点|\n|Get_edge_value(G,x,y)|返回边(x,y)的权值|\n|Set_edge_value(G,x,y,v)|设置边(x,y)的权值为v|\n\n|有向图|无向图|\n|:---|:---|\n|![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/11.png)![数据结构](/img/datastruct/6_graph/graph/11.png)|![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/12.png)![数据结构](/img/datastruct/6_graph/graph/12.png)|\n|![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/13.png)![数据结构](/img/datastruct/6_graph/graph/13.png)|![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/14.png)![数据结构](/img/datastruct/6_graph/graph/14.png)|\n|![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/15.png)![数据结构](/img/datastruct/6_graph/graph/15.png)|![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/15.png)![数据结构](/img/datastruct/6_graph/graph/15.png)|\n|![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/16.png)![数据结构](/img/datastruct/6_graph/graph/16.png)|![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/17.png)![数据结构](/img/datastruct/6_graph/graph/17.png)|\n|![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/18.png)![数据结构](/img/datastruct/6_graph/graph/18.png)|![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/18.png)![数据结构](/img/datastruct/6_graph/graph/18.png)|\n|![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/19.png)![数据结构](/img/datastruct/6_graph/graph/19.png)|![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/19.png)![数据结构](/img/datastruct/6_graph/graph/19.png)|\n|![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/20.png)![数据结构](/img/datastruct/6_graph/graph/20.png)|![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/20.png)![数据结构](/img/datastruct/6_graph/graph/20.png)|\n\n### 6.1-4.1 图的遍历\n- 广度优先遍历（BFS）\n![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/21.png)\n![数据结构](/img/datastruct/6_graph/graph/21.png)\n\n    - 找到所有与v相邻的顶点：FirstNeighbor(G,v)，NextNeighbor(G,v,w)\n    - 标记哪个顶点已经访问过：visited[]\n    - 用队列保存已经访问过的顶点\n![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/22.png)\n![数据结构](/img/datastruct/6_graph/graph/22.png)\n    - 如果图是非连通的，需要对每个连通分量进行BFS\n![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/23.png)\n![数据结构](/img/datastruct/6_graph/graph/23.png)\n    - 广度优先生成树\n![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/24.png)\n![数据结构](/img/datastruct/6_graph/graph/24.png)\n    - 广度优先生成森林\n- 深度优先遍历（DFS）\n![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/25.png)\n![数据结构](/img/datastruct/6_graph/graph/25.png)\n![](../../../../themes/yilia/source/img/datastruct/6_graph/graph/26.png)\n![数据结构](/img/datastruct/6_graph/graph/26.png)\n    - 深度优先生成树\n    - 深度优先生成森林\n","slug":"datastruct/6_graph/1_graph","published":1,"updated":"2023-10-23T12:26:47.885Z","comments":1,"layout":"post","photos":[],"_id":"clvulgd65002d5ovwg99l48gu","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/1.png\"><br><img src=\"/img/datastruct/6_graph/graph/1.png\" alt=\"数据结构\"></p>\n<h2 id=\"6-1-1-定义\"><a href=\"#6-1-1-定义\" class=\"headerlink\" title=\"6.1-1 定义\"></a>6.1-1 定义</h2><ul>\n<li>由顶点的有穷非空集合和顶点之间边的集合组成</li>\n<li>通常表示为：G(V,E)，其中，G表示一个图，V是图G中顶点的集合，E是图G中边的集合。</li>\n<li>线性表可以空，树可以空，图不可以</li>\n</ul>\n<h2 id=\"6-1-2-一些概念\"><a href=\"#6-1-2-一些概念\" class=\"headerlink\" title=\"6.1-2 一些概念\"></a>6.1-2 一些概念</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/2.png\"><br><img src=\"/img/datastruct/6_graph/graph/2.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/3.png\"><br><img src=\"/img/datastruct/6_graph/graph/3.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/4.png\"><br><img src=\"/img/datastruct/6_graph/graph/4.png\" alt=\"数据结构\"></p>\n<ul>\n<li><p>生成子图要包含所有顶点<br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/5.png\"><br><img src=\"/img/datastruct/6_graph/graph/5.png\" alt=\"数据结构\"></p>\n</li>\n<li><p>强连通分量：有向图中</p>\n</li>\n<li><p>连通图的生成树：连通图的极小连通子图</p>\n</li>\n</ul>\n<h2 id=\"6-1-3-图的存储\"><a href=\"#6-1-3-图的存储\" class=\"headerlink\" title=\"6.1-3 图的存储\"></a>6.1-3 图的存储</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/6.png\"><br><img src=\"/img/datastruct/6_graph/graph/6.png\" alt=\"数据结构\"></p>\n<h3 id=\"6-1-3-1-邻接矩阵\"><a href=\"#6-1-3-1-邻接矩阵\" class=\"headerlink\" title=\"6.1-3.1 邻接矩阵\"></a>6.1-3.1 邻接矩阵</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/7.png\"><br><img src=\"/img/datastruct/6_graph/graph/7.png\" alt=\"数据结构\"></p>\n<ul>\n<li>出度：一行中非0元素的个数</li>\n<li>入度：一列中非0元素的个数</li>\n<li>空间复杂度：O(|v|<sup>2</sup>)</li>\n<li>无向图-&gt;对称-&gt;矩阵压缩</li>\n<li>设邻接矩阵A只含0、1，则A<sup>k</sup>中非零元素表示从i到j的长度为k的路径数</li>\n</ul>\n<h3 id=\"6-1-3-2-邻接表\"><a href=\"#6-1-3-2-邻接表\" class=\"headerlink\" title=\"6.1-3.2 邻接表\"></a>6.1-3.2 邻接表</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/8.png\"><br><img src=\"/img/datastruct/6_graph/graph/8.png\" alt=\"数据结构\"></p>\n<ul>\n<li>空间复杂度：O(|v|+|e|)</li>\n</ul>\n<h3 id=\"6-1-3-3-十字链表（有向图）\"><a href=\"#6-1-3-3-十字链表（有向图）\" class=\"headerlink\" title=\"6.1-3.3 十字链表（有向图）\"></a>6.1-3.3 十字链表（有向图）</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/9.png\"><br><img src=\"/img/datastruct/6_graph/graph/9.png\" alt=\"数据结构\"></p>\n<ul>\n<li>空间复杂度：O(|v|+|e|)</li>\n<li>如何找到指定顶点的所有出边：沿着绿色的箭头找</li>\n<li>如何找到指定顶点的所有入边：沿着橙色的箭头找</li>\n</ul>\n<h3 id=\"6-1-3-4-邻接多重表（无向图）\"><a href=\"#6-1-3-4-邻接多重表（无向图）\" class=\"headerlink\" title=\"6.1-3.4 邻接多重表（无向图）\"></a>6.1-3.4 邻接多重表（无向图）</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/10.png\"><br><img src=\"/img/datastruct/6_graph/graph/10.png\" alt=\"数据结构\"></p>\n<ul>\n<li>空间复杂度：O(|v|+|e|)</li>\n<li>删除边、顶点很方便</li>\n</ul>\n<h2 id=\"6-1-4-基本操作\"><a href=\"#6-1-4-基本操作\" class=\"headerlink\" title=\"6.1-4 基本操作\"></a>6.1-4 基本操作</h2><table>\n<thead>\n<tr>\n<th align=\"left\">函数</th>\n<th align=\"left\">功能</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">Adjacent(G,x,y)</td>\n<td align=\"left\">判断是否有从x到y的边，无向图只需判断一次</td>\n</tr>\n<tr>\n<td align=\"left\">Neighbors(G,x)</td>\n<td align=\"left\">返回与x邻接的顶点</td>\n</tr>\n<tr>\n<td align=\"left\">InsertVertex(G,x)</td>\n<td align=\"left\">插入顶点</td>\n</tr>\n<tr>\n<td align=\"left\">DeleteVertex(G,x)</td>\n<td align=\"left\">删除顶点</td>\n</tr>\n<tr>\n<td align=\"left\">AddEdge(G,x,y)</td>\n<td align=\"left\">插入边</td>\n</tr>\n<tr>\n<td align=\"left\">RemoveEdge(G,x,y)</td>\n<td align=\"left\">删除边</td>\n</tr>\n<tr>\n<td align=\"left\">FirstNeighbor(G,x)</td>\n<td align=\"left\">返回x的第一个邻接点</td>\n</tr>\n<tr>\n<td align=\"left\">NextNeighbor(G,x,y)</td>\n<td align=\"left\">返回x相对于y的下一个邻接点</td>\n</tr>\n<tr>\n<td align=\"left\">Get_edge_value(G,x,y)</td>\n<td align=\"left\">返回边(x,y)的权值</td>\n</tr>\n<tr>\n<td align=\"left\">Set_edge_value(G,x,y,v)</td>\n<td align=\"left\">设置边(x,y)的权值为v</td>\n</tr>\n</tbody></table>\n<table>\n<thead>\n<tr>\n<th align=\"left\">有向图</th>\n<th align=\"left\">无向图</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/11.png\"><img src=\"/img/datastruct/6_graph/graph/11.png\" alt=\"数据结构\"></td>\n<td align=\"left\"><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/12.png\"><img src=\"/img/datastruct/6_graph/graph/12.png\" alt=\"数据结构\"></td>\n</tr>\n<tr>\n<td align=\"left\"><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/13.png\"><img src=\"/img/datastruct/6_graph/graph/13.png\" alt=\"数据结构\"></td>\n<td align=\"left\"><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/14.png\"><img src=\"/img/datastruct/6_graph/graph/14.png\" alt=\"数据结构\"></td>\n</tr>\n<tr>\n<td align=\"left\"><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/15.png\"><img src=\"/img/datastruct/6_graph/graph/15.png\" alt=\"数据结构\"></td>\n<td align=\"left\"><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/15.png\"><img src=\"/img/datastruct/6_graph/graph/15.png\" alt=\"数据结构\"></td>\n</tr>\n<tr>\n<td align=\"left\"><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/16.png\"><img src=\"/img/datastruct/6_graph/graph/16.png\" alt=\"数据结构\"></td>\n<td align=\"left\"><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/17.png\"><img src=\"/img/datastruct/6_graph/graph/17.png\" alt=\"数据结构\"></td>\n</tr>\n<tr>\n<td align=\"left\"><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/18.png\"><img src=\"/img/datastruct/6_graph/graph/18.png\" alt=\"数据结构\"></td>\n<td align=\"left\"><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/18.png\"><img src=\"/img/datastruct/6_graph/graph/18.png\" alt=\"数据结构\"></td>\n</tr>\n<tr>\n<td align=\"left\"><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/19.png\"><img src=\"/img/datastruct/6_graph/graph/19.png\" alt=\"数据结构\"></td>\n<td align=\"left\"><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/19.png\"><img src=\"/img/datastruct/6_graph/graph/19.png\" alt=\"数据结构\"></td>\n</tr>\n<tr>\n<td align=\"left\"><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/20.png\"><img src=\"/img/datastruct/6_graph/graph/20.png\" alt=\"数据结构\"></td>\n<td align=\"left\"><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/20.png\"><img src=\"/img/datastruct/6_graph/graph/20.png\" alt=\"数据结构\"></td>\n</tr>\n</tbody></table>\n<h3 id=\"6-1-4-1-图的遍历\"><a href=\"#6-1-4-1-图的遍历\" class=\"headerlink\" title=\"6.1-4.1 图的遍历\"></a>6.1-4.1 图的遍历</h3><ul>\n<li><p>广度优先遍历（BFS）<br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/21.png\"><br><img src=\"/img/datastruct/6_graph/graph/21.png\" alt=\"数据结构\"></p>\n<ul>\n<li>找到所有与v相邻的顶点：FirstNeighbor(G,v)，NextNeighbor(G,v,w)</li>\n<li>标记哪个顶点已经访问过：visited[]</li>\n<li>用队列保存已经访问过的顶点</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/22.png\"><br><img src=\"/img/datastruct/6_graph/graph/22.png\" alt=\"数据结构\"><br>    - 如果图是非连通的，需要对每个连通分量进行BFS<br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/23.png\"><br><img src=\"/img/datastruct/6_graph/graph/23.png\" alt=\"数据结构\"><br>    - 广度优先生成树<br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/24.png\"><br><img src=\"/img/datastruct/6_graph/graph/24.png\" alt=\"数据结构\"><br>    - 广度优先生成森林</p>\n<ul>\n<li>深度优先遍历（DFS）<br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/25.png\"><br><img src=\"/img/datastruct/6_graph/graph/25.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/26.png\"><br><img src=\"/img/datastruct/6_graph/graph/26.png\" alt=\"数据结构\"><ul>\n<li>深度优先生成树</li>\n<li>深度优先生成森林</li>\n</ul>\n</li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<p><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/1.png\"><br><img src=\"/img/datastruct/6_graph/graph/1.png\" alt=\"数据结构\"></p>\n<h2 id=\"6-1-1-定义\"><a href=\"#6-1-1-定义\" class=\"headerlink\" title=\"6.1-1 定义\"></a>6.1-1 定义</h2><ul>\n<li>由顶点的有穷非空集合和顶点之间边的集合组成</li>\n<li>通常表示为：G(V,E)，其中，G表示一个图，V是图G中顶点的集合，E是图G中边的集合。</li>\n<li>线性表可以空，树可以空，图不可以</li>\n</ul>\n<h2 id=\"6-1-2-一些概念\"><a href=\"#6-1-2-一些概念\" class=\"headerlink\" title=\"6.1-2 一些概念\"></a>6.1-2 一些概念</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/2.png\"><br><img src=\"/img/datastruct/6_graph/graph/2.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/3.png\"><br><img src=\"/img/datastruct/6_graph/graph/3.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/4.png\"><br><img src=\"/img/datastruct/6_graph/graph/4.png\" alt=\"数据结构\"></p>\n<ul>\n<li><p>生成子图要包含所有顶点<br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/5.png\"><br><img src=\"/img/datastruct/6_graph/graph/5.png\" alt=\"数据结构\"></p>\n</li>\n<li><p>强连通分量：有向图中</p>\n</li>\n<li><p>连通图的生成树：连通图的极小连通子图</p>\n</li>\n</ul>\n<h2 id=\"6-1-3-图的存储\"><a href=\"#6-1-3-图的存储\" class=\"headerlink\" title=\"6.1-3 图的存储\"></a>6.1-3 图的存储</h2><p><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/6.png\"><br><img src=\"/img/datastruct/6_graph/graph/6.png\" alt=\"数据结构\"></p>\n<h3 id=\"6-1-3-1-邻接矩阵\"><a href=\"#6-1-3-1-邻接矩阵\" class=\"headerlink\" title=\"6.1-3.1 邻接矩阵\"></a>6.1-3.1 邻接矩阵</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/7.png\"><br><img src=\"/img/datastruct/6_graph/graph/7.png\" alt=\"数据结构\"></p>\n<ul>\n<li>出度：一行中非0元素的个数</li>\n<li>入度：一列中非0元素的个数</li>\n<li>空间复杂度：O(|v|<sup>2</sup>)</li>\n<li>无向图-&gt;对称-&gt;矩阵压缩</li>\n<li>设邻接矩阵A只含0、1，则A<sup>k</sup>中非零元素表示从i到j的长度为k的路径数</li>\n</ul>\n<h3 id=\"6-1-3-2-邻接表\"><a href=\"#6-1-3-2-邻接表\" class=\"headerlink\" title=\"6.1-3.2 邻接表\"></a>6.1-3.2 邻接表</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/8.png\"><br><img src=\"/img/datastruct/6_graph/graph/8.png\" alt=\"数据结构\"></p>\n<ul>\n<li>空间复杂度：O(|v|+|e|)</li>\n</ul>\n<h3 id=\"6-1-3-3-十字链表（有向图）\"><a href=\"#6-1-3-3-十字链表（有向图）\" class=\"headerlink\" title=\"6.1-3.3 十字链表（有向图）\"></a>6.1-3.3 十字链表（有向图）</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/9.png\"><br><img src=\"/img/datastruct/6_graph/graph/9.png\" alt=\"数据结构\"></p>\n<ul>\n<li>空间复杂度：O(|v|+|e|)</li>\n<li>如何找到指定顶点的所有出边：沿着绿色的箭头找</li>\n<li>如何找到指定顶点的所有入边：沿着橙色的箭头找</li>\n</ul>\n<h3 id=\"6-1-3-4-邻接多重表（无向图）\"><a href=\"#6-1-3-4-邻接多重表（无向图）\" class=\"headerlink\" title=\"6.1-3.4 邻接多重表（无向图）\"></a>6.1-3.4 邻接多重表（无向图）</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/10.png\"><br><img src=\"/img/datastruct/6_graph/graph/10.png\" alt=\"数据结构\"></p>\n<ul>\n<li>空间复杂度：O(|v|+|e|)</li>\n<li>删除边、顶点很方便</li>\n</ul>\n<h2 id=\"6-1-4-基本操作\"><a href=\"#6-1-4-基本操作\" class=\"headerlink\" title=\"6.1-4 基本操作\"></a>6.1-4 基本操作</h2><table>\n<thead>\n<tr>\n<th align=\"left\">函数</th>\n<th align=\"left\">功能</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">Adjacent(G,x,y)</td>\n<td align=\"left\">判断是否有从x到y的边，无向图只需判断一次</td>\n</tr>\n<tr>\n<td align=\"left\">Neighbors(G,x)</td>\n<td align=\"left\">返回与x邻接的顶点</td>\n</tr>\n<tr>\n<td align=\"left\">InsertVertex(G,x)</td>\n<td align=\"left\">插入顶点</td>\n</tr>\n<tr>\n<td align=\"left\">DeleteVertex(G,x)</td>\n<td align=\"left\">删除顶点</td>\n</tr>\n<tr>\n<td align=\"left\">AddEdge(G,x,y)</td>\n<td align=\"left\">插入边</td>\n</tr>\n<tr>\n<td align=\"left\">RemoveEdge(G,x,y)</td>\n<td align=\"left\">删除边</td>\n</tr>\n<tr>\n<td align=\"left\">FirstNeighbor(G,x)</td>\n<td align=\"left\">返回x的第一个邻接点</td>\n</tr>\n<tr>\n<td align=\"left\">NextNeighbor(G,x,y)</td>\n<td align=\"left\">返回x相对于y的下一个邻接点</td>\n</tr>\n<tr>\n<td align=\"left\">Get_edge_value(G,x,y)</td>\n<td align=\"left\">返回边(x,y)的权值</td>\n</tr>\n<tr>\n<td align=\"left\">Set_edge_value(G,x,y,v)</td>\n<td align=\"left\">设置边(x,y)的权值为v</td>\n</tr>\n</tbody></table>\n<table>\n<thead>\n<tr>\n<th align=\"left\">有向图</th>\n<th align=\"left\">无向图</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/11.png\"><img src=\"/img/datastruct/6_graph/graph/11.png\" alt=\"数据结构\"></td>\n<td align=\"left\"><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/12.png\"><img src=\"/img/datastruct/6_graph/graph/12.png\" alt=\"数据结构\"></td>\n</tr>\n<tr>\n<td align=\"left\"><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/13.png\"><img src=\"/img/datastruct/6_graph/graph/13.png\" alt=\"数据结构\"></td>\n<td align=\"left\"><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/14.png\"><img src=\"/img/datastruct/6_graph/graph/14.png\" alt=\"数据结构\"></td>\n</tr>\n<tr>\n<td align=\"left\"><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/15.png\"><img src=\"/img/datastruct/6_graph/graph/15.png\" alt=\"数据结构\"></td>\n<td align=\"left\"><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/15.png\"><img src=\"/img/datastruct/6_graph/graph/15.png\" alt=\"数据结构\"></td>\n</tr>\n<tr>\n<td align=\"left\"><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/16.png\"><img src=\"/img/datastruct/6_graph/graph/16.png\" alt=\"数据结构\"></td>\n<td align=\"left\"><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/17.png\"><img src=\"/img/datastruct/6_graph/graph/17.png\" alt=\"数据结构\"></td>\n</tr>\n<tr>\n<td align=\"left\"><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/18.png\"><img src=\"/img/datastruct/6_graph/graph/18.png\" alt=\"数据结构\"></td>\n<td align=\"left\"><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/18.png\"><img src=\"/img/datastruct/6_graph/graph/18.png\" alt=\"数据结构\"></td>\n</tr>\n<tr>\n<td align=\"left\"><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/19.png\"><img src=\"/img/datastruct/6_graph/graph/19.png\" alt=\"数据结构\"></td>\n<td align=\"left\"><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/19.png\"><img src=\"/img/datastruct/6_graph/graph/19.png\" alt=\"数据结构\"></td>\n</tr>\n<tr>\n<td align=\"left\"><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/20.png\"><img src=\"/img/datastruct/6_graph/graph/20.png\" alt=\"数据结构\"></td>\n<td align=\"left\"><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/20.png\"><img src=\"/img/datastruct/6_graph/graph/20.png\" alt=\"数据结构\"></td>\n</tr>\n</tbody></table>\n<h3 id=\"6-1-4-1-图的遍历\"><a href=\"#6-1-4-1-图的遍历\" class=\"headerlink\" title=\"6.1-4.1 图的遍历\"></a>6.1-4.1 图的遍历</h3><ul>\n<li><p>广度优先遍历（BFS）<br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/21.png\"><br><img src=\"/img/datastruct/6_graph/graph/21.png\" alt=\"数据结构\"></p>\n<ul>\n<li>找到所有与v相邻的顶点：FirstNeighbor(G,v)，NextNeighbor(G,v,w)</li>\n<li>标记哪个顶点已经访问过：visited[]</li>\n<li>用队列保存已经访问过的顶点</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/22.png\"><br><img src=\"/img/datastruct/6_graph/graph/22.png\" alt=\"数据结构\"><br>    - 如果图是非连通的，需要对每个连通分量进行BFS<br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/23.png\"><br><img src=\"/img/datastruct/6_graph/graph/23.png\" alt=\"数据结构\"><br>    - 广度优先生成树<br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/24.png\"><br><img src=\"/img/datastruct/6_graph/graph/24.png\" alt=\"数据结构\"><br>    - 广度优先生成森林</p>\n<ul>\n<li>深度优先遍历（DFS）<br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/25.png\"><br><img src=\"/img/datastruct/6_graph/graph/25.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/graph/26.png\"><br><img src=\"/img/datastruct/6_graph/graph/26.png\" alt=\"数据结构\"><ul>\n<li>深度优先生成树</li>\n<li>深度优先生成森林</li>\n</ul>\n</li>\n</ul>"},{"title":"6.4 有向无环图","date":"2023-08-06T16:00:00.000Z","comment":false,"toc":true,"_content":"#\n<!--more-->\n\n### \n\n<!--more-->\n### 6.4-1 DAG描述表达式\n![](../../../../themes/yilia/source/img/datastruct/6_graph/DAG/1.png)\n![数据结构](/img/datastruct/6_graph/DAG/1.png)\n![](../../../../themes/yilia/source/img/datastruct/6_graph/DAG/2.png)\n![数据结构](/img/datastruct/6_graph/DAG/2.png)\n### 6.4-2 拓扑排序\n![](../../../../themes/yilia/source/img/datastruct/6_graph/DAG/3.png)\n![数据结构](/img/datastruct/6_graph/DAG/3.png)\n- AOV网（Activity On Vertex Network）：顶点表示活动，弧表示活动之间的优先关系\n- 实现：\n![](../../../../themes/yilia/source/img/datastruct/6_graph/DAG/4.png)\n![数据结构](/img/datastruct/6_graph/DAG/4.png)\n- 时间复杂度：\n    - 邻接表：O(|V|+|E|)\n    - 邻接矩阵：O(|V|<sup>2</sup>)\n- 逆拓扑排序\n![](../../../../themes/yilia/source/img/datastruct/6_graph/DAG/5.png)\n![数据结构](/img/datastruct/6_graph/DAG/5.png)\n![](../../../../themes/yilia/source/img/datastruct/6_graph/DAG/6.png)\n![数据结构](/img/datastruct/6_graph/DAG/6.png)\n![](../../../../themes/yilia/source/img/datastruct/6_graph/DAG/7.png)\n![数据结构](/img/datastruct/6_graph/DAG/7.png)\n\n### 6.4-3 关键路径\n![](../../../../themes/yilia/source/img/datastruct/6_graph/DAG/8.png)\n![数据结构](/img/datastruct/6_graph/DAG/8.png)\n- AOE网（Activity On Edge Network）：弧表示活动，顶点表示事件\n    - 只有某顶点代表的时间发生后，该顶点出发的弧上的活动才能开始\n    - 只有在进入某顶点的所有弧上的活动都完成后，该顶点代表的事件才能发生\n- 关键路径：从源点到汇点的最长路径\n- 关键活动：关键路径上的活动\n- 事件V<sub>k</sub>的最早发生时间ve(k)：决定了所有从V<sub>k</sub>出发的活动的最早开始时间\n- 活动a<sub>i</sub>的最早开始时间e(i)：决定了该活动弧的起点所表示的事件的最早发生时间\n- 事件V<sub>k</sub>的最迟发生时间vl(k)：在不推迟整个工程完成时间的前提下，该事件必须发生的最迟时间\n- 活动a<sub>i</sub>的最迟开始时间l(i)：该活动弧的终点所表示的事件的最迟发生时间与该活动所需的时间之差\n- 求关键路径\n    - 1）正大\n![](../../../../themes/yilia/source/img/datastruct/6_graph/DAG/9.png)\n![数据结构](/img/datastruct/6_graph/DAG/9.png)\n    - 2）反小\n![](../../../../themes/yilia/source/img/datastruct/6_graph/DAG/10.png)\n![数据结构](/img/datastruct/6_graph/DAG/10.png)\n    - 3）=起点\n![](../../../../themes/yilia/source/img/datastruct/6_graph/DAG/11.png)\n![数据结构](/img/datastruct/6_graph/DAG/11.png)\n    - 4）=终点-\n![](../../../../themes/yilia/source/img/datastruct/6_graph/DAG/12.png)\n![数据结构](/img/datastruct/6_graph/DAG/12.png)\n    - 5）相减\n![](../../../../themes/yilia/source/img/datastruct/6_graph/DAG/13.png)\n![数据结构](/img/datastruct/6_graph/DAG/13.png)\n","source":"_posts/datastruct/6_graph/4_DAG.md","raw":"---\ntitle: 6.4 有向无环图\ndate: 2023-08-07 00:00:00\ntags: [数据结构,图]\ncategories: [数据结构]\ncomment: false\ntoc: true\n---\n#\n<!--more-->\n\n### \n\n<!--more-->\n### 6.4-1 DAG描述表达式\n![](../../../../themes/yilia/source/img/datastruct/6_graph/DAG/1.png)\n![数据结构](/img/datastruct/6_graph/DAG/1.png)\n![](../../../../themes/yilia/source/img/datastruct/6_graph/DAG/2.png)\n![数据结构](/img/datastruct/6_graph/DAG/2.png)\n### 6.4-2 拓扑排序\n![](../../../../themes/yilia/source/img/datastruct/6_graph/DAG/3.png)\n![数据结构](/img/datastruct/6_graph/DAG/3.png)\n- AOV网（Activity On Vertex Network）：顶点表示活动，弧表示活动之间的优先关系\n- 实现：\n![](../../../../themes/yilia/source/img/datastruct/6_graph/DAG/4.png)\n![数据结构](/img/datastruct/6_graph/DAG/4.png)\n- 时间复杂度：\n    - 邻接表：O(|V|+|E|)\n    - 邻接矩阵：O(|V|<sup>2</sup>)\n- 逆拓扑排序\n![](../../../../themes/yilia/source/img/datastruct/6_graph/DAG/5.png)\n![数据结构](/img/datastruct/6_graph/DAG/5.png)\n![](../../../../themes/yilia/source/img/datastruct/6_graph/DAG/6.png)\n![数据结构](/img/datastruct/6_graph/DAG/6.png)\n![](../../../../themes/yilia/source/img/datastruct/6_graph/DAG/7.png)\n![数据结构](/img/datastruct/6_graph/DAG/7.png)\n\n### 6.4-3 关键路径\n![](../../../../themes/yilia/source/img/datastruct/6_graph/DAG/8.png)\n![数据结构](/img/datastruct/6_graph/DAG/8.png)\n- AOE网（Activity On Edge Network）：弧表示活动，顶点表示事件\n    - 只有某顶点代表的时间发生后，该顶点出发的弧上的活动才能开始\n    - 只有在进入某顶点的所有弧上的活动都完成后，该顶点代表的事件才能发生\n- 关键路径：从源点到汇点的最长路径\n- 关键活动：关键路径上的活动\n- 事件V<sub>k</sub>的最早发生时间ve(k)：决定了所有从V<sub>k</sub>出发的活动的最早开始时间\n- 活动a<sub>i</sub>的最早开始时间e(i)：决定了该活动弧的起点所表示的事件的最早发生时间\n- 事件V<sub>k</sub>的最迟发生时间vl(k)：在不推迟整个工程完成时间的前提下，该事件必须发生的最迟时间\n- 活动a<sub>i</sub>的最迟开始时间l(i)：该活动弧的终点所表示的事件的最迟发生时间与该活动所需的时间之差\n- 求关键路径\n    - 1）正大\n![](../../../../themes/yilia/source/img/datastruct/6_graph/DAG/9.png)\n![数据结构](/img/datastruct/6_graph/DAG/9.png)\n    - 2）反小\n![](../../../../themes/yilia/source/img/datastruct/6_graph/DAG/10.png)\n![数据结构](/img/datastruct/6_graph/DAG/10.png)\n    - 3）=起点\n![](../../../../themes/yilia/source/img/datastruct/6_graph/DAG/11.png)\n![数据结构](/img/datastruct/6_graph/DAG/11.png)\n    - 4）=终点-\n![](../../../../themes/yilia/source/img/datastruct/6_graph/DAG/12.png)\n![数据结构](/img/datastruct/6_graph/DAG/12.png)\n    - 5）相减\n![](../../../../themes/yilia/source/img/datastruct/6_graph/DAG/13.png)\n![数据结构](/img/datastruct/6_graph/DAG/13.png)\n","slug":"datastruct/6_graph/4_DAG","published":1,"updated":"2023-10-23T12:34:36.586Z","comments":1,"layout":"post","photos":[],"_id":"clvulgd65002h5ovw3zwq9mls","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h3 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a></h3><!--more-->\n<h3 id=\"6-4-1-DAG描述表达式\"><a href=\"#6-4-1-DAG描述表达式\" class=\"headerlink\" title=\"6.4-1 DAG描述表达式\"></a>6.4-1 DAG描述表达式</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/DAG/1.png\"><br><img src=\"/img/datastruct/6_graph/DAG/1.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/DAG/2.png\"><br><img src=\"/img/datastruct/6_graph/DAG/2.png\" alt=\"数据结构\"></p>\n<h3 id=\"6-4-2-拓扑排序\"><a href=\"#6-4-2-拓扑排序\" class=\"headerlink\" title=\"6.4-2 拓扑排序\"></a>6.4-2 拓扑排序</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/DAG/3.png\"><br><img src=\"/img/datastruct/6_graph/DAG/3.png\" alt=\"数据结构\"></p>\n<ul>\n<li>AOV网（Activity On Vertex Network）：顶点表示活动，弧表示活动之间的优先关系</li>\n<li>实现：<br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/DAG/4.png\"><br><img src=\"/img/datastruct/6_graph/DAG/4.png\" alt=\"数据结构\"></li>\n<li>时间复杂度：<ul>\n<li>邻接表：O(|V|+|E|)</li>\n<li>邻接矩阵：O(|V|<sup>2</sup>)</li>\n</ul>\n</li>\n<li>逆拓扑排序<br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/DAG/5.png\"><br><img src=\"/img/datastruct/6_graph/DAG/5.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/DAG/6.png\"><br><img src=\"/img/datastruct/6_graph/DAG/6.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/DAG/7.png\"><br><img src=\"/img/datastruct/6_graph/DAG/7.png\" alt=\"数据结构\"></li>\n</ul>\n<h3 id=\"6-4-3-关键路径\"><a href=\"#6-4-3-关键路径\" class=\"headerlink\" title=\"6.4-3 关键路径\"></a>6.4-3 关键路径</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/DAG/8.png\"><br><img src=\"/img/datastruct/6_graph/DAG/8.png\" alt=\"数据结构\"></p>\n<ul>\n<li>AOE网（Activity On Edge Network）：弧表示活动，顶点表示事件<ul>\n<li>只有某顶点代表的时间发生后，该顶点出发的弧上的活动才能开始</li>\n<li>只有在进入某顶点的所有弧上的活动都完成后，该顶点代表的事件才能发生</li>\n</ul>\n</li>\n<li>关键路径：从源点到汇点的最长路径</li>\n<li>关键活动：关键路径上的活动</li>\n<li>事件V<sub>k</sub>的最早发生时间ve(k)：决定了所有从V<sub>k</sub>出发的活动的最早开始时间</li>\n<li>活动a<sub>i</sub>的最早开始时间e(i)：决定了该活动弧的起点所表示的事件的最早发生时间</li>\n<li>事件V<sub>k</sub>的最迟发生时间vl(k)：在不推迟整个工程完成时间的前提下，该事件必须发生的最迟时间</li>\n<li>活动a<sub>i</sub>的最迟开始时间l(i)：该活动弧的终点所表示的事件的最迟发生时间与该活动所需的时间之差</li>\n<li>求关键路径<ul>\n<li>1）正大<br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/DAG/9.png\"><br><img src=\"/img/datastruct/6_graph/DAG/9.png\" alt=\"数据结构\"></li>\n<li>2）反小<br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/DAG/10.png\"><br><img src=\"/img/datastruct/6_graph/DAG/10.png\" alt=\"数据结构\"></li>\n<li>3）&#x3D;起点<br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/DAG/11.png\"><br><img src=\"/img/datastruct/6_graph/DAG/11.png\" alt=\"数据结构\"></li>\n<li>4）&#x3D;终点-<br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/DAG/12.png\"><br><img src=\"/img/datastruct/6_graph/DAG/12.png\" alt=\"数据结构\"></li>\n<li>5）相减<br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/DAG/13.png\"><br><img src=\"/img/datastruct/6_graph/DAG/13.png\" alt=\"数据结构\"></li>\n</ul>\n</li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h3 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a></h3><!--more-->\n<h3 id=\"6-4-1-DAG描述表达式\"><a href=\"#6-4-1-DAG描述表达式\" class=\"headerlink\" title=\"6.4-1 DAG描述表达式\"></a>6.4-1 DAG描述表达式</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/DAG/1.png\"><br><img src=\"/img/datastruct/6_graph/DAG/1.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/DAG/2.png\"><br><img src=\"/img/datastruct/6_graph/DAG/2.png\" alt=\"数据结构\"></p>\n<h3 id=\"6-4-2-拓扑排序\"><a href=\"#6-4-2-拓扑排序\" class=\"headerlink\" title=\"6.4-2 拓扑排序\"></a>6.4-2 拓扑排序</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/DAG/3.png\"><br><img src=\"/img/datastruct/6_graph/DAG/3.png\" alt=\"数据结构\"></p>\n<ul>\n<li>AOV网（Activity On Vertex Network）：顶点表示活动，弧表示活动之间的优先关系</li>\n<li>实现：<br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/DAG/4.png\"><br><img src=\"/img/datastruct/6_graph/DAG/4.png\" alt=\"数据结构\"></li>\n<li>时间复杂度：<ul>\n<li>邻接表：O(|V|+|E|)</li>\n<li>邻接矩阵：O(|V|<sup>2</sup>)</li>\n</ul>\n</li>\n<li>逆拓扑排序<br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/DAG/5.png\"><br><img src=\"/img/datastruct/6_graph/DAG/5.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/DAG/6.png\"><br><img src=\"/img/datastruct/6_graph/DAG/6.png\" alt=\"数据结构\"><br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/DAG/7.png\"><br><img src=\"/img/datastruct/6_graph/DAG/7.png\" alt=\"数据结构\"></li>\n</ul>\n<h3 id=\"6-4-3-关键路径\"><a href=\"#6-4-3-关键路径\" class=\"headerlink\" title=\"6.4-3 关键路径\"></a>6.4-3 关键路径</h3><p><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/DAG/8.png\"><br><img src=\"/img/datastruct/6_graph/DAG/8.png\" alt=\"数据结构\"></p>\n<ul>\n<li>AOE网（Activity On Edge Network）：弧表示活动，顶点表示事件<ul>\n<li>只有某顶点代表的时间发生后，该顶点出发的弧上的活动才能开始</li>\n<li>只有在进入某顶点的所有弧上的活动都完成后，该顶点代表的事件才能发生</li>\n</ul>\n</li>\n<li>关键路径：从源点到汇点的最长路径</li>\n<li>关键活动：关键路径上的活动</li>\n<li>事件V<sub>k</sub>的最早发生时间ve(k)：决定了所有从V<sub>k</sub>出发的活动的最早开始时间</li>\n<li>活动a<sub>i</sub>的最早开始时间e(i)：决定了该活动弧的起点所表示的事件的最早发生时间</li>\n<li>事件V<sub>k</sub>的最迟发生时间vl(k)：在不推迟整个工程完成时间的前提下，该事件必须发生的最迟时间</li>\n<li>活动a<sub>i</sub>的最迟开始时间l(i)：该活动弧的终点所表示的事件的最迟发生时间与该活动所需的时间之差</li>\n<li>求关键路径<ul>\n<li>1）正大<br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/DAG/9.png\"><br><img src=\"/img/datastruct/6_graph/DAG/9.png\" alt=\"数据结构\"></li>\n<li>2）反小<br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/DAG/10.png\"><br><img src=\"/img/datastruct/6_graph/DAG/10.png\" alt=\"数据结构\"></li>\n<li>3）&#x3D;起点<br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/DAG/11.png\"><br><img src=\"/img/datastruct/6_graph/DAG/11.png\" alt=\"数据结构\"></li>\n<li>4）&#x3D;终点-<br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/DAG/12.png\"><br><img src=\"/img/datastruct/6_graph/DAG/12.png\" alt=\"数据结构\"></li>\n<li>5）相减<br><img src=\"/../../../../themes/yilia/source/img/datastruct/6_graph/DAG/13.png\"><br><img src=\"/img/datastruct/6_graph/DAG/13.png\" alt=\"数据结构\"></li>\n</ul>\n</li>\n</ul>"},{"title":"1. 快速排序","date":"2023-10-01T16:00:00.000Z","comment":false,"toc":true,"_content":"#\n<!--more-->\n### 1. 1 思想\n\n```\n1. 确定分界点\n\t- q[l]\n\t- q[r]\n\t- q[(l+r)/2]\n\t- q[random]\n2. 调整区间（*）\n\t- 左边<=x x0 右边>=x\n\t- x不一定在x0位置\n3. 递归两端\n```\n```\n1. 暴力求解\n\ta[], b[], c[],对c进行排序，<=x的放到a，>=x的放到b。放完后先将a放到c，再将b放到c.\n\t\n2. 优化空间\n\t定义左右指针i,j当c[i]>x时停，当c[j]<x时停。两个都停之后交换。\n```\n### 1.2 算法模板\n```c\n#include<iostream>\nusing namespace std;\nint a[100000];\nvoid qsort(int a[],int l, int r){\n    if(l>=r) return;\n    int i=l-1,j=r+1;\n    int x=a[((r-l)>>1)+l];\n    while(i<j){\n        do i++; while(a[i]<x);\n        do j--; while(a[j]>x);\n        if(i<j) swap(a[i],a[j]);\n    }\n    qsort(a,l,j);\n    qsort(a,j+1,r);\n}\n\nint main(){\n    int n;\n    scanf(\"%d\",&n);\n    for(int i=0;i<n;i++) scanf(\"%d\",&a[i]);\n    qsort(a,0,n-1);\n    for(int i=0;i<n;i++) printf(\"%d \",a[i]);\n}\n```\n- 可以考虑使用i或j\n- 可以考虑使用while(a[i++])\n\n---\n## 2 归并排序\n### 2.1 思想\n```\n1. 确定分界点 a[(l+r)/2]\n2. 递归排序左右\n3. 归并，合二为一（*）\n```\n\n### 2.2 算法模板\n```c\n#include<iostream>\n\nusing namespace std;\n\nint a[100010];\nint temp[100010];\nint n=0;\n\nvoid msort(int a[],int l,int r){\n    if(l>=r) return;\n    int mid=((r-l)>>1)+l;\n    msort(a,l,mid);\n    msort(a,mid+1,r);\n    \n    //merge\n    int i=l,j=mid+1,k=0;\n    while(i<=mid && j<=r){\n        temp[k++]=(a[i]<=a[j]?a[i++]:a[j++]);\n    }\n    while(i<=mid) temp[k++]=a[i++];\n    while(j<=r) temp[k++]=a[j++];\n    //\n    for(i=l,j=0;i<=r;i++,j++) a[i]=temp[j];\n}\n\nint main(){\n    scanf(\"%d\",&n);\n    for(int i=0;i<n;i++) scanf(\"%d\",&a[i]);\n    msort(a,0,n-1);\n    for(int i=0;i<n;i++) printf(\"%d \",a[i]);\n    return 0;\n}\n```\n\n## 3. 整数二分\n单调一定可以二分\n不单调也可能二分","source":"_posts/algorithm/1_base/1_base.md","raw":"---\n\ntitle: 1. 快速排序\n\ndate: 2023-10-02\n\ntags: [算法，排序]\n\ncategories: [算法]\n\ncomment: false\n\ntoc: true\n---\n#\n<!--more-->\n### 1. 1 思想\n\n```\n1. 确定分界点\n\t- q[l]\n\t- q[r]\n\t- q[(l+r)/2]\n\t- q[random]\n2. 调整区间（*）\n\t- 左边<=x x0 右边>=x\n\t- x不一定在x0位置\n3. 递归两端\n```\n```\n1. 暴力求解\n\ta[], b[], c[],对c进行排序，<=x的放到a，>=x的放到b。放完后先将a放到c，再将b放到c.\n\t\n2. 优化空间\n\t定义左右指针i,j当c[i]>x时停，当c[j]<x时停。两个都停之后交换。\n```\n### 1.2 算法模板\n```c\n#include<iostream>\nusing namespace std;\nint a[100000];\nvoid qsort(int a[],int l, int r){\n    if(l>=r) return;\n    int i=l-1,j=r+1;\n    int x=a[((r-l)>>1)+l];\n    while(i<j){\n        do i++; while(a[i]<x);\n        do j--; while(a[j]>x);\n        if(i<j) swap(a[i],a[j]);\n    }\n    qsort(a,l,j);\n    qsort(a,j+1,r);\n}\n\nint main(){\n    int n;\n    scanf(\"%d\",&n);\n    for(int i=0;i<n;i++) scanf(\"%d\",&a[i]);\n    qsort(a,0,n-1);\n    for(int i=0;i<n;i++) printf(\"%d \",a[i]);\n}\n```\n- 可以考虑使用i或j\n- 可以考虑使用while(a[i++])\n\n---\n## 2 归并排序\n### 2.1 思想\n```\n1. 确定分界点 a[(l+r)/2]\n2. 递归排序左右\n3. 归并，合二为一（*）\n```\n\n### 2.2 算法模板\n```c\n#include<iostream>\n\nusing namespace std;\n\nint a[100010];\nint temp[100010];\nint n=0;\n\nvoid msort(int a[],int l,int r){\n    if(l>=r) return;\n    int mid=((r-l)>>1)+l;\n    msort(a,l,mid);\n    msort(a,mid+1,r);\n    \n    //merge\n    int i=l,j=mid+1,k=0;\n    while(i<=mid && j<=r){\n        temp[k++]=(a[i]<=a[j]?a[i++]:a[j++]);\n    }\n    while(i<=mid) temp[k++]=a[i++];\n    while(j<=r) temp[k++]=a[j++];\n    //\n    for(i=l,j=0;i<=r;i++,j++) a[i]=temp[j];\n}\n\nint main(){\n    scanf(\"%d\",&n);\n    for(int i=0;i<n;i++) scanf(\"%d\",&a[i]);\n    msort(a,0,n-1);\n    for(int i=0;i<n;i++) printf(\"%d \",a[i]);\n    return 0;\n}\n```\n\n## 3. 整数二分\n单调一定可以二分\n不单调也可能二分","slug":"algorithm/1_base/1_base","published":1,"updated":"2024-05-06T05:52:09.448Z","comments":1,"layout":"post","photos":[],"_id":"clvulgd66002k5ovw3oiq0s1j","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h3 id=\"1-1-思想\"><a href=\"#1-1-思想\" class=\"headerlink\" title=\"1. 1 思想\"></a>1. 1 思想</h3><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1. 确定分界点</span><br><span class=\"line\">\t- q[l]</span><br><span class=\"line\">\t- q[r]</span><br><span class=\"line\">\t- q[(l+r)/2]</span><br><span class=\"line\">\t- q[random]</span><br><span class=\"line\">2. 调整区间（*）</span><br><span class=\"line\">\t- 左边&lt;=x x0 右边&gt;=x</span><br><span class=\"line\">\t- x不一定在x0位置</span><br><span class=\"line\">3. 递归两端</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1. 暴力求解</span><br><span class=\"line\">\ta[], b[], c[],对c进行排序，&lt;=x的放到a，&gt;=x的放到b。放完后先将a放到c，再将b放到c.</span><br><span class=\"line\">\t</span><br><span class=\"line\">2. 优化空间</span><br><span class=\"line\">\t定义左右指针i,j当c[i]&gt;x时停，当c[j]&lt;x时停。两个都停之后交换。</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-2-算法模板\"><a href=\"#1-2-算法模板\" class=\"headerlink\" title=\"1.2 算法模板\"></a>1.2 算法模板</h3><figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\">using namespace <span class=\"built_in\">std</span>;</span><br><span class=\"line\"><span class=\"type\">int</span> a[<span class=\"number\">100000</span>];</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">qsort</span><span class=\"params\">(<span class=\"type\">int</span> a[],<span class=\"type\">int</span> l, <span class=\"type\">int</span> r)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(l&gt;=r) <span class=\"keyword\">return</span>;</span><br><span class=\"line\">    <span class=\"type\">int</span> i=l<span class=\"number\">-1</span>,j=r+<span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"type\">int</span> x=a[((r-l)&gt;&gt;<span class=\"number\">1</span>)+l];</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(i&lt;j)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">do</span> i++; <span class=\"keyword\">while</span>(a[i]&lt;x);</span><br><span class=\"line\">        <span class=\"keyword\">do</span> j--; <span class=\"keyword\">while</span>(a[j]&gt;x);</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(i&lt;j) swap(a[i],a[j]);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    qsort(a,l,j);</span><br><span class=\"line\">    qsort(a,j+<span class=\"number\">1</span>,r);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> n;</span><br><span class=\"line\">    <span class=\"built_in\">scanf</span>(<span class=\"string\">&quot;%d&quot;</span>,&amp;n);</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">0</span>;i&lt;n;i++) <span class=\"built_in\">scanf</span>(<span class=\"string\">&quot;%d&quot;</span>,&amp;a[i]);</span><br><span class=\"line\">    qsort(a,<span class=\"number\">0</span>,n<span class=\"number\">-1</span>);</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">0</span>;i&lt;n;i++) <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;%d &quot;</span>,a[i]);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>可以考虑使用i或j</li>\n<li>可以考虑使用while(a[i++])</li>\n</ul>\n<hr>\n<h2 id=\"2-归并排序\"><a href=\"#2-归并排序\" class=\"headerlink\" title=\"2 归并排序\"></a>2 归并排序</h2><h3 id=\"2-1-思想\"><a href=\"#2-1-思想\" class=\"headerlink\" title=\"2.1 思想\"></a>2.1 思想</h3><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1. 确定分界点 a[(l+r)/2]</span><br><span class=\"line\">2. 递归排序左右</span><br><span class=\"line\">3. 归并，合二为一（*）</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"2-2-算法模板\"><a href=\"#2-2-算法模板\" class=\"headerlink\" title=\"2.2 算法模板\"></a>2.2 算法模板</h3><figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\">using namespace <span class=\"built_in\">std</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> a[<span class=\"number\">100010</span>];</span><br><span class=\"line\"><span class=\"type\">int</span> temp[<span class=\"number\">100010</span>];</span><br><span class=\"line\"><span class=\"type\">int</span> n=<span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">msort</span><span class=\"params\">(<span class=\"type\">int</span> a[],<span class=\"type\">int</span> l,<span class=\"type\">int</span> r)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(l&gt;=r) <span class=\"keyword\">return</span>;</span><br><span class=\"line\">    <span class=\"type\">int</span> mid=((r-l)&gt;&gt;<span class=\"number\">1</span>)+l;</span><br><span class=\"line\">    msort(a,l,mid);</span><br><span class=\"line\">    msort(a,mid+<span class=\"number\">1</span>,r);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//merge</span></span><br><span class=\"line\">    <span class=\"type\">int</span> i=l,j=mid+<span class=\"number\">1</span>,k=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(i&lt;=mid &amp;&amp; j&lt;=r)&#123;</span><br><span class=\"line\">        temp[k++]=(a[i]&lt;=a[j]?a[i++]:a[j++]);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(i&lt;=mid) temp[k++]=a[i++];</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(j&lt;=r) temp[k++]=a[j++];</span><br><span class=\"line\">    <span class=\"comment\">//</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span>(i=l,j=<span class=\"number\">0</span>;i&lt;=r;i++,j++) a[i]=temp[j];</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">    <span class=\"built_in\">scanf</span>(<span class=\"string\">&quot;%d&quot;</span>,&amp;n);</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">0</span>;i&lt;n;i++) <span class=\"built_in\">scanf</span>(<span class=\"string\">&quot;%d&quot;</span>,&amp;a[i]);</span><br><span class=\"line\">    msort(a,<span class=\"number\">0</span>,n<span class=\"number\">-1</span>);</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">0</span>;i&lt;n;i++) <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;%d &quot;</span>,a[i]);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"3-整数二分\"><a href=\"#3-整数二分\" class=\"headerlink\" title=\"3. 整数二分\"></a>3. 整数二分</h2><p>单调一定可以二分<br>不单调也可能二分</p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h3 id=\"1-1-思想\"><a href=\"#1-1-思想\" class=\"headerlink\" title=\"1. 1 思想\"></a>1. 1 思想</h3><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1. 确定分界点</span><br><span class=\"line\">\t- q[l]</span><br><span class=\"line\">\t- q[r]</span><br><span class=\"line\">\t- q[(l+r)/2]</span><br><span class=\"line\">\t- q[random]</span><br><span class=\"line\">2. 调整区间（*）</span><br><span class=\"line\">\t- 左边&lt;=x x0 右边&gt;=x</span><br><span class=\"line\">\t- x不一定在x0位置</span><br><span class=\"line\">3. 递归两端</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1. 暴力求解</span><br><span class=\"line\">\ta[], b[], c[],对c进行排序，&lt;=x的放到a，&gt;=x的放到b。放完后先将a放到c，再将b放到c.</span><br><span class=\"line\">\t</span><br><span class=\"line\">2. 优化空间</span><br><span class=\"line\">\t定义左右指针i,j当c[i]&gt;x时停，当c[j]&lt;x时停。两个都停之后交换。</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-2-算法模板\"><a href=\"#1-2-算法模板\" class=\"headerlink\" title=\"1.2 算法模板\"></a>1.2 算法模板</h3><figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\">using namespace <span class=\"built_in\">std</span>;</span><br><span class=\"line\"><span class=\"type\">int</span> a[<span class=\"number\">100000</span>];</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">qsort</span><span class=\"params\">(<span class=\"type\">int</span> a[],<span class=\"type\">int</span> l, <span class=\"type\">int</span> r)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(l&gt;=r) <span class=\"keyword\">return</span>;</span><br><span class=\"line\">    <span class=\"type\">int</span> i=l<span class=\"number\">-1</span>,j=r+<span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"type\">int</span> x=a[((r-l)&gt;&gt;<span class=\"number\">1</span>)+l];</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(i&lt;j)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">do</span> i++; <span class=\"keyword\">while</span>(a[i]&lt;x);</span><br><span class=\"line\">        <span class=\"keyword\">do</span> j--; <span class=\"keyword\">while</span>(a[j]&gt;x);</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(i&lt;j) swap(a[i],a[j]);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    qsort(a,l,j);</span><br><span class=\"line\">    qsort(a,j+<span class=\"number\">1</span>,r);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> n;</span><br><span class=\"line\">    <span class=\"built_in\">scanf</span>(<span class=\"string\">&quot;%d&quot;</span>,&amp;n);</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">0</span>;i&lt;n;i++) <span class=\"built_in\">scanf</span>(<span class=\"string\">&quot;%d&quot;</span>,&amp;a[i]);</span><br><span class=\"line\">    qsort(a,<span class=\"number\">0</span>,n<span class=\"number\">-1</span>);</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">0</span>;i&lt;n;i++) <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;%d &quot;</span>,a[i]);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>可以考虑使用i或j</li>\n<li>可以考虑使用while(a[i++])</li>\n</ul>\n<hr>\n<h2 id=\"2-归并排序\"><a href=\"#2-归并排序\" class=\"headerlink\" title=\"2 归并排序\"></a>2 归并排序</h2><h3 id=\"2-1-思想\"><a href=\"#2-1-思想\" class=\"headerlink\" title=\"2.1 思想\"></a>2.1 思想</h3><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1. 确定分界点 a[(l+r)/2]</span><br><span class=\"line\">2. 递归排序左右</span><br><span class=\"line\">3. 归并，合二为一（*）</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"2-2-算法模板\"><a href=\"#2-2-算法模板\" class=\"headerlink\" title=\"2.2 算法模板\"></a>2.2 算法模板</h3><figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\">using namespace <span class=\"built_in\">std</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> a[<span class=\"number\">100010</span>];</span><br><span class=\"line\"><span class=\"type\">int</span> temp[<span class=\"number\">100010</span>];</span><br><span class=\"line\"><span class=\"type\">int</span> n=<span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">msort</span><span class=\"params\">(<span class=\"type\">int</span> a[],<span class=\"type\">int</span> l,<span class=\"type\">int</span> r)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(l&gt;=r) <span class=\"keyword\">return</span>;</span><br><span class=\"line\">    <span class=\"type\">int</span> mid=((r-l)&gt;&gt;<span class=\"number\">1</span>)+l;</span><br><span class=\"line\">    msort(a,l,mid);</span><br><span class=\"line\">    msort(a,mid+<span class=\"number\">1</span>,r);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//merge</span></span><br><span class=\"line\">    <span class=\"type\">int</span> i=l,j=mid+<span class=\"number\">1</span>,k=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(i&lt;=mid &amp;&amp; j&lt;=r)&#123;</span><br><span class=\"line\">        temp[k++]=(a[i]&lt;=a[j]?a[i++]:a[j++]);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(i&lt;=mid) temp[k++]=a[i++];</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(j&lt;=r) temp[k++]=a[j++];</span><br><span class=\"line\">    <span class=\"comment\">//</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span>(i=l,j=<span class=\"number\">0</span>;i&lt;=r;i++,j++) a[i]=temp[j];</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">    <span class=\"built_in\">scanf</span>(<span class=\"string\">&quot;%d&quot;</span>,&amp;n);</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">0</span>;i&lt;n;i++) <span class=\"built_in\">scanf</span>(<span class=\"string\">&quot;%d&quot;</span>,&amp;a[i]);</span><br><span class=\"line\">    msort(a,<span class=\"number\">0</span>,n<span class=\"number\">-1</span>);</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">0</span>;i&lt;n;i++) <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;%d &quot;</span>,a[i]);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"3-整数二分\"><a href=\"#3-整数二分\" class=\"headerlink\" title=\"3. 整数二分\"></a>3. 整数二分</h2><p>单调一定可以二分<br>不单调也可能二分</p>"},{"title":"vscode","date":"2021-03-01T12:00:00.000Z","toc":true,"_content":"# vscode中python相对路径无法使用\n\n<!--more-->\n\n## 解决方案：\n\n- 写绝对路径\n注意用/不要用\\\n- 修改配置文件\n![](../../../../themes/yilia/source/img/experience/vscode/relative_path/1.png)\n![](img/experience/vscode/relative_path/1.png)\n![](../../../../themes/yilia/source/img/experience/vscode/relative_path/2.png)\n![](img/experience/vscode/relative_path/2.png)","source":"_posts/experience/vscode/vscode.md","raw":"---\ntitle: vscode\ndate: 2021-03-01 20:00:00\ntoc: true\ntags: [vscode]\ncategories: [经验]\n---\n# vscode中python相对路径无法使用\n\n<!--more-->\n\n## 解决方案：\n\n- 写绝对路径\n注意用/不要用\\\n- 修改配置文件\n![](../../../../themes/yilia/source/img/experience/vscode/relative_path/1.png)\n![](img/experience/vscode/relative_path/1.png)\n![](../../../../themes/yilia/source/img/experience/vscode/relative_path/2.png)\n![](img/experience/vscode/relative_path/2.png)","slug":"experience/vscode/vscode","published":1,"updated":"2024-04-23T04:56:55.865Z","_id":"clvulgd67002m5ovwdvly80ss","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"vscode中python相对路径无法使用\"><a href=\"#vscode中python相对路径无法使用\" class=\"headerlink\" title=\"vscode中python相对路径无法使用\"></a>vscode中python相对路径无法使用</h1><span id=\"more\"></span>\n\n<h2 id=\"解决方案：\"><a href=\"#解决方案：\" class=\"headerlink\" title=\"解决方案：\"></a>解决方案：</h2><ul>\n<li>写绝对路径<br>注意用&#x2F;不要用\\</li>\n<li>修改配置文件<br><img src=\"/../../../../themes/yilia/source/img/experience/vscode/relative_path/1.png\"><br><img src=\"/img/experience/vscode/relative_path/1.png\"><br><img src=\"/../../../../themes/yilia/source/img/experience/vscode/relative_path/2.png\"><br><img src=\"/img/experience/vscode/relative_path/2.png\"></li>\n</ul>\n","excerpt":"<h1 id=\"vscode中python相对路径无法使用\"><a href=\"#vscode中python相对路径无法使用\" class=\"headerlink\" title=\"vscode中python相对路径无法使用\"></a>vscode中python相对路径无法使用</h1>","more":"<h2 id=\"解决方案：\"><a href=\"#解决方案：\" class=\"headerlink\" title=\"解决方案：\"></a>解决方案：</h2><ul>\n<li>写绝对路径<br>注意用&#x2F;不要用\\</li>\n<li>修改配置文件<br><img src=\"/../../../../themes/yilia/source/img/experience/vscode/relative_path/1.png\"><br><img src=\"/img/experience/vscode/relative_path/1.png\"><br><img src=\"/../../../../themes/yilia/source/img/experience/vscode/relative_path/2.png\"><br><img src=\"/img/experience/vscode/relative_path/2.png\"></li>\n</ul>"},{"title":"win10中windows media player自动同步歌词","date":"2023-12-11T16:00:00.000Z","comment":false,"toc":true,"_content":"\n#\n<!--more-->\n\n- 需要提前下载歌词文件\n\n## 1. 歌词文件\n\n- 如果你的歌词文件是LRC文件，则需要转成SMI文件\n\n  1. [在线转换](https://www.lrccon.com/convert?lang=zh-hans)(只能转单个，想转多个可以找找别的)\n\n     ![](/img/experience/app/lrc_to_smi/2.png)\n\n     ![](../../../../themes/yilia/source/img/experience/app/lrc_to_smi/2.png)\n\n     ![](/img/experience/app/lrc_to_smi/3.png)\n\n     ![](../../../../themes/yilia/source/img/experience/app/lrc_to_smi/3.png)\n\n     注意编码\n  \n     ![](/img/experience/app/lrc_to_smi/4.png)\n  \n     ![](../../../../themes/yilia/source/img/experience/app/lrc_to_smi/4.png)\n  \n  2. 将SMI文件与歌曲mp3文件放在同一文件夹下\n\n## 2. 设置播放器\n\n1. 右键歌曲 -> 打开方式 -> windows media player\n\n![](/img/experience/app/lrc_to_smi/1.png)\n\n![](../../../../themes/yilia/source/img/experience/app/lrc_to_smi/1.png)\n\n2.  右键 -> 歌词 -> 开\n\n3.  ![](/img/experience/app/lrc_to_smi/5.png)\n\n    ![](../../../../themes/yilia/source/img/experience/app/lrc_to_smi/5.png)\n\n     \n\n4. 关闭软件再重新打开音乐就ok了。","source":"_posts/experience/app/lrc_to_smi.md","raw":"---\ntitle: win10中windows media player自动同步歌词\n\ndate: 2023-12-12\n\ntags: [Windows]\n\ncategories: [经验]\n\ncomment: false\n\ntoc: true\n\n---\n\n#\n<!--more-->\n\n- 需要提前下载歌词文件\n\n## 1. 歌词文件\n\n- 如果你的歌词文件是LRC文件，则需要转成SMI文件\n\n  1. [在线转换](https://www.lrccon.com/convert?lang=zh-hans)(只能转单个，想转多个可以找找别的)\n\n     ![](/img/experience/app/lrc_to_smi/2.png)\n\n     ![](../../../../themes/yilia/source/img/experience/app/lrc_to_smi/2.png)\n\n     ![](/img/experience/app/lrc_to_smi/3.png)\n\n     ![](../../../../themes/yilia/source/img/experience/app/lrc_to_smi/3.png)\n\n     注意编码\n  \n     ![](/img/experience/app/lrc_to_smi/4.png)\n  \n     ![](../../../../themes/yilia/source/img/experience/app/lrc_to_smi/4.png)\n  \n  2. 将SMI文件与歌曲mp3文件放在同一文件夹下\n\n## 2. 设置播放器\n\n1. 右键歌曲 -> 打开方式 -> windows media player\n\n![](/img/experience/app/lrc_to_smi/1.png)\n\n![](../../../../themes/yilia/source/img/experience/app/lrc_to_smi/1.png)\n\n2.  右键 -> 歌词 -> 开\n\n3.  ![](/img/experience/app/lrc_to_smi/5.png)\n\n    ![](../../../../themes/yilia/source/img/experience/app/lrc_to_smi/5.png)\n\n     \n\n4. 关闭软件再重新打开音乐就ok了。","slug":"experience/app/lrc_to_smi","published":1,"updated":"2024-03-07T11:19:12.644Z","_id":"clvulgd67002q5ovw9c4e8eo6","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<ul>\n<li>需要提前下载歌词文件</li>\n</ul>\n<h2 id=\"1-歌词文件\"><a href=\"#1-歌词文件\" class=\"headerlink\" title=\"1. 歌词文件\"></a>1. 歌词文件</h2><ul>\n<li><p>如果你的歌词文件是LRC文件，则需要转成SMI文件</p>\n<ol>\n<li><p><a href=\"https://www.lrccon.com/convert?lang=zh-hans\">在线转换</a>(只能转单个，想转多个可以找找别的)</p>\n<p><img src=\"/img/experience/app/lrc_to_smi/2.png\"></p>\n<p><img src=\"/../../../../themes/yilia/source/img/experience/app/lrc_to_smi/2.png\"></p>\n<p><img src=\"/img/experience/app/lrc_to_smi/3.png\"></p>\n<p><img src=\"/../../../../themes/yilia/source/img/experience/app/lrc_to_smi/3.png\"></p>\n<p>注意编码</p>\n<p><img src=\"/img/experience/app/lrc_to_smi/4.png\"></p>\n<p><img src=\"/../../../../themes/yilia/source/img/experience/app/lrc_to_smi/4.png\"></p>\n</li>\n<li><p>将SMI文件与歌曲mp3文件放在同一文件夹下</p>\n</li>\n</ol>\n</li>\n</ul>\n<h2 id=\"2-设置播放器\"><a href=\"#2-设置播放器\" class=\"headerlink\" title=\"2. 设置播放器\"></a>2. 设置播放器</h2><ol>\n<li>右键歌曲 -&gt; 打开方式 -&gt; windows media player</li>\n</ol>\n<p><img src=\"/img/experience/app/lrc_to_smi/1.png\"></p>\n<p><img src=\"/../../../../themes/yilia/source/img/experience/app/lrc_to_smi/1.png\"></p>\n<ol start=\"2\">\n<li><p>右键 -&gt; 歌词 -&gt; 开</p>\n</li>\n<li><p><img src=\"/img/experience/app/lrc_to_smi/5.png\"></p>\n<p><img src=\"/../../../../themes/yilia/source/img/experience/app/lrc_to_smi/5.png\"></p>\n</li>\n<li><p>关闭软件再重新打开音乐就ok了。</p>\n</li>\n</ol>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<ul>\n<li>需要提前下载歌词文件</li>\n</ul>\n<h2 id=\"1-歌词文件\"><a href=\"#1-歌词文件\" class=\"headerlink\" title=\"1. 歌词文件\"></a>1. 歌词文件</h2><ul>\n<li><p>如果你的歌词文件是LRC文件，则需要转成SMI文件</p>\n<ol>\n<li><p><a href=\"https://www.lrccon.com/convert?lang=zh-hans\">在线转换</a>(只能转单个，想转多个可以找找别的)</p>\n<p><img src=\"/img/experience/app/lrc_to_smi/2.png\"></p>\n<p><img src=\"/../../../../themes/yilia/source/img/experience/app/lrc_to_smi/2.png\"></p>\n<p><img src=\"/img/experience/app/lrc_to_smi/3.png\"></p>\n<p><img src=\"/../../../../themes/yilia/source/img/experience/app/lrc_to_smi/3.png\"></p>\n<p>注意编码</p>\n<p><img src=\"/img/experience/app/lrc_to_smi/4.png\"></p>\n<p><img src=\"/../../../../themes/yilia/source/img/experience/app/lrc_to_smi/4.png\"></p>\n</li>\n<li><p>将SMI文件与歌曲mp3文件放在同一文件夹下</p>\n</li>\n</ol>\n</li>\n</ul>\n<h2 id=\"2-设置播放器\"><a href=\"#2-设置播放器\" class=\"headerlink\" title=\"2. 设置播放器\"></a>2. 设置播放器</h2><ol>\n<li>右键歌曲 -&gt; 打开方式 -&gt; windows media player</li>\n</ol>\n<p><img src=\"/img/experience/app/lrc_to_smi/1.png\"></p>\n<p><img src=\"/../../../../themes/yilia/source/img/experience/app/lrc_to_smi/1.png\"></p>\n<ol start=\"2\">\n<li><p>右键 -&gt; 歌词 -&gt; 开</p>\n</li>\n<li><p><img src=\"/img/experience/app/lrc_to_smi/5.png\"></p>\n<p><img src=\"/../../../../themes/yilia/source/img/experience/app/lrc_to_smi/5.png\"></p>\n</li>\n<li><p>关闭软件再重新打开音乐就ok了。</p>\n</li>\n</ol>"},{"title":"pycharm连接服务器--","date":"2023-12-16T16:00:00.000Z","comment":false,"toc":true,"_content":"\n#\n<!--more-->\n\n- 需要提前准备好：IP/端口/user/password\n\n## 1. 连接\n\n![](../../../../themes/yilia/source/img/experience/pycharm/link_server/1.png)\n\n![](/img/experience/pycharm/link_server/1.png)\n\n\n\n![](../../../../themes/yilia/source/img/experience/pycharm/link_server/2.png)\n\n![](/img/experience/pycharm/link_server/2.png)\n\n\n\n![](../../../../themes/yilia/source/img/experience/pycharm/link_server/3.png)\n\n![](/img/experience/pycharm/link_server/3.png)\n\n\n\n![](../../../../themes/yilia/source/img/experience/pycharm/link_server/4.png)\n\n![](/img/experience/pycharm/link_server/4.png)\n\n测试成功即可。\n\n\n\n## 2. 打开服务器\n\n- 打开终端\n\n![](../../../../themes/yilia/source/img/experience/pycharm/link_server/5.png)\n\n![](/img/experience/pycharm/link_server/5.png)\n\n-  打开文件\n\n![](../../../../themes/yilia/source/img/experience/pycharm/link_server/6.png)\n\n![](/img/experience/pycharm/link_server/6.png)\n\n右侧就会显示。如果你想显示特定目录，请在连接的时候填写根路径。\n\n## 3. 使用服务器运行代码\n\n![](../../../../themes/yilia/source/img/experience/pycharm/link_server/7.png)\n\n ![](/img/experience/pycharm/link_server/7.png) \n\n\n\n![](../../../../themes/yilia/source/img/experience/pycharm/link_server/8.png)\n\n ![](/img/experience/pycharm/link_server/8.png) \n\n点击添加。\n\n![](../../../../themes/yilia/source/img/experience/pycharm/link_server/9.png)\n\n ![](/img/experience/pycharm/link_server/9.png) \n\n选择你的python解释器路径（通常在虚拟环境的/bin/python文件\n\n![](../../../../themes/yilia/source/img/experience/pycharm/link_server/10.png)\n\n ![](/img/experience/pycharm/link_server/10.png) \n\n点击右下角即可更改python解释器。\n\n![](../../../../themes/yilia/source/img/experience/pycharm/link_server/11.png)\n\n ![](/img/experience/pycharm/link_server/11.png) \n\n","source":"_posts/experience/pycharm/link_server.md","raw":"---\ntitle: pycharm连接服务器--\n\ndate: 2023-12-17\n\ntags: [pycharm]\n\ncategories: [经验]\n\ncomment: false\n\ntoc: true\n\n---\n\n#\n<!--more-->\n\n- 需要提前准备好：IP/端口/user/password\n\n## 1. 连接\n\n![](../../../../themes/yilia/source/img/experience/pycharm/link_server/1.png)\n\n![](/img/experience/pycharm/link_server/1.png)\n\n\n\n![](../../../../themes/yilia/source/img/experience/pycharm/link_server/2.png)\n\n![](/img/experience/pycharm/link_server/2.png)\n\n\n\n![](../../../../themes/yilia/source/img/experience/pycharm/link_server/3.png)\n\n![](/img/experience/pycharm/link_server/3.png)\n\n\n\n![](../../../../themes/yilia/source/img/experience/pycharm/link_server/4.png)\n\n![](/img/experience/pycharm/link_server/4.png)\n\n测试成功即可。\n\n\n\n## 2. 打开服务器\n\n- 打开终端\n\n![](../../../../themes/yilia/source/img/experience/pycharm/link_server/5.png)\n\n![](/img/experience/pycharm/link_server/5.png)\n\n-  打开文件\n\n![](../../../../themes/yilia/source/img/experience/pycharm/link_server/6.png)\n\n![](/img/experience/pycharm/link_server/6.png)\n\n右侧就会显示。如果你想显示特定目录，请在连接的时候填写根路径。\n\n## 3. 使用服务器运行代码\n\n![](../../../../themes/yilia/source/img/experience/pycharm/link_server/7.png)\n\n ![](/img/experience/pycharm/link_server/7.png) \n\n\n\n![](../../../../themes/yilia/source/img/experience/pycharm/link_server/8.png)\n\n ![](/img/experience/pycharm/link_server/8.png) \n\n点击添加。\n\n![](../../../../themes/yilia/source/img/experience/pycharm/link_server/9.png)\n\n ![](/img/experience/pycharm/link_server/9.png) \n\n选择你的python解释器路径（通常在虚拟环境的/bin/python文件\n\n![](../../../../themes/yilia/source/img/experience/pycharm/link_server/10.png)\n\n ![](/img/experience/pycharm/link_server/10.png) \n\n点击右下角即可更改python解释器。\n\n![](../../../../themes/yilia/source/img/experience/pycharm/link_server/11.png)\n\n ![](/img/experience/pycharm/link_server/11.png) \n\n","slug":"experience/pycharm/link_server","published":1,"updated":"2024-03-07T11:18:44.296Z","_id":"clvulgd68002t5ovw2a7590xu","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<ul>\n<li>需要提前准备好：IP&#x2F;端口&#x2F;user&#x2F;password</li>\n</ul>\n<h2 id=\"1-连接\"><a href=\"#1-连接\" class=\"headerlink\" title=\"1. 连接\"></a>1. 连接</h2><p><img src=\"/../../../../themes/yilia/source/img/experience/pycharm/link_server/1.png\"></p>\n<p><img src=\"/img/experience/pycharm/link_server/1.png\"></p>\n<p><img src=\"/../../../../themes/yilia/source/img/experience/pycharm/link_server/2.png\"></p>\n<p><img src=\"/img/experience/pycharm/link_server/2.png\"></p>\n<p><img src=\"/../../../../themes/yilia/source/img/experience/pycharm/link_server/3.png\"></p>\n<p><img src=\"/img/experience/pycharm/link_server/3.png\"></p>\n<p><img src=\"/../../../../themes/yilia/source/img/experience/pycharm/link_server/4.png\"></p>\n<p><img src=\"/img/experience/pycharm/link_server/4.png\"></p>\n<p>测试成功即可。</p>\n<h2 id=\"2-打开服务器\"><a href=\"#2-打开服务器\" class=\"headerlink\" title=\"2. 打开服务器\"></a>2. 打开服务器</h2><ul>\n<li>打开终端</li>\n</ul>\n<p><img src=\"/../../../../themes/yilia/source/img/experience/pycharm/link_server/5.png\"></p>\n<p><img src=\"/img/experience/pycharm/link_server/5.png\"></p>\n<ul>\n<li>打开文件</li>\n</ul>\n<p><img src=\"/../../../../themes/yilia/source/img/experience/pycharm/link_server/6.png\"></p>\n<p><img src=\"/img/experience/pycharm/link_server/6.png\"></p>\n<p>右侧就会显示。如果你想显示特定目录，请在连接的时候填写根路径。</p>\n<h2 id=\"3-使用服务器运行代码\"><a href=\"#3-使用服务器运行代码\" class=\"headerlink\" title=\"3. 使用服务器运行代码\"></a>3. 使用服务器运行代码</h2><p><img src=\"/../../../../themes/yilia/source/img/experience/pycharm/link_server/7.png\"></p>\n<p> <img src=\"/img/experience/pycharm/link_server/7.png\"> </p>\n<p><img src=\"/../../../../themes/yilia/source/img/experience/pycharm/link_server/8.png\"></p>\n<p> <img src=\"/img/experience/pycharm/link_server/8.png\"> </p>\n<p>点击添加。</p>\n<p><img src=\"/../../../../themes/yilia/source/img/experience/pycharm/link_server/9.png\"></p>\n<p> <img src=\"/img/experience/pycharm/link_server/9.png\"> </p>\n<p>选择你的python解释器路径（通常在虚拟环境的&#x2F;bin&#x2F;python文件</p>\n<p><img src=\"/../../../../themes/yilia/source/img/experience/pycharm/link_server/10.png\"></p>\n<p> <img src=\"/img/experience/pycharm/link_server/10.png\"> </p>\n<p>点击右下角即可更改python解释器。</p>\n<p><img src=\"/../../../../themes/yilia/source/img/experience/pycharm/link_server/11.png\"></p>\n<p> <img src=\"/img/experience/pycharm/link_server/11.png\"> </p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<ul>\n<li>需要提前准备好：IP&#x2F;端口&#x2F;user&#x2F;password</li>\n</ul>\n<h2 id=\"1-连接\"><a href=\"#1-连接\" class=\"headerlink\" title=\"1. 连接\"></a>1. 连接</h2><p><img src=\"/../../../../themes/yilia/source/img/experience/pycharm/link_server/1.png\"></p>\n<p><img src=\"/img/experience/pycharm/link_server/1.png\"></p>\n<p><img src=\"/../../../../themes/yilia/source/img/experience/pycharm/link_server/2.png\"></p>\n<p><img src=\"/img/experience/pycharm/link_server/2.png\"></p>\n<p><img src=\"/../../../../themes/yilia/source/img/experience/pycharm/link_server/3.png\"></p>\n<p><img src=\"/img/experience/pycharm/link_server/3.png\"></p>\n<p><img src=\"/../../../../themes/yilia/source/img/experience/pycharm/link_server/4.png\"></p>\n<p><img src=\"/img/experience/pycharm/link_server/4.png\"></p>\n<p>测试成功即可。</p>\n<h2 id=\"2-打开服务器\"><a href=\"#2-打开服务器\" class=\"headerlink\" title=\"2. 打开服务器\"></a>2. 打开服务器</h2><ul>\n<li>打开终端</li>\n</ul>\n<p><img src=\"/../../../../themes/yilia/source/img/experience/pycharm/link_server/5.png\"></p>\n<p><img src=\"/img/experience/pycharm/link_server/5.png\"></p>\n<ul>\n<li>打开文件</li>\n</ul>\n<p><img src=\"/../../../../themes/yilia/source/img/experience/pycharm/link_server/6.png\"></p>\n<p><img src=\"/img/experience/pycharm/link_server/6.png\"></p>\n<p>右侧就会显示。如果你想显示特定目录，请在连接的时候填写根路径。</p>\n<h2 id=\"3-使用服务器运行代码\"><a href=\"#3-使用服务器运行代码\" class=\"headerlink\" title=\"3. 使用服务器运行代码\"></a>3. 使用服务器运行代码</h2><p><img src=\"/../../../../themes/yilia/source/img/experience/pycharm/link_server/7.png\"></p>\n<p> <img src=\"/img/experience/pycharm/link_server/7.png\"> </p>\n<p><img src=\"/../../../../themes/yilia/source/img/experience/pycharm/link_server/8.png\"></p>\n<p> <img src=\"/img/experience/pycharm/link_server/8.png\"> </p>\n<p>点击添加。</p>\n<p><img src=\"/../../../../themes/yilia/source/img/experience/pycharm/link_server/9.png\"></p>\n<p> <img src=\"/img/experience/pycharm/link_server/9.png\"> </p>\n<p>选择你的python解释器路径（通常在虚拟环境的&#x2F;bin&#x2F;python文件</p>\n<p><img src=\"/../../../../themes/yilia/source/img/experience/pycharm/link_server/10.png\"></p>\n<p> <img src=\"/img/experience/pycharm/link_server/10.png\"> </p>\n<p>点击右下角即可更改python解释器。</p>\n<p><img src=\"/../../../../themes/yilia/source/img/experience/pycharm/link_server/11.png\"></p>\n<p> <img src=\"/img/experience/pycharm/link_server/11.png\"> </p>"},{"title":"CDM论文讲解","date":"2024-01-25T16:00:00.000Z","comment":true,"toc":true,"_content":"\n#\n<!--more-->\n\n- 论文标题：[Cascaded diffusion models for high fidelity image generation](https://dl.acm.org/doi/abs/10.5555/3586589.3586636)\n\n- 来源：  JMLR 2022  \n- 贡献：\n  - Cascaded Diffusion Models(CDM)产生的高保真样本在FID评分和分类准确性评分方面优于BigGAN-deep和VQ-VAE-2。\n  - 为超分辨率模型引入了条件增强，并发现它对实现高样本保真度至关重要。\n\n\n\n# 1. 方法\n\n- 整体思想： Classifier Diffusion Models + SR3 + Tricks的串联模型，应用多个不同分辨率的扩散模型实现超分效果。 \n\n![](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\CDM\\1.png)\n![](img/deeplearning/paper/SR/CDM/1.png)\n![](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\CDM\\2.png)\n![](img/deeplearning/paper/SR/CDM/2.png)\n- SR3中也提到可以用级联的SR3做生成，本文在此基础上提出条件增强以提高生成质量\n\n# 1.1 条件增强\n\n- 截断条件采样：逆向过程中的中间图片Xt输入下一个超分模型（而不是XT，即逆向过程只做一部分）\n- 非截断条件增强：生成XT之后再施加高斯噪声后输入下一个模型\n\n\n\n# 2. 实验\n\n![](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\CDM\\3.png)\n\n![](img/deeplearning/paper/SR/CDM/3.png)\n\n- 效果比直接级联SR3好","source":"_posts/deeplearning/paper/SR/CDM.md","raw":"---\ntitle: CDM论文讲解\ndate: 2024-01-26 00:00:00\ntags: [深度学习,论文,超分]\ncategories: [深度学习]\ncomment: true\ntoc: true\n\n---\n\n#\n<!--more-->\n\n- 论文标题：[Cascaded diffusion models for high fidelity image generation](https://dl.acm.org/doi/abs/10.5555/3586589.3586636)\n\n- 来源：  JMLR 2022  \n- 贡献：\n  - Cascaded Diffusion Models(CDM)产生的高保真样本在FID评分和分类准确性评分方面优于BigGAN-deep和VQ-VAE-2。\n  - 为超分辨率模型引入了条件增强，并发现它对实现高样本保真度至关重要。\n\n\n\n# 1. 方法\n\n- 整体思想： Classifier Diffusion Models + SR3 + Tricks的串联模型，应用多个不同分辨率的扩散模型实现超分效果。 \n\n![](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\CDM\\1.png)\n![](img/deeplearning/paper/SR/CDM/1.png)\n![](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\CDM\\2.png)\n![](img/deeplearning/paper/SR/CDM/2.png)\n- SR3中也提到可以用级联的SR3做生成，本文在此基础上提出条件增强以提高生成质量\n\n# 1.1 条件增强\n\n- 截断条件采样：逆向过程中的中间图片Xt输入下一个超分模型（而不是XT，即逆向过程只做一部分）\n- 非截断条件增强：生成XT之后再施加高斯噪声后输入下一个模型\n\n\n\n# 2. 实验\n\n![](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\CDM\\3.png)\n\n![](img/deeplearning/paper/SR/CDM/3.png)\n\n- 效果比直接级联SR3好","slug":"deeplearning/paper/SR/CDM","published":1,"updated":"2024-01-30T15:36:04.579Z","_id":"clvulgd69002x5ovwad858886","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<ul>\n<li><p>论文标题：<a href=\"https://dl.acm.org/doi/abs/10.5555/3586589.3586636\">Cascaded diffusion models for high fidelity image generation</a></p>\n</li>\n<li><p>来源：  JMLR 2022  </p>\n</li>\n<li><p>贡献：</p>\n<ul>\n<li>Cascaded Diffusion Models(CDM)产生的高保真样本在FID评分和分类准确性评分方面优于BigGAN-deep和VQ-VAE-2。</li>\n<li>为超分辨率模型引入了条件增强，并发现它对实现高样本保真度至关重要。</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"1-方法\"><a href=\"#1-方法\" class=\"headerlink\" title=\"1. 方法\"></a>1. 方法</h1><ul>\n<li>整体思想： Classifier Diffusion Models + SR3 + Tricks的串联模型，应用多个不同分辨率的扩散模型实现超分效果。</li>\n</ul>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\CDM\\1.png\"><br><img src=\"/img/deeplearning/paper/SR/CDM/1.png\"><br><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\CDM\\2.png\"><br><img src=\"/img/deeplearning/paper/SR/CDM/2.png\"></p>\n<ul>\n<li>SR3中也提到可以用级联的SR3做生成，本文在此基础上提出条件增强以提高生成质量</li>\n</ul>\n<h1 id=\"1-1-条件增强\"><a href=\"#1-1-条件增强\" class=\"headerlink\" title=\"1.1 条件增强\"></a>1.1 条件增强</h1><ul>\n<li>截断条件采样：逆向过程中的中间图片Xt输入下一个超分模型（而不是XT，即逆向过程只做一部分）</li>\n<li>非截断条件增强：生成XT之后再施加高斯噪声后输入下一个模型</li>\n</ul>\n<h1 id=\"2-实验\"><a href=\"#2-实验\" class=\"headerlink\" title=\"2. 实验\"></a>2. 实验</h1><p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\CDM\\3.png\"></p>\n<p><img src=\"/img/deeplearning/paper/SR/CDM/3.png\"></p>\n<ul>\n<li>效果比直接级联SR3好</li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<ul>\n<li><p>论文标题：<a href=\"https://dl.acm.org/doi/abs/10.5555/3586589.3586636\">Cascaded diffusion models for high fidelity image generation</a></p>\n</li>\n<li><p>来源：  JMLR 2022  </p>\n</li>\n<li><p>贡献：</p>\n<ul>\n<li>Cascaded Diffusion Models(CDM)产生的高保真样本在FID评分和分类准确性评分方面优于BigGAN-deep和VQ-VAE-2。</li>\n<li>为超分辨率模型引入了条件增强，并发现它对实现高样本保真度至关重要。</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"1-方法\"><a href=\"#1-方法\" class=\"headerlink\" title=\"1. 方法\"></a>1. 方法</h1><ul>\n<li>整体思想： Classifier Diffusion Models + SR3 + Tricks的串联模型，应用多个不同分辨率的扩散模型实现超分效果。</li>\n</ul>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\CDM\\1.png\"><br><img src=\"/img/deeplearning/paper/SR/CDM/1.png\"><br><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\CDM\\2.png\"><br><img src=\"/img/deeplearning/paper/SR/CDM/2.png\"></p>\n<ul>\n<li>SR3中也提到可以用级联的SR3做生成，本文在此基础上提出条件增强以提高生成质量</li>\n</ul>\n<h1 id=\"1-1-条件增强\"><a href=\"#1-1-条件增强\" class=\"headerlink\" title=\"1.1 条件增强\"></a>1.1 条件增强</h1><ul>\n<li>截断条件采样：逆向过程中的中间图片Xt输入下一个超分模型（而不是XT，即逆向过程只做一部分）</li>\n<li>非截断条件增强：生成XT之后再施加高斯噪声后输入下一个模型</li>\n</ul>\n<h1 id=\"2-实验\"><a href=\"#2-实验\" class=\"headerlink\" title=\"2. 实验\"></a>2. 实验</h1><p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\CDM\\3.png\"></p>\n<p><img src=\"/img/deeplearning/paper/SR/CDM/3.png\"></p>\n<ul>\n<li>效果比直接级联SR3好</li>\n</ul>"},{"title":"Resshift论文讲解","date":"2024-01-26T16:00:00.000Z","comment":true,"toc":true,"_content":"\n#\n<!--more-->\n\n- 论文标题：[Resshift: Efficient diffusion model for image super-resolution by residual shifting](https://arxiv.org/abs/2307.12348)\n\n- 来源：NeurIPS. 2023 \n- 贡献：\n  - 提出了一种残差移动方法，加快了扩散模型推理速度，同时保留高性能。\n  - 提出一个噪声设计方案，能够有效控制扩散过程中的噪声强度和转换速度，也可有效控制保真度-真实性之间的权衡。  \n\n# 1. 问题\n\n- 基于diffusion的超分辨率采样速度慢，现有的加速采样技术不可避免地会在一定程度上牺牲性能，导致超分结果过于模糊。\n\n\n\n# 2. 方法\n\n","source":"_posts/deeplearning/paper/SR/Resshift.md","raw":"---\ntitle: Resshift论文讲解\ndate: 2024-01-27 00:00:00\ntags: [深度学习,论文,超分]\ncategories: [深度学习]\ncomment: true\ntoc: true\n\n\n\n---\n\n#\n<!--more-->\n\n- 论文标题：[Resshift: Efficient diffusion model for image super-resolution by residual shifting](https://arxiv.org/abs/2307.12348)\n\n- 来源：NeurIPS. 2023 \n- 贡献：\n  - 提出了一种残差移动方法，加快了扩散模型推理速度，同时保留高性能。\n  - 提出一个噪声设计方案，能够有效控制扩散过程中的噪声强度和转换速度，也可有效控制保真度-真实性之间的权衡。  \n\n# 1. 问题\n\n- 基于diffusion的超分辨率采样速度慢，现有的加速采样技术不可避免地会在一定程度上牺牲性能，导致超分结果过于模糊。\n\n\n\n# 2. 方法\n\n","slug":"deeplearning/paper/SR/Resshift","published":1,"updated":"2024-01-30T15:36:10.587Z","_id":"clvulgd6a00315ovw6j3r5f1e","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<ul>\n<li><p>论文标题：<a href=\"https://arxiv.org/abs/2307.12348\">Resshift: Efficient diffusion model for image super-resolution by residual shifting</a></p>\n</li>\n<li><p>来源：NeurIPS. 2023 </p>\n</li>\n<li><p>贡献：</p>\n<ul>\n<li>提出了一种残差移动方法，加快了扩散模型推理速度，同时保留高性能。</li>\n<li>提出一个噪声设计方案，能够有效控制扩散过程中的噪声强度和转换速度，也可有效控制保真度-真实性之间的权衡。</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"1-问题\"><a href=\"#1-问题\" class=\"headerlink\" title=\"1. 问题\"></a>1. 问题</h1><ul>\n<li>基于diffusion的超分辨率采样速度慢，现有的加速采样技术不可避免地会在一定程度上牺牲性能，导致超分结果过于模糊。</li>\n</ul>\n<h1 id=\"2-方法\"><a href=\"#2-方法\" class=\"headerlink\" title=\"2. 方法\"></a>2. 方法</h1>","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<ul>\n<li><p>论文标题：<a href=\"https://arxiv.org/abs/2307.12348\">Resshift: Efficient diffusion model for image super-resolution by residual shifting</a></p>\n</li>\n<li><p>来源：NeurIPS. 2023 </p>\n</li>\n<li><p>贡献：</p>\n<ul>\n<li>提出了一种残差移动方法，加快了扩散模型推理速度，同时保留高性能。</li>\n<li>提出一个噪声设计方案，能够有效控制扩散过程中的噪声强度和转换速度，也可有效控制保真度-真实性之间的权衡。</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"1-问题\"><a href=\"#1-问题\" class=\"headerlink\" title=\"1. 问题\"></a>1. 问题</h1><ul>\n<li>基于diffusion的超分辨率采样速度慢，现有的加速采样技术不可避免地会在一定程度上牺牲性能，导致超分结果过于模糊。</li>\n</ul>\n<h1 id=\"2-方法\"><a href=\"#2-方法\" class=\"headerlink\" title=\"2. 方法\"></a>2. 方法</h1>"},{"title":"SRDiff论文讲解","date":"2024-01-26T16:00:00.000Z","comment":true,"toc":true,"_content":"\n#\n<!--more-->\n\n- 论文标题：[Srdiff: Single image super-resolution with diffusion probabilistic models](https://www.sciencedirect.com/science/article/pii/S0925231222000522)\n\n- 来源： Neurocomputing 2022  \n- 贡献：\n  - 首次将diffusion用于图像超分，与SR3不同的是：SR3直接预测HR图像，而SRDiff预测LR和HR图像之间的差值，这使得DM能够专注于残差细节，加快收敛速度，稳定训练。其次，SRDiff将LR通过encoder后作为条件输入Unet。\n\n# 1. 问题\n\n- 以往的方案基于PSNR，GAN，flow，会出现过于平滑，模式崩溃，模型开销大等问题\n\n# 2. 解决方案\n\n![](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SRDiff\\1.png)\n\n![](img/deeplearning/paper/SR/SRDiff/1.png)\n\n- 将残差图像拿来做扩散\n- 将LR通过encoder后作为条件\n\n# 3. 实验\n\n ![](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SRDiff\\2.png) \n\n ![](img/deeplearning/paper/SR/SRDiff/2.png) \n\n ![](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SRDiff\\3.png) \n\n ![](img/deeplearning/paper/SR/SRDiff/3.png) ","source":"_posts/deeplearning/paper/SR/SRDiff.md","raw":"---\ntitle: SRDiff论文讲解\ndate: 2024-01-27 00:00:00\ntags: [深度学习,论文,超分]\ncategories: [深度学习]\ncomment: true\ntoc: true\n\n\n---\n\n#\n<!--more-->\n\n- 论文标题：[Srdiff: Single image super-resolution with diffusion probabilistic models](https://www.sciencedirect.com/science/article/pii/S0925231222000522)\n\n- 来源： Neurocomputing 2022  \n- 贡献：\n  - 首次将diffusion用于图像超分，与SR3不同的是：SR3直接预测HR图像，而SRDiff预测LR和HR图像之间的差值，这使得DM能够专注于残差细节，加快收敛速度，稳定训练。其次，SRDiff将LR通过encoder后作为条件输入Unet。\n\n# 1. 问题\n\n- 以往的方案基于PSNR，GAN，flow，会出现过于平滑，模式崩溃，模型开销大等问题\n\n# 2. 解决方案\n\n![](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SRDiff\\1.png)\n\n![](img/deeplearning/paper/SR/SRDiff/1.png)\n\n- 将残差图像拿来做扩散\n- 将LR通过encoder后作为条件\n\n# 3. 实验\n\n ![](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SRDiff\\2.png) \n\n ![](img/deeplearning/paper/SR/SRDiff/2.png) \n\n ![](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SRDiff\\3.png) \n\n ![](img/deeplearning/paper/SR/SRDiff/3.png) ","slug":"deeplearning/paper/SR/SRDiff","published":1,"updated":"2024-01-30T15:36:15.672Z","_id":"clvulgd6a00345ovwh29708fk","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<ul>\n<li><p>论文标题：<a href=\"https://www.sciencedirect.com/science/article/pii/S0925231222000522\">Srdiff: Single image super-resolution with diffusion probabilistic models</a></p>\n</li>\n<li><p>来源： Neurocomputing 2022  </p>\n</li>\n<li><p>贡献：</p>\n<ul>\n<li>首次将diffusion用于图像超分，与SR3不同的是：SR3直接预测HR图像，而SRDiff预测LR和HR图像之间的差值，这使得DM能够专注于残差细节，加快收敛速度，稳定训练。其次，SRDiff将LR通过encoder后作为条件输入Unet。</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"1-问题\"><a href=\"#1-问题\" class=\"headerlink\" title=\"1. 问题\"></a>1. 问题</h1><ul>\n<li>以往的方案基于PSNR，GAN，flow，会出现过于平滑，模式崩溃，模型开销大等问题</li>\n</ul>\n<h1 id=\"2-解决方案\"><a href=\"#2-解决方案\" class=\"headerlink\" title=\"2. 解决方案\"></a>2. 解决方案</h1><p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SRDiff\\1.png\"></p>\n<p><img src=\"/img/deeplearning/paper/SR/SRDiff/1.png\"></p>\n<ul>\n<li>将残差图像拿来做扩散</li>\n<li>将LR通过encoder后作为条件</li>\n</ul>\n<h1 id=\"3-实验\"><a href=\"#3-实验\" class=\"headerlink\" title=\"3. 实验\"></a>3. 实验</h1><p> <img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SRDiff\\2.png\"> </p>\n<p> <img src=\"/img/deeplearning/paper/SR/SRDiff/2.png\"> </p>\n<p> <img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SRDiff\\3.png\"> </p>\n<p> <img src=\"/img/deeplearning/paper/SR/SRDiff/3.png\"> </p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<ul>\n<li><p>论文标题：<a href=\"https://www.sciencedirect.com/science/article/pii/S0925231222000522\">Srdiff: Single image super-resolution with diffusion probabilistic models</a></p>\n</li>\n<li><p>来源： Neurocomputing 2022  </p>\n</li>\n<li><p>贡献：</p>\n<ul>\n<li>首次将diffusion用于图像超分，与SR3不同的是：SR3直接预测HR图像，而SRDiff预测LR和HR图像之间的差值，这使得DM能够专注于残差细节，加快收敛速度，稳定训练。其次，SRDiff将LR通过encoder后作为条件输入Unet。</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"1-问题\"><a href=\"#1-问题\" class=\"headerlink\" title=\"1. 问题\"></a>1. 问题</h1><ul>\n<li>以往的方案基于PSNR，GAN，flow，会出现过于平滑，模式崩溃，模型开销大等问题</li>\n</ul>\n<h1 id=\"2-解决方案\"><a href=\"#2-解决方案\" class=\"headerlink\" title=\"2. 解决方案\"></a>2. 解决方案</h1><p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SRDiff\\1.png\"></p>\n<p><img src=\"/img/deeplearning/paper/SR/SRDiff/1.png\"></p>\n<ul>\n<li>将残差图像拿来做扩散</li>\n<li>将LR通过encoder后作为条件</li>\n</ul>\n<h1 id=\"3-实验\"><a href=\"#3-实验\" class=\"headerlink\" title=\"3. 实验\"></a>3. 实验</h1><p> <img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SRDiff\\2.png\"> </p>\n<p> <img src=\"/img/deeplearning/paper/SR/SRDiff/2.png\"> </p>\n<p> <img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\SRDiff\\3.png\"> </p>\n<p> <img src=\"/img/deeplearning/paper/SR/SRDiff/3.png\"> </p>"},{"title":"HAT论文讲解","date":"2024-01-24T16:00:00.000Z","comment":false,"toc":true,"_content":"\n#\n<!--more-->\n\n- 论文标题：[Activating more pixels in image super-resolution transformer](https://scholar.google.com/scholar?cluster=4970683343699562565&hl=en&as_sdt=2005&sciodt=0,5)\n\n- 来源：CVPR  2023\n- 贡献：\n  -  在结构上，本文设计的HAT结合了通道注意力与自注意力，在以往Transformer结构的基础上进一步提升了模型利用输入信息的范围。同时设计了一个重叠交叉注意力模块，对Swin结构利用跨窗口信息的能力进行了有效增强。 \n  -  在预训练策略上，本文提出的在相同任务上做预训练的方法，使得模型的性能进一步增强。 \n  -  HAT大幅超越了当前超分方法的性能，这表明该任务或许远没有达到上限，可能依然还有很大的探索空间。 \n  \n  \n\n# 1. 问题\n\n \t本文首先对不同方法的LAM 结果进行了对比。LAM是一种为SR任务设计的归因方法，它能够显示模型在进行超分辨率重建的过程中哪些像素起到了作用。 \n\n​\t 如下图所示，LAM图中红色标记点表示：模型在重建左上图红框标记块时，对重建结果会产生影响的像素（LAM结果下面的值为DI值，它可以定量地反映被利用像素的范围。DI值越大，表示重建时利用的像素范围越大）。 \n\n![](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\1.png)\n\n![](img/deeplearning/paper/SR/HAT/1.png)\n\n​\t 一般来说，被利用像素的范围越大，重建的效果往往越好，该结论在对比基于CNN的方法EDSR与RCAN时可以得到明显体现。然而，当对比RCAN与基于Transformer的SwinIR方法时，却出现了结论相反的现象： SwinIR取得了更高的PSNR/SSIM，但相比RCAN并没有使用更大范围的像素信息，并且由于其有限的信息使用范围，在蓝色框区域恢复出了错误的纹理。这与以往普遍认为Transformer结构是通过更好地利用long-range信息来取得性能优势的直觉是相悖的。 \n\n​\t这些现象说明：\n\n- SwinIR结构拥有更强的局部表征能力，能够使用更少的信息来达到更高的性能；\n- SwinIR依然有较大提升空间，如果更多的像素能够被利用，那么应该会取得更大的性能提升。\n\n​\t除此之外，本文发现在SwinIR网络前几层产生的中间特征会出现明显的块状效应。这是由于模型在计算自注意力时的窗口划分导致的，因此本文认为现有结构进行跨窗口信息交互的方式也应该被改进。\n\n\n\n# 2. 解决方案\n\n## 2.1  **网络结构设计** \n\n​\tHAT的整体架构采用了与SwinIR相似的Residual in Residual结构，如下图3所示。主要的不同之处在于混合注意力模块（Hybrid Attention Block， HAB）与重叠的交叉注意力模块（Overlapping Cross-Attention Block， OCAB）的设计。\n\n​\t其中对于HAB，本文采用了并联的方式来结合通道注意力和自注意力。通道注意力能够利用全局信息；自注意力具有强大的表征能力。HAB模块的目的在于能够同时结合这两者的优势。\n\n![](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\2.png)\n\n ![](img/deeplearning/paper/SR/HAT/2.png) \n\n​\t对于OCAB的设计，本文使用了一种重叠的窗口划分机制，如下图所示。\n\n![](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\3.png)\n\n ![](img/deeplearning/paper/SR/HAT/3.png) \n\n​\t相对于原始基于窗口的self-attention中Q、K和V来自于同一个窗口特征，OCA中的K/V来自更大的窗口特征，这允许attention能够被跨窗口地计算，以增强相邻窗口间信息的交互。\n\n\n\n## 2.2  **预训练策略** \n\n​\t本文提出了一种直接使用相同的任务，但是使用更大的数据集（比如ImageNet）进行预训练的策略。\n\n​\t相比于之前用于超分任务的预训练方案，该策略更简单，但却能带来更多的性能增益。\n\n\n\n# 3. 实验\n\n## 3.1 更大的窗口尺寸\n\n-  通过对于不同窗口尺寸的定量和定性比较，可以看到16窗口尺寸有明显提升，HAT使用窗口尺寸16作为默认设置。 \n\n![](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\4.png)\n![](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\5.png)\n\n ![](img/deeplearning/paper/SR/HAT/4.png) \n ![](img/deeplearning/paper/SR/HAT/5.png) \n\n\n\n## 3.2 消融实验\n\n- 本文提供了消融实验来验证CAB和OCAB的影响，定量和定性分析结果如下表2和图6所示。\n\n![](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\6.png)\n\n![](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\7.png)![](img/deeplearning/paper/SR/HAT/6.png) \n![](img/deeplearning/paper/SR/HAT/7.png) \n\n- 可以看到文中所提的两个模块在定量指标上均带来了不小的提升，在LAM和视觉效果上相对于Baseline也具有明显改善。\n\n\n\n## 3.3 主实验结果\n\n- 在基准数据集上进行定量对比实验的结果如下表所示。\n\n![](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\8.png)\n![](img/deeplearning/paper/SR/HAT/8.png) \n\n​\t从定量指标上看，没有使用ImageNet预训练策略的HAT的性能已经明显超越SwinIR，甚至在很多情况下超越了经过ImageNet预训练的EDT。\n\n​\t使用了ImageNet预训练的HAT则更是大幅超越了SwinIR与EDT的性能，在2倍超分的Urban100数据集上，超越SwinIR 1dB。\n\n​\t更大容量的模型HAT-L带来了更大的性能提升，最高在2倍超分的Urban100数据集上超越SwinIR达1.28dB，超越EDT达0.85dB。\n\n-  视觉效果对比如下图所示。可以看出HAT能够恢复更多更清晰的细节，由于对于重复纹理较多的情况，HAT具有显著优势。在文字的恢复上，HAT相比其他方法也能够恢复出更清晰的文字边缘。 \n\n![](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\9.png)\n![](img/deeplearning/paper/SR/HAT/8.png) \n\n\n\n## 3.4 **预训练策略对比** \n\n​\t 本文对于不同的预训练策略进行了对比，如下表所示。相对于EDT 提出使用相关任务进行预训练的策略，本文提出的使用相同任务进行预训练的策略无论是在预训练阶段还是微调后的结果，性能都要更优。 \n\n![](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\11.png)\n![](img/deeplearning/paper/SR/HAT/11.png) \n\n\n\n\n【转自：https://mmlab.siat.ac.cn/research/1/area?id=r2022052401&menu=1】\n\n","source":"_posts/deeplearning/paper/SR/HAT.md","raw":"---\ntitle: HAT论文讲解\ndate: 2024-01-25 00:00:00\ntags: [深度学习,论文,超分]\ncategories: [深度学习]\ncomment: false\ntoc: true\n\n\n---\n\n#\n<!--more-->\n\n- 论文标题：[Activating more pixels in image super-resolution transformer](https://scholar.google.com/scholar?cluster=4970683343699562565&hl=en&as_sdt=2005&sciodt=0,5)\n\n- 来源：CVPR  2023\n- 贡献：\n  -  在结构上，本文设计的HAT结合了通道注意力与自注意力，在以往Transformer结构的基础上进一步提升了模型利用输入信息的范围。同时设计了一个重叠交叉注意力模块，对Swin结构利用跨窗口信息的能力进行了有效增强。 \n  -  在预训练策略上，本文提出的在相同任务上做预训练的方法，使得模型的性能进一步增强。 \n  -  HAT大幅超越了当前超分方法的性能，这表明该任务或许远没有达到上限，可能依然还有很大的探索空间。 \n  \n  \n\n# 1. 问题\n\n \t本文首先对不同方法的LAM 结果进行了对比。LAM是一种为SR任务设计的归因方法，它能够显示模型在进行超分辨率重建的过程中哪些像素起到了作用。 \n\n​\t 如下图所示，LAM图中红色标记点表示：模型在重建左上图红框标记块时，对重建结果会产生影响的像素（LAM结果下面的值为DI值，它可以定量地反映被利用像素的范围。DI值越大，表示重建时利用的像素范围越大）。 \n\n![](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\1.png)\n\n![](img/deeplearning/paper/SR/HAT/1.png)\n\n​\t 一般来说，被利用像素的范围越大，重建的效果往往越好，该结论在对比基于CNN的方法EDSR与RCAN时可以得到明显体现。然而，当对比RCAN与基于Transformer的SwinIR方法时，却出现了结论相反的现象： SwinIR取得了更高的PSNR/SSIM，但相比RCAN并没有使用更大范围的像素信息，并且由于其有限的信息使用范围，在蓝色框区域恢复出了错误的纹理。这与以往普遍认为Transformer结构是通过更好地利用long-range信息来取得性能优势的直觉是相悖的。 \n\n​\t这些现象说明：\n\n- SwinIR结构拥有更强的局部表征能力，能够使用更少的信息来达到更高的性能；\n- SwinIR依然有较大提升空间，如果更多的像素能够被利用，那么应该会取得更大的性能提升。\n\n​\t除此之外，本文发现在SwinIR网络前几层产生的中间特征会出现明显的块状效应。这是由于模型在计算自注意力时的窗口划分导致的，因此本文认为现有结构进行跨窗口信息交互的方式也应该被改进。\n\n\n\n# 2. 解决方案\n\n## 2.1  **网络结构设计** \n\n​\tHAT的整体架构采用了与SwinIR相似的Residual in Residual结构，如下图3所示。主要的不同之处在于混合注意力模块（Hybrid Attention Block， HAB）与重叠的交叉注意力模块（Overlapping Cross-Attention Block， OCAB）的设计。\n\n​\t其中对于HAB，本文采用了并联的方式来结合通道注意力和自注意力。通道注意力能够利用全局信息；自注意力具有强大的表征能力。HAB模块的目的在于能够同时结合这两者的优势。\n\n![](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\2.png)\n\n ![](img/deeplearning/paper/SR/HAT/2.png) \n\n​\t对于OCAB的设计，本文使用了一种重叠的窗口划分机制，如下图所示。\n\n![](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\3.png)\n\n ![](img/deeplearning/paper/SR/HAT/3.png) \n\n​\t相对于原始基于窗口的self-attention中Q、K和V来自于同一个窗口特征，OCA中的K/V来自更大的窗口特征，这允许attention能够被跨窗口地计算，以增强相邻窗口间信息的交互。\n\n\n\n## 2.2  **预训练策略** \n\n​\t本文提出了一种直接使用相同的任务，但是使用更大的数据集（比如ImageNet）进行预训练的策略。\n\n​\t相比于之前用于超分任务的预训练方案，该策略更简单，但却能带来更多的性能增益。\n\n\n\n# 3. 实验\n\n## 3.1 更大的窗口尺寸\n\n-  通过对于不同窗口尺寸的定量和定性比较，可以看到16窗口尺寸有明显提升，HAT使用窗口尺寸16作为默认设置。 \n\n![](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\4.png)\n![](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\5.png)\n\n ![](img/deeplearning/paper/SR/HAT/4.png) \n ![](img/deeplearning/paper/SR/HAT/5.png) \n\n\n\n## 3.2 消融实验\n\n- 本文提供了消融实验来验证CAB和OCAB的影响，定量和定性分析结果如下表2和图6所示。\n\n![](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\6.png)\n\n![](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\7.png)![](img/deeplearning/paper/SR/HAT/6.png) \n![](img/deeplearning/paper/SR/HAT/7.png) \n\n- 可以看到文中所提的两个模块在定量指标上均带来了不小的提升，在LAM和视觉效果上相对于Baseline也具有明显改善。\n\n\n\n## 3.3 主实验结果\n\n- 在基准数据集上进行定量对比实验的结果如下表所示。\n\n![](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\8.png)\n![](img/deeplearning/paper/SR/HAT/8.png) \n\n​\t从定量指标上看，没有使用ImageNet预训练策略的HAT的性能已经明显超越SwinIR，甚至在很多情况下超越了经过ImageNet预训练的EDT。\n\n​\t使用了ImageNet预训练的HAT则更是大幅超越了SwinIR与EDT的性能，在2倍超分的Urban100数据集上，超越SwinIR 1dB。\n\n​\t更大容量的模型HAT-L带来了更大的性能提升，最高在2倍超分的Urban100数据集上超越SwinIR达1.28dB，超越EDT达0.85dB。\n\n-  视觉效果对比如下图所示。可以看出HAT能够恢复更多更清晰的细节，由于对于重复纹理较多的情况，HAT具有显著优势。在文字的恢复上，HAT相比其他方法也能够恢复出更清晰的文字边缘。 \n\n![](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\9.png)\n![](img/deeplearning/paper/SR/HAT/8.png) \n\n\n\n## 3.4 **预训练策略对比** \n\n​\t 本文对于不同的预训练策略进行了对比，如下表所示。相对于EDT 提出使用相关任务进行预训练的策略，本文提出的使用相同任务进行预训练的策略无论是在预训练阶段还是微调后的结果，性能都要更优。 \n\n![](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\11.png)\n![](img/deeplearning/paper/SR/HAT/11.png) \n\n\n\n\n【转自：https://mmlab.siat.ac.cn/research/1/area?id=r2022052401&menu=1】\n\n","slug":"deeplearning/paper/SR/HAT","published":1,"updated":"2024-01-26T11:15:23.399Z","comments":1,"layout":"post","photos":[],"_id":"clvulgd6b00385ovwfka40h8y","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<ul>\n<li><p>论文标题：<a href=\"https://scholar.google.com/scholar?cluster=4970683343699562565&hl=en&as_sdt=2005&sciodt=0,5\">Activating more pixels in image super-resolution transformer</a></p>\n</li>\n<li><p>来源：CVPR  2023</p>\n</li>\n<li><p>贡献：</p>\n<ul>\n<li>在结构上，本文设计的HAT结合了通道注意力与自注意力，在以往Transformer结构的基础上进一步提升了模型利用输入信息的范围。同时设计了一个重叠交叉注意力模块，对Swin结构利用跨窗口信息的能力进行了有效增强。 </li>\n<li>在预训练策略上，本文提出的在相同任务上做预训练的方法，使得模型的性能进一步增强。 </li>\n<li>HAT大幅超越了当前超分方法的性能，这表明该任务或许远没有达到上限，可能依然还有很大的探索空间。</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"1-问题\"><a href=\"#1-问题\" class=\"headerlink\" title=\"1. 问题\"></a>1. 问题</h1><pre><code> 本文首先对不同方法的LAM 结果进行了对比。LAM是一种为SR任务设计的归因方法，它能够显示模型在进行超分辨率重建的过程中哪些像素起到了作用。 \n</code></pre>\n<p>​\t 如下图所示，LAM图中红色标记点表示：模型在重建左上图红框标记块时，对重建结果会产生影响的像素（LAM结果下面的值为DI值，它可以定量地反映被利用像素的范围。DI值越大，表示重建时利用的像素范围越大）。 </p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\1.png\"></p>\n<p><img src=\"/img/deeplearning/paper/SR/HAT/1.png\"></p>\n<p>​\t 一般来说，被利用像素的范围越大，重建的效果往往越好，该结论在对比基于CNN的方法EDSR与RCAN时可以得到明显体现。然而，当对比RCAN与基于Transformer的SwinIR方法时，却出现了结论相反的现象： SwinIR取得了更高的PSNR&#x2F;SSIM，但相比RCAN并没有使用更大范围的像素信息，并且由于其有限的信息使用范围，在蓝色框区域恢复出了错误的纹理。这与以往普遍认为Transformer结构是通过更好地利用long-range信息来取得性能优势的直觉是相悖的。 </p>\n<p>​\t这些现象说明：</p>\n<ul>\n<li>SwinIR结构拥有更强的局部表征能力，能够使用更少的信息来达到更高的性能；</li>\n<li>SwinIR依然有较大提升空间，如果更多的像素能够被利用，那么应该会取得更大的性能提升。</li>\n</ul>\n<p>​\t除此之外，本文发现在SwinIR网络前几层产生的中间特征会出现明显的块状效应。这是由于模型在计算自注意力时的窗口划分导致的，因此本文认为现有结构进行跨窗口信息交互的方式也应该被改进。</p>\n<h1 id=\"2-解决方案\"><a href=\"#2-解决方案\" class=\"headerlink\" title=\"2. 解决方案\"></a>2. 解决方案</h1><h2 id=\"2-1-网络结构设计\"><a href=\"#2-1-网络结构设计\" class=\"headerlink\" title=\"2.1  网络结构设计\"></a>2.1  <strong>网络结构设计</strong></h2><p>​\tHAT的整体架构采用了与SwinIR相似的Residual in Residual结构，如下图3所示。主要的不同之处在于混合注意力模块（Hybrid Attention Block， HAB）与重叠的交叉注意力模块（Overlapping Cross-Attention Block， OCAB）的设计。</p>\n<p>​\t其中对于HAB，本文采用了并联的方式来结合通道注意力和自注意力。通道注意力能够利用全局信息；自注意力具有强大的表征能力。HAB模块的目的在于能够同时结合这两者的优势。</p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\2.png\"></p>\n<p> <img src=\"/img/deeplearning/paper/SR/HAT/2.png\"> </p>\n<p>​\t对于OCAB的设计，本文使用了一种重叠的窗口划分机制，如下图所示。</p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\3.png\"></p>\n<p> <img src=\"/img/deeplearning/paper/SR/HAT/3.png\"> </p>\n<p>​\t相对于原始基于窗口的self-attention中Q、K和V来自于同一个窗口特征，OCA中的K&#x2F;V来自更大的窗口特征，这允许attention能够被跨窗口地计算，以增强相邻窗口间信息的交互。</p>\n<h2 id=\"2-2-预训练策略\"><a href=\"#2-2-预训练策略\" class=\"headerlink\" title=\"2.2  预训练策略\"></a>2.2  <strong>预训练策略</strong></h2><p>​\t本文提出了一种直接使用相同的任务，但是使用更大的数据集（比如ImageNet）进行预训练的策略。</p>\n<p>​\t相比于之前用于超分任务的预训练方案，该策略更简单，但却能带来更多的性能增益。</p>\n<h1 id=\"3-实验\"><a href=\"#3-实验\" class=\"headerlink\" title=\"3. 实验\"></a>3. 实验</h1><h2 id=\"3-1-更大的窗口尺寸\"><a href=\"#3-1-更大的窗口尺寸\" class=\"headerlink\" title=\"3.1 更大的窗口尺寸\"></a>3.1 更大的窗口尺寸</h2><ul>\n<li>通过对于不同窗口尺寸的定量和定性比较，可以看到16窗口尺寸有明显提升，HAT使用窗口尺寸16作为默认设置。</li>\n</ul>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\4.png\"><br><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\5.png\"></p>\n<p> <img src=\"/img/deeplearning/paper/SR/HAT/4.png\"><br> <img src=\"/img/deeplearning/paper/SR/HAT/5.png\"> </p>\n<h2 id=\"3-2-消融实验\"><a href=\"#3-2-消融实验\" class=\"headerlink\" title=\"3.2 消融实验\"></a>3.2 消融实验</h2><ul>\n<li>本文提供了消融实验来验证CAB和OCAB的影响，定量和定性分析结果如下表2和图6所示。</li>\n</ul>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\6.png\"></p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\7.png\"><img src=\"/img/deeplearning/paper/SR/HAT/6.png\"><br><img src=\"/img/deeplearning/paper/SR/HAT/7.png\"> </p>\n<ul>\n<li>可以看到文中所提的两个模块在定量指标上均带来了不小的提升，在LAM和视觉效果上相对于Baseline也具有明显改善。</li>\n</ul>\n<h2 id=\"3-3-主实验结果\"><a href=\"#3-3-主实验结果\" class=\"headerlink\" title=\"3.3 主实验结果\"></a>3.3 主实验结果</h2><ul>\n<li>在基准数据集上进行定量对比实验的结果如下表所示。</li>\n</ul>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\8.png\"><br><img src=\"/img/deeplearning/paper/SR/HAT/8.png\"> </p>\n<p>​\t从定量指标上看，没有使用ImageNet预训练策略的HAT的性能已经明显超越SwinIR，甚至在很多情况下超越了经过ImageNet预训练的EDT。</p>\n<p>​\t使用了ImageNet预训练的HAT则更是大幅超越了SwinIR与EDT的性能，在2倍超分的Urban100数据集上，超越SwinIR 1dB。</p>\n<p>​\t更大容量的模型HAT-L带来了更大的性能提升，最高在2倍超分的Urban100数据集上超越SwinIR达1.28dB，超越EDT达0.85dB。</p>\n<ul>\n<li>视觉效果对比如下图所示。可以看出HAT能够恢复更多更清晰的细节，由于对于重复纹理较多的情况，HAT具有显著优势。在文字的恢复上，HAT相比其他方法也能够恢复出更清晰的文字边缘。</li>\n</ul>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\9.png\"><br><img src=\"/img/deeplearning/paper/SR/HAT/8.png\"> </p>\n<h2 id=\"3-4-预训练策略对比\"><a href=\"#3-4-预训练策略对比\" class=\"headerlink\" title=\"3.4 预训练策略对比\"></a>3.4 <strong>预训练策略对比</strong></h2><p>​\t 本文对于不同的预训练策略进行了对比，如下表所示。相对于EDT 提出使用相关任务进行预训练的策略，本文提出的使用相同任务进行预训练的策略无论是在预训练阶段还是微调后的结果，性能都要更优。 </p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\11.png\"><br><img src=\"/img/deeplearning/paper/SR/HAT/11.png\"> </p>\n<p>【转自：<a href=\"https://mmlab.siat.ac.cn/research/1/area?id=r2022052401&menu=1%E3%80%91\">https://mmlab.siat.ac.cn/research/1/area?id=r2022052401&amp;menu=1】</a></p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<ul>\n<li><p>论文标题：<a href=\"https://scholar.google.com/scholar?cluster=4970683343699562565&hl=en&as_sdt=2005&sciodt=0,5\">Activating more pixels in image super-resolution transformer</a></p>\n</li>\n<li><p>来源：CVPR  2023</p>\n</li>\n<li><p>贡献：</p>\n<ul>\n<li>在结构上，本文设计的HAT结合了通道注意力与自注意力，在以往Transformer结构的基础上进一步提升了模型利用输入信息的范围。同时设计了一个重叠交叉注意力模块，对Swin结构利用跨窗口信息的能力进行了有效增强。 </li>\n<li>在预训练策略上，本文提出的在相同任务上做预训练的方法，使得模型的性能进一步增强。 </li>\n<li>HAT大幅超越了当前超分方法的性能，这表明该任务或许远没有达到上限，可能依然还有很大的探索空间。</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"1-问题\"><a href=\"#1-问题\" class=\"headerlink\" title=\"1. 问题\"></a>1. 问题</h1><pre><code> 本文首先对不同方法的LAM 结果进行了对比。LAM是一种为SR任务设计的归因方法，它能够显示模型在进行超分辨率重建的过程中哪些像素起到了作用。 \n</code></pre>\n<p>​\t 如下图所示，LAM图中红色标记点表示：模型在重建左上图红框标记块时，对重建结果会产生影响的像素（LAM结果下面的值为DI值，它可以定量地反映被利用像素的范围。DI值越大，表示重建时利用的像素范围越大）。 </p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\1.png\"></p>\n<p><img src=\"/img/deeplearning/paper/SR/HAT/1.png\"></p>\n<p>​\t 一般来说，被利用像素的范围越大，重建的效果往往越好，该结论在对比基于CNN的方法EDSR与RCAN时可以得到明显体现。然而，当对比RCAN与基于Transformer的SwinIR方法时，却出现了结论相反的现象： SwinIR取得了更高的PSNR&#x2F;SSIM，但相比RCAN并没有使用更大范围的像素信息，并且由于其有限的信息使用范围，在蓝色框区域恢复出了错误的纹理。这与以往普遍认为Transformer结构是通过更好地利用long-range信息来取得性能优势的直觉是相悖的。 </p>\n<p>​\t这些现象说明：</p>\n<ul>\n<li>SwinIR结构拥有更强的局部表征能力，能够使用更少的信息来达到更高的性能；</li>\n<li>SwinIR依然有较大提升空间，如果更多的像素能够被利用，那么应该会取得更大的性能提升。</li>\n</ul>\n<p>​\t除此之外，本文发现在SwinIR网络前几层产生的中间特征会出现明显的块状效应。这是由于模型在计算自注意力时的窗口划分导致的，因此本文认为现有结构进行跨窗口信息交互的方式也应该被改进。</p>\n<h1 id=\"2-解决方案\"><a href=\"#2-解决方案\" class=\"headerlink\" title=\"2. 解决方案\"></a>2. 解决方案</h1><h2 id=\"2-1-网络结构设计\"><a href=\"#2-1-网络结构设计\" class=\"headerlink\" title=\"2.1  网络结构设计\"></a>2.1  <strong>网络结构设计</strong></h2><p>​\tHAT的整体架构采用了与SwinIR相似的Residual in Residual结构，如下图3所示。主要的不同之处在于混合注意力模块（Hybrid Attention Block， HAB）与重叠的交叉注意力模块（Overlapping Cross-Attention Block， OCAB）的设计。</p>\n<p>​\t其中对于HAB，本文采用了并联的方式来结合通道注意力和自注意力。通道注意力能够利用全局信息；自注意力具有强大的表征能力。HAB模块的目的在于能够同时结合这两者的优势。</p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\2.png\"></p>\n<p> <img src=\"/img/deeplearning/paper/SR/HAT/2.png\"> </p>\n<p>​\t对于OCAB的设计，本文使用了一种重叠的窗口划分机制，如下图所示。</p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\3.png\"></p>\n<p> <img src=\"/img/deeplearning/paper/SR/HAT/3.png\"> </p>\n<p>​\t相对于原始基于窗口的self-attention中Q、K和V来自于同一个窗口特征，OCA中的K&#x2F;V来自更大的窗口特征，这允许attention能够被跨窗口地计算，以增强相邻窗口间信息的交互。</p>\n<h2 id=\"2-2-预训练策略\"><a href=\"#2-2-预训练策略\" class=\"headerlink\" title=\"2.2  预训练策略\"></a>2.2  <strong>预训练策略</strong></h2><p>​\t本文提出了一种直接使用相同的任务，但是使用更大的数据集（比如ImageNet）进行预训练的策略。</p>\n<p>​\t相比于之前用于超分任务的预训练方案，该策略更简单，但却能带来更多的性能增益。</p>\n<h1 id=\"3-实验\"><a href=\"#3-实验\" class=\"headerlink\" title=\"3. 实验\"></a>3. 实验</h1><h2 id=\"3-1-更大的窗口尺寸\"><a href=\"#3-1-更大的窗口尺寸\" class=\"headerlink\" title=\"3.1 更大的窗口尺寸\"></a>3.1 更大的窗口尺寸</h2><ul>\n<li>通过对于不同窗口尺寸的定量和定性比较，可以看到16窗口尺寸有明显提升，HAT使用窗口尺寸16作为默认设置。</li>\n</ul>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\4.png\"><br><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\5.png\"></p>\n<p> <img src=\"/img/deeplearning/paper/SR/HAT/4.png\"><br> <img src=\"/img/deeplearning/paper/SR/HAT/5.png\"> </p>\n<h2 id=\"3-2-消融实验\"><a href=\"#3-2-消融实验\" class=\"headerlink\" title=\"3.2 消融实验\"></a>3.2 消融实验</h2><ul>\n<li>本文提供了消融实验来验证CAB和OCAB的影响，定量和定性分析结果如下表2和图6所示。</li>\n</ul>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\6.png\"></p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\7.png\"><img src=\"/img/deeplearning/paper/SR/HAT/6.png\"><br><img src=\"/img/deeplearning/paper/SR/HAT/7.png\"> </p>\n<ul>\n<li>可以看到文中所提的两个模块在定量指标上均带来了不小的提升，在LAM和视觉效果上相对于Baseline也具有明显改善。</li>\n</ul>\n<h2 id=\"3-3-主实验结果\"><a href=\"#3-3-主实验结果\" class=\"headerlink\" title=\"3.3 主实验结果\"></a>3.3 主实验结果</h2><ul>\n<li>在基准数据集上进行定量对比实验的结果如下表所示。</li>\n</ul>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\8.png\"><br><img src=\"/img/deeplearning/paper/SR/HAT/8.png\"> </p>\n<p>​\t从定量指标上看，没有使用ImageNet预训练策略的HAT的性能已经明显超越SwinIR，甚至在很多情况下超越了经过ImageNet预训练的EDT。</p>\n<p>​\t使用了ImageNet预训练的HAT则更是大幅超越了SwinIR与EDT的性能，在2倍超分的Urban100数据集上，超越SwinIR 1dB。</p>\n<p>​\t更大容量的模型HAT-L带来了更大的性能提升，最高在2倍超分的Urban100数据集上超越SwinIR达1.28dB，超越EDT达0.85dB。</p>\n<ul>\n<li>视觉效果对比如下图所示。可以看出HAT能够恢复更多更清晰的细节，由于对于重复纹理较多的情况，HAT具有显著优势。在文字的恢复上，HAT相比其他方法也能够恢复出更清晰的文字边缘。</li>\n</ul>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\9.png\"><br><img src=\"/img/deeplearning/paper/SR/HAT/8.png\"> </p>\n<h2 id=\"3-4-预训练策略对比\"><a href=\"#3-4-预训练策略对比\" class=\"headerlink\" title=\"3.4 预训练策略对比\"></a>3.4 <strong>预训练策略对比</strong></h2><p>​\t 本文对于不同的预训练策略进行了对比，如下表所示。相对于EDT 提出使用相关任务进行预训练的策略，本文提出的使用相同任务进行预训练的策略无论是在预训练阶段还是微调后的结果，性能都要更优。 </p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\paper\\SR\\HAT\\11.png\"><br><img src=\"/img/deeplearning/paper/SR/HAT/11.png\"> </p>\n<p>【转自：<a href=\"https://mmlab.siat.ac.cn/research/1/area?id=r2022052401&menu=1%E3%80%91\">https://mmlab.siat.ac.cn/research/1/area?id=r2022052401&amp;menu=1】</a></p>"},{"title":"SwinIR论文讲解","date":"2024-01-24T16:00:00.000Z","comment":true,"toc":true,"_content":"\n#\n<!--more-->\n\n- 论文标题：[**Swinir**: Image restoration using swin transformer](https://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.html)\n\n- 来源：ICCV  2021\n- 贡献：\n  - 提出基于Swin Transformer的图像修复模型：SwinIR\n  - 通过大量实验表明SwinIR在SR、denoising、JPEG block removal上都是sota\n\n# 1. 背景\n\n## 1-1 图像修复\n\n- 从一个劣质图片修复成一个干净的图片，如：超分、降噪、JPEG压缩区块失真![](D:/blog/themes/yilia/source/img/deeplearning/paper/SR/SwinIR/1.png)\n\n  \n\n![](img/deeplearning/paper/SR/SwinIR/1.png)\n\n\n\n# 2. 相关工作\n\n## 2.1 NLP(Neural Language Processing)\n\n- 可以看成一个翻译任务（Seq2Seq)\n\n- 翻译的难点：一词多义 ->根据上下文的信息\n\n- 常用模型：RNN、LSTM ->全局信息损失\n\n## 2.2 Self-Attention\n\n- Attention($Q$ ,$K$,$V$) = softmax$(\\frac{QK^T}{\\sqrt{d_k}})V$\n\n- 计算一个单词与每一个单词的关系（包括自己）\n\n  ![](D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/2.png)\n\n![](img/deeplearning/paper/SR/SwinIR/2.png)\n\n## 2.3 Multi-head Self-Attention (MSA)\n\n- Ensemble 的self attention，集成多个attention再平均\n\n## 2.4 Transformer on NLP\n\n![](D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/3.png)\n\n![](img/deeplearning/paper/SR/SwinIR/3.png)\n\n- Transformer在NLP领域效果很好，但是在CV上不太好。\n\n## 2.5 Transformer on CV\n\n- 第一篇将Transformer应用到CV的是：ViT\n\n  - Transformer接收的是一个序列 -> 将图片分割成多个patch\n\n    ![](D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/4.png)\n\n    ![](img/deeplearning/paper/SR/SwinIR/4.png)\n\n  - 可用于图片分割、图片分类、物体检测\n\n## 2.6 Swin Transformer\n\n- 虽然ViT效果还算不错，但是仍然无法超过一些主流的卷积网络，主要原因是Transformer一开始是针对NLP的，NLP是一个一维的问题，图片是高维的问题。\n\n- 直接分块然后独立地丢入Transformer后会有一个问题：最后得出的结果会有边界的问题（各个分块相交的地方）\n\n- Swin Transformer的一个模块包括两个层：\n  \n  ​\t![](D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/5.png)\n  \n  ​\t![](img/deeplearning/paper/SR/SwinIR/5.png)\n  \n  - W-MSA：在原来的小分割patch基础上有一个local的概念，先在local上先做。（可以想象成背景有一个大网格）\n  - SW-MSA：背景表格不动，将图片往左上或右下做一个shift，这样分割出来的local跟第一层的分割效果就不同了，这样可以解决边界问题。\n  \n- 效果非常好，基本可以取代卷积运算\n\n  \n\n# 3 SwinIR\n\n## 3.1 结构\n\n![](D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/6.png)\n\n![](img/deeplearning/paper/SR/SwinIR/6.png)\n\n- 浅层特征提取层 -> 深层特征提取层 -> 重建层\n\n  - Shallow Feature Extraction：用一个3*3卷积获得低频信息（颜色或者纹理等基础特性）\n  - Deep Feature Extraction：由于Swin Transformer原本使用在高阶的图像处理任务上，这里让他应用到低阶的任务上，效果非常好。\n  - Reconstruction：\n    - SR：用sub-pixel做up-sample\n    - Denoising/JPEG：用3*3卷积\n\n  - Loss function:\n    - SR：L1 loss -> $l = || I_{RHQ}-I_{HQ}||_1$\n    - Denoising & JPEG：Charbonnier loss -> $l=\\sqrt{||I_{PHQ}-I_{HQ}||^2 + \\epsilon^2}$\n\n\n\n# 4. 实验\n\n## 4.1 Ablation Studies\n\n![](D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/7.png)\n\n![](img/deeplearning/paper/SR/SwinIR/7.png)\n\n- Super Resolution\n\n  - Classical SR\n\n    ![](D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/8.png)\n\n    ![](img/deeplearning/paper/SR/SwinIR/8.png)\n\n  - Lightweight image SR(轻量的网络架构)\n\n    ![](D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/9.png)\n\n    ![](img/deeplearning/paper/SR/SwinIR/9.png)\n\n  - Real-world image SR\n\n    ![](D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/10.png)\n\n    ![](img/deeplearning/paper/SR/SwinIR/10.png)\n\n- JPEG block removal\n\n  ![](D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/11.png)\n\n  ![](img/deeplearning/paper/SR/SwinIR/11.png)\n\n- Denoising\n\n  - Grayscale image denoising\n\n  - Color image denoising\n\n    ![](D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/12.png)\n\n    ![](img/deeplearning/paper/SR/SwinIR/12.png)\n\n    ![](D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/13.png) \n\n     ![](img/deeplearning/paper/SR/SwinIR/13.png) \n\n # 5 结论\n\n- 提出基于Swin Transformer的图像修复模型：SwinIR\n- 通过大量实验表明SwinIR在SR、denoising、JPEG block removal上都是sota","source":"_posts/deeplearning/paper/SR/SwinIR.md","raw":"---\ntitle: SwinIR论文讲解\ndate: 2024-01-25 00:00:00\ntags: [深度学习,论文,超分]\ncategories: [深度学习]\ncomment: true\ntoc: true\n\n---\n\n#\n<!--more-->\n\n- 论文标题：[**Swinir**: Image restoration using swin transformer](https://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.html)\n\n- 来源：ICCV  2021\n- 贡献：\n  - 提出基于Swin Transformer的图像修复模型：SwinIR\n  - 通过大量实验表明SwinIR在SR、denoising、JPEG block removal上都是sota\n\n# 1. 背景\n\n## 1-1 图像修复\n\n- 从一个劣质图片修复成一个干净的图片，如：超分、降噪、JPEG压缩区块失真![](D:/blog/themes/yilia/source/img/deeplearning/paper/SR/SwinIR/1.png)\n\n  \n\n![](img/deeplearning/paper/SR/SwinIR/1.png)\n\n\n\n# 2. 相关工作\n\n## 2.1 NLP(Neural Language Processing)\n\n- 可以看成一个翻译任务（Seq2Seq)\n\n- 翻译的难点：一词多义 ->根据上下文的信息\n\n- 常用模型：RNN、LSTM ->全局信息损失\n\n## 2.2 Self-Attention\n\n- Attention($Q$ ,$K$,$V$) = softmax$(\\frac{QK^T}{\\sqrt{d_k}})V$\n\n- 计算一个单词与每一个单词的关系（包括自己）\n\n  ![](D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/2.png)\n\n![](img/deeplearning/paper/SR/SwinIR/2.png)\n\n## 2.3 Multi-head Self-Attention (MSA)\n\n- Ensemble 的self attention，集成多个attention再平均\n\n## 2.4 Transformer on NLP\n\n![](D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/3.png)\n\n![](img/deeplearning/paper/SR/SwinIR/3.png)\n\n- Transformer在NLP领域效果很好，但是在CV上不太好。\n\n## 2.5 Transformer on CV\n\n- 第一篇将Transformer应用到CV的是：ViT\n\n  - Transformer接收的是一个序列 -> 将图片分割成多个patch\n\n    ![](D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/4.png)\n\n    ![](img/deeplearning/paper/SR/SwinIR/4.png)\n\n  - 可用于图片分割、图片分类、物体检测\n\n## 2.6 Swin Transformer\n\n- 虽然ViT效果还算不错，但是仍然无法超过一些主流的卷积网络，主要原因是Transformer一开始是针对NLP的，NLP是一个一维的问题，图片是高维的问题。\n\n- 直接分块然后独立地丢入Transformer后会有一个问题：最后得出的结果会有边界的问题（各个分块相交的地方）\n\n- Swin Transformer的一个模块包括两个层：\n  \n  ​\t![](D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/5.png)\n  \n  ​\t![](img/deeplearning/paper/SR/SwinIR/5.png)\n  \n  - W-MSA：在原来的小分割patch基础上有一个local的概念，先在local上先做。（可以想象成背景有一个大网格）\n  - SW-MSA：背景表格不动，将图片往左上或右下做一个shift，这样分割出来的local跟第一层的分割效果就不同了，这样可以解决边界问题。\n  \n- 效果非常好，基本可以取代卷积运算\n\n  \n\n# 3 SwinIR\n\n## 3.1 结构\n\n![](D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/6.png)\n\n![](img/deeplearning/paper/SR/SwinIR/6.png)\n\n- 浅层特征提取层 -> 深层特征提取层 -> 重建层\n\n  - Shallow Feature Extraction：用一个3*3卷积获得低频信息（颜色或者纹理等基础特性）\n  - Deep Feature Extraction：由于Swin Transformer原本使用在高阶的图像处理任务上，这里让他应用到低阶的任务上，效果非常好。\n  - Reconstruction：\n    - SR：用sub-pixel做up-sample\n    - Denoising/JPEG：用3*3卷积\n\n  - Loss function:\n    - SR：L1 loss -> $l = || I_{RHQ}-I_{HQ}||_1$\n    - Denoising & JPEG：Charbonnier loss -> $l=\\sqrt{||I_{PHQ}-I_{HQ}||^2 + \\epsilon^2}$\n\n\n\n# 4. 实验\n\n## 4.1 Ablation Studies\n\n![](D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/7.png)\n\n![](img/deeplearning/paper/SR/SwinIR/7.png)\n\n- Super Resolution\n\n  - Classical SR\n\n    ![](D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/8.png)\n\n    ![](img/deeplearning/paper/SR/SwinIR/8.png)\n\n  - Lightweight image SR(轻量的网络架构)\n\n    ![](D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/9.png)\n\n    ![](img/deeplearning/paper/SR/SwinIR/9.png)\n\n  - Real-world image SR\n\n    ![](D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/10.png)\n\n    ![](img/deeplearning/paper/SR/SwinIR/10.png)\n\n- JPEG block removal\n\n  ![](D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/11.png)\n\n  ![](img/deeplearning/paper/SR/SwinIR/11.png)\n\n- Denoising\n\n  - Grayscale image denoising\n\n  - Color image denoising\n\n    ![](D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/12.png)\n\n    ![](img/deeplearning/paper/SR/SwinIR/12.png)\n\n    ![](D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/13.png) \n\n     ![](img/deeplearning/paper/SR/SwinIR/13.png) \n\n # 5 结论\n\n- 提出基于Swin Transformer的图像修复模型：SwinIR\n- 通过大量实验表明SwinIR在SR、denoising、JPEG block removal上都是sota","slug":"deeplearning/paper/SR/SwinIR","published":1,"updated":"2024-01-30T14:37:18.986Z","comments":1,"layout":"post","photos":[],"_id":"clvulgd6b003c5ovw4qm6gvjw","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<ul>\n<li><p>论文标题：<a href=\"https://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.html\"><strong>Swinir</strong>: Image restoration using swin transformer</a></p>\n</li>\n<li><p>来源：ICCV  2021</p>\n</li>\n<li><p>贡献：</p>\n<ul>\n<li>提出基于Swin Transformer的图像修复模型：SwinIR</li>\n<li>通过大量实验表明SwinIR在SR、denoising、JPEG block removal上都是sota</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"1-背景\"><a href=\"#1-背景\" class=\"headerlink\" title=\"1. 背景\"></a>1. 背景</h1><h2 id=\"1-1-图像修复\"><a href=\"#1-1-图像修复\" class=\"headerlink\" title=\"1-1 图像修复\"></a>1-1 图像修复</h2><ul>\n<li>从一个劣质图片修复成一个干净的图片，如：超分、降噪、JPEG压缩区块失真<img src=\"D:/blog/themes/yilia/source/img/deeplearning/paper/SR/SwinIR/1.png\"></li>\n</ul>\n<p><img src=\"/img/deeplearning/paper/SR/SwinIR/1.png\"></p>\n<h1 id=\"2-相关工作\"><a href=\"#2-相关工作\" class=\"headerlink\" title=\"2. 相关工作\"></a>2. 相关工作</h1><h2 id=\"2-1-NLP-Neural-Language-Processing\"><a href=\"#2-1-NLP-Neural-Language-Processing\" class=\"headerlink\" title=\"2.1 NLP(Neural Language Processing)\"></a>2.1 NLP(Neural Language Processing)</h2><ul>\n<li><p>可以看成一个翻译任务（Seq2Seq)</p>\n</li>\n<li><p>翻译的难点：一词多义 -&gt;根据上下文的信息</p>\n</li>\n<li><p>常用模型：RNN、LSTM -&gt;全局信息损失</p>\n</li>\n</ul>\n<h2 id=\"2-2-Self-Attention\"><a href=\"#2-2-Self-Attention\" class=\"headerlink\" title=\"2.2 Self-Attention\"></a>2.2 Self-Attention</h2><ul>\n<li><p>Attention($Q$ ,$K$,$V$) &#x3D; softmax$(\\frac{QK^T}{\\sqrt{d_k}})V$</p>\n</li>\n<li><p>计算一个单词与每一个单词的关系（包括自己）</p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/2.png\"></p>\n</li>\n</ul>\n<p><img src=\"/img/deeplearning/paper/SR/SwinIR/2.png\"></p>\n<h2 id=\"2-3-Multi-head-Self-Attention-MSA\"><a href=\"#2-3-Multi-head-Self-Attention-MSA\" class=\"headerlink\" title=\"2.3 Multi-head Self-Attention (MSA)\"></a>2.3 Multi-head Self-Attention (MSA)</h2><ul>\n<li>Ensemble 的self attention，集成多个attention再平均</li>\n</ul>\n<h2 id=\"2-4-Transformer-on-NLP\"><a href=\"#2-4-Transformer-on-NLP\" class=\"headerlink\" title=\"2.4 Transformer on NLP\"></a>2.4 Transformer on NLP</h2><p><img src=\"D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/3.png\"></p>\n<p><img src=\"/img/deeplearning/paper/SR/SwinIR/3.png\"></p>\n<ul>\n<li>Transformer在NLP领域效果很好，但是在CV上不太好。</li>\n</ul>\n<h2 id=\"2-5-Transformer-on-CV\"><a href=\"#2-5-Transformer-on-CV\" class=\"headerlink\" title=\"2.5 Transformer on CV\"></a>2.5 Transformer on CV</h2><ul>\n<li><p>第一篇将Transformer应用到CV的是：ViT</p>\n<ul>\n<li><p>Transformer接收的是一个序列 -&gt; 将图片分割成多个patch</p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/4.png\"></p>\n<p><img src=\"/img/deeplearning/paper/SR/SwinIR/4.png\"></p>\n</li>\n<li><p>可用于图片分割、图片分类、物体检测</p>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"2-6-Swin-Transformer\"><a href=\"#2-6-Swin-Transformer\" class=\"headerlink\" title=\"2.6 Swin Transformer\"></a>2.6 Swin Transformer</h2><ul>\n<li><p>虽然ViT效果还算不错，但是仍然无法超过一些主流的卷积网络，主要原因是Transformer一开始是针对NLP的，NLP是一个一维的问题，图片是高维的问题。</p>\n</li>\n<li><p>直接分块然后独立地丢入Transformer后会有一个问题：最后得出的结果会有边界的问题（各个分块相交的地方）</p>\n</li>\n<li><p>Swin Transformer的一个模块包括两个层：</p>\n<p>​\t<img src=\"D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/5.png\"></p>\n<p>​\t<img src=\"/img/deeplearning/paper/SR/SwinIR/5.png\"></p>\n<ul>\n<li>W-MSA：在原来的小分割patch基础上有一个local的概念，先在local上先做。（可以想象成背景有一个大网格）</li>\n<li>SW-MSA：背景表格不动，将图片往左上或右下做一个shift，这样分割出来的local跟第一层的分割效果就不同了，这样可以解决边界问题。</li>\n</ul>\n</li>\n<li><p>效果非常好，基本可以取代卷积运算</p>\n</li>\n</ul>\n<h1 id=\"3-SwinIR\"><a href=\"#3-SwinIR\" class=\"headerlink\" title=\"3 SwinIR\"></a>3 SwinIR</h1><h2 id=\"3-1-结构\"><a href=\"#3-1-结构\" class=\"headerlink\" title=\"3.1 结构\"></a>3.1 结构</h2><p><img src=\"D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/6.png\"></p>\n<p><img src=\"/img/deeplearning/paper/SR/SwinIR/6.png\"></p>\n<ul>\n<li><p>浅层特征提取层 -&gt; 深层特征提取层 -&gt; 重建层</p>\n<ul>\n<li><p>Shallow Feature Extraction：用一个3*3卷积获得低频信息（颜色或者纹理等基础特性）</p>\n</li>\n<li><p>Deep Feature Extraction：由于Swin Transformer原本使用在高阶的图像处理任务上，这里让他应用到低阶的任务上，效果非常好。</p>\n</li>\n<li><p>Reconstruction：</p>\n<ul>\n<li>SR：用sub-pixel做up-sample</li>\n<li>Denoising&#x2F;JPEG：用3*3卷积</li>\n</ul>\n</li>\n<li><p>Loss function:</p>\n<ul>\n<li>SR：L1 loss -&gt; $l &#x3D; || I_{RHQ}-I_{HQ}||_1$</li>\n<li>Denoising &amp; JPEG：Charbonnier loss -&gt; $l&#x3D;\\sqrt{||I_{PHQ}-I_{HQ}||^2 + \\epsilon^2}$</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"4-实验\"><a href=\"#4-实验\" class=\"headerlink\" title=\"4. 实验\"></a>4. 实验</h1><h2 id=\"4-1-Ablation-Studies\"><a href=\"#4-1-Ablation-Studies\" class=\"headerlink\" title=\"4.1 Ablation Studies\"></a>4.1 Ablation Studies</h2><p><img src=\"D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/7.png\"></p>\n<p><img src=\"/img/deeplearning/paper/SR/SwinIR/7.png\"></p>\n<ul>\n<li><p>Super Resolution</p>\n<ul>\n<li><p>Classical SR</p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/8.png\"></p>\n<p><img src=\"/img/deeplearning/paper/SR/SwinIR/8.png\"></p>\n</li>\n<li><p>Lightweight image SR(轻量的网络架构)</p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/9.png\"></p>\n<p><img src=\"/img/deeplearning/paper/SR/SwinIR/9.png\"></p>\n</li>\n<li><p>Real-world image SR</p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/10.png\"></p>\n<p><img src=\"/img/deeplearning/paper/SR/SwinIR/10.png\"></p>\n</li>\n</ul>\n</li>\n<li><p>JPEG block removal</p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/11.png\"></p>\n<p><img src=\"/img/deeplearning/paper/SR/SwinIR/11.png\"></p>\n</li>\n<li><p>Denoising</p>\n<ul>\n<li><p>Grayscale image denoising</p>\n</li>\n<li><p>Color image denoising</p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/12.png\"></p>\n<p><img src=\"/img/deeplearning/paper/SR/SwinIR/12.png\"></p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/13.png\"> </p>\n<p> <img src=\"/img/deeplearning/paper/SR/SwinIR/13.png\"></p>\n</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"5-结论\"><a href=\"#5-结论\" class=\"headerlink\" title=\"5 结论\"></a>5 结论</h1><ul>\n<li>提出基于Swin Transformer的图像修复模型：SwinIR</li>\n<li>通过大量实验表明SwinIR在SR、denoising、JPEG block removal上都是sota</li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<ul>\n<li><p>论文标题：<a href=\"https://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.html\"><strong>Swinir</strong>: Image restoration using swin transformer</a></p>\n</li>\n<li><p>来源：ICCV  2021</p>\n</li>\n<li><p>贡献：</p>\n<ul>\n<li>提出基于Swin Transformer的图像修复模型：SwinIR</li>\n<li>通过大量实验表明SwinIR在SR、denoising、JPEG block removal上都是sota</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"1-背景\"><a href=\"#1-背景\" class=\"headerlink\" title=\"1. 背景\"></a>1. 背景</h1><h2 id=\"1-1-图像修复\"><a href=\"#1-1-图像修复\" class=\"headerlink\" title=\"1-1 图像修复\"></a>1-1 图像修复</h2><ul>\n<li>从一个劣质图片修复成一个干净的图片，如：超分、降噪、JPEG压缩区块失真<img src=\"D:/blog/themes/yilia/source/img/deeplearning/paper/SR/SwinIR/1.png\"></li>\n</ul>\n<p><img src=\"/img/deeplearning/paper/SR/SwinIR/1.png\"></p>\n<h1 id=\"2-相关工作\"><a href=\"#2-相关工作\" class=\"headerlink\" title=\"2. 相关工作\"></a>2. 相关工作</h1><h2 id=\"2-1-NLP-Neural-Language-Processing\"><a href=\"#2-1-NLP-Neural-Language-Processing\" class=\"headerlink\" title=\"2.1 NLP(Neural Language Processing)\"></a>2.1 NLP(Neural Language Processing)</h2><ul>\n<li><p>可以看成一个翻译任务（Seq2Seq)</p>\n</li>\n<li><p>翻译的难点：一词多义 -&gt;根据上下文的信息</p>\n</li>\n<li><p>常用模型：RNN、LSTM -&gt;全局信息损失</p>\n</li>\n</ul>\n<h2 id=\"2-2-Self-Attention\"><a href=\"#2-2-Self-Attention\" class=\"headerlink\" title=\"2.2 Self-Attention\"></a>2.2 Self-Attention</h2><ul>\n<li><p>Attention($Q$ ,$K$,$V$) &#x3D; softmax$(\\frac{QK^T}{\\sqrt{d_k}})V$</p>\n</li>\n<li><p>计算一个单词与每一个单词的关系（包括自己）</p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/2.png\"></p>\n</li>\n</ul>\n<p><img src=\"/img/deeplearning/paper/SR/SwinIR/2.png\"></p>\n<h2 id=\"2-3-Multi-head-Self-Attention-MSA\"><a href=\"#2-3-Multi-head-Self-Attention-MSA\" class=\"headerlink\" title=\"2.3 Multi-head Self-Attention (MSA)\"></a>2.3 Multi-head Self-Attention (MSA)</h2><ul>\n<li>Ensemble 的self attention，集成多个attention再平均</li>\n</ul>\n<h2 id=\"2-4-Transformer-on-NLP\"><a href=\"#2-4-Transformer-on-NLP\" class=\"headerlink\" title=\"2.4 Transformer on NLP\"></a>2.4 Transformer on NLP</h2><p><img src=\"D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/3.png\"></p>\n<p><img src=\"/img/deeplearning/paper/SR/SwinIR/3.png\"></p>\n<ul>\n<li>Transformer在NLP领域效果很好，但是在CV上不太好。</li>\n</ul>\n<h2 id=\"2-5-Transformer-on-CV\"><a href=\"#2-5-Transformer-on-CV\" class=\"headerlink\" title=\"2.5 Transformer on CV\"></a>2.5 Transformer on CV</h2><ul>\n<li><p>第一篇将Transformer应用到CV的是：ViT</p>\n<ul>\n<li><p>Transformer接收的是一个序列 -&gt; 将图片分割成多个patch</p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/4.png\"></p>\n<p><img src=\"/img/deeplearning/paper/SR/SwinIR/4.png\"></p>\n</li>\n<li><p>可用于图片分割、图片分类、物体检测</p>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"2-6-Swin-Transformer\"><a href=\"#2-6-Swin-Transformer\" class=\"headerlink\" title=\"2.6 Swin Transformer\"></a>2.6 Swin Transformer</h2><ul>\n<li><p>虽然ViT效果还算不错，但是仍然无法超过一些主流的卷积网络，主要原因是Transformer一开始是针对NLP的，NLP是一个一维的问题，图片是高维的问题。</p>\n</li>\n<li><p>直接分块然后独立地丢入Transformer后会有一个问题：最后得出的结果会有边界的问题（各个分块相交的地方）</p>\n</li>\n<li><p>Swin Transformer的一个模块包括两个层：</p>\n<p>​\t<img src=\"D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/5.png\"></p>\n<p>​\t<img src=\"/img/deeplearning/paper/SR/SwinIR/5.png\"></p>\n<ul>\n<li>W-MSA：在原来的小分割patch基础上有一个local的概念，先在local上先做。（可以想象成背景有一个大网格）</li>\n<li>SW-MSA：背景表格不动，将图片往左上或右下做一个shift，这样分割出来的local跟第一层的分割效果就不同了，这样可以解决边界问题。</li>\n</ul>\n</li>\n<li><p>效果非常好，基本可以取代卷积运算</p>\n</li>\n</ul>\n<h1 id=\"3-SwinIR\"><a href=\"#3-SwinIR\" class=\"headerlink\" title=\"3 SwinIR\"></a>3 SwinIR</h1><h2 id=\"3-1-结构\"><a href=\"#3-1-结构\" class=\"headerlink\" title=\"3.1 结构\"></a>3.1 结构</h2><p><img src=\"D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/6.png\"></p>\n<p><img src=\"/img/deeplearning/paper/SR/SwinIR/6.png\"></p>\n<ul>\n<li><p>浅层特征提取层 -&gt; 深层特征提取层 -&gt; 重建层</p>\n<ul>\n<li><p>Shallow Feature Extraction：用一个3*3卷积获得低频信息（颜色或者纹理等基础特性）</p>\n</li>\n<li><p>Deep Feature Extraction：由于Swin Transformer原本使用在高阶的图像处理任务上，这里让他应用到低阶的任务上，效果非常好。</p>\n</li>\n<li><p>Reconstruction：</p>\n<ul>\n<li>SR：用sub-pixel做up-sample</li>\n<li>Denoising&#x2F;JPEG：用3*3卷积</li>\n</ul>\n</li>\n<li><p>Loss function:</p>\n<ul>\n<li>SR：L1 loss -&gt; $l &#x3D; || I_{RHQ}-I_{HQ}||_1$</li>\n<li>Denoising &amp; JPEG：Charbonnier loss -&gt; $l&#x3D;\\sqrt{||I_{PHQ}-I_{HQ}||^2 + \\epsilon^2}$</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"4-实验\"><a href=\"#4-实验\" class=\"headerlink\" title=\"4. 实验\"></a>4. 实验</h1><h2 id=\"4-1-Ablation-Studies\"><a href=\"#4-1-Ablation-Studies\" class=\"headerlink\" title=\"4.1 Ablation Studies\"></a>4.1 Ablation Studies</h2><p><img src=\"D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/7.png\"></p>\n<p><img src=\"/img/deeplearning/paper/SR/SwinIR/7.png\"></p>\n<ul>\n<li><p>Super Resolution</p>\n<ul>\n<li><p>Classical SR</p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/8.png\"></p>\n<p><img src=\"/img/deeplearning/paper/SR/SwinIR/8.png\"></p>\n</li>\n<li><p>Lightweight image SR(轻量的网络架构)</p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/9.png\"></p>\n<p><img src=\"/img/deeplearning/paper/SR/SwinIR/9.png\"></p>\n</li>\n<li><p>Real-world image SR</p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/10.png\"></p>\n<p><img src=\"/img/deeplearning/paper/SR/SwinIR/10.png\"></p>\n</li>\n</ul>\n</li>\n<li><p>JPEG block removal</p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/11.png\"></p>\n<p><img src=\"/img/deeplearning/paper/SR/SwinIR/11.png\"></p>\n</li>\n<li><p>Denoising</p>\n<ul>\n<li><p>Grayscale image denoising</p>\n</li>\n<li><p>Color image denoising</p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/12.png\"></p>\n<p><img src=\"/img/deeplearning/paper/SR/SwinIR/12.png\"></p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img/deeplearning/paper/SR/SwinIR/13.png\"> </p>\n<p> <img src=\"/img/deeplearning/paper/SR/SwinIR/13.png\"></p>\n</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"5-结论\"><a href=\"#5-结论\" class=\"headerlink\" title=\"5 结论\"></a>5 结论</h1><ul>\n<li>提出基于Swin Transformer的图像修复模型：SwinIR</li>\n<li>通过大量实验表明SwinIR在SR、denoising、JPEG block removal上都是sota</li>\n</ul>"},{"title":"1.1 pytorch数据操作","date":"2023-11-11T06:00:00.000Z","comment":false,"toc":true,"_content":"#  \n<!--more-->\n\n\n# 1. pytorch数据操作\n\n- pytorch与tensorflow中的Tensor类似于numpy的ndarray\n- pytorch中的Tensor可以在GPU上运行\n- pytorch中的Tensor可以用于自动求导\n\n\n\n## 1.1 张量\n\n- 创建张量\n\n\n```python\n# 1. 导入torch，不是pytorch\nimport torch\n\n# 2. 创建范围张量\nx=torch.arange(12) # 0-11,默认为int64，可以指定dtype，默认存储在CPU上\ny=torch.arange(12,dtype=torch.float32) # 指定dtype\n\n# 3. 创建全0张量\nzeros=torch.zeros(2,3,4) # 2*3*4的全0张量\n\n# 4. 创建全1张量\nones=torch.ones(2,3,4) # 2*3*4的全1张量\n\n# 5. 创建采样张量\nsample=torch.randn(3,4) #从标准高斯分布中采样\n\n# 6. 列表张量\nlists=torch.tensor([[1,2,3],[4,5,6]])\n\n\nprint(x)\nprint(y)\nprint(zeros)\nprint(ones)\nprint(sample)\nprint(lists)\n\n```\n\n    tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n    tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])\n    tensor([[[0., 0., 0., 0.],\n             [0., 0., 0., 0.],\n             [0., 0., 0., 0.]],\n    \n            [[0., 0., 0., 0.],\n             [0., 0., 0., 0.],\n             [0., 0., 0., 0.]]])\n    tensor([[[1., 1., 1., 1.],\n             [1., 1., 1., 1.],\n             [1., 1., 1., 1.]],\n    \n            [[1., 1., 1., 1.],\n             [1., 1., 1., 1.],\n             [1., 1., 1., 1.]]])\n    tensor([[-0.0847, -0.2406,  0.1735, -1.5543],\n            [-0.2820, -0.6689,  0.0565,  0.4746],\n            [ 0.9841, -1.6116, -0.1587, -1.5121]])\n    tensor([[1, 2, 3],\n            [4, 5, 6]])\n    \n\n- 张量的属性\n\n\n```python\n# 3. 查看张量的形状\nprint('x的规模：',x.shape)\n\n# 4. 查看张量的元素总数\nprint('x中元素个数：',x.numel())\n\n# 5. 改变张量的形状\nx=x.reshape(3,4)\nprint('改变x的形状：',x)\nprint('自动计算的形状：',x.reshape(-1,6)) # -1表示自动计算这个维度\n```\n\n    x的规模： torch.Size([12])\n    x中元素个数： 12\n    改变x的形状： tensor([[ 0,  1,  2,  3],\n            [ 4,  5,  6,  7],\n            [ 8,  9, 10, 11]])\n    自动计算的形状： tensor([[ 0,  1,  2,  3,  4,  5],\n            [ 6,  7,  8,  9, 10, 11]])\n    \n\n## 1.2 运算符\n\n\n```python\nx=torch.arange(4)\n\n# 1. 按元素运算\nprint('按元素加法：',x+x)\nprint('按元素减法：',x-x)\nprint('按元素乘法：',x*x)\nprint('按元素除法：',x/x)\nprint('按元素幂运算：',x**x)\nprint('按元素开方：',x.sqrt())\nprint('求幂运算：',torch.exp(x))\n\n# 2. 矩阵运算\n\n# 3. 合并张量(记得加括号)\nx=torch.zeros(12).reshape(3,4)\ny=torch.ones(12).reshape(3,4)\nprint('沿行合并：',torch.cat((x,y),dim=0)) # 沿行合并\nprint('沿列合并：',torch.cat((x,y),dim=1)) # 沿列合并\n\n# 4. 逻辑运算\nprint(x==y)\n\n# 5. 求和\nprint('求和：',y.sum())\n```\n\n    按元素加法： tensor([0, 2, 4, 6])\n    按元素减法： tensor([0, 0, 0, 0])\n    按元素乘法： tensor([0, 1, 4, 9])\n    按元素除法： tensor([nan, 1., 1., 1.])\n    按元素幂运算： tensor([ 1,  1,  4, 27])\n    按元素开方： tensor([0.0000, 1.0000, 1.4142, 1.7321])\n    求幂运算： tensor([ 1.0000,  2.7183,  7.3891, 20.0855])\n    沿行合并： tensor([[0., 0., 0., 0.],\n            [0., 0., 0., 0.],\n            [0., 0., 0., 0.],\n            [1., 1., 1., 1.],\n            [1., 1., 1., 1.],\n            [1., 1., 1., 1.]])\n    沿列合并： tensor([[0., 0., 0., 0., 1., 1., 1., 1.],\n            [0., 0., 0., 0., 1., 1., 1., 1.],\n            [0., 0., 0., 0., 1., 1., 1., 1.]])\n    tensor([[False, False, False, False],\n            [False, False, False, False],\n            [False, False, False, False]])\n    求和： tensor(12.)\n    \n\n## 1.3 广播机制\n- 对两个形状不同的张量按元素运算时，可能会触发广播机制：先适当复制元素使这两个张量形状相同后再按元素运算\n\n1. 如果两个张量的维度数不同，可以在较小的张量的形状前面补1，直到两者的维度数相同。\n2. 如果两个张量在某个维度上的大小不同，但其中一个张量在该维度上的大小为1，那么可以通过在该维度上重复扩展该张量，使得两个张量在该维度上的大小相同。\n3. 如果两个张量在某个维度上的大小都不为1，且大小不同，那么会发生形状不匹配，导致无法进行广播\n\n```python\na=torch.arange(3).reshape(3,1)\nb=torch.arange(2).reshape(1,2)\nprint('a:',a)\nprint('b:',b)\nprint('a+b:',a+b) #通常沿着长度为1的维度进行广播\n```\n\n    a: tensor([[0],\n            [1],\n            [2]])\n    b: tensor([[0, 1]])\n    a+b: tensor([[0, 1],\n            [1, 2],\n            [2, 3]])\n    \n\n## 1.4 索引和切片\n\n\n```python\nprint('x:',x)\nprint('x[-1]:',x[-1])\nprint('x[1:3]:',x[1:3]) #选择第2和第3个元素\nx[1,2]=9 # 修改元素\nprint('x[1,2]=9后 x:',x)\nx[0:2,:]=12 # 修改一行\nprint('x[0:2,:]=12后 x:',x)\n```\n\n    x: tensor([[0., 0., 0., 0.],\n            [0., 0., 0., 0.],\n            [0., 0., 0., 0.]])\n    x[-1]: tensor([0., 0., 0., 0.])\n    x[1:3]: tensor([[0., 0., 0., 0.],\n            [0., 0., 0., 0.]])\n    x[1,2]=9后 x: tensor([[0., 0., 0., 0.],\n            [0., 0., 9., 0.],\n            [0., 0., 0., 0.]])\n    x[0:2,:]=12后 x: tensor([[12., 12., 12., 12.],\n            [12., 12., 12., 12.],\n            [ 0.,  0.,  0.,  0.]])\n    \n\n## 1.5 节省内存\n\n\n```python\nz=torch.zeros(1)\nbefore=id(z)\nz=z+x #对于这种类似列表的数据结构，加号会导致新的内存分配\nprint('z:',z)\nprint('id(z)==before:',id(z)==before) # False，说明z指向了新的地址\n\nz=torch.zeros(3,4) #使用如下操作可以避免新的内存分配，但是无法广播，所以这里要求x和z的形状一致\nbefore=id(z)\nz+=x # +=不会导致新的内存分配\nz[:]=x+z # 也不会导致新的内存分配\nprint('id(z)==before:',id(z)==before) # True\n```\n\n    z: tensor([[12., 12., 12., 12.],\n            [12., 12., 12., 12.],\n            [ 0.,  0.,  0.,  0.]])\n    id(z)==before: False\n    id(z)==before: True\n    \n\n## 1.6 转换为其他python对象\n- tensor与numpy数组\n\n\n```python\nA=x.numpy() # 将张量转换为numpy数组\nB=torch.tensor(A) # 将numpy数组转换为张量\nprint(A,B,sep='\\n')\n\nA[1,1]=3\nprint(A,B,x,sep='\\n') #A与X共享内存\n```\n\n    [[12. 12. 12. 12.]\n     [12.  3. 12. 12.]\n     [ 0.  0.  0.  0.]]\n    tensor([[12., 12., 12., 12.],\n            [12.,  3., 12., 12.],\n            [ 0.,  0.,  0.,  0.]])\n    [[12. 12. 12. 12.]\n     [12.  3. 12. 12.]\n     [ 0.  0.  0.  0.]]\n    tensor([[12., 12., 12., 12.],\n            [12.,  3., 12., 12.],\n            [ 0.,  0.,  0.,  0.]])\n    tensor([[12., 12., 12., 12.],\n            [12.,  3., 12., 12.],\n            [ 0.,  0.,  0.,  0.]])\n    \n\n- 张量转为python标量\n\n\n```python\na=torch.tensor([1.3])\nprint(a)\n\n#由于pytorch中小数用32位，python中用64位；尝试输出.20f就能发现精确度问题\n#由于舍入（四舍五入到最接近的偶数），1.3在pytorch中表示是1.299多，而在python中是1.300多\nprint(a.item()) #使用item函数\nprint(float(a)) #python内置函数\n```\n\n    tensor([1.3000])\n    1.2999999523162842\n    1.2999999523162842\n    \n","source":"_posts/deeplearning/code/pytorch/1_prepare/1_dataoperation.md","raw":"---\ntitle: 1.1 pytorch数据操作\ndate: 2023-11-11 14:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: false\ntoc: true\n---\n#  \n<!--more-->\n\n\n# 1. pytorch数据操作\n\n- pytorch与tensorflow中的Tensor类似于numpy的ndarray\n- pytorch中的Tensor可以在GPU上运行\n- pytorch中的Tensor可以用于自动求导\n\n\n\n## 1.1 张量\n\n- 创建张量\n\n\n```python\n# 1. 导入torch，不是pytorch\nimport torch\n\n# 2. 创建范围张量\nx=torch.arange(12) # 0-11,默认为int64，可以指定dtype，默认存储在CPU上\ny=torch.arange(12,dtype=torch.float32) # 指定dtype\n\n# 3. 创建全0张量\nzeros=torch.zeros(2,3,4) # 2*3*4的全0张量\n\n# 4. 创建全1张量\nones=torch.ones(2,3,4) # 2*3*4的全1张量\n\n# 5. 创建采样张量\nsample=torch.randn(3,4) #从标准高斯分布中采样\n\n# 6. 列表张量\nlists=torch.tensor([[1,2,3],[4,5,6]])\n\n\nprint(x)\nprint(y)\nprint(zeros)\nprint(ones)\nprint(sample)\nprint(lists)\n\n```\n\n    tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n    tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])\n    tensor([[[0., 0., 0., 0.],\n             [0., 0., 0., 0.],\n             [0., 0., 0., 0.]],\n    \n            [[0., 0., 0., 0.],\n             [0., 0., 0., 0.],\n             [0., 0., 0., 0.]]])\n    tensor([[[1., 1., 1., 1.],\n             [1., 1., 1., 1.],\n             [1., 1., 1., 1.]],\n    \n            [[1., 1., 1., 1.],\n             [1., 1., 1., 1.],\n             [1., 1., 1., 1.]]])\n    tensor([[-0.0847, -0.2406,  0.1735, -1.5543],\n            [-0.2820, -0.6689,  0.0565,  0.4746],\n            [ 0.9841, -1.6116, -0.1587, -1.5121]])\n    tensor([[1, 2, 3],\n            [4, 5, 6]])\n    \n\n- 张量的属性\n\n\n```python\n# 3. 查看张量的形状\nprint('x的规模：',x.shape)\n\n# 4. 查看张量的元素总数\nprint('x中元素个数：',x.numel())\n\n# 5. 改变张量的形状\nx=x.reshape(3,4)\nprint('改变x的形状：',x)\nprint('自动计算的形状：',x.reshape(-1,6)) # -1表示自动计算这个维度\n```\n\n    x的规模： torch.Size([12])\n    x中元素个数： 12\n    改变x的形状： tensor([[ 0,  1,  2,  3],\n            [ 4,  5,  6,  7],\n            [ 8,  9, 10, 11]])\n    自动计算的形状： tensor([[ 0,  1,  2,  3,  4,  5],\n            [ 6,  7,  8,  9, 10, 11]])\n    \n\n## 1.2 运算符\n\n\n```python\nx=torch.arange(4)\n\n# 1. 按元素运算\nprint('按元素加法：',x+x)\nprint('按元素减法：',x-x)\nprint('按元素乘法：',x*x)\nprint('按元素除法：',x/x)\nprint('按元素幂运算：',x**x)\nprint('按元素开方：',x.sqrt())\nprint('求幂运算：',torch.exp(x))\n\n# 2. 矩阵运算\n\n# 3. 合并张量(记得加括号)\nx=torch.zeros(12).reshape(3,4)\ny=torch.ones(12).reshape(3,4)\nprint('沿行合并：',torch.cat((x,y),dim=0)) # 沿行合并\nprint('沿列合并：',torch.cat((x,y),dim=1)) # 沿列合并\n\n# 4. 逻辑运算\nprint(x==y)\n\n# 5. 求和\nprint('求和：',y.sum())\n```\n\n    按元素加法： tensor([0, 2, 4, 6])\n    按元素减法： tensor([0, 0, 0, 0])\n    按元素乘法： tensor([0, 1, 4, 9])\n    按元素除法： tensor([nan, 1., 1., 1.])\n    按元素幂运算： tensor([ 1,  1,  4, 27])\n    按元素开方： tensor([0.0000, 1.0000, 1.4142, 1.7321])\n    求幂运算： tensor([ 1.0000,  2.7183,  7.3891, 20.0855])\n    沿行合并： tensor([[0., 0., 0., 0.],\n            [0., 0., 0., 0.],\n            [0., 0., 0., 0.],\n            [1., 1., 1., 1.],\n            [1., 1., 1., 1.],\n            [1., 1., 1., 1.]])\n    沿列合并： tensor([[0., 0., 0., 0., 1., 1., 1., 1.],\n            [0., 0., 0., 0., 1., 1., 1., 1.],\n            [0., 0., 0., 0., 1., 1., 1., 1.]])\n    tensor([[False, False, False, False],\n            [False, False, False, False],\n            [False, False, False, False]])\n    求和： tensor(12.)\n    \n\n## 1.3 广播机制\n- 对两个形状不同的张量按元素运算时，可能会触发广播机制：先适当复制元素使这两个张量形状相同后再按元素运算\n\n1. 如果两个张量的维度数不同，可以在较小的张量的形状前面补1，直到两者的维度数相同。\n2. 如果两个张量在某个维度上的大小不同，但其中一个张量在该维度上的大小为1，那么可以通过在该维度上重复扩展该张量，使得两个张量在该维度上的大小相同。\n3. 如果两个张量在某个维度上的大小都不为1，且大小不同，那么会发生形状不匹配，导致无法进行广播\n\n```python\na=torch.arange(3).reshape(3,1)\nb=torch.arange(2).reshape(1,2)\nprint('a:',a)\nprint('b:',b)\nprint('a+b:',a+b) #通常沿着长度为1的维度进行广播\n```\n\n    a: tensor([[0],\n            [1],\n            [2]])\n    b: tensor([[0, 1]])\n    a+b: tensor([[0, 1],\n            [1, 2],\n            [2, 3]])\n    \n\n## 1.4 索引和切片\n\n\n```python\nprint('x:',x)\nprint('x[-1]:',x[-1])\nprint('x[1:3]:',x[1:3]) #选择第2和第3个元素\nx[1,2]=9 # 修改元素\nprint('x[1,2]=9后 x:',x)\nx[0:2,:]=12 # 修改一行\nprint('x[0:2,:]=12后 x:',x)\n```\n\n    x: tensor([[0., 0., 0., 0.],\n            [0., 0., 0., 0.],\n            [0., 0., 0., 0.]])\n    x[-1]: tensor([0., 0., 0., 0.])\n    x[1:3]: tensor([[0., 0., 0., 0.],\n            [0., 0., 0., 0.]])\n    x[1,2]=9后 x: tensor([[0., 0., 0., 0.],\n            [0., 0., 9., 0.],\n            [0., 0., 0., 0.]])\n    x[0:2,:]=12后 x: tensor([[12., 12., 12., 12.],\n            [12., 12., 12., 12.],\n            [ 0.,  0.,  0.,  0.]])\n    \n\n## 1.5 节省内存\n\n\n```python\nz=torch.zeros(1)\nbefore=id(z)\nz=z+x #对于这种类似列表的数据结构，加号会导致新的内存分配\nprint('z:',z)\nprint('id(z)==before:',id(z)==before) # False，说明z指向了新的地址\n\nz=torch.zeros(3,4) #使用如下操作可以避免新的内存分配，但是无法广播，所以这里要求x和z的形状一致\nbefore=id(z)\nz+=x # +=不会导致新的内存分配\nz[:]=x+z # 也不会导致新的内存分配\nprint('id(z)==before:',id(z)==before) # True\n```\n\n    z: tensor([[12., 12., 12., 12.],\n            [12., 12., 12., 12.],\n            [ 0.,  0.,  0.,  0.]])\n    id(z)==before: False\n    id(z)==before: True\n    \n\n## 1.6 转换为其他python对象\n- tensor与numpy数组\n\n\n```python\nA=x.numpy() # 将张量转换为numpy数组\nB=torch.tensor(A) # 将numpy数组转换为张量\nprint(A,B,sep='\\n')\n\nA[1,1]=3\nprint(A,B,x,sep='\\n') #A与X共享内存\n```\n\n    [[12. 12. 12. 12.]\n     [12.  3. 12. 12.]\n     [ 0.  0.  0.  0.]]\n    tensor([[12., 12., 12., 12.],\n            [12.,  3., 12., 12.],\n            [ 0.,  0.,  0.,  0.]])\n    [[12. 12. 12. 12.]\n     [12.  3. 12. 12.]\n     [ 0.  0.  0.  0.]]\n    tensor([[12., 12., 12., 12.],\n            [12.,  3., 12., 12.],\n            [ 0.,  0.,  0.,  0.]])\n    tensor([[12., 12., 12., 12.],\n            [12.,  3., 12., 12.],\n            [ 0.,  0.,  0.,  0.]])\n    \n\n- 张量转为python标量\n\n\n```python\na=torch.tensor([1.3])\nprint(a)\n\n#由于pytorch中小数用32位，python中用64位；尝试输出.20f就能发现精确度问题\n#由于舍入（四舍五入到最接近的偶数），1.3在pytorch中表示是1.299多，而在python中是1.300多\nprint(a.item()) #使用item函数\nprint(float(a)) #python内置函数\n```\n\n    tensor([1.3000])\n    1.2999999523162842\n    1.2999999523162842\n    \n","slug":"deeplearning/code/pytorch/1_prepare/1_dataoperation","published":1,"updated":"2024-05-02T08:55:44.030Z","_id":"clvulgd6c003f5ovwfukoh6k0","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n\n<h1 id=\"1-pytorch数据操作\"><a href=\"#1-pytorch数据操作\" class=\"headerlink\" title=\"1. pytorch数据操作\"></a>1. pytorch数据操作</h1><ul>\n<li>pytorch与tensorflow中的Tensor类似于numpy的ndarray</li>\n<li>pytorch中的Tensor可以在GPU上运行</li>\n<li>pytorch中的Tensor可以用于自动求导</li>\n</ul>\n<h2 id=\"1-1-张量\"><a href=\"#1-1-张量\" class=\"headerlink\" title=\"1.1 张量\"></a>1.1 张量</h2><ul>\n<li>创建张量</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 1. 导入torch，不是pytorch</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2. 创建范围张量</span></span><br><span class=\"line\">x=torch.arange(<span class=\"number\">12</span>) <span class=\"comment\"># 0-11,默认为int64，可以指定dtype，默认存储在CPU上</span></span><br><span class=\"line\">y=torch.arange(<span class=\"number\">12</span>,dtype=torch.float32) <span class=\"comment\"># 指定dtype</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 3. 创建全0张量</span></span><br><span class=\"line\">zeros=torch.zeros(<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>) <span class=\"comment\"># 2*3*4的全0张量</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 4. 创建全1张量</span></span><br><span class=\"line\">ones=torch.ones(<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>) <span class=\"comment\"># 2*3*4的全1张量</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 5. 创建采样张量</span></span><br><span class=\"line\">sample=torch.randn(<span class=\"number\">3</span>,<span class=\"number\">4</span>) <span class=\"comment\">#从标准高斯分布中采样</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 6. 列表张量</span></span><br><span class=\"line\">lists=torch.tensor([[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>],[<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>]])</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(x)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(y)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(zeros)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(ones)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(sample)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(lists)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\ntensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])\ntensor([[[0., 0., 0., 0.],\n         [0., 0., 0., 0.],\n         [0., 0., 0., 0.]],\n\n        [[0., 0., 0., 0.],\n         [0., 0., 0., 0.],\n         [0., 0., 0., 0.]]])\ntensor([[[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]],\n\n        [[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]]])\ntensor([[-0.0847, -0.2406,  0.1735, -1.5543],\n        [-0.2820, -0.6689,  0.0565,  0.4746],\n        [ 0.9841, -1.6116, -0.1587, -1.5121]])\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n</code></pre>\n<ul>\n<li>张量的属性</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 3. 查看张量的形状</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;x的规模：&#x27;</span>,x.shape)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 4. 查看张量的元素总数</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;x中元素个数：&#x27;</span>,x.numel())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 5. 改变张量的形状</span></span><br><span class=\"line\">x=x.reshape(<span class=\"number\">3</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;改变x的形状：&#x27;</span>,x)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;自动计算的形状：&#x27;</span>,x.reshape(-<span class=\"number\">1</span>,<span class=\"number\">6</span>)) <span class=\"comment\"># -1表示自动计算这个维度</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>x的规模： torch.Size([12])\nx中元素个数： 12\n改变x的形状： tensor([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]])\n自动计算的形状： tensor([[ 0,  1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10, 11]])\n</code></pre>\n<h2 id=\"1-2-运算符\"><a href=\"#1-2-运算符\" class=\"headerlink\" title=\"1.2 运算符\"></a>1.2 运算符</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=torch.arange(<span class=\"number\">4</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 1. 按元素运算</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;按元素加法：&#x27;</span>,x+x)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;按元素减法：&#x27;</span>,x-x)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;按元素乘法：&#x27;</span>,x*x)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;按元素除法：&#x27;</span>,x/x)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;按元素幂运算：&#x27;</span>,x**x)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;按元素开方：&#x27;</span>,x.sqrt())</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;求幂运算：&#x27;</span>,torch.exp(x))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2. 矩阵运算</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 3. 合并张量(记得加括号)</span></span><br><span class=\"line\">x=torch.zeros(<span class=\"number\">12</span>).reshape(<span class=\"number\">3</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\">y=torch.ones(<span class=\"number\">12</span>).reshape(<span class=\"number\">3</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;沿行合并：&#x27;</span>,torch.cat((x,y),dim=<span class=\"number\">0</span>)) <span class=\"comment\"># 沿行合并</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;沿列合并：&#x27;</span>,torch.cat((x,y),dim=<span class=\"number\">1</span>)) <span class=\"comment\"># 沿列合并</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 4. 逻辑运算</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(x==y)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 5. 求和</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;求和：&#x27;</span>,y.<span class=\"built_in\">sum</span>())</span><br></pre></td></tr></table></figure>\n\n<pre><code>按元素加法： tensor([0, 2, 4, 6])\n按元素减法： tensor([0, 0, 0, 0])\n按元素乘法： tensor([0, 1, 4, 9])\n按元素除法： tensor([nan, 1., 1., 1.])\n按元素幂运算： tensor([ 1,  1,  4, 27])\n按元素开方： tensor([0.0000, 1.0000, 1.4142, 1.7321])\n求幂运算： tensor([ 1.0000,  2.7183,  7.3891, 20.0855])\n沿行合并： tensor([[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]])\n沿列合并： tensor([[0., 0., 0., 0., 1., 1., 1., 1.],\n        [0., 0., 0., 0., 1., 1., 1., 1.],\n        [0., 0., 0., 0., 1., 1., 1., 1.]])\ntensor([[False, False, False, False],\n        [False, False, False, False],\n        [False, False, False, False]])\n求和： tensor(12.)\n</code></pre>\n<h2 id=\"1-3-广播机制\"><a href=\"#1-3-广播机制\" class=\"headerlink\" title=\"1.3 广播机制\"></a>1.3 广播机制</h2><ul>\n<li>对两个形状不同的张量按元素运算时，可能会触发广播机制：先适当复制元素使这两个张量形状相同后再按元素运算</li>\n</ul>\n<ol>\n<li>如果两个张量的维度数不同，可以在较小的张量的形状前面补1，直到两者的维度数相同。</li>\n<li>如果两个张量在某个维度上的大小不同，但其中一个张量在该维度上的大小为1，那么可以通过在该维度上重复扩展该张量，使得两个张量在该维度上的大小相同。</li>\n<li>如果两个张量在某个维度上的大小都不为1，且大小不同，那么会发生形状不匹配，导致无法进行广播</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=torch.arange(<span class=\"number\">3</span>).reshape(<span class=\"number\">3</span>,<span class=\"number\">1</span>)</span><br><span class=\"line\">b=torch.arange(<span class=\"number\">2</span>).reshape(<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;a:&#x27;</span>,a)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;b:&#x27;</span>,b)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;a+b:&#x27;</span>,a+b) <span class=\"comment\">#通常沿着长度为1的维度进行广播</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>a: tensor([[0],\n        [1],\n        [2]])\nb: tensor([[0, 1]])\na+b: tensor([[0, 1],\n        [1, 2],\n        [2, 3]])\n</code></pre>\n<h2 id=\"1-4-索引和切片\"><a href=\"#1-4-索引和切片\" class=\"headerlink\" title=\"1.4 索引和切片\"></a>1.4 索引和切片</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;x:&#x27;</span>,x)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;x[-1]:&#x27;</span>,x[-<span class=\"number\">1</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;x[1:3]:&#x27;</span>,x[<span class=\"number\">1</span>:<span class=\"number\">3</span>]) <span class=\"comment\">#选择第2和第3个元素</span></span><br><span class=\"line\">x[<span class=\"number\">1</span>,<span class=\"number\">2</span>]=<span class=\"number\">9</span> <span class=\"comment\"># 修改元素</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;x[1,2]=9后 x:&#x27;</span>,x)</span><br><span class=\"line\">x[<span class=\"number\">0</span>:<span class=\"number\">2</span>,:]=<span class=\"number\">12</span> <span class=\"comment\"># 修改一行</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;x[0:2,:]=12后 x:&#x27;</span>,x)</span><br></pre></td></tr></table></figure>\n\n<pre><code>x: tensor([[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]])\nx[-1]: tensor([0., 0., 0., 0.])\nx[1:3]: tensor([[0., 0., 0., 0.],\n        [0., 0., 0., 0.]])\nx[1,2]=9后 x: tensor([[0., 0., 0., 0.],\n        [0., 0., 9., 0.],\n        [0., 0., 0., 0.]])\nx[0:2,:]=12后 x: tensor([[12., 12., 12., 12.],\n        [12., 12., 12., 12.],\n        [ 0.,  0.,  0.,  0.]])\n</code></pre>\n<h2 id=\"1-5-节省内存\"><a href=\"#1-5-节省内存\" class=\"headerlink\" title=\"1.5 节省内存\"></a>1.5 节省内存</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">z=torch.zeros(<span class=\"number\">1</span>)</span><br><span class=\"line\">before=<span class=\"built_in\">id</span>(z)</span><br><span class=\"line\">z=z+x <span class=\"comment\">#对于这种类似列表的数据结构，加号会导致新的内存分配</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;z:&#x27;</span>,z)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;id(z)==before:&#x27;</span>,<span class=\"built_in\">id</span>(z)==before) <span class=\"comment\"># False，说明z指向了新的地址</span></span><br><span class=\"line\"></span><br><span class=\"line\">z=torch.zeros(<span class=\"number\">3</span>,<span class=\"number\">4</span>) <span class=\"comment\">#使用如下操作可以避免新的内存分配，但是无法广播，所以这里要求x和z的形状一致</span></span><br><span class=\"line\">before=<span class=\"built_in\">id</span>(z)</span><br><span class=\"line\">z+=x <span class=\"comment\"># +=不会导致新的内存分配</span></span><br><span class=\"line\">z[:]=x+z <span class=\"comment\"># 也不会导致新的内存分配</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;id(z)==before:&#x27;</span>,<span class=\"built_in\">id</span>(z)==before) <span class=\"comment\"># True</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>z: tensor([[12., 12., 12., 12.],\n        [12., 12., 12., 12.],\n        [ 0.,  0.,  0.,  0.]])\nid(z)==before: False\nid(z)==before: True\n</code></pre>\n<h2 id=\"1-6-转换为其他python对象\"><a href=\"#1-6-转换为其他python对象\" class=\"headerlink\" title=\"1.6 转换为其他python对象\"></a>1.6 转换为其他python对象</h2><ul>\n<li>tensor与numpy数组</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A=x.numpy() <span class=\"comment\"># 将张量转换为numpy数组</span></span><br><span class=\"line\">B=torch.tensor(A) <span class=\"comment\"># 将numpy数组转换为张量</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(A,B,sep=<span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">A[<span class=\"number\">1</span>,<span class=\"number\">1</span>]=<span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(A,B,x,sep=<span class=\"string\">&#x27;\\n&#x27;</span>) <span class=\"comment\">#A与X共享内存</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>[[12. 12. 12. 12.]\n [12.  3. 12. 12.]\n [ 0.  0.  0.  0.]]\ntensor([[12., 12., 12., 12.],\n        [12.,  3., 12., 12.],\n        [ 0.,  0.,  0.,  0.]])\n[[12. 12. 12. 12.]\n [12.  3. 12. 12.]\n [ 0.  0.  0.  0.]]\ntensor([[12., 12., 12., 12.],\n        [12.,  3., 12., 12.],\n        [ 0.,  0.,  0.,  0.]])\ntensor([[12., 12., 12., 12.],\n        [12.,  3., 12., 12.],\n        [ 0.,  0.,  0.,  0.]])\n</code></pre>\n<ul>\n<li>张量转为python标量</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=torch.tensor([<span class=\"number\">1.3</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(a)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#由于pytorch中小数用32位，python中用64位；尝试输出.20f就能发现精确度问题</span></span><br><span class=\"line\"><span class=\"comment\">#由于舍入（四舍五入到最接近的偶数），1.3在pytorch中表示是1.299多，而在python中是1.300多</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(a.item()) <span class=\"comment\">#使用item函数</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">float</span>(a)) <span class=\"comment\">#python内置函数</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([1.3000])\n1.2999999523162842\n1.2999999523162842\n</code></pre>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"1-pytorch数据操作\"><a href=\"#1-pytorch数据操作\" class=\"headerlink\" title=\"1. pytorch数据操作\"></a>1. pytorch数据操作</h1><ul>\n<li>pytorch与tensorflow中的Tensor类似于numpy的ndarray</li>\n<li>pytorch中的Tensor可以在GPU上运行</li>\n<li>pytorch中的Tensor可以用于自动求导</li>\n</ul>\n<h2 id=\"1-1-张量\"><a href=\"#1-1-张量\" class=\"headerlink\" title=\"1.1 张量\"></a>1.1 张量</h2><ul>\n<li>创建张量</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 1. 导入torch，不是pytorch</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2. 创建范围张量</span></span><br><span class=\"line\">x=torch.arange(<span class=\"number\">12</span>) <span class=\"comment\"># 0-11,默认为int64，可以指定dtype，默认存储在CPU上</span></span><br><span class=\"line\">y=torch.arange(<span class=\"number\">12</span>,dtype=torch.float32) <span class=\"comment\"># 指定dtype</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 3. 创建全0张量</span></span><br><span class=\"line\">zeros=torch.zeros(<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>) <span class=\"comment\"># 2*3*4的全0张量</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 4. 创建全1张量</span></span><br><span class=\"line\">ones=torch.ones(<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>) <span class=\"comment\"># 2*3*4的全1张量</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 5. 创建采样张量</span></span><br><span class=\"line\">sample=torch.randn(<span class=\"number\">3</span>,<span class=\"number\">4</span>) <span class=\"comment\">#从标准高斯分布中采样</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 6. 列表张量</span></span><br><span class=\"line\">lists=torch.tensor([[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>],[<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>]])</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(x)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(y)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(zeros)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(ones)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(sample)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(lists)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\ntensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])\ntensor([[[0., 0., 0., 0.],\n         [0., 0., 0., 0.],\n         [0., 0., 0., 0.]],\n\n        [[0., 0., 0., 0.],\n         [0., 0., 0., 0.],\n         [0., 0., 0., 0.]]])\ntensor([[[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]],\n\n        [[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]]])\ntensor([[-0.0847, -0.2406,  0.1735, -1.5543],\n        [-0.2820, -0.6689,  0.0565,  0.4746],\n        [ 0.9841, -1.6116, -0.1587, -1.5121]])\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n</code></pre>\n<ul>\n<li>张量的属性</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 3. 查看张量的形状</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;x的规模：&#x27;</span>,x.shape)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 4. 查看张量的元素总数</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;x中元素个数：&#x27;</span>,x.numel())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 5. 改变张量的形状</span></span><br><span class=\"line\">x=x.reshape(<span class=\"number\">3</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;改变x的形状：&#x27;</span>,x)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;自动计算的形状：&#x27;</span>,x.reshape(-<span class=\"number\">1</span>,<span class=\"number\">6</span>)) <span class=\"comment\"># -1表示自动计算这个维度</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>x的规模： torch.Size([12])\nx中元素个数： 12\n改变x的形状： tensor([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]])\n自动计算的形状： tensor([[ 0,  1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10, 11]])\n</code></pre>\n<h2 id=\"1-2-运算符\"><a href=\"#1-2-运算符\" class=\"headerlink\" title=\"1.2 运算符\"></a>1.2 运算符</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=torch.arange(<span class=\"number\">4</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 1. 按元素运算</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;按元素加法：&#x27;</span>,x+x)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;按元素减法：&#x27;</span>,x-x)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;按元素乘法：&#x27;</span>,x*x)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;按元素除法：&#x27;</span>,x/x)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;按元素幂运算：&#x27;</span>,x**x)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;按元素开方：&#x27;</span>,x.sqrt())</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;求幂运算：&#x27;</span>,torch.exp(x))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2. 矩阵运算</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 3. 合并张量(记得加括号)</span></span><br><span class=\"line\">x=torch.zeros(<span class=\"number\">12</span>).reshape(<span class=\"number\">3</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\">y=torch.ones(<span class=\"number\">12</span>).reshape(<span class=\"number\">3</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;沿行合并：&#x27;</span>,torch.cat((x,y),dim=<span class=\"number\">0</span>)) <span class=\"comment\"># 沿行合并</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;沿列合并：&#x27;</span>,torch.cat((x,y),dim=<span class=\"number\">1</span>)) <span class=\"comment\"># 沿列合并</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 4. 逻辑运算</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(x==y)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 5. 求和</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;求和：&#x27;</span>,y.<span class=\"built_in\">sum</span>())</span><br></pre></td></tr></table></figure>\n\n<pre><code>按元素加法： tensor([0, 2, 4, 6])\n按元素减法： tensor([0, 0, 0, 0])\n按元素乘法： tensor([0, 1, 4, 9])\n按元素除法： tensor([nan, 1., 1., 1.])\n按元素幂运算： tensor([ 1,  1,  4, 27])\n按元素开方： tensor([0.0000, 1.0000, 1.4142, 1.7321])\n求幂运算： tensor([ 1.0000,  2.7183,  7.3891, 20.0855])\n沿行合并： tensor([[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]])\n沿列合并： tensor([[0., 0., 0., 0., 1., 1., 1., 1.],\n        [0., 0., 0., 0., 1., 1., 1., 1.],\n        [0., 0., 0., 0., 1., 1., 1., 1.]])\ntensor([[False, False, False, False],\n        [False, False, False, False],\n        [False, False, False, False]])\n求和： tensor(12.)\n</code></pre>\n<h2 id=\"1-3-广播机制\"><a href=\"#1-3-广播机制\" class=\"headerlink\" title=\"1.3 广播机制\"></a>1.3 广播机制</h2><ul>\n<li>对两个形状不同的张量按元素运算时，可能会触发广播机制：先适当复制元素使这两个张量形状相同后再按元素运算</li>\n</ul>\n<ol>\n<li>如果两个张量的维度数不同，可以在较小的张量的形状前面补1，直到两者的维度数相同。</li>\n<li>如果两个张量在某个维度上的大小不同，但其中一个张量在该维度上的大小为1，那么可以通过在该维度上重复扩展该张量，使得两个张量在该维度上的大小相同。</li>\n<li>如果两个张量在某个维度上的大小都不为1，且大小不同，那么会发生形状不匹配，导致无法进行广播</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=torch.arange(<span class=\"number\">3</span>).reshape(<span class=\"number\">3</span>,<span class=\"number\">1</span>)</span><br><span class=\"line\">b=torch.arange(<span class=\"number\">2</span>).reshape(<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;a:&#x27;</span>,a)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;b:&#x27;</span>,b)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;a+b:&#x27;</span>,a+b) <span class=\"comment\">#通常沿着长度为1的维度进行广播</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>a: tensor([[0],\n        [1],\n        [2]])\nb: tensor([[0, 1]])\na+b: tensor([[0, 1],\n        [1, 2],\n        [2, 3]])\n</code></pre>\n<h2 id=\"1-4-索引和切片\"><a href=\"#1-4-索引和切片\" class=\"headerlink\" title=\"1.4 索引和切片\"></a>1.4 索引和切片</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;x:&#x27;</span>,x)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;x[-1]:&#x27;</span>,x[-<span class=\"number\">1</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;x[1:3]:&#x27;</span>,x[<span class=\"number\">1</span>:<span class=\"number\">3</span>]) <span class=\"comment\">#选择第2和第3个元素</span></span><br><span class=\"line\">x[<span class=\"number\">1</span>,<span class=\"number\">2</span>]=<span class=\"number\">9</span> <span class=\"comment\"># 修改元素</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;x[1,2]=9后 x:&#x27;</span>,x)</span><br><span class=\"line\">x[<span class=\"number\">0</span>:<span class=\"number\">2</span>,:]=<span class=\"number\">12</span> <span class=\"comment\"># 修改一行</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;x[0:2,:]=12后 x:&#x27;</span>,x)</span><br></pre></td></tr></table></figure>\n\n<pre><code>x: tensor([[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]])\nx[-1]: tensor([0., 0., 0., 0.])\nx[1:3]: tensor([[0., 0., 0., 0.],\n        [0., 0., 0., 0.]])\nx[1,2]=9后 x: tensor([[0., 0., 0., 0.],\n        [0., 0., 9., 0.],\n        [0., 0., 0., 0.]])\nx[0:2,:]=12后 x: tensor([[12., 12., 12., 12.],\n        [12., 12., 12., 12.],\n        [ 0.,  0.,  0.,  0.]])\n</code></pre>\n<h2 id=\"1-5-节省内存\"><a href=\"#1-5-节省内存\" class=\"headerlink\" title=\"1.5 节省内存\"></a>1.5 节省内存</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">z=torch.zeros(<span class=\"number\">1</span>)</span><br><span class=\"line\">before=<span class=\"built_in\">id</span>(z)</span><br><span class=\"line\">z=z+x <span class=\"comment\">#对于这种类似列表的数据结构，加号会导致新的内存分配</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;z:&#x27;</span>,z)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;id(z)==before:&#x27;</span>,<span class=\"built_in\">id</span>(z)==before) <span class=\"comment\"># False，说明z指向了新的地址</span></span><br><span class=\"line\"></span><br><span class=\"line\">z=torch.zeros(<span class=\"number\">3</span>,<span class=\"number\">4</span>) <span class=\"comment\">#使用如下操作可以避免新的内存分配，但是无法广播，所以这里要求x和z的形状一致</span></span><br><span class=\"line\">before=<span class=\"built_in\">id</span>(z)</span><br><span class=\"line\">z+=x <span class=\"comment\"># +=不会导致新的内存分配</span></span><br><span class=\"line\">z[:]=x+z <span class=\"comment\"># 也不会导致新的内存分配</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;id(z)==before:&#x27;</span>,<span class=\"built_in\">id</span>(z)==before) <span class=\"comment\"># True</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>z: tensor([[12., 12., 12., 12.],\n        [12., 12., 12., 12.],\n        [ 0.,  0.,  0.,  0.]])\nid(z)==before: False\nid(z)==before: True\n</code></pre>\n<h2 id=\"1-6-转换为其他python对象\"><a href=\"#1-6-转换为其他python对象\" class=\"headerlink\" title=\"1.6 转换为其他python对象\"></a>1.6 转换为其他python对象</h2><ul>\n<li>tensor与numpy数组</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A=x.numpy() <span class=\"comment\"># 将张量转换为numpy数组</span></span><br><span class=\"line\">B=torch.tensor(A) <span class=\"comment\"># 将numpy数组转换为张量</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(A,B,sep=<span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">A[<span class=\"number\">1</span>,<span class=\"number\">1</span>]=<span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(A,B,x,sep=<span class=\"string\">&#x27;\\n&#x27;</span>) <span class=\"comment\">#A与X共享内存</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>[[12. 12. 12. 12.]\n [12.  3. 12. 12.]\n [ 0.  0.  0.  0.]]\ntensor([[12., 12., 12., 12.],\n        [12.,  3., 12., 12.],\n        [ 0.,  0.,  0.,  0.]])\n[[12. 12. 12. 12.]\n [12.  3. 12. 12.]\n [ 0.  0.  0.  0.]]\ntensor([[12., 12., 12., 12.],\n        [12.,  3., 12., 12.],\n        [ 0.,  0.,  0.,  0.]])\ntensor([[12., 12., 12., 12.],\n        [12.,  3., 12., 12.],\n        [ 0.,  0.,  0.,  0.]])\n</code></pre>\n<ul>\n<li>张量转为python标量</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=torch.tensor([<span class=\"number\">1.3</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(a)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#由于pytorch中小数用32位，python中用64位；尝试输出.20f就能发现精确度问题</span></span><br><span class=\"line\"><span class=\"comment\">#由于舍入（四舍五入到最接近的偶数），1.3在pytorch中表示是1.299多，而在python中是1.300多</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(a.item()) <span class=\"comment\">#使用item函数</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">float</span>(a)) <span class=\"comment\">#python内置函数</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([1.3000])\n1.2999999523162842\n1.2999999523162842\n</code></pre>"},{"title":"1.2 pytorch数据预处理","date":"2023-11-27T06:00:00.000Z","comment":false,"toc":true,"_content":"#  \n<!--more-->\n\n# 2. pytorch数据预处理\n- 主要通过pandas预处理\n\n## 2.1 读取数据集\n\n\n```python\n# 1. 先自己准备一个数据集\nimport os\nos.makedirs(os.path.join('..','data'),exist_ok=True)\ndata_file=os.path.join('..','data','house_tiny.csv')\nwith open(data_file,'w') as f:\n    f.write('NumRooms, Alley, Price\\n') #列名\n    f.write('NA,Pave, 127500\\n') #每行表示一个数据样本\n    f.write('2,NA, 106000\\n')\n    f.write('4,NA, 178100\\n')\n    f.write('NA,NA, 14000\\n')\n\n# 2. 读取数据集\nimport pandas as pd\ndata=pd.read_csv(data_file)\nprint(data)\n```\n\n       NumRooms  Alley   Price\n    0       NaN   Pave  127500\n    1       2.0    NaN  106000\n    2       4.0    NaN  178100\n    3       NaN    NaN   14000\n    \n\n## 2.2 缺失值处理\n- 删除\n- 插值\n\n### 2.2.1 数值类型\n\n\n```python\n# 1. 位置索引iloc将data分为输入与输出\ninputs=data.iloc[:,0:2] #前两列作为输入\noutputs=data.iloc[:,2] #第三列作为输出\n\n# 2. 用每一列的均值替换空值\ninputs=inputs.fillna(inputs.mean())\nprint(inputs)\n```\n\n       NumRooms  Alley\n    0       3.0   Pave\n    1       2.0    NaN\n    2       4.0    NaN\n    3       3.0    NaN\n    \n\n### 2.2.2 类别类型或离散值\n- 将NaN视为一个类别，根据这一列类别的个数分出n列，每一列代表一个类别，如果该行的值为该列的类别，则为1，否则为0\n\n\n```python\ninputs=pd.get_dummies(inputs, dummy_na=True)\nprint(inputs)\n```\n\n       NumRooms   Alley_Pave   Alley_nan\n    0       3.0            1           0\n    1       2.0            0           1\n    2       4.0            0           1\n    3       3.0            0           1\n    \n\n## 2.3 转换为张量\n- torch.tensor()\n\n\n```python\nimport torch\nx=torch.tensor(inputs.to_numpy(dtype=float))\ny=torch.tensor(outputs.to_numpy(dtype=float))\nx,y\n```\n\n\n\n\n    (tensor([[3., 1., 0.],\n             [2., 0., 1.],\n             [4., 0., 1.],\n             [3., 0., 1.]], dtype=torch.float64),\n     tensor([127500., 106000., 178100.,  14000.], dtype=torch.float64))\n\n\n\n## 练习\n- 1. 创建一个更多行和列的数据集\n\n\n\n```python\nimport os\nos.makedirs(os.path.join('..','data'),exist_ok=True)\ndata_file=os.path.join('..','data','house_tiny.csv')\nwith open(data_file, 'w') as f:\n    f.write('NumRooms, Alley, Price, test\\n') # 列名\n    f.write('NA, Pave, 127500, 1\\n') # 每行表示一个数据样本\n    f.write('2, NA, 106000, 0\\n')\n    f.write('4, NA, 178100, 0\\n')\n    f.write('NA, NA, 140000, 1\\n')\n    f.write('2, Pave, 127500, 0\\n')\n\n#读取数据集\nimport pandas as pd\nprint(pd.read_csv(data_file))\n```\n\n       NumRooms  Alley   Price   test\n    0       NaN   Pave  127500      1\n    1       2.0     NA  106000      0\n    2       4.0     NA  178100      0\n    3       NaN     NA  140000      1\n    4       2.0   Pave  127500      0\n    \n\n- 2. 删除缺失值最多的列\n\n\n```python\n#删除缺失值最多的列\nimport pandas as pd\ndata=pd.read_csv(data_file)\n\n#计算每一列的缺失值个数\nmissing=data.isnull().sum() #按列求和\nprint(missing)\ncolumn=missing.idxmax() #返回缺失值最多的列名\nprint(column)\ndata=data.drop(columns=[column])\nprint(data)\n\n#保存处理后的数据集\ndata.to_csv(data_file,index=False) #index=False表示不保存行索引\n```\n\n    NumRooms    2\n     Alley      0\n     Price      0\n     test       0\n    dtype: int64\n    NumRooms\n       Alley   Price   test\n    0   Pave  127500      1\n    1     NA  106000      0\n    2     NA  178100      0\n    3     NA  140000      1\n    4   Pave  127500      0\n    \n\n- 3. 将处理后的数据集转换为张量\n\n\n```python\n#将数据集转换为张量格式\nimport torch\nimport os\ndata_file=os.path.join('..','data','house_tiny.csv')\ndata=pd.read_csv(data_file)\n#输出data的shape\nprint(data,data.shape,sep='\\n')\n\n#将字符串类型进行one-hot编码\ndata=pd.get_dummies(data,dummy_na=True) #dummy_na=True表示将缺失值也当作合法的特征值并为其创建指示特征\nprint(data)\n#将dataframe格式转换为张量格式\ndata=torch.tensor(data.to_numpy(dtype=float))\nprint(data)\n```\n\n       Alley   Price   test\n    0   Pave  127500      1\n    1     NA  106000      0\n    2     NA  178100      0\n    3     NA  140000      1\n    4   Pave  127500      0\n    (5, 3)\n        Price   test   Alley_ NA   Alley_ Pave   Alley_nan\n    0  127500      1           0             1           0\n    1  106000      0           1             0           0\n    2  178100      0           1             0           0\n    3  140000      1           1             0           0\n    4  127500      0           0             1           0\n    tensor([[1.2750e+05, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00],\n            [1.0600e+05, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00],\n            [1.7810e+05, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00],\n            [1.4000e+05, 1.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00],\n            [1.2750e+05, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00]],\n           dtype=torch.float64)\n    \n","source":"_posts/deeplearning/code/pytorch/1_prepare/2_preprocessing.md","raw":"---\ntitle: 1.2 pytorch数据预处理\ndate: 2023-11-27 14:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: false\ntoc: true\n---\n#  \n<!--more-->\n\n# 2. pytorch数据预处理\n- 主要通过pandas预处理\n\n## 2.1 读取数据集\n\n\n```python\n# 1. 先自己准备一个数据集\nimport os\nos.makedirs(os.path.join('..','data'),exist_ok=True)\ndata_file=os.path.join('..','data','house_tiny.csv')\nwith open(data_file,'w') as f:\n    f.write('NumRooms, Alley, Price\\n') #列名\n    f.write('NA,Pave, 127500\\n') #每行表示一个数据样本\n    f.write('2,NA, 106000\\n')\n    f.write('4,NA, 178100\\n')\n    f.write('NA,NA, 14000\\n')\n\n# 2. 读取数据集\nimport pandas as pd\ndata=pd.read_csv(data_file)\nprint(data)\n```\n\n       NumRooms  Alley   Price\n    0       NaN   Pave  127500\n    1       2.0    NaN  106000\n    2       4.0    NaN  178100\n    3       NaN    NaN   14000\n    \n\n## 2.2 缺失值处理\n- 删除\n- 插值\n\n### 2.2.1 数值类型\n\n\n```python\n# 1. 位置索引iloc将data分为输入与输出\ninputs=data.iloc[:,0:2] #前两列作为输入\noutputs=data.iloc[:,2] #第三列作为输出\n\n# 2. 用每一列的均值替换空值\ninputs=inputs.fillna(inputs.mean())\nprint(inputs)\n```\n\n       NumRooms  Alley\n    0       3.0   Pave\n    1       2.0    NaN\n    2       4.0    NaN\n    3       3.0    NaN\n    \n\n### 2.2.2 类别类型或离散值\n- 将NaN视为一个类别，根据这一列类别的个数分出n列，每一列代表一个类别，如果该行的值为该列的类别，则为1，否则为0\n\n\n```python\ninputs=pd.get_dummies(inputs, dummy_na=True)\nprint(inputs)\n```\n\n       NumRooms   Alley_Pave   Alley_nan\n    0       3.0            1           0\n    1       2.0            0           1\n    2       4.0            0           1\n    3       3.0            0           1\n    \n\n## 2.3 转换为张量\n- torch.tensor()\n\n\n```python\nimport torch\nx=torch.tensor(inputs.to_numpy(dtype=float))\ny=torch.tensor(outputs.to_numpy(dtype=float))\nx,y\n```\n\n\n\n\n    (tensor([[3., 1., 0.],\n             [2., 0., 1.],\n             [4., 0., 1.],\n             [3., 0., 1.]], dtype=torch.float64),\n     tensor([127500., 106000., 178100.,  14000.], dtype=torch.float64))\n\n\n\n## 练习\n- 1. 创建一个更多行和列的数据集\n\n\n\n```python\nimport os\nos.makedirs(os.path.join('..','data'),exist_ok=True)\ndata_file=os.path.join('..','data','house_tiny.csv')\nwith open(data_file, 'w') as f:\n    f.write('NumRooms, Alley, Price, test\\n') # 列名\n    f.write('NA, Pave, 127500, 1\\n') # 每行表示一个数据样本\n    f.write('2, NA, 106000, 0\\n')\n    f.write('4, NA, 178100, 0\\n')\n    f.write('NA, NA, 140000, 1\\n')\n    f.write('2, Pave, 127500, 0\\n')\n\n#读取数据集\nimport pandas as pd\nprint(pd.read_csv(data_file))\n```\n\n       NumRooms  Alley   Price   test\n    0       NaN   Pave  127500      1\n    1       2.0     NA  106000      0\n    2       4.0     NA  178100      0\n    3       NaN     NA  140000      1\n    4       2.0   Pave  127500      0\n    \n\n- 2. 删除缺失值最多的列\n\n\n```python\n#删除缺失值最多的列\nimport pandas as pd\ndata=pd.read_csv(data_file)\n\n#计算每一列的缺失值个数\nmissing=data.isnull().sum() #按列求和\nprint(missing)\ncolumn=missing.idxmax() #返回缺失值最多的列名\nprint(column)\ndata=data.drop(columns=[column])\nprint(data)\n\n#保存处理后的数据集\ndata.to_csv(data_file,index=False) #index=False表示不保存行索引\n```\n\n    NumRooms    2\n     Alley      0\n     Price      0\n     test       0\n    dtype: int64\n    NumRooms\n       Alley   Price   test\n    0   Pave  127500      1\n    1     NA  106000      0\n    2     NA  178100      0\n    3     NA  140000      1\n    4   Pave  127500      0\n    \n\n- 3. 将处理后的数据集转换为张量\n\n\n```python\n#将数据集转换为张量格式\nimport torch\nimport os\ndata_file=os.path.join('..','data','house_tiny.csv')\ndata=pd.read_csv(data_file)\n#输出data的shape\nprint(data,data.shape,sep='\\n')\n\n#将字符串类型进行one-hot编码\ndata=pd.get_dummies(data,dummy_na=True) #dummy_na=True表示将缺失值也当作合法的特征值并为其创建指示特征\nprint(data)\n#将dataframe格式转换为张量格式\ndata=torch.tensor(data.to_numpy(dtype=float))\nprint(data)\n```\n\n       Alley   Price   test\n    0   Pave  127500      1\n    1     NA  106000      0\n    2     NA  178100      0\n    3     NA  140000      1\n    4   Pave  127500      0\n    (5, 3)\n        Price   test   Alley_ NA   Alley_ Pave   Alley_nan\n    0  127500      1           0             1           0\n    1  106000      0           1             0           0\n    2  178100      0           1             0           0\n    3  140000      1           1             0           0\n    4  127500      0           0             1           0\n    tensor([[1.2750e+05, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00],\n            [1.0600e+05, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00],\n            [1.7810e+05, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00],\n            [1.4000e+05, 1.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00],\n            [1.2750e+05, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00]],\n           dtype=torch.float64)\n    \n","slug":"deeplearning/code/pytorch/1_prepare/2_preprocessing","published":1,"updated":"2024-05-02T08:56:01.084Z","_id":"clvulgd6d003h5ovw7ohf656t","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h1 id=\"2-pytorch数据预处理\"><a href=\"#2-pytorch数据预处理\" class=\"headerlink\" title=\"2. pytorch数据预处理\"></a>2. pytorch数据预处理</h1><ul>\n<li>主要通过pandas预处理</li>\n</ul>\n<h2 id=\"2-1-读取数据集\"><a href=\"#2-1-读取数据集\" class=\"headerlink\" title=\"2.1 读取数据集\"></a>2.1 读取数据集</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 1. 先自己准备一个数据集</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.makedirs(os.path.join(<span class=\"string\">&#x27;..&#x27;</span>,<span class=\"string\">&#x27;data&#x27;</span>),exist_ok=<span class=\"literal\">True</span>)</span><br><span class=\"line\">data_file=os.path.join(<span class=\"string\">&#x27;..&#x27;</span>,<span class=\"string\">&#x27;data&#x27;</span>,<span class=\"string\">&#x27;house_tiny.csv&#x27;</span>)</span><br><span class=\"line\"><span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(data_file,<span class=\"string\">&#x27;w&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">    f.write(<span class=\"string\">&#x27;NumRooms, Alley, Price\\n&#x27;</span>) <span class=\"comment\">#列名</span></span><br><span class=\"line\">    f.write(<span class=\"string\">&#x27;NA,Pave, 127500\\n&#x27;</span>) <span class=\"comment\">#每行表示一个数据样本</span></span><br><span class=\"line\">    f.write(<span class=\"string\">&#x27;2,NA, 106000\\n&#x27;</span>)</span><br><span class=\"line\">    f.write(<span class=\"string\">&#x27;4,NA, 178100\\n&#x27;</span>)</span><br><span class=\"line\">    f.write(<span class=\"string\">&#x27;NA,NA, 14000\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2. 读取数据集</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\">data=pd.read_csv(data_file)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(data)</span><br></pre></td></tr></table></figure>\n\n<pre><code>   NumRooms  Alley   Price\n0       NaN   Pave  127500\n1       2.0    NaN  106000\n2       4.0    NaN  178100\n3       NaN    NaN   14000\n</code></pre>\n<h2 id=\"2-2-缺失值处理\"><a href=\"#2-2-缺失值处理\" class=\"headerlink\" title=\"2.2 缺失值处理\"></a>2.2 缺失值处理</h2><ul>\n<li>删除</li>\n<li>插值</li>\n</ul>\n<h3 id=\"2-2-1-数值类型\"><a href=\"#2-2-1-数值类型\" class=\"headerlink\" title=\"2.2.1 数值类型\"></a>2.2.1 数值类型</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 1. 位置索引iloc将data分为输入与输出</span></span><br><span class=\"line\">inputs=data.iloc[:,<span class=\"number\">0</span>:<span class=\"number\">2</span>] <span class=\"comment\">#前两列作为输入</span></span><br><span class=\"line\">outputs=data.iloc[:,<span class=\"number\">2</span>] <span class=\"comment\">#第三列作为输出</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2. 用每一列的均值替换空值</span></span><br><span class=\"line\">inputs=inputs.fillna(inputs.mean())</span><br><span class=\"line\"><span class=\"built_in\">print</span>(inputs)</span><br></pre></td></tr></table></figure>\n\n<pre><code>   NumRooms  Alley\n0       3.0   Pave\n1       2.0    NaN\n2       4.0    NaN\n3       3.0    NaN\n</code></pre>\n<h3 id=\"2-2-2-类别类型或离散值\"><a href=\"#2-2-2-类别类型或离散值\" class=\"headerlink\" title=\"2.2.2 类别类型或离散值\"></a>2.2.2 类别类型或离散值</h3><ul>\n<li>将NaN视为一个类别，根据这一列类别的个数分出n列，每一列代表一个类别，如果该行的值为该列的类别，则为1，否则为0</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inputs=pd.get_dummies(inputs, dummy_na=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(inputs)</span><br></pre></td></tr></table></figure>\n\n<pre><code>   NumRooms   Alley_Pave   Alley_nan\n0       3.0            1           0\n1       2.0            0           1\n2       4.0            0           1\n3       3.0            0           1\n</code></pre>\n<h2 id=\"2-3-转换为张量\"><a href=\"#2-3-转换为张量\" class=\"headerlink\" title=\"2.3 转换为张量\"></a>2.3 转换为张量</h2><ul>\n<li>torch.tensor()</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\">x=torch.tensor(inputs.to_numpy(dtype=<span class=\"built_in\">float</span>))</span><br><span class=\"line\">y=torch.tensor(outputs.to_numpy(dtype=<span class=\"built_in\">float</span>))</span><br><span class=\"line\">x,y</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>(tensor([[3., 1., 0.],\n         [2., 0., 1.],\n         [4., 0., 1.],\n         [3., 0., 1.]], dtype=torch.float64),\n tensor([127500., 106000., 178100.,  14000.], dtype=torch.float64))\n</code></pre>\n<h2 id=\"练习\"><a href=\"#练习\" class=\"headerlink\" title=\"练习\"></a>练习</h2><ul>\n<li><ol>\n<li>创建一个更多行和列的数据集</li>\n</ol>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.makedirs(os.path.join(<span class=\"string\">&#x27;..&#x27;</span>,<span class=\"string\">&#x27;data&#x27;</span>),exist_ok=<span class=\"literal\">True</span>)</span><br><span class=\"line\">data_file=os.path.join(<span class=\"string\">&#x27;..&#x27;</span>,<span class=\"string\">&#x27;data&#x27;</span>,<span class=\"string\">&#x27;house_tiny.csv&#x27;</span>)</span><br><span class=\"line\"><span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(data_file, <span class=\"string\">&#x27;w&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">    f.write(<span class=\"string\">&#x27;NumRooms, Alley, Price, test\\n&#x27;</span>) <span class=\"comment\"># 列名</span></span><br><span class=\"line\">    f.write(<span class=\"string\">&#x27;NA, Pave, 127500, 1\\n&#x27;</span>) <span class=\"comment\"># 每行表示一个数据样本</span></span><br><span class=\"line\">    f.write(<span class=\"string\">&#x27;2, NA, 106000, 0\\n&#x27;</span>)</span><br><span class=\"line\">    f.write(<span class=\"string\">&#x27;4, NA, 178100, 0\\n&#x27;</span>)</span><br><span class=\"line\">    f.write(<span class=\"string\">&#x27;NA, NA, 140000, 1\\n&#x27;</span>)</span><br><span class=\"line\">    f.write(<span class=\"string\">&#x27;2, Pave, 127500, 0\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#读取数据集</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"built_in\">print</span>(pd.read_csv(data_file))</span><br></pre></td></tr></table></figure>\n\n<pre><code>   NumRooms  Alley   Price   test\n0       NaN   Pave  127500      1\n1       2.0     NA  106000      0\n2       4.0     NA  178100      0\n3       NaN     NA  140000      1\n4       2.0   Pave  127500      0\n</code></pre>\n<ul>\n<li><ol start=\"2\">\n<li>删除缺失值最多的列</li>\n</ol>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#删除缺失值最多的列</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\">data=pd.read_csv(data_file)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#计算每一列的缺失值个数</span></span><br><span class=\"line\">missing=data.isnull().<span class=\"built_in\">sum</span>() <span class=\"comment\">#按列求和</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(missing)</span><br><span class=\"line\">column=missing.idxmax() <span class=\"comment\">#返回缺失值最多的列名</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(column)</span><br><span class=\"line\">data=data.drop(columns=[column])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(data)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#保存处理后的数据集</span></span><br><span class=\"line\">data.to_csv(data_file,index=<span class=\"literal\">False</span>) <span class=\"comment\">#index=False表示不保存行索引</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>NumRooms    2\n Alley      0\n Price      0\n test       0\ndtype: int64\nNumRooms\n   Alley   Price   test\n0   Pave  127500      1\n1     NA  106000      0\n2     NA  178100      0\n3     NA  140000      1\n4   Pave  127500      0\n</code></pre>\n<ul>\n<li><ol start=\"3\">\n<li>将处理后的数据集转换为张量</li>\n</ol>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#将数据集转换为张量格式</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">data_file=os.path.join(<span class=\"string\">&#x27;..&#x27;</span>,<span class=\"string\">&#x27;data&#x27;</span>,<span class=\"string\">&#x27;house_tiny.csv&#x27;</span>)</span><br><span class=\"line\">data=pd.read_csv(data_file)</span><br><span class=\"line\"><span class=\"comment\">#输出data的shape</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(data,data.shape,sep=<span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#将字符串类型进行one-hot编码</span></span><br><span class=\"line\">data=pd.get_dummies(data,dummy_na=<span class=\"literal\">True</span>) <span class=\"comment\">#dummy_na=True表示将缺失值也当作合法的特征值并为其创建指示特征</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(data)</span><br><span class=\"line\"><span class=\"comment\">#将dataframe格式转换为张量格式</span></span><br><span class=\"line\">data=torch.tensor(data.to_numpy(dtype=<span class=\"built_in\">float</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(data)</span><br></pre></td></tr></table></figure>\n\n<pre><code>   Alley   Price   test\n0   Pave  127500      1\n1     NA  106000      0\n2     NA  178100      0\n3     NA  140000      1\n4   Pave  127500      0\n(5, 3)\n    Price   test   Alley_ NA   Alley_ Pave   Alley_nan\n0  127500      1           0             1           0\n1  106000      0           1             0           0\n2  178100      0           1             0           0\n3  140000      1           1             0           0\n4  127500      0           0             1           0\ntensor([[1.2750e+05, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00],\n        [1.0600e+05, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00],\n        [1.7810e+05, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00],\n        [1.4000e+05, 1.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00],\n        [1.2750e+05, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00]],\n       dtype=torch.float64)\n</code></pre>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"2-pytorch数据预处理\"><a href=\"#2-pytorch数据预处理\" class=\"headerlink\" title=\"2. pytorch数据预处理\"></a>2. pytorch数据预处理</h1><ul>\n<li>主要通过pandas预处理</li>\n</ul>\n<h2 id=\"2-1-读取数据集\"><a href=\"#2-1-读取数据集\" class=\"headerlink\" title=\"2.1 读取数据集\"></a>2.1 读取数据集</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 1. 先自己准备一个数据集</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.makedirs(os.path.join(<span class=\"string\">&#x27;..&#x27;</span>,<span class=\"string\">&#x27;data&#x27;</span>),exist_ok=<span class=\"literal\">True</span>)</span><br><span class=\"line\">data_file=os.path.join(<span class=\"string\">&#x27;..&#x27;</span>,<span class=\"string\">&#x27;data&#x27;</span>,<span class=\"string\">&#x27;house_tiny.csv&#x27;</span>)</span><br><span class=\"line\"><span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(data_file,<span class=\"string\">&#x27;w&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">    f.write(<span class=\"string\">&#x27;NumRooms, Alley, Price\\n&#x27;</span>) <span class=\"comment\">#列名</span></span><br><span class=\"line\">    f.write(<span class=\"string\">&#x27;NA,Pave, 127500\\n&#x27;</span>) <span class=\"comment\">#每行表示一个数据样本</span></span><br><span class=\"line\">    f.write(<span class=\"string\">&#x27;2,NA, 106000\\n&#x27;</span>)</span><br><span class=\"line\">    f.write(<span class=\"string\">&#x27;4,NA, 178100\\n&#x27;</span>)</span><br><span class=\"line\">    f.write(<span class=\"string\">&#x27;NA,NA, 14000\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2. 读取数据集</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\">data=pd.read_csv(data_file)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(data)</span><br></pre></td></tr></table></figure>\n\n<pre><code>   NumRooms  Alley   Price\n0       NaN   Pave  127500\n1       2.0    NaN  106000\n2       4.0    NaN  178100\n3       NaN    NaN   14000\n</code></pre>\n<h2 id=\"2-2-缺失值处理\"><a href=\"#2-2-缺失值处理\" class=\"headerlink\" title=\"2.2 缺失值处理\"></a>2.2 缺失值处理</h2><ul>\n<li>删除</li>\n<li>插值</li>\n</ul>\n<h3 id=\"2-2-1-数值类型\"><a href=\"#2-2-1-数值类型\" class=\"headerlink\" title=\"2.2.1 数值类型\"></a>2.2.1 数值类型</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 1. 位置索引iloc将data分为输入与输出</span></span><br><span class=\"line\">inputs=data.iloc[:,<span class=\"number\">0</span>:<span class=\"number\">2</span>] <span class=\"comment\">#前两列作为输入</span></span><br><span class=\"line\">outputs=data.iloc[:,<span class=\"number\">2</span>] <span class=\"comment\">#第三列作为输出</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2. 用每一列的均值替换空值</span></span><br><span class=\"line\">inputs=inputs.fillna(inputs.mean())</span><br><span class=\"line\"><span class=\"built_in\">print</span>(inputs)</span><br></pre></td></tr></table></figure>\n\n<pre><code>   NumRooms  Alley\n0       3.0   Pave\n1       2.0    NaN\n2       4.0    NaN\n3       3.0    NaN\n</code></pre>\n<h3 id=\"2-2-2-类别类型或离散值\"><a href=\"#2-2-2-类别类型或离散值\" class=\"headerlink\" title=\"2.2.2 类别类型或离散值\"></a>2.2.2 类别类型或离散值</h3><ul>\n<li>将NaN视为一个类别，根据这一列类别的个数分出n列，每一列代表一个类别，如果该行的值为该列的类别，则为1，否则为0</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inputs=pd.get_dummies(inputs, dummy_na=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(inputs)</span><br></pre></td></tr></table></figure>\n\n<pre><code>   NumRooms   Alley_Pave   Alley_nan\n0       3.0            1           0\n1       2.0            0           1\n2       4.0            0           1\n3       3.0            0           1\n</code></pre>\n<h2 id=\"2-3-转换为张量\"><a href=\"#2-3-转换为张量\" class=\"headerlink\" title=\"2.3 转换为张量\"></a>2.3 转换为张量</h2><ul>\n<li>torch.tensor()</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\">x=torch.tensor(inputs.to_numpy(dtype=<span class=\"built_in\">float</span>))</span><br><span class=\"line\">y=torch.tensor(outputs.to_numpy(dtype=<span class=\"built_in\">float</span>))</span><br><span class=\"line\">x,y</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>(tensor([[3., 1., 0.],\n         [2., 0., 1.],\n         [4., 0., 1.],\n         [3., 0., 1.]], dtype=torch.float64),\n tensor([127500., 106000., 178100.,  14000.], dtype=torch.float64))\n</code></pre>\n<h2 id=\"练习\"><a href=\"#练习\" class=\"headerlink\" title=\"练习\"></a>练习</h2><ul>\n<li><ol>\n<li>创建一个更多行和列的数据集</li>\n</ol>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.makedirs(os.path.join(<span class=\"string\">&#x27;..&#x27;</span>,<span class=\"string\">&#x27;data&#x27;</span>),exist_ok=<span class=\"literal\">True</span>)</span><br><span class=\"line\">data_file=os.path.join(<span class=\"string\">&#x27;..&#x27;</span>,<span class=\"string\">&#x27;data&#x27;</span>,<span class=\"string\">&#x27;house_tiny.csv&#x27;</span>)</span><br><span class=\"line\"><span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(data_file, <span class=\"string\">&#x27;w&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">    f.write(<span class=\"string\">&#x27;NumRooms, Alley, Price, test\\n&#x27;</span>) <span class=\"comment\"># 列名</span></span><br><span class=\"line\">    f.write(<span class=\"string\">&#x27;NA, Pave, 127500, 1\\n&#x27;</span>) <span class=\"comment\"># 每行表示一个数据样本</span></span><br><span class=\"line\">    f.write(<span class=\"string\">&#x27;2, NA, 106000, 0\\n&#x27;</span>)</span><br><span class=\"line\">    f.write(<span class=\"string\">&#x27;4, NA, 178100, 0\\n&#x27;</span>)</span><br><span class=\"line\">    f.write(<span class=\"string\">&#x27;NA, NA, 140000, 1\\n&#x27;</span>)</span><br><span class=\"line\">    f.write(<span class=\"string\">&#x27;2, Pave, 127500, 0\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#读取数据集</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"built_in\">print</span>(pd.read_csv(data_file))</span><br></pre></td></tr></table></figure>\n\n<pre><code>   NumRooms  Alley   Price   test\n0       NaN   Pave  127500      1\n1       2.0     NA  106000      0\n2       4.0     NA  178100      0\n3       NaN     NA  140000      1\n4       2.0   Pave  127500      0\n</code></pre>\n<ul>\n<li><ol start=\"2\">\n<li>删除缺失值最多的列</li>\n</ol>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#删除缺失值最多的列</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\">data=pd.read_csv(data_file)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#计算每一列的缺失值个数</span></span><br><span class=\"line\">missing=data.isnull().<span class=\"built_in\">sum</span>() <span class=\"comment\">#按列求和</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(missing)</span><br><span class=\"line\">column=missing.idxmax() <span class=\"comment\">#返回缺失值最多的列名</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(column)</span><br><span class=\"line\">data=data.drop(columns=[column])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(data)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#保存处理后的数据集</span></span><br><span class=\"line\">data.to_csv(data_file,index=<span class=\"literal\">False</span>) <span class=\"comment\">#index=False表示不保存行索引</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>NumRooms    2\n Alley      0\n Price      0\n test       0\ndtype: int64\nNumRooms\n   Alley   Price   test\n0   Pave  127500      1\n1     NA  106000      0\n2     NA  178100      0\n3     NA  140000      1\n4   Pave  127500      0\n</code></pre>\n<ul>\n<li><ol start=\"3\">\n<li>将处理后的数据集转换为张量</li>\n</ol>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#将数据集转换为张量格式</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">data_file=os.path.join(<span class=\"string\">&#x27;..&#x27;</span>,<span class=\"string\">&#x27;data&#x27;</span>,<span class=\"string\">&#x27;house_tiny.csv&#x27;</span>)</span><br><span class=\"line\">data=pd.read_csv(data_file)</span><br><span class=\"line\"><span class=\"comment\">#输出data的shape</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(data,data.shape,sep=<span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#将字符串类型进行one-hot编码</span></span><br><span class=\"line\">data=pd.get_dummies(data,dummy_na=<span class=\"literal\">True</span>) <span class=\"comment\">#dummy_na=True表示将缺失值也当作合法的特征值并为其创建指示特征</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(data)</span><br><span class=\"line\"><span class=\"comment\">#将dataframe格式转换为张量格式</span></span><br><span class=\"line\">data=torch.tensor(data.to_numpy(dtype=<span class=\"built_in\">float</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(data)</span><br></pre></td></tr></table></figure>\n\n<pre><code>   Alley   Price   test\n0   Pave  127500      1\n1     NA  106000      0\n2     NA  178100      0\n3     NA  140000      1\n4   Pave  127500      0\n(5, 3)\n    Price   test   Alley_ NA   Alley_ Pave   Alley_nan\n0  127500      1           0             1           0\n1  106000      0           1             0           0\n2  178100      0           1             0           0\n3  140000      1           1             0           0\n4  127500      0           0             1           0\ntensor([[1.2750e+05, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00],\n        [1.0600e+05, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00],\n        [1.7810e+05, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00],\n        [1.4000e+05, 1.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00],\n        [1.2750e+05, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00]],\n       dtype=torch.float64)\n</code></pre>"},{"title":"1-1. 快速排序","date":"2023-10-01T16:00:00.000Z","comment":false,"toc":true,"_content":"#\n<!--more-->\n\n# 1 快速排序\n\n### 1. 1 思想\n\n```\n1. 确定分界点\n\t- q[l]\n\t- q[r]\n\t- q[(l+r)/2]\n\t- q[random]\n2. 调整区间（*）\n\t- 左边<=x x0 右边>=x\n\t- x不一定在x0位置\n3. 递归两端\n```\n```\n1. 暴力求解\n\ta[], b[], c[],对c进行排序，<=x的放到a，>=x的放到b。放完后先将a放到c，再将b放到c.\n\t\n2. 优化空间\n\t定义左右指针i,j当c[i]>x时停，当c[j]<x时停。两个都停之后交换。\n```\n### 1.2 算法模板\n```c\n#include<iostream>\nusing namespace std;\nint a[100000];\nvoid qsort(int a[],int l, int r){\n    if(l>=r) return;\n    int i=l-1,j=r+1;\n    int x=a[((r-l)>>1)+l]; //这里要用a[]不要只计算索引，因为后续该索引位置上的元素可能会改变。\n    while(i<j){\n        do i++; while(a[i]<x);\n        do j--; while(a[j]>x);\n        if(i<j) swap(a[i],a[j]);\n    }\n    qsort(a,l,j);\n    qsort(a,j+1,r);\n}\n\nint main(){\n    int n;\n    scanf(\"%d\",&n);\n    for(int i=0;i<n;i++) scanf(\"%d\",&a[i]);\n    qsort(a,0,n-1);\n    for(int i=0;i<n;i++) printf(\"%d \",a[i]);\n}\n```\n```\n//输入\n5\n5 4 3 2 1\n```\n\n\n\n- 可以考虑使用i或j\n- 可以考虑使用while(a[i++])\n\n- 快排不稳定，如果要变成稳定的，可以变成<a[i], i>\n\n\n","source":"_posts/algorithm/1_base/1_qsort.md","raw":"---\ntitle: 1-1. 快速排序\n\ndate: 2023-10-02\n\ntags: [算法，排序]\n\ncategories: [算法]\n\ncomment: false\n\ntoc: true\n---\n#\n<!--more-->\n\n# 1 快速排序\n\n### 1. 1 思想\n\n```\n1. 确定分界点\n\t- q[l]\n\t- q[r]\n\t- q[(l+r)/2]\n\t- q[random]\n2. 调整区间（*）\n\t- 左边<=x x0 右边>=x\n\t- x不一定在x0位置\n3. 递归两端\n```\n```\n1. 暴力求解\n\ta[], b[], c[],对c进行排序，<=x的放到a，>=x的放到b。放完后先将a放到c，再将b放到c.\n\t\n2. 优化空间\n\t定义左右指针i,j当c[i]>x时停，当c[j]<x时停。两个都停之后交换。\n```\n### 1.2 算法模板\n```c\n#include<iostream>\nusing namespace std;\nint a[100000];\nvoid qsort(int a[],int l, int r){\n    if(l>=r) return;\n    int i=l-1,j=r+1;\n    int x=a[((r-l)>>1)+l]; //这里要用a[]不要只计算索引，因为后续该索引位置上的元素可能会改变。\n    while(i<j){\n        do i++; while(a[i]<x);\n        do j--; while(a[j]>x);\n        if(i<j) swap(a[i],a[j]);\n    }\n    qsort(a,l,j);\n    qsort(a,j+1,r);\n}\n\nint main(){\n    int n;\n    scanf(\"%d\",&n);\n    for(int i=0;i<n;i++) scanf(\"%d\",&a[i]);\n    qsort(a,0,n-1);\n    for(int i=0;i<n;i++) printf(\"%d \",a[i]);\n}\n```\n```\n//输入\n5\n5 4 3 2 1\n```\n\n\n\n- 可以考虑使用i或j\n- 可以考虑使用while(a[i++])\n\n- 快排不稳定，如果要变成稳定的，可以变成<a[i], i>\n\n\n","slug":"algorithm/1_base/1_qsort","published":1,"updated":"2024-04-19T14:40:16.578Z","_id":"clvuli5p50000zsvwaswv0eae","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h1 id=\"1-快速排序\"><a href=\"#1-快速排序\" class=\"headerlink\" title=\"1 快速排序\"></a>1 快速排序</h1><h3 id=\"1-1-思想\"><a href=\"#1-1-思想\" class=\"headerlink\" title=\"1. 1 思想\"></a>1. 1 思想</h3><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1. 确定分界点</span><br><span class=\"line\">\t- q[l]</span><br><span class=\"line\">\t- q[r]</span><br><span class=\"line\">\t- q[(l+r)/2]</span><br><span class=\"line\">\t- q[random]</span><br><span class=\"line\">2. 调整区间（*）</span><br><span class=\"line\">\t- 左边&lt;=x x0 右边&gt;=x</span><br><span class=\"line\">\t- x不一定在x0位置</span><br><span class=\"line\">3. 递归两端</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1. 暴力求解</span><br><span class=\"line\">\ta[], b[], c[],对c进行排序，&lt;=x的放到a，&gt;=x的放到b。放完后先将a放到c，再将b放到c.</span><br><span class=\"line\">\t</span><br><span class=\"line\">2. 优化空间</span><br><span class=\"line\">\t定义左右指针i,j当c[i]&gt;x时停，当c[j]&lt;x时停。两个都停之后交换。</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-2-算法模板\"><a href=\"#1-2-算法模板\" class=\"headerlink\" title=\"1.2 算法模板\"></a>1.2 算法模板</h3><figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\">using namespace <span class=\"built_in\">std</span>;</span><br><span class=\"line\"><span class=\"type\">int</span> a[<span class=\"number\">100000</span>];</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">qsort</span><span class=\"params\">(<span class=\"type\">int</span> a[],<span class=\"type\">int</span> l, <span class=\"type\">int</span> r)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(l&gt;=r) <span class=\"keyword\">return</span>;</span><br><span class=\"line\">    <span class=\"type\">int</span> i=l<span class=\"number\">-1</span>,j=r+<span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"type\">int</span> x=a[((r-l)&gt;&gt;<span class=\"number\">1</span>)+l]; <span class=\"comment\">//这里要用a[]不要只计算索引，因为后续该索引位置上的元素可能会改变。</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span>(i&lt;j)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">do</span> i++; <span class=\"keyword\">while</span>(a[i]&lt;x);</span><br><span class=\"line\">        <span class=\"keyword\">do</span> j--; <span class=\"keyword\">while</span>(a[j]&gt;x);</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(i&lt;j) swap(a[i],a[j]);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    qsort(a,l,j);</span><br><span class=\"line\">    qsort(a,j+<span class=\"number\">1</span>,r);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> n;</span><br><span class=\"line\">    <span class=\"built_in\">scanf</span>(<span class=\"string\">&quot;%d&quot;</span>,&amp;n);</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">0</span>;i&lt;n;i++) <span class=\"built_in\">scanf</span>(<span class=\"string\">&quot;%d&quot;</span>,&amp;a[i]);</span><br><span class=\"line\">    qsort(a,<span class=\"number\">0</span>,n<span class=\"number\">-1</span>);</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">0</span>;i&lt;n;i++) <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;%d &quot;</span>,a[i]);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//输入</span><br><span class=\"line\">5</span><br><span class=\"line\">5 4 3 2 1</span><br></pre></td></tr></table></figure>\n\n\n\n<ul>\n<li><p>可以考虑使用i或j</p>\n</li>\n<li><p>可以考虑使用while(a[i++])</p>\n</li>\n<li><p>快排不稳定，如果要变成稳定的，可以变成&lt;a[i], i&gt;</p>\n</li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"1-快速排序\"><a href=\"#1-快速排序\" class=\"headerlink\" title=\"1 快速排序\"></a>1 快速排序</h1><h3 id=\"1-1-思想\"><a href=\"#1-1-思想\" class=\"headerlink\" title=\"1. 1 思想\"></a>1. 1 思想</h3><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1. 确定分界点</span><br><span class=\"line\">\t- q[l]</span><br><span class=\"line\">\t- q[r]</span><br><span class=\"line\">\t- q[(l+r)/2]</span><br><span class=\"line\">\t- q[random]</span><br><span class=\"line\">2. 调整区间（*）</span><br><span class=\"line\">\t- 左边&lt;=x x0 右边&gt;=x</span><br><span class=\"line\">\t- x不一定在x0位置</span><br><span class=\"line\">3. 递归两端</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1. 暴力求解</span><br><span class=\"line\">\ta[], b[], c[],对c进行排序，&lt;=x的放到a，&gt;=x的放到b。放完后先将a放到c，再将b放到c.</span><br><span class=\"line\">\t</span><br><span class=\"line\">2. 优化空间</span><br><span class=\"line\">\t定义左右指针i,j当c[i]&gt;x时停，当c[j]&lt;x时停。两个都停之后交换。</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-2-算法模板\"><a href=\"#1-2-算法模板\" class=\"headerlink\" title=\"1.2 算法模板\"></a>1.2 算法模板</h3><figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\">using namespace <span class=\"built_in\">std</span>;</span><br><span class=\"line\"><span class=\"type\">int</span> a[<span class=\"number\">100000</span>];</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">qsort</span><span class=\"params\">(<span class=\"type\">int</span> a[],<span class=\"type\">int</span> l, <span class=\"type\">int</span> r)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(l&gt;=r) <span class=\"keyword\">return</span>;</span><br><span class=\"line\">    <span class=\"type\">int</span> i=l<span class=\"number\">-1</span>,j=r+<span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"type\">int</span> x=a[((r-l)&gt;&gt;<span class=\"number\">1</span>)+l]; <span class=\"comment\">//这里要用a[]不要只计算索引，因为后续该索引位置上的元素可能会改变。</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span>(i&lt;j)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">do</span> i++; <span class=\"keyword\">while</span>(a[i]&lt;x);</span><br><span class=\"line\">        <span class=\"keyword\">do</span> j--; <span class=\"keyword\">while</span>(a[j]&gt;x);</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(i&lt;j) swap(a[i],a[j]);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    qsort(a,l,j);</span><br><span class=\"line\">    qsort(a,j+<span class=\"number\">1</span>,r);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> n;</span><br><span class=\"line\">    <span class=\"built_in\">scanf</span>(<span class=\"string\">&quot;%d&quot;</span>,&amp;n);</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">0</span>;i&lt;n;i++) <span class=\"built_in\">scanf</span>(<span class=\"string\">&quot;%d&quot;</span>,&amp;a[i]);</span><br><span class=\"line\">    qsort(a,<span class=\"number\">0</span>,n<span class=\"number\">-1</span>);</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">0</span>;i&lt;n;i++) <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;%d &quot;</span>,a[i]);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//输入</span><br><span class=\"line\">5</span><br><span class=\"line\">5 4 3 2 1</span><br></pre></td></tr></table></figure>\n\n\n\n<ul>\n<li><p>可以考虑使用i或j</p>\n</li>\n<li><p>可以考虑使用while(a[i++])</p>\n</li>\n<li><p>快排不稳定，如果要变成稳定的，可以变成&lt;a[i], i&gt;</p>\n</li>\n</ul>"},{"title":"1-2 归并排序","date":"2023-10-01T16:00:00.000Z","comment":false,"toc":true,"_content":"\n#\n<!--more-->\n\n## 2 归并排序\n\n### 2.1 思想\n\n```\n1. 确定分界点 a[(l+r)/2]\n2. 递归排序左右\n3. 归并，合二为一（*）\n```\n\n### 2.2 算法模板\n```c\n#include<iostream>\n\nusing namespace std;\n\nint a[100010];\nint temp[100010];\nint n=0;\n\nvoid msort(int a[],int l,int r){\n    if(l>=r) return;\n    int mid=((r-l)>>1)+l;\n    msort(a,l,mid);\n    msort(a,mid+1,r);\n    \n    //merge\n    int i=l,j=mid+1,k=0;\n    while(i<=mid && j<=r){\n        temp[k++]=(a[i]<=a[j]?a[i++]:a[j++]);\n    }\n    while(i<=mid) temp[k++]=a[i++];\n    while(j<=r) temp[k++]=a[j++];\n    //\n    for(i=l,j=0;i<=r;i++,j++) a[i]=temp[j];\n}\n\nint main(){\n    scanf(\"%d\",&n);\n    for(int i=0;i<n;i++) scanf(\"%d\",&a[i]);\n    msort(a,0,n-1);\n    for(int i=0;i<n;i++) printf(\"%d \",a[i]);\n    return 0;\n}\n```\n\n```c++\n//测试\n5\n5 4 3 2 1\n```\n\n","source":"_posts/algorithm/1_base/2_mergesort.md","raw":"---\ntitle: 1-2 归并排序\n\ndate: 2023-10-02\n\ntags: [算法，排序]\n\ncategories: [算法]\n\ncomment: false\n\ntoc: true\n\n---\n\n#\n<!--more-->\n\n## 2 归并排序\n\n### 2.1 思想\n\n```\n1. 确定分界点 a[(l+r)/2]\n2. 递归排序左右\n3. 归并，合二为一（*）\n```\n\n### 2.2 算法模板\n```c\n#include<iostream>\n\nusing namespace std;\n\nint a[100010];\nint temp[100010];\nint n=0;\n\nvoid msort(int a[],int l,int r){\n    if(l>=r) return;\n    int mid=((r-l)>>1)+l;\n    msort(a,l,mid);\n    msort(a,mid+1,r);\n    \n    //merge\n    int i=l,j=mid+1,k=0;\n    while(i<=mid && j<=r){\n        temp[k++]=(a[i]<=a[j]?a[i++]:a[j++]);\n    }\n    while(i<=mid) temp[k++]=a[i++];\n    while(j<=r) temp[k++]=a[j++];\n    //\n    for(i=l,j=0;i<=r;i++,j++) a[i]=temp[j];\n}\n\nint main(){\n    scanf(\"%d\",&n);\n    for(int i=0;i<n;i++) scanf(\"%d\",&a[i]);\n    msort(a,0,n-1);\n    for(int i=0;i<n;i++) printf(\"%d \",a[i]);\n    return 0;\n}\n```\n\n```c++\n//测试\n5\n5 4 3 2 1\n```\n\n","slug":"algorithm/1_base/2_mergesort","published":1,"updated":"2024-04-18T05:33:39.799Z","_id":"clvuli5qi0003zsvwfe0a1deh","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h2 id=\"2-归并排序\"><a href=\"#2-归并排序\" class=\"headerlink\" title=\"2 归并排序\"></a>2 归并排序</h2><h3 id=\"2-1-思想\"><a href=\"#2-1-思想\" class=\"headerlink\" title=\"2.1 思想\"></a>2.1 思想</h3><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1. 确定分界点 a[(l+r)/2]</span><br><span class=\"line\">2. 递归排序左右</span><br><span class=\"line\">3. 归并，合二为一（*）</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"2-2-算法模板\"><a href=\"#2-2-算法模板\" class=\"headerlink\" title=\"2.2 算法模板\"></a>2.2 算法模板</h3><figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\">using namespace <span class=\"built_in\">std</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> a[<span class=\"number\">100010</span>];</span><br><span class=\"line\"><span class=\"type\">int</span> temp[<span class=\"number\">100010</span>];</span><br><span class=\"line\"><span class=\"type\">int</span> n=<span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">msort</span><span class=\"params\">(<span class=\"type\">int</span> a[],<span class=\"type\">int</span> l,<span class=\"type\">int</span> r)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(l&gt;=r) <span class=\"keyword\">return</span>;</span><br><span class=\"line\">    <span class=\"type\">int</span> mid=((r-l)&gt;&gt;<span class=\"number\">1</span>)+l;</span><br><span class=\"line\">    msort(a,l,mid);</span><br><span class=\"line\">    msort(a,mid+<span class=\"number\">1</span>,r);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//merge</span></span><br><span class=\"line\">    <span class=\"type\">int</span> i=l,j=mid+<span class=\"number\">1</span>,k=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(i&lt;=mid &amp;&amp; j&lt;=r)&#123;</span><br><span class=\"line\">        temp[k++]=(a[i]&lt;=a[j]?a[i++]:a[j++]);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(i&lt;=mid) temp[k++]=a[i++];</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(j&lt;=r) temp[k++]=a[j++];</span><br><span class=\"line\">    <span class=\"comment\">//</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span>(i=l,j=<span class=\"number\">0</span>;i&lt;=r;i++,j++) a[i]=temp[j];</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">    <span class=\"built_in\">scanf</span>(<span class=\"string\">&quot;%d&quot;</span>,&amp;n);</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">0</span>;i&lt;n;i++) <span class=\"built_in\">scanf</span>(<span class=\"string\">&quot;%d&quot;</span>,&amp;a[i]);</span><br><span class=\"line\">    msort(a,<span class=\"number\">0</span>,n<span class=\"number\">-1</span>);</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">0</span>;i&lt;n;i++) <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;%d &quot;</span>,a[i]);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//测试</span></span><br><span class=\"line\"><span class=\"number\">5</span></span><br><span class=\"line\"><span class=\"number\">5</span> <span class=\"number\">4</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">1</span></span><br></pre></td></tr></table></figure>\n\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h2 id=\"2-归并排序\"><a href=\"#2-归并排序\" class=\"headerlink\" title=\"2 归并排序\"></a>2 归并排序</h2><h3 id=\"2-1-思想\"><a href=\"#2-1-思想\" class=\"headerlink\" title=\"2.1 思想\"></a>2.1 思想</h3><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1. 确定分界点 a[(l+r)/2]</span><br><span class=\"line\">2. 递归排序左右</span><br><span class=\"line\">3. 归并，合二为一（*）</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"2-2-算法模板\"><a href=\"#2-2-算法模板\" class=\"headerlink\" title=\"2.2 算法模板\"></a>2.2 算法模板</h3><figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\">using namespace <span class=\"built_in\">std</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> a[<span class=\"number\">100010</span>];</span><br><span class=\"line\"><span class=\"type\">int</span> temp[<span class=\"number\">100010</span>];</span><br><span class=\"line\"><span class=\"type\">int</span> n=<span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">msort</span><span class=\"params\">(<span class=\"type\">int</span> a[],<span class=\"type\">int</span> l,<span class=\"type\">int</span> r)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(l&gt;=r) <span class=\"keyword\">return</span>;</span><br><span class=\"line\">    <span class=\"type\">int</span> mid=((r-l)&gt;&gt;<span class=\"number\">1</span>)+l;</span><br><span class=\"line\">    msort(a,l,mid);</span><br><span class=\"line\">    msort(a,mid+<span class=\"number\">1</span>,r);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//merge</span></span><br><span class=\"line\">    <span class=\"type\">int</span> i=l,j=mid+<span class=\"number\">1</span>,k=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(i&lt;=mid &amp;&amp; j&lt;=r)&#123;</span><br><span class=\"line\">        temp[k++]=(a[i]&lt;=a[j]?a[i++]:a[j++]);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(i&lt;=mid) temp[k++]=a[i++];</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(j&lt;=r) temp[k++]=a[j++];</span><br><span class=\"line\">    <span class=\"comment\">//</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span>(i=l,j=<span class=\"number\">0</span>;i&lt;=r;i++,j++) a[i]=temp[j];</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">    <span class=\"built_in\">scanf</span>(<span class=\"string\">&quot;%d&quot;</span>,&amp;n);</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">0</span>;i&lt;n;i++) <span class=\"built_in\">scanf</span>(<span class=\"string\">&quot;%d&quot;</span>,&amp;a[i]);</span><br><span class=\"line\">    msort(a,<span class=\"number\">0</span>,n<span class=\"number\">-1</span>);</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">0</span>;i&lt;n;i++) <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;%d &quot;</span>,a[i]);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//测试</span></span><br><span class=\"line\"><span class=\"number\">5</span></span><br><span class=\"line\"><span class=\"number\">5</span> <span class=\"number\">4</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">1</span></span><br></pre></td></tr></table></figure>"},{"title":"1-3 二分","date":"2024-04-17T16:00:00.000Z","comment":false,"toc":true,"_content":"\n#\n<!--more-->\n\n# 3 整数二分\n\n## 3.1思想\n\n- 二分的本质并不是单调性：有单调性一定可以二分，可以二分不一定有单调性。\n\n- 如果可以找到一个性质，将区间分为一边满足，一边不满足，那么二分就可以寻找这个性质的边界。\n\n- 每次分一半，保证答案处在目标区间里。二分一定会有解。\n\n## 3.2 模板\n\n```c++\n//目标在右区间。区间[l, r]被划分成[l, mid], [mid+1, r]\nint bsearch_1(int l, int r){\n    while(l < r){\n        int mid = ((r-l)>>1)+l; //多一个+1\n        if(check(mid)) r=mid;  //check()查看mid是否符合性质\n        else l = mid + 1;\n    }\n    return l;\n}\n\n//目标在左区间。区间[l, r]被划分成[l, mid-1]，[mid, r]\nint bsearch_2(int l, int r){\n    while(l<r){\n        int mid = ((r-l+1)>>1)+l;\n        if (check(mid)) l=mid;\n        else r=mid-1;\n    }\n    return l;\n}\n```\n\n\n\n## 3.3 例题\n\n- 给定一长度为n的升序数组，以及q个查询。对于每次查询，输入一个数k，返回元素k的起始位置和终止位置（从0开始）。不存在则返回-1 -1\n\n```c++\n//测试\n6 3\n1 2 2 3 3 4\n3\n4\n5\n    \n//输出\n3 4\n5 5\n-1 -1\n```\n\n```c++\n#include<iostream>\nusing namespace std;\nint a[100010];\nint n=0,m=0;\n\nint main(){\n        scanf(\"%d%d\",&n,&m);\n        for(int i=0; i<n; i++) scanf(\"%d\",&a[i]);\n        while(m--){\n                int x;\n                scanf(\"%d\", &x);\n\n                int l=0, r=n-1;\n                while(l<r){\n                        int mid = ((r-l)>>1)+l;\n                        if (a[mid] >= x) r=mid;\n                        else l=mid+1;\n                }\n                if (a[l] != x) cout<<\"-1 -1\"<<endl;\n                else{\n                        cout <<l <<' ';\n\n                        int l =0, r=n-1;\n                        while(l<r){\n                                int mid = ((r-l+1)>>1)+l;\n                                if (a[mid] <=x) l=mid;\n                                else r=mid-1;\n                        }\n                        cout<<l<<endl;\n                }\n        }\n        return 0;\n}\n```\n\n# 4 浮点数二分\n\n- 更简单，不用考虑+1问题。\n\n## 4.1 例题\n\n- 计算一个数（>1）的平方根\n\n```c++\n#include<iostream>\nusing namespace std;\n\nint main(){\n        double x=0;\n        cin >>x;\n        double l=0, r=x;\n        while(r-l>1e-8){\n                double mid = ((r-l)/2)+l;\n                if(mid*mid<x) l=mid;\n                else r=mid;\n        }\n        cout<<l<<endl;\n        return 0;\n}\n```\n\n","source":"_posts/algorithm/1_base/3_dichotomy.md","raw":"---\ntitle: 1-3 二分\n\ndate: 2024-4-18\n\ntags: [算法，排序]\n\ncategories: [算法]\n\ncomment: false\n\ntoc: true\n\n\n---\n\n#\n<!--more-->\n\n# 3 整数二分\n\n## 3.1思想\n\n- 二分的本质并不是单调性：有单调性一定可以二分，可以二分不一定有单调性。\n\n- 如果可以找到一个性质，将区间分为一边满足，一边不满足，那么二分就可以寻找这个性质的边界。\n\n- 每次分一半，保证答案处在目标区间里。二分一定会有解。\n\n## 3.2 模板\n\n```c++\n//目标在右区间。区间[l, r]被划分成[l, mid], [mid+1, r]\nint bsearch_1(int l, int r){\n    while(l < r){\n        int mid = ((r-l)>>1)+l; //多一个+1\n        if(check(mid)) r=mid;  //check()查看mid是否符合性质\n        else l = mid + 1;\n    }\n    return l;\n}\n\n//目标在左区间。区间[l, r]被划分成[l, mid-1]，[mid, r]\nint bsearch_2(int l, int r){\n    while(l<r){\n        int mid = ((r-l+1)>>1)+l;\n        if (check(mid)) l=mid;\n        else r=mid-1;\n    }\n    return l;\n}\n```\n\n\n\n## 3.3 例题\n\n- 给定一长度为n的升序数组，以及q个查询。对于每次查询，输入一个数k，返回元素k的起始位置和终止位置（从0开始）。不存在则返回-1 -1\n\n```c++\n//测试\n6 3\n1 2 2 3 3 4\n3\n4\n5\n    \n//输出\n3 4\n5 5\n-1 -1\n```\n\n```c++\n#include<iostream>\nusing namespace std;\nint a[100010];\nint n=0,m=0;\n\nint main(){\n        scanf(\"%d%d\",&n,&m);\n        for(int i=0; i<n; i++) scanf(\"%d\",&a[i]);\n        while(m--){\n                int x;\n                scanf(\"%d\", &x);\n\n                int l=0, r=n-1;\n                while(l<r){\n                        int mid = ((r-l)>>1)+l;\n                        if (a[mid] >= x) r=mid;\n                        else l=mid+1;\n                }\n                if (a[l] != x) cout<<\"-1 -1\"<<endl;\n                else{\n                        cout <<l <<' ';\n\n                        int l =0, r=n-1;\n                        while(l<r){\n                                int mid = ((r-l+1)>>1)+l;\n                                if (a[mid] <=x) l=mid;\n                                else r=mid-1;\n                        }\n                        cout<<l<<endl;\n                }\n        }\n        return 0;\n}\n```\n\n# 4 浮点数二分\n\n- 更简单，不用考虑+1问题。\n\n## 4.1 例题\n\n- 计算一个数（>1）的平方根\n\n```c++\n#include<iostream>\nusing namespace std;\n\nint main(){\n        double x=0;\n        cin >>x;\n        double l=0, r=x;\n        while(r-l>1e-8){\n                double mid = ((r-l)/2)+l;\n                if(mid*mid<x) l=mid;\n                else r=mid;\n        }\n        cout<<l<<endl;\n        return 0;\n}\n```\n\n","slug":"algorithm/1_base/3_dichotomy","published":1,"updated":"2024-04-18T11:20:24.711Z","_id":"clvuli5qj0004zsvw90t2hkf5","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h1 id=\"3-整数二分\"><a href=\"#3-整数二分\" class=\"headerlink\" title=\"3 整数二分\"></a>3 整数二分</h1><h2 id=\"3-1思想\"><a href=\"#3-1思想\" class=\"headerlink\" title=\"3.1思想\"></a>3.1思想</h2><ul>\n<li><p>二分的本质并不是单调性：有单调性一定可以二分，可以二分不一定有单调性。</p>\n</li>\n<li><p>如果可以找到一个性质，将区间分为一边满足，一边不满足，那么二分就可以寻找这个性质的边界。</p>\n</li>\n<li><p>每次分一半，保证答案处在目标区间里。二分一定会有解。</p>\n</li>\n</ul>\n<h2 id=\"3-2-模板\"><a href=\"#3-2-模板\" class=\"headerlink\" title=\"3.2 模板\"></a>3.2 模板</h2><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//目标在右区间。区间[l, r]被划分成[l, mid], [mid+1, r]</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">bsearch_1</span><span class=\"params\">(<span class=\"type\">int</span> l, <span class=\"type\">int</span> r)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(l &lt; r)&#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> mid = ((r-l)&gt;&gt;<span class=\"number\">1</span>)+l; <span class=\"comment\">//多一个+1</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span>(<span class=\"built_in\">check</span>(mid)) r=mid;  <span class=\"comment\">//check()查看mid是否符合性质</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span> l = mid + <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> l;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//目标在左区间。区间[l, r]被划分成[l, mid-1]，[mid, r]</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">bsearch_2</span><span class=\"params\">(<span class=\"type\">int</span> l, <span class=\"type\">int</span> r)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(l&lt;r)&#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> mid = ((r-l+<span class=\"number\">1</span>)&gt;&gt;<span class=\"number\">1</span>)+l;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (<span class=\"built_in\">check</span>(mid)) l=mid;</span><br><span class=\"line\">        <span class=\"keyword\">else</span> r=mid<span class=\"number\">-1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> l;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<h2 id=\"3-3-例题\"><a href=\"#3-3-例题\" class=\"headerlink\" title=\"3.3 例题\"></a>3.3 例题</h2><ul>\n<li>给定一长度为n的升序数组，以及q个查询。对于每次查询，输入一个数k，返回元素k的起始位置和终止位置（从0开始）。不存在则返回-1 -1</li>\n</ul>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//测试</span></span><br><span class=\"line\"><span class=\"number\">6</span> <span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">4</span></span><br><span class=\"line\"><span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"number\">4</span></span><br><span class=\"line\"><span class=\"number\">5</span></span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"comment\">//输出</span></span><br><span class=\"line\"><span class=\"number\">3</span> <span class=\"number\">4</span></span><br><span class=\"line\"><span class=\"number\">5</span> <span class=\"number\">5</span></span><br><span class=\"line\"><span class=\"number\">-1</span> <span class=\"number\">-1</span></span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> std;</span><br><span class=\"line\"><span class=\"type\">int</span> a[<span class=\"number\">100010</span>];</span><br><span class=\"line\"><span class=\"type\">int</span> n=<span class=\"number\">0</span>,m=<span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">        <span class=\"built_in\">scanf</span>(<span class=\"string\">&quot;%d%d&quot;</span>,&amp;n,&amp;m);</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">0</span>; i&lt;n; i++) <span class=\"built_in\">scanf</span>(<span class=\"string\">&quot;%d&quot;</span>,&amp;a[i]);</span><br><span class=\"line\">        <span class=\"keyword\">while</span>(m--)&#123;</span><br><span class=\"line\">                <span class=\"type\">int</span> x;</span><br><span class=\"line\">                <span class=\"built_in\">scanf</span>(<span class=\"string\">&quot;%d&quot;</span>, &amp;x);</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"type\">int</span> l=<span class=\"number\">0</span>, r=n<span class=\"number\">-1</span>;</span><br><span class=\"line\">                <span class=\"keyword\">while</span>(l&lt;r)&#123;</span><br><span class=\"line\">                        <span class=\"type\">int</span> mid = ((r-l)&gt;&gt;<span class=\"number\">1</span>)+l;</span><br><span class=\"line\">                        <span class=\"keyword\">if</span> (a[mid] &gt;= x) r=mid;</span><br><span class=\"line\">                        <span class=\"keyword\">else</span> l=mid+<span class=\"number\">1</span>;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (a[l] != x) cout&lt;&lt;<span class=\"string\">&quot;-1 -1&quot;</span>&lt;&lt;endl;</span><br><span class=\"line\">                <span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">                        cout &lt;&lt;l &lt;&lt;<span class=\"string\">&#x27; &#x27;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">                        <span class=\"type\">int</span> l =<span class=\"number\">0</span>, r=n<span class=\"number\">-1</span>;</span><br><span class=\"line\">                        <span class=\"keyword\">while</span>(l&lt;r)&#123;</span><br><span class=\"line\">                                <span class=\"type\">int</span> mid = ((r-l+<span class=\"number\">1</span>)&gt;&gt;<span class=\"number\">1</span>)+l;</span><br><span class=\"line\">                                <span class=\"keyword\">if</span> (a[mid] &lt;=x) l=mid;</span><br><span class=\"line\">                                <span class=\"keyword\">else</span> r=mid<span class=\"number\">-1</span>;</span><br><span class=\"line\">                        &#125;</span><br><span class=\"line\">                        cout&lt;&lt;l&lt;&lt;endl;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"4-浮点数二分\"><a href=\"#4-浮点数二分\" class=\"headerlink\" title=\"4 浮点数二分\"></a>4 浮点数二分</h1><ul>\n<li>更简单，不用考虑+1问题。</li>\n</ul>\n<h2 id=\"4-1-例题\"><a href=\"#4-1-例题\" class=\"headerlink\" title=\"4.1 例题\"></a>4.1 例题</h2><ul>\n<li>计算一个数（&gt;1）的平方根</li>\n</ul>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> std;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">        <span class=\"type\">double</span> x=<span class=\"number\">0</span>;</span><br><span class=\"line\">        cin &gt;&gt;x;</span><br><span class=\"line\">        <span class=\"type\">double</span> l=<span class=\"number\">0</span>, r=x;</span><br><span class=\"line\">        <span class=\"keyword\">while</span>(r-l&gt;<span class=\"number\">1e-8</span>)&#123;</span><br><span class=\"line\">                <span class=\"type\">double</span> mid = ((r-l)/<span class=\"number\">2</span>)+l;</span><br><span class=\"line\">                <span class=\"keyword\">if</span>(mid*mid&lt;x) l=mid;</span><br><span class=\"line\">                <span class=\"keyword\">else</span> r=mid;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        cout&lt;&lt;l&lt;&lt;endl;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"3-整数二分\"><a href=\"#3-整数二分\" class=\"headerlink\" title=\"3 整数二分\"></a>3 整数二分</h1><h2 id=\"3-1思想\"><a href=\"#3-1思想\" class=\"headerlink\" title=\"3.1思想\"></a>3.1思想</h2><ul>\n<li><p>二分的本质并不是单调性：有单调性一定可以二分，可以二分不一定有单调性。</p>\n</li>\n<li><p>如果可以找到一个性质，将区间分为一边满足，一边不满足，那么二分就可以寻找这个性质的边界。</p>\n</li>\n<li><p>每次分一半，保证答案处在目标区间里。二分一定会有解。</p>\n</li>\n</ul>\n<h2 id=\"3-2-模板\"><a href=\"#3-2-模板\" class=\"headerlink\" title=\"3.2 模板\"></a>3.2 模板</h2><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//目标在右区间。区间[l, r]被划分成[l, mid], [mid+1, r]</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">bsearch_1</span><span class=\"params\">(<span class=\"type\">int</span> l, <span class=\"type\">int</span> r)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(l &lt; r)&#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> mid = ((r-l)&gt;&gt;<span class=\"number\">1</span>)+l; <span class=\"comment\">//多一个+1</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span>(<span class=\"built_in\">check</span>(mid)) r=mid;  <span class=\"comment\">//check()查看mid是否符合性质</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span> l = mid + <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> l;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//目标在左区间。区间[l, r]被划分成[l, mid-1]，[mid, r]</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">bsearch_2</span><span class=\"params\">(<span class=\"type\">int</span> l, <span class=\"type\">int</span> r)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(l&lt;r)&#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> mid = ((r-l+<span class=\"number\">1</span>)&gt;&gt;<span class=\"number\">1</span>)+l;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (<span class=\"built_in\">check</span>(mid)) l=mid;</span><br><span class=\"line\">        <span class=\"keyword\">else</span> r=mid<span class=\"number\">-1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> l;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<h2 id=\"3-3-例题\"><a href=\"#3-3-例题\" class=\"headerlink\" title=\"3.3 例题\"></a>3.3 例题</h2><ul>\n<li>给定一长度为n的升序数组，以及q个查询。对于每次查询，输入一个数k，返回元素k的起始位置和终止位置（从0开始）。不存在则返回-1 -1</li>\n</ul>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//测试</span></span><br><span class=\"line\"><span class=\"number\">6</span> <span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">4</span></span><br><span class=\"line\"><span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"number\">4</span></span><br><span class=\"line\"><span class=\"number\">5</span></span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"comment\">//输出</span></span><br><span class=\"line\"><span class=\"number\">3</span> <span class=\"number\">4</span></span><br><span class=\"line\"><span class=\"number\">5</span> <span class=\"number\">5</span></span><br><span class=\"line\"><span class=\"number\">-1</span> <span class=\"number\">-1</span></span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> std;</span><br><span class=\"line\"><span class=\"type\">int</span> a[<span class=\"number\">100010</span>];</span><br><span class=\"line\"><span class=\"type\">int</span> n=<span class=\"number\">0</span>,m=<span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">        <span class=\"built_in\">scanf</span>(<span class=\"string\">&quot;%d%d&quot;</span>,&amp;n,&amp;m);</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">0</span>; i&lt;n; i++) <span class=\"built_in\">scanf</span>(<span class=\"string\">&quot;%d&quot;</span>,&amp;a[i]);</span><br><span class=\"line\">        <span class=\"keyword\">while</span>(m--)&#123;</span><br><span class=\"line\">                <span class=\"type\">int</span> x;</span><br><span class=\"line\">                <span class=\"built_in\">scanf</span>(<span class=\"string\">&quot;%d&quot;</span>, &amp;x);</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"type\">int</span> l=<span class=\"number\">0</span>, r=n<span class=\"number\">-1</span>;</span><br><span class=\"line\">                <span class=\"keyword\">while</span>(l&lt;r)&#123;</span><br><span class=\"line\">                        <span class=\"type\">int</span> mid = ((r-l)&gt;&gt;<span class=\"number\">1</span>)+l;</span><br><span class=\"line\">                        <span class=\"keyword\">if</span> (a[mid] &gt;= x) r=mid;</span><br><span class=\"line\">                        <span class=\"keyword\">else</span> l=mid+<span class=\"number\">1</span>;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (a[l] != x) cout&lt;&lt;<span class=\"string\">&quot;-1 -1&quot;</span>&lt;&lt;endl;</span><br><span class=\"line\">                <span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">                        cout &lt;&lt;l &lt;&lt;<span class=\"string\">&#x27; &#x27;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">                        <span class=\"type\">int</span> l =<span class=\"number\">0</span>, r=n<span class=\"number\">-1</span>;</span><br><span class=\"line\">                        <span class=\"keyword\">while</span>(l&lt;r)&#123;</span><br><span class=\"line\">                                <span class=\"type\">int</span> mid = ((r-l+<span class=\"number\">1</span>)&gt;&gt;<span class=\"number\">1</span>)+l;</span><br><span class=\"line\">                                <span class=\"keyword\">if</span> (a[mid] &lt;=x) l=mid;</span><br><span class=\"line\">                                <span class=\"keyword\">else</span> r=mid<span class=\"number\">-1</span>;</span><br><span class=\"line\">                        &#125;</span><br><span class=\"line\">                        cout&lt;&lt;l&lt;&lt;endl;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"4-浮点数二分\"><a href=\"#4-浮点数二分\" class=\"headerlink\" title=\"4 浮点数二分\"></a>4 浮点数二分</h1><ul>\n<li>更简单，不用考虑+1问题。</li>\n</ul>\n<h2 id=\"4-1-例题\"><a href=\"#4-1-例题\" class=\"headerlink\" title=\"4.1 例题\"></a>4.1 例题</h2><ul>\n<li>计算一个数（&gt;1）的平方根</li>\n</ul>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> std;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">        <span class=\"type\">double</span> x=<span class=\"number\">0</span>;</span><br><span class=\"line\">        cin &gt;&gt;x;</span><br><span class=\"line\">        <span class=\"type\">double</span> l=<span class=\"number\">0</span>, r=x;</span><br><span class=\"line\">        <span class=\"keyword\">while</span>(r-l&gt;<span class=\"number\">1e-8</span>)&#123;</span><br><span class=\"line\">                <span class=\"type\">double</span> mid = ((r-l)/<span class=\"number\">2</span>)+l;</span><br><span class=\"line\">                <span class=\"keyword\">if</span>(mid*mid&lt;x) l=mid;</span><br><span class=\"line\">                <span class=\"keyword\">else</span> r=mid;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        cout&lt;&lt;l&lt;&lt;endl;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>"},{"title":"2.2 线性回归实现","date":"2024-02-03T06:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 2. 线性回归实现\n\n\n\n```python\nimport random\nimport torch\nfrom d2l import torch as d2l\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n```\n\n# 2.1 生成数据集\n- 使用线性模型参数w = [2, −3.4]⊤、b = 4.2 和噪声项ϵ~N(0,0.01)生成数据集及其标签：\n\n\n```python\n#1 生成数据集\ndef synthetic_data(w,b,num_examples): #@save\n    \"\"\"生成 y = Xw + b + 噪声。\"\"\"\n    X = torch.normal(0,1,(num_examples,len(w))) #生成服从正态分布的数据\n    y = torch.matmul(X,w) + b #矩阵乘法\n    y += torch.normal(0,0.01,y.shape) #加上噪声\n    return X,y.reshape((-1,1))\ntrue_w = torch.tensor([2,-3.4])\ntrue_b = 4.2\nfeatures,labels = synthetic_data(true_w,true_b,1000)\nprint('features:',features[0],'\\nlabel:',labels[0])\n\n#2 可以看到x1与y是线性关系\nd2l.set_figsize()\nd2l.plt.scatter(features[:,1].detach().numpy(),labels.detach().numpy(),1)\n\n```\n\n    features: tensor([1.2604, 0.9351]) \n    label: tensor([3.5493])\n    \n\n\n\n\n    <matplotlib.collections.PathCollection at 0x168296a63a0>\n\n\n\n\n    \n![svg](2_linear_regression_realize_files/2_linear_regression_realize_3_2.svg)\n![](img/deeplearning/code/pytorch/2_linear_neural_network/2_linear_regression_realize_files/2_linear_regression_realize_3_2.svg)\n    \n\n\n## 2.2 读取数据集\n- 定义一个函数打乱样本并返回批量数据：\n    - 输入：批量大小、特征、标签\n    - 输出：批量数据\n\n\n```python\ndef data_iter(batch_size,features,labels):\n    num_examples = len(features) #1 样本数\n    indices = list(range(num_examples)) #2 生成下标\n    random.shuffle(indices) #3 打乱下标\n    for i in range(0,num_examples,batch_size): #4 遍历数据集，每次取出batch_size个样本\n        batch_indices = torch.tensor(indices[i:min(i+batch_size,num_examples)])\n        yield features[batch_indices],labels[batch_indices] #5 可迭代对象，每次返回batch_size个样本\n\nbatch_size = 10\nfor X,y in data_iter(batch_size,features,labels):\n    print(X,'\\n',y)\n    break\n```\n\n    tensor([[ 0.1474, -0.8037],\n            [ 0.3242,  0.3905],\n            [-0.1883,  0.1497],\n            [ 0.4741,  0.2937],\n            [ 1.6234, -0.3555],\n            [ 1.2714, -1.5737],\n            [-0.6498,  1.9078],\n            [ 0.6298,  0.4764],\n            [ 0.2344, -1.3566],\n            [-0.4173, -0.4298]]) \n     tensor([[ 7.2271],\n            [ 3.5186],\n            [ 3.3331],\n            [ 4.1500],\n            [ 8.6549],\n            [12.0805],\n            [-3.5749],\n            [ 3.8355],\n            [ 9.2662],\n            [ 4.8142]])\n    \n\n## 2.3 初始化模型参数\n- 从均值为0、标准差为0.01的正态分布中采样随机数来初始化权重，并将偏置初始化为0。\n\n\n\n```python\nw = torch.normal(0,0.01,size=(2,1),requires_grad=True)\nb = torch.zeros(1,requires_grad=True)\n```\n\n## 2.4 定义模型\n\n\n\n```python\ndef linreg(X,w,b): #@save\n    \"\"\"线性回归模型。\"\"\"\n    return torch.matmul(X,w) + b\n```\n\n## 2.5 定义损失函数\n- 平方损失函数\n\n\n```python\ndef squred_loss(y_hat,y): #@save\n    \"\"\"均方损失。\"\"\"\n    return (y_hat - y.reshape(y_hat.shape))**2/2\n```\n\n## 2.6 定义优化算法\n\n\n\n```python\ndef sge(params,lr,batch_size): #@save\n    \"\"\"小批量随机梯度下降。\"\"\"\n    with torch.no_grad(): #在该模块下，所有计算得出的tensor的requires_grad都自动设置为False ???\n        for param in params:\n            param -= lr*param.grad/batch_size #这是一个计算操作，会产生新的梯度信息，所以需要关闭梯度信息 ???\n            param.grad.zero_() #将param.grad清零\n```\n\n## 2.7 训练\n- 初始化参数\n- 重复以下训练，知道完成：\n    - 计算梯度$g \\leftarrow \\partial_{(\\mathbf{w},b)}\\frac{1}{|\\mathcal{B}|}\\sum_{i\\in\\mathcal{B}}l(x^{(i)},y^{(i)},\\mathbf{w},b)$\n    - 更新参数$(\\mathbf{w},b) \\leftarrow (\\mathbf{w},b) - \\eta g$\n\n\n```python\n#1 超参数\nlr = 0.03\nnum_epochs = 3\nnet = linreg\nloss = squred_loss\n\n#2 训练\nfor epoch in range(num_epochs):\n    for X,y in data_iter(batch_size,features,labels):\n        l = loss(net(X,w,b),y) #l是有关小批量X和y的损失\n        l.sum().backward() #小批量的损失对模型参数求梯度,因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，\n        sge([w,b],lr,batch_size) #使用小批量随机梯度下降迭代模型参数\n    with torch.no_grad():\n        train_l = loss(net(features,w,b),labels)\n        print(f'epoch {epoch+1},loss {float(train_l.mean()):f}')\n\n#3 检验\nprint(f'w的估计误差：{true_w - w.reshape(true_w.shape)}')\nprint(f'b的估计误差：{true_b - b}')\n```\n\n    epoch 1,loss 0.030588\n    epoch 2,loss 0.000105\n    epoch 3,loss 0.000049\n    w的估计误差：tensor([ 0.0010, -0.0008], grad_fn=<SubBackward0>)\n    b的估计误差：tensor([0.0008], grad_fn=<RsubBackward1>)\n    \n","source":"_posts/deeplearning/code/pytorch/2_linear_neural_network/2_linear_regression_realize.md","raw":"---\ntitle: 2.2 线性回归实现\ndate: 2024-2-3 14:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 2. 线性回归实现\n\n\n\n```python\nimport random\nimport torch\nfrom d2l import torch as d2l\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n```\n\n# 2.1 生成数据集\n- 使用线性模型参数w = [2, −3.4]⊤、b = 4.2 和噪声项ϵ~N(0,0.01)生成数据集及其标签：\n\n\n```python\n#1 生成数据集\ndef synthetic_data(w,b,num_examples): #@save\n    \"\"\"生成 y = Xw + b + 噪声。\"\"\"\n    X = torch.normal(0,1,(num_examples,len(w))) #生成服从正态分布的数据\n    y = torch.matmul(X,w) + b #矩阵乘法\n    y += torch.normal(0,0.01,y.shape) #加上噪声\n    return X,y.reshape((-1,1))\ntrue_w = torch.tensor([2,-3.4])\ntrue_b = 4.2\nfeatures,labels = synthetic_data(true_w,true_b,1000)\nprint('features:',features[0],'\\nlabel:',labels[0])\n\n#2 可以看到x1与y是线性关系\nd2l.set_figsize()\nd2l.plt.scatter(features[:,1].detach().numpy(),labels.detach().numpy(),1)\n\n```\n\n    features: tensor([1.2604, 0.9351]) \n    label: tensor([3.5493])\n    \n\n\n\n\n    <matplotlib.collections.PathCollection at 0x168296a63a0>\n\n\n\n\n    \n![svg](2_linear_regression_realize_files/2_linear_regression_realize_3_2.svg)\n![](img/deeplearning/code/pytorch/2_linear_neural_network/2_linear_regression_realize_files/2_linear_regression_realize_3_2.svg)\n    \n\n\n## 2.2 读取数据集\n- 定义一个函数打乱样本并返回批量数据：\n    - 输入：批量大小、特征、标签\n    - 输出：批量数据\n\n\n```python\ndef data_iter(batch_size,features,labels):\n    num_examples = len(features) #1 样本数\n    indices = list(range(num_examples)) #2 生成下标\n    random.shuffle(indices) #3 打乱下标\n    for i in range(0,num_examples,batch_size): #4 遍历数据集，每次取出batch_size个样本\n        batch_indices = torch.tensor(indices[i:min(i+batch_size,num_examples)])\n        yield features[batch_indices],labels[batch_indices] #5 可迭代对象，每次返回batch_size个样本\n\nbatch_size = 10\nfor X,y in data_iter(batch_size,features,labels):\n    print(X,'\\n',y)\n    break\n```\n\n    tensor([[ 0.1474, -0.8037],\n            [ 0.3242,  0.3905],\n            [-0.1883,  0.1497],\n            [ 0.4741,  0.2937],\n            [ 1.6234, -0.3555],\n            [ 1.2714, -1.5737],\n            [-0.6498,  1.9078],\n            [ 0.6298,  0.4764],\n            [ 0.2344, -1.3566],\n            [-0.4173, -0.4298]]) \n     tensor([[ 7.2271],\n            [ 3.5186],\n            [ 3.3331],\n            [ 4.1500],\n            [ 8.6549],\n            [12.0805],\n            [-3.5749],\n            [ 3.8355],\n            [ 9.2662],\n            [ 4.8142]])\n    \n\n## 2.3 初始化模型参数\n- 从均值为0、标准差为0.01的正态分布中采样随机数来初始化权重，并将偏置初始化为0。\n\n\n\n```python\nw = torch.normal(0,0.01,size=(2,1),requires_grad=True)\nb = torch.zeros(1,requires_grad=True)\n```\n\n## 2.4 定义模型\n\n\n\n```python\ndef linreg(X,w,b): #@save\n    \"\"\"线性回归模型。\"\"\"\n    return torch.matmul(X,w) + b\n```\n\n## 2.5 定义损失函数\n- 平方损失函数\n\n\n```python\ndef squred_loss(y_hat,y): #@save\n    \"\"\"均方损失。\"\"\"\n    return (y_hat - y.reshape(y_hat.shape))**2/2\n```\n\n## 2.6 定义优化算法\n\n\n\n```python\ndef sge(params,lr,batch_size): #@save\n    \"\"\"小批量随机梯度下降。\"\"\"\n    with torch.no_grad(): #在该模块下，所有计算得出的tensor的requires_grad都自动设置为False ???\n        for param in params:\n            param -= lr*param.grad/batch_size #这是一个计算操作，会产生新的梯度信息，所以需要关闭梯度信息 ???\n            param.grad.zero_() #将param.grad清零\n```\n\n## 2.7 训练\n- 初始化参数\n- 重复以下训练，知道完成：\n    - 计算梯度$g \\leftarrow \\partial_{(\\mathbf{w},b)}\\frac{1}{|\\mathcal{B}|}\\sum_{i\\in\\mathcal{B}}l(x^{(i)},y^{(i)},\\mathbf{w},b)$\n    - 更新参数$(\\mathbf{w},b) \\leftarrow (\\mathbf{w},b) - \\eta g$\n\n\n```python\n#1 超参数\nlr = 0.03\nnum_epochs = 3\nnet = linreg\nloss = squred_loss\n\n#2 训练\nfor epoch in range(num_epochs):\n    for X,y in data_iter(batch_size,features,labels):\n        l = loss(net(X,w,b),y) #l是有关小批量X和y的损失\n        l.sum().backward() #小批量的损失对模型参数求梯度,因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，\n        sge([w,b],lr,batch_size) #使用小批量随机梯度下降迭代模型参数\n    with torch.no_grad():\n        train_l = loss(net(features,w,b),labels)\n        print(f'epoch {epoch+1},loss {float(train_l.mean()):f}')\n\n#3 检验\nprint(f'w的估计误差：{true_w - w.reshape(true_w.shape)}')\nprint(f'b的估计误差：{true_b - b}')\n```\n\n    epoch 1,loss 0.030588\n    epoch 2,loss 0.000105\n    epoch 3,loss 0.000049\n    w的估计误差：tensor([ 0.0010, -0.0008], grad_fn=<SubBackward0>)\n    b的估计误差：tensor([0.0008], grad_fn=<RsubBackward1>)\n    \n","slug":"deeplearning/code/pytorch/2_linear_neural_network/2_linear_regression_realize","published":1,"updated":"2024-05-02T08:54:42.138Z","_id":"clvuli5qy0009zsvw4zcaeia8","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"2-线性回归实现\"><a href=\"#2-线性回归实现\" class=\"headerlink\" title=\"2. 线性回归实现\"></a>2. 线性回归实现</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> random</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>] = <span class=\"string\">&#x27;True&#x27;</span></span><br></pre></td></tr></table></figure>\n\n<h1 id=\"2-1-生成数据集\"><a href=\"#2-1-生成数据集\" class=\"headerlink\" title=\"2.1 生成数据集\"></a>2.1 生成数据集</h1><ul>\n<li>使用线性模型参数w &#x3D; [2, −3.4]⊤、b &#x3D; 4.2 和噪声项ϵ~N(0,0.01)生成数据集及其标签：</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#1 生成数据集</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">synthetic_data</span>(<span class=\"params\">w,b,num_examples</span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;生成 y = Xw + b + 噪声。&quot;&quot;&quot;</span></span><br><span class=\"line\">    X = torch.normal(<span class=\"number\">0</span>,<span class=\"number\">1</span>,(num_examples,<span class=\"built_in\">len</span>(w))) <span class=\"comment\">#生成服从正态分布的数据</span></span><br><span class=\"line\">    y = torch.matmul(X,w) + b <span class=\"comment\">#矩阵乘法</span></span><br><span class=\"line\">    y += torch.normal(<span class=\"number\">0</span>,<span class=\"number\">0.01</span>,y.shape) <span class=\"comment\">#加上噪声</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> X,y.reshape((-<span class=\"number\">1</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">true_w = torch.tensor([<span class=\"number\">2</span>,-<span class=\"number\">3.4</span>])</span><br><span class=\"line\">true_b = <span class=\"number\">4.2</span></span><br><span class=\"line\">features,labels = synthetic_data(true_w,true_b,<span class=\"number\">1000</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;features:&#x27;</span>,features[<span class=\"number\">0</span>],<span class=\"string\">&#x27;\\nlabel:&#x27;</span>,labels[<span class=\"number\">0</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#2 可以看到x1与y是线性关系</span></span><br><span class=\"line\">d2l.set_figsize()</span><br><span class=\"line\">d2l.plt.scatter(features[:,<span class=\"number\">1</span>].detach().numpy(),labels.detach().numpy(),<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<pre><code>features: tensor([1.2604, 0.9351]) \nlabel: tensor([3.5493])\n\n\n\n\n\n&lt;matplotlib.collections.PathCollection at 0x168296a63a0&gt;\n</code></pre>\n<p><img src=\"/2_linear_regression_realize_files/2_linear_regression_realize_3_2.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/2_linear_neural_network/2_linear_regression_realize_files/2_linear_regression_realize_3_2.svg\"></p>\n<h2 id=\"2-2-读取数据集\"><a href=\"#2-2-读取数据集\" class=\"headerlink\" title=\"2.2 读取数据集\"></a>2.2 读取数据集</h2><ul>\n<li>定义一个函数打乱样本并返回批量数据：<ul>\n<li>输入：批量大小、特征、标签</li>\n<li>输出：批量数据</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">data_iter</span>(<span class=\"params\">batch_size,features,labels</span>):</span><br><span class=\"line\">    num_examples = <span class=\"built_in\">len</span>(features) <span class=\"comment\">#1 样本数</span></span><br><span class=\"line\">    indices = <span class=\"built_in\">list</span>(<span class=\"built_in\">range</span>(num_examples)) <span class=\"comment\">#2 生成下标</span></span><br><span class=\"line\">    random.shuffle(indices) <span class=\"comment\">#3 打乱下标</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">0</span>,num_examples,batch_size): <span class=\"comment\">#4 遍历数据集，每次取出batch_size个样本</span></span><br><span class=\"line\">        batch_indices = torch.tensor(indices[i:<span class=\"built_in\">min</span>(i+batch_size,num_examples)])</span><br><span class=\"line\">        <span class=\"keyword\">yield</span> features[batch_indices],labels[batch_indices] <span class=\"comment\">#5 可迭代对象，每次返回batch_size个样本</span></span><br><span class=\"line\"></span><br><span class=\"line\">batch_size = <span class=\"number\">10</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> X,y <span class=\"keyword\">in</span> data_iter(batch_size,features,labels):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(X,<span class=\"string\">&#x27;\\n&#x27;</span>,y)</span><br><span class=\"line\">    <span class=\"keyword\">break</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[ 0.1474, -0.8037],\n        [ 0.3242,  0.3905],\n        [-0.1883,  0.1497],\n        [ 0.4741,  0.2937],\n        [ 1.6234, -0.3555],\n        [ 1.2714, -1.5737],\n        [-0.6498,  1.9078],\n        [ 0.6298,  0.4764],\n        [ 0.2344, -1.3566],\n        [-0.4173, -0.4298]]) \n tensor([[ 7.2271],\n        [ 3.5186],\n        [ 3.3331],\n        [ 4.1500],\n        [ 8.6549],\n        [12.0805],\n        [-3.5749],\n        [ 3.8355],\n        [ 9.2662],\n        [ 4.8142]])\n</code></pre>\n<h2 id=\"2-3-初始化模型参数\"><a href=\"#2-3-初始化模型参数\" class=\"headerlink\" title=\"2.3 初始化模型参数\"></a>2.3 初始化模型参数</h2><ul>\n<li>从均值为0、标准差为0.01的正态分布中采样随机数来初始化权重，并将偏置初始化为0。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">w = torch.normal(<span class=\"number\">0</span>,<span class=\"number\">0.01</span>,size=(<span class=\"number\">2</span>,<span class=\"number\">1</span>),requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">b = torch.zeros(<span class=\"number\">1</span>,requires_grad=<span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"2-4-定义模型\"><a href=\"#2-4-定义模型\" class=\"headerlink\" title=\"2.4 定义模型\"></a>2.4 定义模型</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">linreg</span>(<span class=\"params\">X,w,b</span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;线性回归模型。&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.matmul(X,w) + b</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"2-5-定义损失函数\"><a href=\"#2-5-定义损失函数\" class=\"headerlink\" title=\"2.5 定义损失函数\"></a>2.5 定义损失函数</h2><ul>\n<li>平方损失函数</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">squred_loss</span>(<span class=\"params\">y_hat,y</span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;均方损失。&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> (y_hat - y.reshape(y_hat.shape))**<span class=\"number\">2</span>/<span class=\"number\">2</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"2-6-定义优化算法\"><a href=\"#2-6-定义优化算法\" class=\"headerlink\" title=\"2.6 定义优化算法\"></a>2.6 定义优化算法</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">sge</span>(<span class=\"params\">params,lr,batch_size</span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;小批量随机梯度下降。&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">with</span> torch.no_grad(): <span class=\"comment\">#在该模块下，所有计算得出的tensor的requires_grad都自动设置为False ???</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> param <span class=\"keyword\">in</span> params:</span><br><span class=\"line\">            param -= lr*param.grad/batch_size <span class=\"comment\">#这是一个计算操作，会产生新的梯度信息，所以需要关闭梯度信息 ???</span></span><br><span class=\"line\">            param.grad.zero_() <span class=\"comment\">#将param.grad清零</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"2-7-训练\"><a href=\"#2-7-训练\" class=\"headerlink\" title=\"2.7 训练\"></a>2.7 训练</h2><ul>\n<li>初始化参数</li>\n<li>重复以下训练，知道完成：<ul>\n<li>计算梯度$g \\leftarrow \\partial_{(\\mathbf{w},b)}\\frac{1}{|\\mathcal{B}|}\\sum_{i\\in\\mathcal{B}}l(x^{(i)},y^{(i)},\\mathbf{w},b)$</li>\n<li>更新参数$(\\mathbf{w},b) \\leftarrow (\\mathbf{w},b) - \\eta g$</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#1 超参数</span></span><br><span class=\"line\">lr = <span class=\"number\">0.03</span></span><br><span class=\"line\">num_epochs = <span class=\"number\">3</span></span><br><span class=\"line\">net = linreg</span><br><span class=\"line\">loss = squred_loss</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#2 训练</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">    <span class=\"keyword\">for</span> X,y <span class=\"keyword\">in</span> data_iter(batch_size,features,labels):</span><br><span class=\"line\">        l = loss(net(X,w,b),y) <span class=\"comment\">#l是有关小批量X和y的损失</span></span><br><span class=\"line\">        l.<span class=\"built_in\">sum</span>().backward() <span class=\"comment\">#小批量的损失对模型参数求梯度,因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，</span></span><br><span class=\"line\">        sge([w,b],lr,batch_size) <span class=\"comment\">#使用小批量随机梯度下降迭代模型参数</span></span><br><span class=\"line\">    <span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">        train_l = loss(net(features,w,b),labels)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;epoch <span class=\"subst\">&#123;epoch+<span class=\"number\">1</span>&#125;</span>,loss <span class=\"subst\">&#123;<span class=\"built_in\">float</span>(train_l.mean()):f&#125;</span>&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#3 检验</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;w的估计误差：<span class=\"subst\">&#123;true_w - w.reshape(true_w.shape)&#125;</span>&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;b的估计误差：<span class=\"subst\">&#123;true_b - b&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>epoch 1,loss 0.030588\nepoch 2,loss 0.000105\nepoch 3,loss 0.000049\nw的估计误差：tensor([ 0.0010, -0.0008], grad_fn=&lt;SubBackward0&gt;)\nb的估计误差：tensor([0.0008], grad_fn=&lt;RsubBackward1&gt;)\n</code></pre>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"2-线性回归实现\"><a href=\"#2-线性回归实现\" class=\"headerlink\" title=\"2. 线性回归实现\"></a>2. 线性回归实现</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> random</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>] = <span class=\"string\">&#x27;True&#x27;</span></span><br></pre></td></tr></table></figure>\n\n<h1 id=\"2-1-生成数据集\"><a href=\"#2-1-生成数据集\" class=\"headerlink\" title=\"2.1 生成数据集\"></a>2.1 生成数据集</h1><ul>\n<li>使用线性模型参数w &#x3D; [2, −3.4]⊤、b &#x3D; 4.2 和噪声项ϵ~N(0,0.01)生成数据集及其标签：</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#1 生成数据集</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">synthetic_data</span>(<span class=\"params\">w,b,num_examples</span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;生成 y = Xw + b + 噪声。&quot;&quot;&quot;</span></span><br><span class=\"line\">    X = torch.normal(<span class=\"number\">0</span>,<span class=\"number\">1</span>,(num_examples,<span class=\"built_in\">len</span>(w))) <span class=\"comment\">#生成服从正态分布的数据</span></span><br><span class=\"line\">    y = torch.matmul(X,w) + b <span class=\"comment\">#矩阵乘法</span></span><br><span class=\"line\">    y += torch.normal(<span class=\"number\">0</span>,<span class=\"number\">0.01</span>,y.shape) <span class=\"comment\">#加上噪声</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> X,y.reshape((-<span class=\"number\">1</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">true_w = torch.tensor([<span class=\"number\">2</span>,-<span class=\"number\">3.4</span>])</span><br><span class=\"line\">true_b = <span class=\"number\">4.2</span></span><br><span class=\"line\">features,labels = synthetic_data(true_w,true_b,<span class=\"number\">1000</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;features:&#x27;</span>,features[<span class=\"number\">0</span>],<span class=\"string\">&#x27;\\nlabel:&#x27;</span>,labels[<span class=\"number\">0</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#2 可以看到x1与y是线性关系</span></span><br><span class=\"line\">d2l.set_figsize()</span><br><span class=\"line\">d2l.plt.scatter(features[:,<span class=\"number\">1</span>].detach().numpy(),labels.detach().numpy(),<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<pre><code>features: tensor([1.2604, 0.9351]) \nlabel: tensor([3.5493])\n\n\n\n\n\n&lt;matplotlib.collections.PathCollection at 0x168296a63a0&gt;\n</code></pre>\n<p><img src=\"/2_linear_regression_realize_files/2_linear_regression_realize_3_2.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/2_linear_neural_network/2_linear_regression_realize_files/2_linear_regression_realize_3_2.svg\"></p>\n<h2 id=\"2-2-读取数据集\"><a href=\"#2-2-读取数据集\" class=\"headerlink\" title=\"2.2 读取数据集\"></a>2.2 读取数据集</h2><ul>\n<li>定义一个函数打乱样本并返回批量数据：<ul>\n<li>输入：批量大小、特征、标签</li>\n<li>输出：批量数据</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">data_iter</span>(<span class=\"params\">batch_size,features,labels</span>):</span><br><span class=\"line\">    num_examples = <span class=\"built_in\">len</span>(features) <span class=\"comment\">#1 样本数</span></span><br><span class=\"line\">    indices = <span class=\"built_in\">list</span>(<span class=\"built_in\">range</span>(num_examples)) <span class=\"comment\">#2 生成下标</span></span><br><span class=\"line\">    random.shuffle(indices) <span class=\"comment\">#3 打乱下标</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">0</span>,num_examples,batch_size): <span class=\"comment\">#4 遍历数据集，每次取出batch_size个样本</span></span><br><span class=\"line\">        batch_indices = torch.tensor(indices[i:<span class=\"built_in\">min</span>(i+batch_size,num_examples)])</span><br><span class=\"line\">        <span class=\"keyword\">yield</span> features[batch_indices],labels[batch_indices] <span class=\"comment\">#5 可迭代对象，每次返回batch_size个样本</span></span><br><span class=\"line\"></span><br><span class=\"line\">batch_size = <span class=\"number\">10</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> X,y <span class=\"keyword\">in</span> data_iter(batch_size,features,labels):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(X,<span class=\"string\">&#x27;\\n&#x27;</span>,y)</span><br><span class=\"line\">    <span class=\"keyword\">break</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[ 0.1474, -0.8037],\n        [ 0.3242,  0.3905],\n        [-0.1883,  0.1497],\n        [ 0.4741,  0.2937],\n        [ 1.6234, -0.3555],\n        [ 1.2714, -1.5737],\n        [-0.6498,  1.9078],\n        [ 0.6298,  0.4764],\n        [ 0.2344, -1.3566],\n        [-0.4173, -0.4298]]) \n tensor([[ 7.2271],\n        [ 3.5186],\n        [ 3.3331],\n        [ 4.1500],\n        [ 8.6549],\n        [12.0805],\n        [-3.5749],\n        [ 3.8355],\n        [ 9.2662],\n        [ 4.8142]])\n</code></pre>\n<h2 id=\"2-3-初始化模型参数\"><a href=\"#2-3-初始化模型参数\" class=\"headerlink\" title=\"2.3 初始化模型参数\"></a>2.3 初始化模型参数</h2><ul>\n<li>从均值为0、标准差为0.01的正态分布中采样随机数来初始化权重，并将偏置初始化为0。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">w = torch.normal(<span class=\"number\">0</span>,<span class=\"number\">0.01</span>,size=(<span class=\"number\">2</span>,<span class=\"number\">1</span>),requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">b = torch.zeros(<span class=\"number\">1</span>,requires_grad=<span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"2-4-定义模型\"><a href=\"#2-4-定义模型\" class=\"headerlink\" title=\"2.4 定义模型\"></a>2.4 定义模型</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">linreg</span>(<span class=\"params\">X,w,b</span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;线性回归模型。&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.matmul(X,w) + b</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"2-5-定义损失函数\"><a href=\"#2-5-定义损失函数\" class=\"headerlink\" title=\"2.5 定义损失函数\"></a>2.5 定义损失函数</h2><ul>\n<li>平方损失函数</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">squred_loss</span>(<span class=\"params\">y_hat,y</span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;均方损失。&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> (y_hat - y.reshape(y_hat.shape))**<span class=\"number\">2</span>/<span class=\"number\">2</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"2-6-定义优化算法\"><a href=\"#2-6-定义优化算法\" class=\"headerlink\" title=\"2.6 定义优化算法\"></a>2.6 定义优化算法</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">sge</span>(<span class=\"params\">params,lr,batch_size</span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;小批量随机梯度下降。&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">with</span> torch.no_grad(): <span class=\"comment\">#在该模块下，所有计算得出的tensor的requires_grad都自动设置为False ???</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> param <span class=\"keyword\">in</span> params:</span><br><span class=\"line\">            param -= lr*param.grad/batch_size <span class=\"comment\">#这是一个计算操作，会产生新的梯度信息，所以需要关闭梯度信息 ???</span></span><br><span class=\"line\">            param.grad.zero_() <span class=\"comment\">#将param.grad清零</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"2-7-训练\"><a href=\"#2-7-训练\" class=\"headerlink\" title=\"2.7 训练\"></a>2.7 训练</h2><ul>\n<li>初始化参数</li>\n<li>重复以下训练，知道完成：<ul>\n<li>计算梯度$g \\leftarrow \\partial_{(\\mathbf{w},b)}\\frac{1}{|\\mathcal{B}|}\\sum_{i\\in\\mathcal{B}}l(x^{(i)},y^{(i)},\\mathbf{w},b)$</li>\n<li>更新参数$(\\mathbf{w},b) \\leftarrow (\\mathbf{w},b) - \\eta g$</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#1 超参数</span></span><br><span class=\"line\">lr = <span class=\"number\">0.03</span></span><br><span class=\"line\">num_epochs = <span class=\"number\">3</span></span><br><span class=\"line\">net = linreg</span><br><span class=\"line\">loss = squred_loss</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#2 训练</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">    <span class=\"keyword\">for</span> X,y <span class=\"keyword\">in</span> data_iter(batch_size,features,labels):</span><br><span class=\"line\">        l = loss(net(X,w,b),y) <span class=\"comment\">#l是有关小批量X和y的损失</span></span><br><span class=\"line\">        l.<span class=\"built_in\">sum</span>().backward() <span class=\"comment\">#小批量的损失对模型参数求梯度,因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，</span></span><br><span class=\"line\">        sge([w,b],lr,batch_size) <span class=\"comment\">#使用小批量随机梯度下降迭代模型参数</span></span><br><span class=\"line\">    <span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">        train_l = loss(net(features,w,b),labels)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;epoch <span class=\"subst\">&#123;epoch+<span class=\"number\">1</span>&#125;</span>,loss <span class=\"subst\">&#123;<span class=\"built_in\">float</span>(train_l.mean()):f&#125;</span>&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#3 检验</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;w的估计误差：<span class=\"subst\">&#123;true_w - w.reshape(true_w.shape)&#125;</span>&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;b的估计误差：<span class=\"subst\">&#123;true_b - b&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>epoch 1,loss 0.030588\nepoch 2,loss 0.000105\nepoch 3,loss 0.000049\nw的估计误差：tensor([ 0.0010, -0.0008], grad_fn=&lt;SubBackward0&gt;)\nb的估计误差：tensor([0.0008], grad_fn=&lt;RsubBackward1&gt;)\n</code></pre>"},{"title":"2.1 线性回归","date":"2024-02-03T06:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 1. 线性回归\n## 1.1 线性回归的基本元素\n- 基于几个简单的假设：\n    - 假设自变量x和因变量y之间的关系是线性的\n    - 假设y的观测值是通过均值为0的噪声扰动后得到的\n- 一些概念：\n    - 训练集：训练模型的数据集\n    - 样本：每行数据\n    - 标签：试图预测的值\n    - 特征（feature，协变量（covariate））：预测所依赖的输入变量\n### 1.1.1 线性模型\n$$\\hat y = w_1x1+w_2x_2+b \\rightarrow \\hat y = \\mathbf{w}^T\\mathbf{x}+b$$\n- $w_1$和$w_2$是权重\n- $b$是偏置(bias),也叫偏移量(offset),截距(intercept)：当所有特征都取值为0时，预测值应该为多少。\n- 严格来说，上述公式是输入特征的一个 仿射变换（affine transformation）：仿射变换的特点是通过加权和对特征进行线性变换（linear transformation），并通过偏置项来进行平移（translation）。\n- 对于整个数据集，特征集合是一个矩阵：$\\hat y = \\mathbf{X}\\mathbf{w}+b$\n### 1.1.2 损失函数\n- 量化目标的实际值与预测值之间的差距。\n- 回归问题中最常用的损失函数是平方误差函数:\n    - $l^{(i)}(\\mathbf{w},b) = \\frac{1}{2}(\\hat y^{(i)}-y^{(i)})^2$\n- 训练目标：寻找一组参数（$\\mathbf{w^*},b^*$）,使得所有样本的损失函数之和最小：\n$$\\mathbf{w^*},b^* = L(\\mathbf{w},b) \\underset{\\mathbf{w},b}{\\arg\\min} \\frac{1}{n}\\sum_{i=1}^n l^{(i)}(\\mathbf{w},b)$$\n### 1.1.3 解析解\n$$\\mathbf{w^*} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$\n### 1.1.4 随机梯度下降\n- $$(\\mathbf{w},b) \\leftarrow (\\mathbf{w},b) - \\frac{\\eta}{|\\mathcal{B}|}\\sum_{i\\in\\mathcal{B}}\\partial_{(\\mathbf{w},b)}l^{(i)}(\\mathbf{w},b)$$\n- 通过不断地在损失函数递减的方向上更新参数来降低误差。\n- 梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值）关于模型参数的导数（在这里也可以称为梯度）。但实际中的执行可能会非常慢：因为在每一次更新参数之前，我们必须遍历整个数据集。因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本$\\mathcal{B}$，并计算损失函数关于这些样本的平均损失的梯度。\n- 算法过程：\n    - 1）初始化模型参数的值，如随机初始化\n    - 2）从数据集中随机抽取一小批样本且在负梯度方向上更新参数\n    - 3）重复步骤2直到收敛\n    - 例如对于平方损失和放射变换：\n        - $\\mathbf{w} \\leftarrow \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|}\\sum\\limits_{i\\in\\mathcal{B}}\\mathbf{x}^{(i)}(\\mathbf{w}^T\\mathbf{x}^{(i)}+b-y^{(i)})$\n        - $b \\leftarrow b - \\frac{\\eta}{|\\mathcal{B}|}\\sum\\limits_{i\\in\\mathcal{B}}(\\mathbf{w}^T\\mathbf{x}^{(i)}+b-y^{(i)})$\n            - $\\eta$是学习率\n            - $\\mathcal{B}$是小批量样本的大小(batch size)\n    - 对像深度神经网络这样复杂的模型来说，损失平面上通常包含多个最小值。深度学习实践者很少会去花费大力气寻找这样一组参数，使得在训练集上的损失达到最小。事实上，更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失，这一挑战被称为泛化（generalization）。\n### 1.1.5 用模型进行预测\n$$\\hat y = \\mathbf{\\hat w}^T\\mathbf{x}+\\hat b$$\n## 1.2 矢量化加速\n- 在训练我们的模型时，我们经常希望能够同时处理整个小批量的样本。而不是在Python中编写开销高昂的for循环。\n\n\n\n\n```python\nimport math\nimport time\nimport numpy as np\nimport torch\nfrom d2l import torch as d2l\n#计时器\nclass Timer: #@save\n    \"\"\"记录多次运行时间\"\"\"\n    def __init__(self):\n        self.times = []\n        self.start()\n    def start(self):\n        \"\"\"启动计时器\"\"\"\n        self.tik = time.time()\n    def stop(self):\n        \"\"\"停止计时器并将时间记录在列表中\"\"\"\n        self.times.append(time.time() - self.tik)\n        return self.times[-1]\n    def avg(self):\n        \"\"\"返回平均时间\"\"\"\n        return sum(self.times) / len(self.times)\n    def sum(self):\n        \"\"\"返回时间总和\"\"\"\n        return sum(self.times)\n    def cumsum(self):\n        \"\"\"返回累计时间\"\"\"\n        return np.array(self.times).cumsum().tolist()\n```\n\n\n```python\n#1 两个10000维向量\nn = 10000\na = torch.ones(n)\nb = torch.ones(n)\nc = torch.zeros(n)\n\n#2 for循环计算\ntimer = Timer()\nfor i in range(n):\n    c[i] = a[i] + b[i]\nprint(f'{timer.stop():.5f} sec')\n\n#3 一次性计算\ntimer.start()\nd = a + b\nprint(f'{timer.stop():.7f} sec')\n```\n\n    0.11051 sec\n    0.0000000 sec\n    \n\n## 1.3 正态分布与平方损失\n- 正态分布（高斯分布）：\n$$p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{(x-\\mu)^2}{2\\sigma^2})$$\n\n\n```python\nimport os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n#1 定义一个函数来计算高斯分布\ndef normal(x,mu,sigma):\n    p = 1 / math.sqrt(2 * math.pi * sigma**2)\n    return p * np.exp(-0.5 / sigma**2 * (x - mu)**2)\n\n#2 可视化高斯分布\nx = np.arange(-7,7,0.01)\n#均值和标准差对\nparams = [(0,1),(0,2),(3,1)]\nd2l.plot(x,[normal(x,mu,sigma) for mu,sigma in params],xlabel='x',ylabel='p(x)',legend=[f'mean {mu},std {sigma}' for mu,sigma in params])\n```\n\n\n    \n![svg](1_linear_regression_files/1_linear_regression_4_0.svg)\n![](img/deeplearning/code/pytorch/2_linear_neural_network/1_linear_regression_files/1_linear_regression_4_0.svg)\n    \n\n\n- 假设观测中包含噪声：$$y = \\mathbf{w}^T\\mathbf{x}+b+\\epsilon$$\n    - 噪声$\\epsilon \\sim N(0,\\sigma^2)$\n    - 因此有：(y服从正态分布，y的均值是$\\mathbf{w}^T\\mathbf{x}+b$，方差是$\\sigma^2$)\n        - 1）通过给定的$\\mathbf{x}$观测到特定y的似然（likelihood）：$$p(y|\\mathbf{x}) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{(y-\\mathbf{w}^T\\mathbf{x}-b)^2}{2\\sigma^2})$$\n        - 2）根据极大似然估计，参数（$\\mathbf{w},b$）的最优值使整个数据集的似然最大：$$p(\\mathbf{y}|\\mathbf{X}) = \\prod \\limits_{i=1}^n p(y^{(i)}|\\mathbf{x}^{(i)})$$\n        - 3）根据极大似然估计法选择的估计量称为极大似然估计量。虽然使许多指数函数的乘积最大化看起来很困难，但是我们可以在不改变目标的前提下，通过最大化似然对数来简化：$$-log P(\\mathbf{y}|\\mathbf{X}) = \\frac{n}{2}log(2\\pi\\sigma^2)+\\frac{1}{2\\sigma^2}\\sum \\limits_{i=1}^n(y^{(i)}-\\mathbf{w}^T\\mathbf{x}^{(i)}-b)^2$$\n        - 4）假设$\\sigma$是常数，则可以忽略第一项；第二项除了常数项$\\frac{1}{\\sigma^2}$之外，就是平方损失函数。因此，在高斯噪声的假设下，最小化均方误差等价于对线性模型的极大似然估计。\n\n## 1.4 从线性回归到深度网络\n### 1.4.1 神经网络\n- 线性回归是一个单层神经网络（隐去了权重和偏置）：\n- 每个输入都与每个输出相连，成为全连接层（fully connected layer）或稠密层（dense layer）。\n![](1_linear_regression_files/1.png)\n![](img/deeplearning/code/pytorch/2_linear_neural_network/1_linear_regression_files/1.png)\n","source":"_posts/deeplearning/code/pytorch/2_linear_neural_network/1_linear_regression.md","raw":"---\ntitle: 2.1 线性回归\ndate: 2024-2-3 14:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 1. 线性回归\n## 1.1 线性回归的基本元素\n- 基于几个简单的假设：\n    - 假设自变量x和因变量y之间的关系是线性的\n    - 假设y的观测值是通过均值为0的噪声扰动后得到的\n- 一些概念：\n    - 训练集：训练模型的数据集\n    - 样本：每行数据\n    - 标签：试图预测的值\n    - 特征（feature，协变量（covariate））：预测所依赖的输入变量\n### 1.1.1 线性模型\n$$\\hat y = w_1x1+w_2x_2+b \\rightarrow \\hat y = \\mathbf{w}^T\\mathbf{x}+b$$\n- $w_1$和$w_2$是权重\n- $b$是偏置(bias),也叫偏移量(offset),截距(intercept)：当所有特征都取值为0时，预测值应该为多少。\n- 严格来说，上述公式是输入特征的一个 仿射变换（affine transformation）：仿射变换的特点是通过加权和对特征进行线性变换（linear transformation），并通过偏置项来进行平移（translation）。\n- 对于整个数据集，特征集合是一个矩阵：$\\hat y = \\mathbf{X}\\mathbf{w}+b$\n### 1.1.2 损失函数\n- 量化目标的实际值与预测值之间的差距。\n- 回归问题中最常用的损失函数是平方误差函数:\n    - $l^{(i)}(\\mathbf{w},b) = \\frac{1}{2}(\\hat y^{(i)}-y^{(i)})^2$\n- 训练目标：寻找一组参数（$\\mathbf{w^*},b^*$）,使得所有样本的损失函数之和最小：\n$$\\mathbf{w^*},b^* = L(\\mathbf{w},b) \\underset{\\mathbf{w},b}{\\arg\\min} \\frac{1}{n}\\sum_{i=1}^n l^{(i)}(\\mathbf{w},b)$$\n### 1.1.3 解析解\n$$\\mathbf{w^*} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$\n### 1.1.4 随机梯度下降\n- $$(\\mathbf{w},b) \\leftarrow (\\mathbf{w},b) - \\frac{\\eta}{|\\mathcal{B}|}\\sum_{i\\in\\mathcal{B}}\\partial_{(\\mathbf{w},b)}l^{(i)}(\\mathbf{w},b)$$\n- 通过不断地在损失函数递减的方向上更新参数来降低误差。\n- 梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值）关于模型参数的导数（在这里也可以称为梯度）。但实际中的执行可能会非常慢：因为在每一次更新参数之前，我们必须遍历整个数据集。因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本$\\mathcal{B}$，并计算损失函数关于这些样本的平均损失的梯度。\n- 算法过程：\n    - 1）初始化模型参数的值，如随机初始化\n    - 2）从数据集中随机抽取一小批样本且在负梯度方向上更新参数\n    - 3）重复步骤2直到收敛\n    - 例如对于平方损失和放射变换：\n        - $\\mathbf{w} \\leftarrow \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|}\\sum\\limits_{i\\in\\mathcal{B}}\\mathbf{x}^{(i)}(\\mathbf{w}^T\\mathbf{x}^{(i)}+b-y^{(i)})$\n        - $b \\leftarrow b - \\frac{\\eta}{|\\mathcal{B}|}\\sum\\limits_{i\\in\\mathcal{B}}(\\mathbf{w}^T\\mathbf{x}^{(i)}+b-y^{(i)})$\n            - $\\eta$是学习率\n            - $\\mathcal{B}$是小批量样本的大小(batch size)\n    - 对像深度神经网络这样复杂的模型来说，损失平面上通常包含多个最小值。深度学习实践者很少会去花费大力气寻找这样一组参数，使得在训练集上的损失达到最小。事实上，更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失，这一挑战被称为泛化（generalization）。\n### 1.1.5 用模型进行预测\n$$\\hat y = \\mathbf{\\hat w}^T\\mathbf{x}+\\hat b$$\n## 1.2 矢量化加速\n- 在训练我们的模型时，我们经常希望能够同时处理整个小批量的样本。而不是在Python中编写开销高昂的for循环。\n\n\n\n\n```python\nimport math\nimport time\nimport numpy as np\nimport torch\nfrom d2l import torch as d2l\n#计时器\nclass Timer: #@save\n    \"\"\"记录多次运行时间\"\"\"\n    def __init__(self):\n        self.times = []\n        self.start()\n    def start(self):\n        \"\"\"启动计时器\"\"\"\n        self.tik = time.time()\n    def stop(self):\n        \"\"\"停止计时器并将时间记录在列表中\"\"\"\n        self.times.append(time.time() - self.tik)\n        return self.times[-1]\n    def avg(self):\n        \"\"\"返回平均时间\"\"\"\n        return sum(self.times) / len(self.times)\n    def sum(self):\n        \"\"\"返回时间总和\"\"\"\n        return sum(self.times)\n    def cumsum(self):\n        \"\"\"返回累计时间\"\"\"\n        return np.array(self.times).cumsum().tolist()\n```\n\n\n```python\n#1 两个10000维向量\nn = 10000\na = torch.ones(n)\nb = torch.ones(n)\nc = torch.zeros(n)\n\n#2 for循环计算\ntimer = Timer()\nfor i in range(n):\n    c[i] = a[i] + b[i]\nprint(f'{timer.stop():.5f} sec')\n\n#3 一次性计算\ntimer.start()\nd = a + b\nprint(f'{timer.stop():.7f} sec')\n```\n\n    0.11051 sec\n    0.0000000 sec\n    \n\n## 1.3 正态分布与平方损失\n- 正态分布（高斯分布）：\n$$p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{(x-\\mu)^2}{2\\sigma^2})$$\n\n\n```python\nimport os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n#1 定义一个函数来计算高斯分布\ndef normal(x,mu,sigma):\n    p = 1 / math.sqrt(2 * math.pi * sigma**2)\n    return p * np.exp(-0.5 / sigma**2 * (x - mu)**2)\n\n#2 可视化高斯分布\nx = np.arange(-7,7,0.01)\n#均值和标准差对\nparams = [(0,1),(0,2),(3,1)]\nd2l.plot(x,[normal(x,mu,sigma) for mu,sigma in params],xlabel='x',ylabel='p(x)',legend=[f'mean {mu},std {sigma}' for mu,sigma in params])\n```\n\n\n    \n![svg](1_linear_regression_files/1_linear_regression_4_0.svg)\n![](img/deeplearning/code/pytorch/2_linear_neural_network/1_linear_regression_files/1_linear_regression_4_0.svg)\n    \n\n\n- 假设观测中包含噪声：$$y = \\mathbf{w}^T\\mathbf{x}+b+\\epsilon$$\n    - 噪声$\\epsilon \\sim N(0,\\sigma^2)$\n    - 因此有：(y服从正态分布，y的均值是$\\mathbf{w}^T\\mathbf{x}+b$，方差是$\\sigma^2$)\n        - 1）通过给定的$\\mathbf{x}$观测到特定y的似然（likelihood）：$$p(y|\\mathbf{x}) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{(y-\\mathbf{w}^T\\mathbf{x}-b)^2}{2\\sigma^2})$$\n        - 2）根据极大似然估计，参数（$\\mathbf{w},b$）的最优值使整个数据集的似然最大：$$p(\\mathbf{y}|\\mathbf{X}) = \\prod \\limits_{i=1}^n p(y^{(i)}|\\mathbf{x}^{(i)})$$\n        - 3）根据极大似然估计法选择的估计量称为极大似然估计量。虽然使许多指数函数的乘积最大化看起来很困难，但是我们可以在不改变目标的前提下，通过最大化似然对数来简化：$$-log P(\\mathbf{y}|\\mathbf{X}) = \\frac{n}{2}log(2\\pi\\sigma^2)+\\frac{1}{2\\sigma^2}\\sum \\limits_{i=1}^n(y^{(i)}-\\mathbf{w}^T\\mathbf{x}^{(i)}-b)^2$$\n        - 4）假设$\\sigma$是常数，则可以忽略第一项；第二项除了常数项$\\frac{1}{\\sigma^2}$之外，就是平方损失函数。因此，在高斯噪声的假设下，最小化均方误差等价于对线性模型的极大似然估计。\n\n## 1.4 从线性回归到深度网络\n### 1.4.1 神经网络\n- 线性回归是一个单层神经网络（隐去了权重和偏置）：\n- 每个输入都与每个输出相连，成为全连接层（fully connected layer）或稠密层（dense layer）。\n![](1_linear_regression_files/1.png)\n![](img/deeplearning/code/pytorch/2_linear_neural_network/1_linear_regression_files/1.png)\n","slug":"deeplearning/code/pytorch/2_linear_neural_network/1_linear_regression","published":1,"updated":"2024-05-02T08:54:33.883Z","_id":"clvuli5qz000azsvw34t1cfb7","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"1-线性回归\"><a href=\"#1-线性回归\" class=\"headerlink\" title=\"1. 线性回归\"></a>1. 线性回归</h1><h2 id=\"1-1-线性回归的基本元素\"><a href=\"#1-1-线性回归的基本元素\" class=\"headerlink\" title=\"1.1 线性回归的基本元素\"></a>1.1 线性回归的基本元素</h2><ul>\n<li>基于几个简单的假设：<ul>\n<li>假设自变量x和因变量y之间的关系是线性的</li>\n<li>假设y的观测值是通过均值为0的噪声扰动后得到的</li>\n</ul>\n</li>\n<li>一些概念：<ul>\n<li>训练集：训练模型的数据集</li>\n<li>样本：每行数据</li>\n<li>标签：试图预测的值</li>\n<li>特征（feature，协变量（covariate））：预测所依赖的输入变量</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"1-1-1-线性模型\"><a href=\"#1-1-1-线性模型\" class=\"headerlink\" title=\"1.1.1 线性模型\"></a>1.1.1 线性模型</h3><p>$$\\hat y &#x3D; w_1x1+w_2x_2+b \\rightarrow \\hat y &#x3D; \\mathbf{w}^T\\mathbf{x}+b$$</p>\n<ul>\n<li>$w_1$和$w_2$是权重</li>\n<li>$b$是偏置(bias),也叫偏移量(offset),截距(intercept)：当所有特征都取值为0时，预测值应该为多少。</li>\n<li>严格来说，上述公式是输入特征的一个 仿射变换（affine transformation）：仿射变换的特点是通过加权和对特征进行线性变换（linear transformation），并通过偏置项来进行平移（translation）。</li>\n<li>对于整个数据集，特征集合是一个矩阵：$\\hat y &#x3D; \\mathbf{X}\\mathbf{w}+b$</li>\n</ul>\n<h3 id=\"1-1-2-损失函数\"><a href=\"#1-1-2-损失函数\" class=\"headerlink\" title=\"1.1.2 损失函数\"></a>1.1.2 损失函数</h3><ul>\n<li>量化目标的实际值与预测值之间的差距。</li>\n<li>回归问题中最常用的损失函数是平方误差函数:<ul>\n<li>$l^{(i)}(\\mathbf{w},b) &#x3D; \\frac{1}{2}(\\hat y^{(i)}-y^{(i)})^2$</li>\n</ul>\n</li>\n<li>训练目标：寻找一组参数（$\\mathbf{w^*},b^*$）,使得所有样本的损失函数之和最小：<br>$$\\mathbf{w^*},b^* &#x3D; L(\\mathbf{w},b) \\underset{\\mathbf{w},b}{\\arg\\min} \\frac{1}{n}\\sum_{i&#x3D;1}^n l^{(i)}(\\mathbf{w},b)$$</li>\n</ul>\n<h3 id=\"1-1-3-解析解\"><a href=\"#1-1-3-解析解\" class=\"headerlink\" title=\"1.1.3 解析解\"></a>1.1.3 解析解</h3><p>$$\\mathbf{w^*} &#x3D; (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$</p>\n<h3 id=\"1-1-4-随机梯度下降\"><a href=\"#1-1-4-随机梯度下降\" class=\"headerlink\" title=\"1.1.4 随机梯度下降\"></a>1.1.4 随机梯度下降</h3><ul>\n<li>$$(\\mathbf{w},b) \\leftarrow (\\mathbf{w},b) - \\frac{\\eta}{|\\mathcal{B}|}\\sum_{i\\in\\mathcal{B}}\\partial_{(\\mathbf{w},b)}l^{(i)}(\\mathbf{w},b)$$</li>\n<li>通过不断地在损失函数递减的方向上更新参数来降低误差。</li>\n<li>梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值）关于模型参数的导数（在这里也可以称为梯度）。但实际中的执行可能会非常慢：因为在每一次更新参数之前，我们必须遍历整个数据集。因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本$\\mathcal{B}$，并计算损失函数关于这些样本的平均损失的梯度。</li>\n<li>算法过程：<ul>\n<li>1）初始化模型参数的值，如随机初始化</li>\n<li>2）从数据集中随机抽取一小批样本且在负梯度方向上更新参数</li>\n<li>3）重复步骤2直到收敛</li>\n<li>例如对于平方损失和放射变换：<ul>\n<li>$\\mathbf{w} \\leftarrow \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|}\\sum\\limits_{i\\in\\mathcal{B}}\\mathbf{x}^{(i)}(\\mathbf{w}^T\\mathbf{x}^{(i)}+b-y^{(i)})$</li>\n<li>$b \\leftarrow b - \\frac{\\eta}{|\\mathcal{B}|}\\sum\\limits_{i\\in\\mathcal{B}}(\\mathbf{w}^T\\mathbf{x}^{(i)}+b-y^{(i)})$<ul>\n<li>$\\eta$是学习率</li>\n<li>$\\mathcal{B}$是小批量样本的大小(batch size)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>对像深度神经网络这样复杂的模型来说，损失平面上通常包含多个最小值。深度学习实践者很少会去花费大力气寻找这样一组参数，使得在训练集上的损失达到最小。事实上，更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失，这一挑战被称为泛化（generalization）。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"1-1-5-用模型进行预测\"><a href=\"#1-1-5-用模型进行预测\" class=\"headerlink\" title=\"1.1.5 用模型进行预测\"></a>1.1.5 用模型进行预测</h3><p>$$\\hat y &#x3D; \\mathbf{\\hat w}^T\\mathbf{x}+\\hat b$$</p>\n<h2 id=\"1-2-矢量化加速\"><a href=\"#1-2-矢量化加速\" class=\"headerlink\" title=\"1.2 矢量化加速\"></a>1.2 矢量化加速</h2><ul>\n<li>在训练我们的模型时，我们经常希望能够同时处理整个小批量的样本。而不是在Python中编写开销高昂的for循环。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> math</span><br><span class=\"line\"><span class=\"keyword\">import</span> time</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"><span class=\"comment\">#计时器</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Timer</span>: <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;记录多次运行时间&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        self.times = []</span><br><span class=\"line\">        self.start()</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">start</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;启动计时器&quot;&quot;&quot;</span></span><br><span class=\"line\">        self.tik = time.time()</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">stop</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;停止计时器并将时间记录在列表中&quot;&quot;&quot;</span></span><br><span class=\"line\">        self.times.append(time.time() - self.tik)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.times[-<span class=\"number\">1</span>]</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">avg</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;返回平均时间&quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"built_in\">sum</span>(self.times) / <span class=\"built_in\">len</span>(self.times)</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">sum</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;返回时间总和&quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"built_in\">sum</span>(self.times)</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">cumsum</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;返回累计时间&quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> np.array(self.times).cumsum().tolist()</span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#1 两个10000维向量</span></span><br><span class=\"line\">n = <span class=\"number\">10000</span></span><br><span class=\"line\">a = torch.ones(n)</span><br><span class=\"line\">b = torch.ones(n)</span><br><span class=\"line\">c = torch.zeros(n)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#2 for循环计算</span></span><br><span class=\"line\">timer = Timer()</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(n):</span><br><span class=\"line\">    c[i] = a[i] + b[i]</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;<span class=\"subst\">&#123;timer.stop():<span class=\"number\">.5</span>f&#125;</span> sec&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#3 一次性计算</span></span><br><span class=\"line\">timer.start()</span><br><span class=\"line\">d = a + b</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;<span class=\"subst\">&#123;timer.stop():<span class=\"number\">.7</span>f&#125;</span> sec&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>0.11051 sec\n0.0000000 sec\n</code></pre>\n<h2 id=\"1-3-正态分布与平方损失\"><a href=\"#1-3-正态分布与平方损失\" class=\"headerlink\" title=\"1.3 正态分布与平方损失\"></a>1.3 正态分布与平方损失</h2><ul>\n<li>正态分布（高斯分布）：<br>$$p(x) &#x3D; \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{(x-\\mu)^2}{2\\sigma^2})$$</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>] = <span class=\"string\">&quot;TRUE&quot;</span></span><br><span class=\"line\"><span class=\"comment\">#1 定义一个函数来计算高斯分布</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">normal</span>(<span class=\"params\">x,mu,sigma</span>):</span><br><span class=\"line\">    p = <span class=\"number\">1</span> / math.sqrt(<span class=\"number\">2</span> * math.pi * sigma**<span class=\"number\">2</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> p * np.exp(-<span class=\"number\">0.5</span> / sigma**<span class=\"number\">2</span> * (x - mu)**<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#2 可视化高斯分布</span></span><br><span class=\"line\">x = np.arange(-<span class=\"number\">7</span>,<span class=\"number\">7</span>,<span class=\"number\">0.01</span>)</span><br><span class=\"line\"><span class=\"comment\">#均值和标准差对</span></span><br><span class=\"line\">params = [(<span class=\"number\">0</span>,<span class=\"number\">1</span>),(<span class=\"number\">0</span>,<span class=\"number\">2</span>),(<span class=\"number\">3</span>,<span class=\"number\">1</span>)]</span><br><span class=\"line\">d2l.plot(x,[normal(x,mu,sigma) <span class=\"keyword\">for</span> mu,sigma <span class=\"keyword\">in</span> params],xlabel=<span class=\"string\">&#x27;x&#x27;</span>,ylabel=<span class=\"string\">&#x27;p(x)&#x27;</span>,legend=[<span class=\"string\">f&#x27;mean <span class=\"subst\">&#123;mu&#125;</span>,std <span class=\"subst\">&#123;sigma&#125;</span>&#x27;</span> <span class=\"keyword\">for</span> mu,sigma <span class=\"keyword\">in</span> params])</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/1_linear_regression_files/1_linear_regression_4_0.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/2_linear_neural_network/1_linear_regression_files/1_linear_regression_4_0.svg\"></p>\n<ul>\n<li>假设观测中包含噪声：$$y &#x3D; \\mathbf{w}^T\\mathbf{x}+b+\\epsilon$$<ul>\n<li>噪声$\\epsilon \\sim N(0,\\sigma^2)$</li>\n<li>因此有：(y服从正态分布，y的均值是$\\mathbf{w}^T\\mathbf{x}+b$，方差是$\\sigma^2$)<ul>\n<li>1）通过给定的$\\mathbf{x}$观测到特定y的似然（likelihood）：$$p(y|\\mathbf{x}) &#x3D; \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{(y-\\mathbf{w}^T\\mathbf{x}-b)^2}{2\\sigma^2})$$</li>\n<li>2）根据极大似然估计，参数（$\\mathbf{w},b$）的最优值使整个数据集的似然最大：$$p(\\mathbf{y}|\\mathbf{X}) &#x3D; \\prod \\limits_{i&#x3D;1}^n p(y^{(i)}|\\mathbf{x}^{(i)})$$</li>\n<li>3）根据极大似然估计法选择的估计量称为极大似然估计量。虽然使许多指数函数的乘积最大化看起来很困难，但是我们可以在不改变目标的前提下，通过最大化似然对数来简化：$$-log P(\\mathbf{y}|\\mathbf{X}) &#x3D; \\frac{n}{2}log(2\\pi\\sigma^2)+\\frac{1}{2\\sigma^2}\\sum \\limits_{i&#x3D;1}^n(y^{(i)}-\\mathbf{w}^T\\mathbf{x}^{(i)}-b)^2$$</li>\n<li>4）假设$\\sigma$是常数，则可以忽略第一项；第二项除了常数项$\\frac{1}{\\sigma^2}$之外，就是平方损失函数。因此，在高斯噪声的假设下，最小化均方误差等价于对线性模型的极大似然估计。</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"1-4-从线性回归到深度网络\"><a href=\"#1-4-从线性回归到深度网络\" class=\"headerlink\" title=\"1.4 从线性回归到深度网络\"></a>1.4 从线性回归到深度网络</h2><h3 id=\"1-4-1-神经网络\"><a href=\"#1-4-1-神经网络\" class=\"headerlink\" title=\"1.4.1 神经网络\"></a>1.4.1 神经网络</h3><ul>\n<li>线性回归是一个单层神经网络（隐去了权重和偏置）：</li>\n<li>每个输入都与每个输出相连，成为全连接层（fully connected layer）或稠密层（dense layer）。<br><img src=\"/1_linear_regression_files/1.png\"><br><img src=\"/img/deeplearning/code/pytorch/2_linear_neural_network/1_linear_regression_files/1.png\"></li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"1-线性回归\"><a href=\"#1-线性回归\" class=\"headerlink\" title=\"1. 线性回归\"></a>1. 线性回归</h1><h2 id=\"1-1-线性回归的基本元素\"><a href=\"#1-1-线性回归的基本元素\" class=\"headerlink\" title=\"1.1 线性回归的基本元素\"></a>1.1 线性回归的基本元素</h2><ul>\n<li>基于几个简单的假设：<ul>\n<li>假设自变量x和因变量y之间的关系是线性的</li>\n<li>假设y的观测值是通过均值为0的噪声扰动后得到的</li>\n</ul>\n</li>\n<li>一些概念：<ul>\n<li>训练集：训练模型的数据集</li>\n<li>样本：每行数据</li>\n<li>标签：试图预测的值</li>\n<li>特征（feature，协变量（covariate））：预测所依赖的输入变量</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"1-1-1-线性模型\"><a href=\"#1-1-1-线性模型\" class=\"headerlink\" title=\"1.1.1 线性模型\"></a>1.1.1 线性模型</h3><p>$$\\hat y &#x3D; w_1x1+w_2x_2+b \\rightarrow \\hat y &#x3D; \\mathbf{w}^T\\mathbf{x}+b$$</p>\n<ul>\n<li>$w_1$和$w_2$是权重</li>\n<li>$b$是偏置(bias),也叫偏移量(offset),截距(intercept)：当所有特征都取值为0时，预测值应该为多少。</li>\n<li>严格来说，上述公式是输入特征的一个 仿射变换（affine transformation）：仿射变换的特点是通过加权和对特征进行线性变换（linear transformation），并通过偏置项来进行平移（translation）。</li>\n<li>对于整个数据集，特征集合是一个矩阵：$\\hat y &#x3D; \\mathbf{X}\\mathbf{w}+b$</li>\n</ul>\n<h3 id=\"1-1-2-损失函数\"><a href=\"#1-1-2-损失函数\" class=\"headerlink\" title=\"1.1.2 损失函数\"></a>1.1.2 损失函数</h3><ul>\n<li>量化目标的实际值与预测值之间的差距。</li>\n<li>回归问题中最常用的损失函数是平方误差函数:<ul>\n<li>$l^{(i)}(\\mathbf{w},b) &#x3D; \\frac{1}{2}(\\hat y^{(i)}-y^{(i)})^2$</li>\n</ul>\n</li>\n<li>训练目标：寻找一组参数（$\\mathbf{w^*},b^*$）,使得所有样本的损失函数之和最小：<br>$$\\mathbf{w^*},b^* &#x3D; L(\\mathbf{w},b) \\underset{\\mathbf{w},b}{\\arg\\min} \\frac{1}{n}\\sum_{i&#x3D;1}^n l^{(i)}(\\mathbf{w},b)$$</li>\n</ul>\n<h3 id=\"1-1-3-解析解\"><a href=\"#1-1-3-解析解\" class=\"headerlink\" title=\"1.1.3 解析解\"></a>1.1.3 解析解</h3><p>$$\\mathbf{w^*} &#x3D; (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$</p>\n<h3 id=\"1-1-4-随机梯度下降\"><a href=\"#1-1-4-随机梯度下降\" class=\"headerlink\" title=\"1.1.4 随机梯度下降\"></a>1.1.4 随机梯度下降</h3><ul>\n<li>$$(\\mathbf{w},b) \\leftarrow (\\mathbf{w},b) - \\frac{\\eta}{|\\mathcal{B}|}\\sum_{i\\in\\mathcal{B}}\\partial_{(\\mathbf{w},b)}l^{(i)}(\\mathbf{w},b)$$</li>\n<li>通过不断地在损失函数递减的方向上更新参数来降低误差。</li>\n<li>梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值）关于模型参数的导数（在这里也可以称为梯度）。但实际中的执行可能会非常慢：因为在每一次更新参数之前，我们必须遍历整个数据集。因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本$\\mathcal{B}$，并计算损失函数关于这些样本的平均损失的梯度。</li>\n<li>算法过程：<ul>\n<li>1）初始化模型参数的值，如随机初始化</li>\n<li>2）从数据集中随机抽取一小批样本且在负梯度方向上更新参数</li>\n<li>3）重复步骤2直到收敛</li>\n<li>例如对于平方损失和放射变换：<ul>\n<li>$\\mathbf{w} \\leftarrow \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|}\\sum\\limits_{i\\in\\mathcal{B}}\\mathbf{x}^{(i)}(\\mathbf{w}^T\\mathbf{x}^{(i)}+b-y^{(i)})$</li>\n<li>$b \\leftarrow b - \\frac{\\eta}{|\\mathcal{B}|}\\sum\\limits_{i\\in\\mathcal{B}}(\\mathbf{w}^T\\mathbf{x}^{(i)}+b-y^{(i)})$<ul>\n<li>$\\eta$是学习率</li>\n<li>$\\mathcal{B}$是小批量样本的大小(batch size)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>对像深度神经网络这样复杂的模型来说，损失平面上通常包含多个最小值。深度学习实践者很少会去花费大力气寻找这样一组参数，使得在训练集上的损失达到最小。事实上，更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失，这一挑战被称为泛化（generalization）。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"1-1-5-用模型进行预测\"><a href=\"#1-1-5-用模型进行预测\" class=\"headerlink\" title=\"1.1.5 用模型进行预测\"></a>1.1.5 用模型进行预测</h3><p>$$\\hat y &#x3D; \\mathbf{\\hat w}^T\\mathbf{x}+\\hat b$$</p>\n<h2 id=\"1-2-矢量化加速\"><a href=\"#1-2-矢量化加速\" class=\"headerlink\" title=\"1.2 矢量化加速\"></a>1.2 矢量化加速</h2><ul>\n<li>在训练我们的模型时，我们经常希望能够同时处理整个小批量的样本。而不是在Python中编写开销高昂的for循环。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> math</span><br><span class=\"line\"><span class=\"keyword\">import</span> time</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"><span class=\"comment\">#计时器</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Timer</span>: <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;记录多次运行时间&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        self.times = []</span><br><span class=\"line\">        self.start()</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">start</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;启动计时器&quot;&quot;&quot;</span></span><br><span class=\"line\">        self.tik = time.time()</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">stop</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;停止计时器并将时间记录在列表中&quot;&quot;&quot;</span></span><br><span class=\"line\">        self.times.append(time.time() - self.tik)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.times[-<span class=\"number\">1</span>]</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">avg</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;返回平均时间&quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"built_in\">sum</span>(self.times) / <span class=\"built_in\">len</span>(self.times)</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">sum</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;返回时间总和&quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"built_in\">sum</span>(self.times)</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">cumsum</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;返回累计时间&quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> np.array(self.times).cumsum().tolist()</span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#1 两个10000维向量</span></span><br><span class=\"line\">n = <span class=\"number\">10000</span></span><br><span class=\"line\">a = torch.ones(n)</span><br><span class=\"line\">b = torch.ones(n)</span><br><span class=\"line\">c = torch.zeros(n)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#2 for循环计算</span></span><br><span class=\"line\">timer = Timer()</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(n):</span><br><span class=\"line\">    c[i] = a[i] + b[i]</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;<span class=\"subst\">&#123;timer.stop():<span class=\"number\">.5</span>f&#125;</span> sec&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#3 一次性计算</span></span><br><span class=\"line\">timer.start()</span><br><span class=\"line\">d = a + b</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;<span class=\"subst\">&#123;timer.stop():<span class=\"number\">.7</span>f&#125;</span> sec&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>0.11051 sec\n0.0000000 sec\n</code></pre>\n<h2 id=\"1-3-正态分布与平方损失\"><a href=\"#1-3-正态分布与平方损失\" class=\"headerlink\" title=\"1.3 正态分布与平方损失\"></a>1.3 正态分布与平方损失</h2><ul>\n<li>正态分布（高斯分布）：<br>$$p(x) &#x3D; \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{(x-\\mu)^2}{2\\sigma^2})$$</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>] = <span class=\"string\">&quot;TRUE&quot;</span></span><br><span class=\"line\"><span class=\"comment\">#1 定义一个函数来计算高斯分布</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">normal</span>(<span class=\"params\">x,mu,sigma</span>):</span><br><span class=\"line\">    p = <span class=\"number\">1</span> / math.sqrt(<span class=\"number\">2</span> * math.pi * sigma**<span class=\"number\">2</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> p * np.exp(-<span class=\"number\">0.5</span> / sigma**<span class=\"number\">2</span> * (x - mu)**<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#2 可视化高斯分布</span></span><br><span class=\"line\">x = np.arange(-<span class=\"number\">7</span>,<span class=\"number\">7</span>,<span class=\"number\">0.01</span>)</span><br><span class=\"line\"><span class=\"comment\">#均值和标准差对</span></span><br><span class=\"line\">params = [(<span class=\"number\">0</span>,<span class=\"number\">1</span>),(<span class=\"number\">0</span>,<span class=\"number\">2</span>),(<span class=\"number\">3</span>,<span class=\"number\">1</span>)]</span><br><span class=\"line\">d2l.plot(x,[normal(x,mu,sigma) <span class=\"keyword\">for</span> mu,sigma <span class=\"keyword\">in</span> params],xlabel=<span class=\"string\">&#x27;x&#x27;</span>,ylabel=<span class=\"string\">&#x27;p(x)&#x27;</span>,legend=[<span class=\"string\">f&#x27;mean <span class=\"subst\">&#123;mu&#125;</span>,std <span class=\"subst\">&#123;sigma&#125;</span>&#x27;</span> <span class=\"keyword\">for</span> mu,sigma <span class=\"keyword\">in</span> params])</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/1_linear_regression_files/1_linear_regression_4_0.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/2_linear_neural_network/1_linear_regression_files/1_linear_regression_4_0.svg\"></p>\n<ul>\n<li>假设观测中包含噪声：$$y &#x3D; \\mathbf{w}^T\\mathbf{x}+b+\\epsilon$$<ul>\n<li>噪声$\\epsilon \\sim N(0,\\sigma^2)$</li>\n<li>因此有：(y服从正态分布，y的均值是$\\mathbf{w}^T\\mathbf{x}+b$，方差是$\\sigma^2$)<ul>\n<li>1）通过给定的$\\mathbf{x}$观测到特定y的似然（likelihood）：$$p(y|\\mathbf{x}) &#x3D; \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{(y-\\mathbf{w}^T\\mathbf{x}-b)^2}{2\\sigma^2})$$</li>\n<li>2）根据极大似然估计，参数（$\\mathbf{w},b$）的最优值使整个数据集的似然最大：$$p(\\mathbf{y}|\\mathbf{X}) &#x3D; \\prod \\limits_{i&#x3D;1}^n p(y^{(i)}|\\mathbf{x}^{(i)})$$</li>\n<li>3）根据极大似然估计法选择的估计量称为极大似然估计量。虽然使许多指数函数的乘积最大化看起来很困难，但是我们可以在不改变目标的前提下，通过最大化似然对数来简化：$$-log P(\\mathbf{y}|\\mathbf{X}) &#x3D; \\frac{n}{2}log(2\\pi\\sigma^2)+\\frac{1}{2\\sigma^2}\\sum \\limits_{i&#x3D;1}^n(y^{(i)}-\\mathbf{w}^T\\mathbf{x}^{(i)}-b)^2$$</li>\n<li>4）假设$\\sigma$是常数，则可以忽略第一项；第二项除了常数项$\\frac{1}{\\sigma^2}$之外，就是平方损失函数。因此，在高斯噪声的假设下，最小化均方误差等价于对线性模型的极大似然估计。</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"1-4-从线性回归到深度网络\"><a href=\"#1-4-从线性回归到深度网络\" class=\"headerlink\" title=\"1.4 从线性回归到深度网络\"></a>1.4 从线性回归到深度网络</h2><h3 id=\"1-4-1-神经网络\"><a href=\"#1-4-1-神经网络\" class=\"headerlink\" title=\"1.4.1 神经网络\"></a>1.4.1 神经网络</h3><ul>\n<li>线性回归是一个单层神经网络（隐去了权重和偏置）：</li>\n<li>每个输入都与每个输出相连，成为全连接层（fully connected layer）或稠密层（dense layer）。<br><img src=\"/1_linear_regression_files/1.png\"><br><img src=\"/img/deeplearning/code/pytorch/2_linear_neural_network/1_linear_regression_files/1.png\"></li>\n</ul>"},{"title":"2.5 图像分类数据集","date":"2024-02-04T06:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 5 图像分类数据集\n- MNIST是一个常见图像分类数据集，但过于简单。我们将使用一个更复杂的图像数据集Fashion-MNIST,10个类60000+10000张28*28的灰度图像。\n\n\n```python\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\nimport torch\nimport torchvision\nfrom torch.utils import data\nfrom torchvision import transforms\nfrom d2l import torch as d2l\nd2l.use_svg_display()\n\n```\n\n## 5.1 读取数据集\n- 我们可以通过框架中的内置函数将Fashion-MNIST下载并读取到内存中。\n\n\n```python\n#通过ToTensor实例将图像数据从PIL类型变换成32位浮点数格式,并除以255使得所有像素的数值均在0到1之间\ntrans= transforms.ToTensor()\nmnist_train = torchvision.datasets.FashionMNIST(root=\"D:/fashionmnist\", train=True, transform=trans, download=True)\nmnist_test = torchvision.datasets.FashionMNIST(root=\"D:/fashionmnist\", train=False, transform=trans, download=True)\n\nprint(len(mnist_train), len(mnist_test))\nprint(mnist_train[0][0].shape)\n\n# 数字标签->文本标签\ndef get_fashion_mnist_labels(labels): #@save\n    \"\"\"返回Fashion-MNIST数据集的文本标签。\"\"\"\n    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n    return [text_labels[int(i)] for i in labels]\n# 可视化样本\ndef show_images(imgs, num_rows, num_cols, titles=None, scale=1.5): #@save\n    \"\"\"绘制图像列表。\"\"\"\n    figsize = (num_cols * scale, num_rows * scale)\n    _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)\n    axes = axes.flatten() # 返回一个展平的数组\n    for i, (ax, img) in enumerate(zip(axes, imgs)):\n        if torch.is_tensor(img):\n            # 图片张量\n            ax.imshow(img.numpy()) #将图片放在axes的ax位置\n        else:\n            # PIL图片\n            ax.imshow(img)\n        ax.axes.get_xaxis().set_visible(False) #不显示x轴\n        ax.axes.get_yaxis().set_visible(False)\n        if titles:\n            ax.set_title(titles[i])\n    return axes\n\n# 可视化样本\nX, y = next(iter(data.DataLoader(mnist_train, batch_size=18))) #iter()返回一个迭代器\nshow_images(X.reshape(18, 28, 28), 2, 9, titles=get_fashion_mnist_labels(y));\n```\n\n    60000 10000\n    torch.Size([1, 28, 28])\n    \n\n\n    \n![svg](5_image_classification_dataset_files/5_image_classification_dataset_3_1.svg)\n![](img/deeplearning/code/pytorch/2_linear_neural_network/5_image_classification_dataset_files/5_image_classification_dataset_3_1.svg)\n    \n\n\n## 5.2 读取小批量\n- 使用内置数据迭代器，可以随机打乱所有样本\n\n\n\n```python\n# 读取小批量\nbatch_size = 256\ndef get_dataloader_workers(): #@save\n    \"\"\"使用4个进程来读取数据。\"\"\"\n    return 4\ntrain_iter = data.DataLoader(mnist_train, batch_size, shuffle=True, num_workers=get_dataloader_workers())\n\n#读取数据需要的时间\ntimer = d2l.Timer()\nfor X,y in train_iter:\n    continue\nprint(f'{timer.stop():.2f} sec')\n```\n\n    3.66 sec\n    \n\n## 5.3 整合\n\n\n\n```python\n#1 读取数据集，返回迭代器\ndef load_data_fashion_mnist(batch_size, resize=None): #@save\n    \"\"\"下载Fashion-MNIST数据集，然后将其加载到内存中\"\"\"\n    trans = [transforms.ToTensor()]\n    if resize:\n        trans.insert(0, transforms.Resize(resize))\n    trans = transforms.Compose(trans)\n    mnist_train= torchvision.datasets.FashionMNIST(root=\"D:/fashionmnist\", train=True, transform=trans, download=True)\n    mnist_test = torchvision.datasets.FashionMNIST(root=\"D:/fashionmnist\", train=False, transform=trans, download=True)\n    return (data.DataLoader(mnist_train, batch_size, shuffle=True, num_workers=get_dataloader_workers()), \n            data.DataLoader(mnist_test, batch_size, shuffle=False, num_workers=get_dataloader_workers()))\n#2 resize图像大小\ntrain_iter, test_iter = load_data_fashion_mnist(32, resize=64) #resize为64\nfor X, y in train_iter:\n    print(X.shape, X.dtype, y.shape, y.dtype)\n    break\n```\n\n    torch.Size([32, 1, 64, 64]) torch.float32 torch.Size([32]) torch.int64\n    \n","source":"_posts/deeplearning/code/pytorch/2_linear_neural_network/5_image_classification_dataset.md","raw":"---\ntitle: 2.5 图像分类数据集\ndate: 2024-2-4 14:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 5 图像分类数据集\n- MNIST是一个常见图像分类数据集，但过于简单。我们将使用一个更复杂的图像数据集Fashion-MNIST,10个类60000+10000张28*28的灰度图像。\n\n\n```python\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\nimport torch\nimport torchvision\nfrom torch.utils import data\nfrom torchvision import transforms\nfrom d2l import torch as d2l\nd2l.use_svg_display()\n\n```\n\n## 5.1 读取数据集\n- 我们可以通过框架中的内置函数将Fashion-MNIST下载并读取到内存中。\n\n\n```python\n#通过ToTensor实例将图像数据从PIL类型变换成32位浮点数格式,并除以255使得所有像素的数值均在0到1之间\ntrans= transforms.ToTensor()\nmnist_train = torchvision.datasets.FashionMNIST(root=\"D:/fashionmnist\", train=True, transform=trans, download=True)\nmnist_test = torchvision.datasets.FashionMNIST(root=\"D:/fashionmnist\", train=False, transform=trans, download=True)\n\nprint(len(mnist_train), len(mnist_test))\nprint(mnist_train[0][0].shape)\n\n# 数字标签->文本标签\ndef get_fashion_mnist_labels(labels): #@save\n    \"\"\"返回Fashion-MNIST数据集的文本标签。\"\"\"\n    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n    return [text_labels[int(i)] for i in labels]\n# 可视化样本\ndef show_images(imgs, num_rows, num_cols, titles=None, scale=1.5): #@save\n    \"\"\"绘制图像列表。\"\"\"\n    figsize = (num_cols * scale, num_rows * scale)\n    _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)\n    axes = axes.flatten() # 返回一个展平的数组\n    for i, (ax, img) in enumerate(zip(axes, imgs)):\n        if torch.is_tensor(img):\n            # 图片张量\n            ax.imshow(img.numpy()) #将图片放在axes的ax位置\n        else:\n            # PIL图片\n            ax.imshow(img)\n        ax.axes.get_xaxis().set_visible(False) #不显示x轴\n        ax.axes.get_yaxis().set_visible(False)\n        if titles:\n            ax.set_title(titles[i])\n    return axes\n\n# 可视化样本\nX, y = next(iter(data.DataLoader(mnist_train, batch_size=18))) #iter()返回一个迭代器\nshow_images(X.reshape(18, 28, 28), 2, 9, titles=get_fashion_mnist_labels(y));\n```\n\n    60000 10000\n    torch.Size([1, 28, 28])\n    \n\n\n    \n![svg](5_image_classification_dataset_files/5_image_classification_dataset_3_1.svg)\n![](img/deeplearning/code/pytorch/2_linear_neural_network/5_image_classification_dataset_files/5_image_classification_dataset_3_1.svg)\n    \n\n\n## 5.2 读取小批量\n- 使用内置数据迭代器，可以随机打乱所有样本\n\n\n\n```python\n# 读取小批量\nbatch_size = 256\ndef get_dataloader_workers(): #@save\n    \"\"\"使用4个进程来读取数据。\"\"\"\n    return 4\ntrain_iter = data.DataLoader(mnist_train, batch_size, shuffle=True, num_workers=get_dataloader_workers())\n\n#读取数据需要的时间\ntimer = d2l.Timer()\nfor X,y in train_iter:\n    continue\nprint(f'{timer.stop():.2f} sec')\n```\n\n    3.66 sec\n    \n\n## 5.3 整合\n\n\n\n```python\n#1 读取数据集，返回迭代器\ndef load_data_fashion_mnist(batch_size, resize=None): #@save\n    \"\"\"下载Fashion-MNIST数据集，然后将其加载到内存中\"\"\"\n    trans = [transforms.ToTensor()]\n    if resize:\n        trans.insert(0, transforms.Resize(resize))\n    trans = transforms.Compose(trans)\n    mnist_train= torchvision.datasets.FashionMNIST(root=\"D:/fashionmnist\", train=True, transform=trans, download=True)\n    mnist_test = torchvision.datasets.FashionMNIST(root=\"D:/fashionmnist\", train=False, transform=trans, download=True)\n    return (data.DataLoader(mnist_train, batch_size, shuffle=True, num_workers=get_dataloader_workers()), \n            data.DataLoader(mnist_test, batch_size, shuffle=False, num_workers=get_dataloader_workers()))\n#2 resize图像大小\ntrain_iter, test_iter = load_data_fashion_mnist(32, resize=64) #resize为64\nfor X, y in train_iter:\n    print(X.shape, X.dtype, y.shape, y.dtype)\n    break\n```\n\n    torch.Size([32, 1, 64, 64]) torch.float32 torch.Size([32]) torch.int64\n    \n","slug":"deeplearning/code/pytorch/2_linear_neural_network/5_image_classification_dataset","published":1,"updated":"2024-05-02T08:55:08.942Z","_id":"clvuli5r0000dzsvwf76e8bno","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"5-图像分类数据集\"><a href=\"#5-图像分类数据集\" class=\"headerlink\" title=\"5 图像分类数据集\"></a>5 图像分类数据集</h1><ul>\n<li>MNIST是一个常见图像分类数据集，但过于简单。我们将使用一个更复杂的图像数据集Fashion-MNIST,10个类60000+10000张28*28的灰度图像。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>] = <span class=\"string\">&#x27;True&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torchvision</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils <span class=\"keyword\">import</span> data</span><br><span class=\"line\"><span class=\"keyword\">from</span> torchvision <span class=\"keyword\">import</span> transforms</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\">d2l.use_svg_display()</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"5-1-读取数据集\"><a href=\"#5-1-读取数据集\" class=\"headerlink\" title=\"5.1 读取数据集\"></a>5.1 读取数据集</h2><ul>\n<li>我们可以通过框架中的内置函数将Fashion-MNIST下载并读取到内存中。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#通过ToTensor实例将图像数据从PIL类型变换成32位浮点数格式,并除以255使得所有像素的数值均在0到1之间</span></span><br><span class=\"line\">trans= transforms.ToTensor()</span><br><span class=\"line\">mnist_train = torchvision.datasets.FashionMNIST(root=<span class=\"string\">&quot;D:/fashionmnist&quot;</span>, train=<span class=\"literal\">True</span>, transform=trans, download=<span class=\"literal\">True</span>)</span><br><span class=\"line\">mnist_test = torchvision.datasets.FashionMNIST(root=<span class=\"string\">&quot;D:/fashionmnist&quot;</span>, train=<span class=\"literal\">False</span>, transform=trans, download=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">len</span>(mnist_train), <span class=\"built_in\">len</span>(mnist_test))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(mnist_train[<span class=\"number\">0</span>][<span class=\"number\">0</span>].shape)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 数字标签-&gt;文本标签</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">get_fashion_mnist_labels</span>(<span class=\"params\">labels</span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;返回Fashion-MNIST数据集的文本标签。&quot;&quot;&quot;</span></span><br><span class=\"line\">    text_labels = [<span class=\"string\">&#x27;t-shirt&#x27;</span>, <span class=\"string\">&#x27;trouser&#x27;</span>, <span class=\"string\">&#x27;pullover&#x27;</span>, <span class=\"string\">&#x27;dress&#x27;</span>, <span class=\"string\">&#x27;coat&#x27;</span>,</span><br><span class=\"line\">                   <span class=\"string\">&#x27;sandal&#x27;</span>, <span class=\"string\">&#x27;shirt&#x27;</span>, <span class=\"string\">&#x27;sneaker&#x27;</span>, <span class=\"string\">&#x27;bag&#x27;</span>, <span class=\"string\">&#x27;ankle boot&#x27;</span>]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> [text_labels[<span class=\"built_in\">int</span>(i)] <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> labels]</span><br><span class=\"line\"><span class=\"comment\"># 可视化样本</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">show_images</span>(<span class=\"params\">imgs, num_rows, num_cols, titles=<span class=\"literal\">None</span>, scale=<span class=\"number\">1.5</span></span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;绘制图像列表。&quot;&quot;&quot;</span></span><br><span class=\"line\">    figsize = (num_cols * scale, num_rows * scale)</span><br><span class=\"line\">    _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)</span><br><span class=\"line\">    axes = axes.flatten() <span class=\"comment\"># 返回一个展平的数组</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i, (ax, img) <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(<span class=\"built_in\">zip</span>(axes, imgs)):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> torch.is_tensor(img):</span><br><span class=\"line\">            <span class=\"comment\"># 图片张量</span></span><br><span class=\"line\">            ax.imshow(img.numpy()) <span class=\"comment\">#将图片放在axes的ax位置</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"comment\"># PIL图片</span></span><br><span class=\"line\">            ax.imshow(img)</span><br><span class=\"line\">        ax.axes.get_xaxis().set_visible(<span class=\"literal\">False</span>) <span class=\"comment\">#不显示x轴</span></span><br><span class=\"line\">        ax.axes.get_yaxis().set_visible(<span class=\"literal\">False</span>)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> titles:</span><br><span class=\"line\">            ax.set_title(titles[i])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> axes</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 可视化样本</span></span><br><span class=\"line\">X, y = <span class=\"built_in\">next</span>(<span class=\"built_in\">iter</span>(data.DataLoader(mnist_train, batch_size=<span class=\"number\">18</span>))) <span class=\"comment\">#iter()返回一个迭代器</span></span><br><span class=\"line\">show_images(X.reshape(<span class=\"number\">18</span>, <span class=\"number\">28</span>, <span class=\"number\">28</span>), <span class=\"number\">2</span>, <span class=\"number\">9</span>, titles=get_fashion_mnist_labels(y));</span><br></pre></td></tr></table></figure>\n\n<pre><code>60000 10000\ntorch.Size([1, 28, 28])\n</code></pre>\n<p><img src=\"/5_image_classification_dataset_files/5_image_classification_dataset_3_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/2_linear_neural_network/5_image_classification_dataset_files/5_image_classification_dataset_3_1.svg\"></p>\n<h2 id=\"5-2-读取小批量\"><a href=\"#5-2-读取小批量\" class=\"headerlink\" title=\"5.2 读取小批量\"></a>5.2 读取小批量</h2><ul>\n<li>使用内置数据迭代器，可以随机打乱所有样本</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 读取小批量</span></span><br><span class=\"line\">batch_size = <span class=\"number\">256</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">get_dataloader_workers</span>(): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;使用4个进程来读取数据。&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">4</span></span><br><span class=\"line\">train_iter = data.DataLoader(mnist_train, batch_size, shuffle=<span class=\"literal\">True</span>, num_workers=get_dataloader_workers())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#读取数据需要的时间</span></span><br><span class=\"line\">timer = d2l.Timer()</span><br><span class=\"line\"><span class=\"keyword\">for</span> X,y <span class=\"keyword\">in</span> train_iter:</span><br><span class=\"line\">    <span class=\"keyword\">continue</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;<span class=\"subst\">&#123;timer.stop():<span class=\"number\">.2</span>f&#125;</span> sec&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>3.66 sec\n</code></pre>\n<h2 id=\"5-3-整合\"><a href=\"#5-3-整合\" class=\"headerlink\" title=\"5.3 整合\"></a>5.3 整合</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#1 读取数据集，返回迭代器</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">load_data_fashion_mnist</span>(<span class=\"params\">batch_size, resize=<span class=\"literal\">None</span></span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;下载Fashion-MNIST数据集，然后将其加载到内存中&quot;&quot;&quot;</span></span><br><span class=\"line\">    trans = [transforms.ToTensor()]</span><br><span class=\"line\">    <span class=\"keyword\">if</span> resize:</span><br><span class=\"line\">        trans.insert(<span class=\"number\">0</span>, transforms.Resize(resize))</span><br><span class=\"line\">    trans = transforms.Compose(trans)</span><br><span class=\"line\">    mnist_train= torchvision.datasets.FashionMNIST(root=<span class=\"string\">&quot;D:/fashionmnist&quot;</span>, train=<span class=\"literal\">True</span>, transform=trans, download=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    mnist_test = torchvision.datasets.FashionMNIST(root=<span class=\"string\">&quot;D:/fashionmnist&quot;</span>, train=<span class=\"literal\">False</span>, transform=trans, download=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> (data.DataLoader(mnist_train, batch_size, shuffle=<span class=\"literal\">True</span>, num_workers=get_dataloader_workers()), </span><br><span class=\"line\">            data.DataLoader(mnist_test, batch_size, shuffle=<span class=\"literal\">False</span>, num_workers=get_dataloader_workers()))</span><br><span class=\"line\"><span class=\"comment\">#2 resize图像大小</span></span><br><span class=\"line\">train_iter, test_iter = load_data_fashion_mnist(<span class=\"number\">32</span>, resize=<span class=\"number\">64</span>) <span class=\"comment\">#resize为64</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> train_iter:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(X.shape, X.dtype, y.shape, y.dtype)</span><br><span class=\"line\">    <span class=\"keyword\">break</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>torch.Size([32, 1, 64, 64]) torch.float32 torch.Size([32]) torch.int64\n</code></pre>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"5-图像分类数据集\"><a href=\"#5-图像分类数据集\" class=\"headerlink\" title=\"5 图像分类数据集\"></a>5 图像分类数据集</h1><ul>\n<li>MNIST是一个常见图像分类数据集，但过于简单。我们将使用一个更复杂的图像数据集Fashion-MNIST,10个类60000+10000张28*28的灰度图像。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>] = <span class=\"string\">&#x27;True&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torchvision</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils <span class=\"keyword\">import</span> data</span><br><span class=\"line\"><span class=\"keyword\">from</span> torchvision <span class=\"keyword\">import</span> transforms</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\">d2l.use_svg_display()</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"5-1-读取数据集\"><a href=\"#5-1-读取数据集\" class=\"headerlink\" title=\"5.1 读取数据集\"></a>5.1 读取数据集</h2><ul>\n<li>我们可以通过框架中的内置函数将Fashion-MNIST下载并读取到内存中。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#通过ToTensor实例将图像数据从PIL类型变换成32位浮点数格式,并除以255使得所有像素的数值均在0到1之间</span></span><br><span class=\"line\">trans= transforms.ToTensor()</span><br><span class=\"line\">mnist_train = torchvision.datasets.FashionMNIST(root=<span class=\"string\">&quot;D:/fashionmnist&quot;</span>, train=<span class=\"literal\">True</span>, transform=trans, download=<span class=\"literal\">True</span>)</span><br><span class=\"line\">mnist_test = torchvision.datasets.FashionMNIST(root=<span class=\"string\">&quot;D:/fashionmnist&quot;</span>, train=<span class=\"literal\">False</span>, transform=trans, download=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">len</span>(mnist_train), <span class=\"built_in\">len</span>(mnist_test))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(mnist_train[<span class=\"number\">0</span>][<span class=\"number\">0</span>].shape)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 数字标签-&gt;文本标签</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">get_fashion_mnist_labels</span>(<span class=\"params\">labels</span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;返回Fashion-MNIST数据集的文本标签。&quot;&quot;&quot;</span></span><br><span class=\"line\">    text_labels = [<span class=\"string\">&#x27;t-shirt&#x27;</span>, <span class=\"string\">&#x27;trouser&#x27;</span>, <span class=\"string\">&#x27;pullover&#x27;</span>, <span class=\"string\">&#x27;dress&#x27;</span>, <span class=\"string\">&#x27;coat&#x27;</span>,</span><br><span class=\"line\">                   <span class=\"string\">&#x27;sandal&#x27;</span>, <span class=\"string\">&#x27;shirt&#x27;</span>, <span class=\"string\">&#x27;sneaker&#x27;</span>, <span class=\"string\">&#x27;bag&#x27;</span>, <span class=\"string\">&#x27;ankle boot&#x27;</span>]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> [text_labels[<span class=\"built_in\">int</span>(i)] <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> labels]</span><br><span class=\"line\"><span class=\"comment\"># 可视化样本</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">show_images</span>(<span class=\"params\">imgs, num_rows, num_cols, titles=<span class=\"literal\">None</span>, scale=<span class=\"number\">1.5</span></span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;绘制图像列表。&quot;&quot;&quot;</span></span><br><span class=\"line\">    figsize = (num_cols * scale, num_rows * scale)</span><br><span class=\"line\">    _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)</span><br><span class=\"line\">    axes = axes.flatten() <span class=\"comment\"># 返回一个展平的数组</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i, (ax, img) <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(<span class=\"built_in\">zip</span>(axes, imgs)):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> torch.is_tensor(img):</span><br><span class=\"line\">            <span class=\"comment\"># 图片张量</span></span><br><span class=\"line\">            ax.imshow(img.numpy()) <span class=\"comment\">#将图片放在axes的ax位置</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"comment\"># PIL图片</span></span><br><span class=\"line\">            ax.imshow(img)</span><br><span class=\"line\">        ax.axes.get_xaxis().set_visible(<span class=\"literal\">False</span>) <span class=\"comment\">#不显示x轴</span></span><br><span class=\"line\">        ax.axes.get_yaxis().set_visible(<span class=\"literal\">False</span>)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> titles:</span><br><span class=\"line\">            ax.set_title(titles[i])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> axes</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 可视化样本</span></span><br><span class=\"line\">X, y = <span class=\"built_in\">next</span>(<span class=\"built_in\">iter</span>(data.DataLoader(mnist_train, batch_size=<span class=\"number\">18</span>))) <span class=\"comment\">#iter()返回一个迭代器</span></span><br><span class=\"line\">show_images(X.reshape(<span class=\"number\">18</span>, <span class=\"number\">28</span>, <span class=\"number\">28</span>), <span class=\"number\">2</span>, <span class=\"number\">9</span>, titles=get_fashion_mnist_labels(y));</span><br></pre></td></tr></table></figure>\n\n<pre><code>60000 10000\ntorch.Size([1, 28, 28])\n</code></pre>\n<p><img src=\"/5_image_classification_dataset_files/5_image_classification_dataset_3_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/2_linear_neural_network/5_image_classification_dataset_files/5_image_classification_dataset_3_1.svg\"></p>\n<h2 id=\"5-2-读取小批量\"><a href=\"#5-2-读取小批量\" class=\"headerlink\" title=\"5.2 读取小批量\"></a>5.2 读取小批量</h2><ul>\n<li>使用内置数据迭代器，可以随机打乱所有样本</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 读取小批量</span></span><br><span class=\"line\">batch_size = <span class=\"number\">256</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">get_dataloader_workers</span>(): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;使用4个进程来读取数据。&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">4</span></span><br><span class=\"line\">train_iter = data.DataLoader(mnist_train, batch_size, shuffle=<span class=\"literal\">True</span>, num_workers=get_dataloader_workers())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#读取数据需要的时间</span></span><br><span class=\"line\">timer = d2l.Timer()</span><br><span class=\"line\"><span class=\"keyword\">for</span> X,y <span class=\"keyword\">in</span> train_iter:</span><br><span class=\"line\">    <span class=\"keyword\">continue</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;<span class=\"subst\">&#123;timer.stop():<span class=\"number\">.2</span>f&#125;</span> sec&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>3.66 sec\n</code></pre>\n<h2 id=\"5-3-整合\"><a href=\"#5-3-整合\" class=\"headerlink\" title=\"5.3 整合\"></a>5.3 整合</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#1 读取数据集，返回迭代器</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">load_data_fashion_mnist</span>(<span class=\"params\">batch_size, resize=<span class=\"literal\">None</span></span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;下载Fashion-MNIST数据集，然后将其加载到内存中&quot;&quot;&quot;</span></span><br><span class=\"line\">    trans = [transforms.ToTensor()]</span><br><span class=\"line\">    <span class=\"keyword\">if</span> resize:</span><br><span class=\"line\">        trans.insert(<span class=\"number\">0</span>, transforms.Resize(resize))</span><br><span class=\"line\">    trans = transforms.Compose(trans)</span><br><span class=\"line\">    mnist_train= torchvision.datasets.FashionMNIST(root=<span class=\"string\">&quot;D:/fashionmnist&quot;</span>, train=<span class=\"literal\">True</span>, transform=trans, download=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    mnist_test = torchvision.datasets.FashionMNIST(root=<span class=\"string\">&quot;D:/fashionmnist&quot;</span>, train=<span class=\"literal\">False</span>, transform=trans, download=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> (data.DataLoader(mnist_train, batch_size, shuffle=<span class=\"literal\">True</span>, num_workers=get_dataloader_workers()), </span><br><span class=\"line\">            data.DataLoader(mnist_test, batch_size, shuffle=<span class=\"literal\">False</span>, num_workers=get_dataloader_workers()))</span><br><span class=\"line\"><span class=\"comment\">#2 resize图像大小</span></span><br><span class=\"line\">train_iter, test_iter = load_data_fashion_mnist(<span class=\"number\">32</span>, resize=<span class=\"number\">64</span>) <span class=\"comment\">#resize为64</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> train_iter:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(X.shape, X.dtype, y.shape, y.dtype)</span><br><span class=\"line\">    <span class=\"keyword\">break</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>torch.Size([32, 1, 64, 64]) torch.float32 torch.Size([32]) torch.int64\n</code></pre>"},{"title":"2.3 线性回归简洁实现","date":"2024-02-03T06:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 3 线性回归简洁实现\n\n\n```python\nimport numpy as np\nimport torch\nfrom torch.utils import data\nfrom d2l import torch as d2l\n\n#1 生成数据集\ntrue_w = torch.tensor([2, -3.4])\ntrue_b = 4.2\nfeatures, labels = d2l.synthetic_data(true_w, true_b, 1000) #生成方式为y = Xw + b + e，e为噪声默认为服从N(0,1)的正态分布\n\n\n#2 读取数据集\ndef load_array(data_arrays, batch_size, is_train=True):  #@save\n    \"\"\"构造一个PyTorch数据迭代器。\"\"\"\n    dataset = data.TensorDataset(*data_arrays)\n    return data.DataLoader(dataset, batch_size, shuffle=is_train)\nbatch_size = 10\ndata_iter = load_array((features, labels), batch_size)\nprint(next(iter(data_iter)))\n\n#3 定义模型\nfrom torch import nn\nnet = nn.Sequential(nn.Linear(2, 1))\n\n#4 初始化模型参数\nnet[0].weight.data.normal_(0, 0.01) #将第一层的权重初始化为均值为0，标准差为0.01的正态分布\nnet[0].bias.data.fill_(0) #将偏置初始化为0\n\n#5 定义损失函数\nloss = nn.MSELoss() #均方误差损失函数\n\n#6 定义优化算法\ntrainer = torch.optim.SGD(net.parameters(), lr=0.03)\n```\n\n    [tensor([[ 0.9365, -0.9840],\n            [ 0.9369,  1.5472],\n            [ 0.1462,  0.3326],\n            [-0.1900,  0.8358],\n            [ 1.1384, -1.4987],\n            [-0.4850, -0.8085],\n            [-2.1479, -0.8616],\n            [ 0.9232,  0.0479],\n            [ 0.3618, -0.7924],\n            [-1.2253,  0.1714]]), tensor([[ 9.4372],\n            [ 0.8145],\n            [ 3.3749],\n            [ 0.9582],\n            [11.5770],\n            [ 5.9775],\n            [ 2.8450],\n            [ 5.8905],\n            [ 7.6133],\n            [ 1.1836]])]\n    \n\n## 2.7 训练\n- 重复以下训练，知道完成：\n    - net(x)生成预测并计算损失l（正向传播）\n    - backward()计算梯度（反向传播）\n    - 优化器更新模型参数\n\n\n```python\n#1 超参数\nnum_epochs = 3\n\n#2 训练\nfor epoch in range(num_epochs):\n    for X,y in data_iter:\n        l = loss(net(X),y) #l是小批量X和y的损失\n        trainer.zero_grad()\n        l.backward()\n        trainer.step() #更新模型参数\n    l = loss(net(features),labels) #整个数据集的损失\n    print(f'epoch {epoch + 1}, loss {l:f}')\n\n#3 检验\nw = net[0].weight.data\nprint('w的估计误差：', true_w - w.reshape(true_w.shape))\nb = net[0].bias.data\nprint('b的估计误差：',true_b-b)\n```\n\n    epoch 1, loss 0.000270\n    epoch 2, loss 0.000103\n    epoch 3, loss 0.000103\n    w的估计误差： tensor([-0.0003, -0.0005])\n    b的估计误差： tensor([0.0007])\n    \n","source":"_posts/deeplearning/code/pytorch/2_linear_neural_network/3_linear_regression_simplerealize.md","raw":"---\ntitle: 2.3 线性回归简洁实现\ndate: 2024-2-3 14:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 3 线性回归简洁实现\n\n\n```python\nimport numpy as np\nimport torch\nfrom torch.utils import data\nfrom d2l import torch as d2l\n\n#1 生成数据集\ntrue_w = torch.tensor([2, -3.4])\ntrue_b = 4.2\nfeatures, labels = d2l.synthetic_data(true_w, true_b, 1000) #生成方式为y = Xw + b + e，e为噪声默认为服从N(0,1)的正态分布\n\n\n#2 读取数据集\ndef load_array(data_arrays, batch_size, is_train=True):  #@save\n    \"\"\"构造一个PyTorch数据迭代器。\"\"\"\n    dataset = data.TensorDataset(*data_arrays)\n    return data.DataLoader(dataset, batch_size, shuffle=is_train)\nbatch_size = 10\ndata_iter = load_array((features, labels), batch_size)\nprint(next(iter(data_iter)))\n\n#3 定义模型\nfrom torch import nn\nnet = nn.Sequential(nn.Linear(2, 1))\n\n#4 初始化模型参数\nnet[0].weight.data.normal_(0, 0.01) #将第一层的权重初始化为均值为0，标准差为0.01的正态分布\nnet[0].bias.data.fill_(0) #将偏置初始化为0\n\n#5 定义损失函数\nloss = nn.MSELoss() #均方误差损失函数\n\n#6 定义优化算法\ntrainer = torch.optim.SGD(net.parameters(), lr=0.03)\n```\n\n    [tensor([[ 0.9365, -0.9840],\n            [ 0.9369,  1.5472],\n            [ 0.1462,  0.3326],\n            [-0.1900,  0.8358],\n            [ 1.1384, -1.4987],\n            [-0.4850, -0.8085],\n            [-2.1479, -0.8616],\n            [ 0.9232,  0.0479],\n            [ 0.3618, -0.7924],\n            [-1.2253,  0.1714]]), tensor([[ 9.4372],\n            [ 0.8145],\n            [ 3.3749],\n            [ 0.9582],\n            [11.5770],\n            [ 5.9775],\n            [ 2.8450],\n            [ 5.8905],\n            [ 7.6133],\n            [ 1.1836]])]\n    \n\n## 2.7 训练\n- 重复以下训练，知道完成：\n    - net(x)生成预测并计算损失l（正向传播）\n    - backward()计算梯度（反向传播）\n    - 优化器更新模型参数\n\n\n```python\n#1 超参数\nnum_epochs = 3\n\n#2 训练\nfor epoch in range(num_epochs):\n    for X,y in data_iter:\n        l = loss(net(X),y) #l是小批量X和y的损失\n        trainer.zero_grad()\n        l.backward()\n        trainer.step() #更新模型参数\n    l = loss(net(features),labels) #整个数据集的损失\n    print(f'epoch {epoch + 1}, loss {l:f}')\n\n#3 检验\nw = net[0].weight.data\nprint('w的估计误差：', true_w - w.reshape(true_w.shape))\nb = net[0].bias.data\nprint('b的估计误差：',true_b-b)\n```\n\n    epoch 1, loss 0.000270\n    epoch 2, loss 0.000103\n    epoch 3, loss 0.000103\n    w的估计误差： tensor([-0.0003, -0.0005])\n    b的估计误差： tensor([0.0007])\n    \n","slug":"deeplearning/code/pytorch/2_linear_neural_network/3_linear_regression_simplerealize","published":1,"updated":"2024-05-02T08:54:50.462Z","_id":"clvuli5r0000fzsvwbe0hgd80","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"3-线性回归简洁实现\"><a href=\"#3-线性回归简洁实现\" class=\"headerlink\" title=\"3 线性回归简洁实现\"></a>3 线性回归简洁实现</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils <span class=\"keyword\">import</span> data</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#1 生成数据集</span></span><br><span class=\"line\">true_w = torch.tensor([<span class=\"number\">2</span>, -<span class=\"number\">3.4</span>])</span><br><span class=\"line\">true_b = <span class=\"number\">4.2</span></span><br><span class=\"line\">features, labels = d2l.synthetic_data(true_w, true_b, <span class=\"number\">1000</span>) <span class=\"comment\">#生成方式为y = Xw + b + e，e为噪声默认为服从N(0,1)的正态分布</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#2 读取数据集</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">load_array</span>(<span class=\"params\">data_arrays, batch_size, is_train=<span class=\"literal\">True</span></span>):  <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;构造一个PyTorch数据迭代器。&quot;&quot;&quot;</span></span><br><span class=\"line\">    dataset = data.TensorDataset(*data_arrays)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> data.DataLoader(dataset, batch_size, shuffle=is_train)</span><br><span class=\"line\">batch_size = <span class=\"number\">10</span></span><br><span class=\"line\">data_iter = load_array((features, labels), batch_size)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">next</span>(<span class=\"built_in\">iter</span>(data_iter)))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#3 定义模型</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\">net = nn.Sequential(nn.Linear(<span class=\"number\">2</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#4 初始化模型参数</span></span><br><span class=\"line\">net[<span class=\"number\">0</span>].weight.data.normal_(<span class=\"number\">0</span>, <span class=\"number\">0.01</span>) <span class=\"comment\">#将第一层的权重初始化为均值为0，标准差为0.01的正态分布</span></span><br><span class=\"line\">net[<span class=\"number\">0</span>].bias.data.fill_(<span class=\"number\">0</span>) <span class=\"comment\">#将偏置初始化为0</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#5 定义损失函数</span></span><br><span class=\"line\">loss = nn.MSELoss() <span class=\"comment\">#均方误差损失函数</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#6 定义优化算法</span></span><br><span class=\"line\">trainer = torch.optim.SGD(net.parameters(), lr=<span class=\"number\">0.03</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>[tensor([[ 0.9365, -0.9840],\n        [ 0.9369,  1.5472],\n        [ 0.1462,  0.3326],\n        [-0.1900,  0.8358],\n        [ 1.1384, -1.4987],\n        [-0.4850, -0.8085],\n        [-2.1479, -0.8616],\n        [ 0.9232,  0.0479],\n        [ 0.3618, -0.7924],\n        [-1.2253,  0.1714]]), tensor([[ 9.4372],\n        [ 0.8145],\n        [ 3.3749],\n        [ 0.9582],\n        [11.5770],\n        [ 5.9775],\n        [ 2.8450],\n        [ 5.8905],\n        [ 7.6133],\n        [ 1.1836]])]\n</code></pre>\n<h2 id=\"2-7-训练\"><a href=\"#2-7-训练\" class=\"headerlink\" title=\"2.7 训练\"></a>2.7 训练</h2><ul>\n<li>重复以下训练，知道完成：<ul>\n<li>net(x)生成预测并计算损失l（正向传播）</li>\n<li>backward()计算梯度（反向传播）</li>\n<li>优化器更新模型参数</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#1 超参数</span></span><br><span class=\"line\">num_epochs = <span class=\"number\">3</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#2 训练</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">    <span class=\"keyword\">for</span> X,y <span class=\"keyword\">in</span> data_iter:</span><br><span class=\"line\">        l = loss(net(X),y) <span class=\"comment\">#l是小批量X和y的损失</span></span><br><span class=\"line\">        trainer.zero_grad()</span><br><span class=\"line\">        l.backward()</span><br><span class=\"line\">        trainer.step() <span class=\"comment\">#更新模型参数</span></span><br><span class=\"line\">    l = loss(net(features),labels) <span class=\"comment\">#整个数据集的损失</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;epoch <span class=\"subst\">&#123;epoch + <span class=\"number\">1</span>&#125;</span>, loss <span class=\"subst\">&#123;l:f&#125;</span>&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#3 检验</span></span><br><span class=\"line\">w = net[<span class=\"number\">0</span>].weight.data</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;w的估计误差：&#x27;</span>, true_w - w.reshape(true_w.shape))</span><br><span class=\"line\">b = net[<span class=\"number\">0</span>].bias.data</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;b的估计误差：&#x27;</span>,true_b-b)</span><br></pre></td></tr></table></figure>\n\n<pre><code>epoch 1, loss 0.000270\nepoch 2, loss 0.000103\nepoch 3, loss 0.000103\nw的估计误差： tensor([-0.0003, -0.0005])\nb的估计误差： tensor([0.0007])\n</code></pre>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"3-线性回归简洁实现\"><a href=\"#3-线性回归简洁实现\" class=\"headerlink\" title=\"3 线性回归简洁实现\"></a>3 线性回归简洁实现</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils <span class=\"keyword\">import</span> data</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#1 生成数据集</span></span><br><span class=\"line\">true_w = torch.tensor([<span class=\"number\">2</span>, -<span class=\"number\">3.4</span>])</span><br><span class=\"line\">true_b = <span class=\"number\">4.2</span></span><br><span class=\"line\">features, labels = d2l.synthetic_data(true_w, true_b, <span class=\"number\">1000</span>) <span class=\"comment\">#生成方式为y = Xw + b + e，e为噪声默认为服从N(0,1)的正态分布</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#2 读取数据集</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">load_array</span>(<span class=\"params\">data_arrays, batch_size, is_train=<span class=\"literal\">True</span></span>):  <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;构造一个PyTorch数据迭代器。&quot;&quot;&quot;</span></span><br><span class=\"line\">    dataset = data.TensorDataset(*data_arrays)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> data.DataLoader(dataset, batch_size, shuffle=is_train)</span><br><span class=\"line\">batch_size = <span class=\"number\">10</span></span><br><span class=\"line\">data_iter = load_array((features, labels), batch_size)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">next</span>(<span class=\"built_in\">iter</span>(data_iter)))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#3 定义模型</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\">net = nn.Sequential(nn.Linear(<span class=\"number\">2</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#4 初始化模型参数</span></span><br><span class=\"line\">net[<span class=\"number\">0</span>].weight.data.normal_(<span class=\"number\">0</span>, <span class=\"number\">0.01</span>) <span class=\"comment\">#将第一层的权重初始化为均值为0，标准差为0.01的正态分布</span></span><br><span class=\"line\">net[<span class=\"number\">0</span>].bias.data.fill_(<span class=\"number\">0</span>) <span class=\"comment\">#将偏置初始化为0</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#5 定义损失函数</span></span><br><span class=\"line\">loss = nn.MSELoss() <span class=\"comment\">#均方误差损失函数</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#6 定义优化算法</span></span><br><span class=\"line\">trainer = torch.optim.SGD(net.parameters(), lr=<span class=\"number\">0.03</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>[tensor([[ 0.9365, -0.9840],\n        [ 0.9369,  1.5472],\n        [ 0.1462,  0.3326],\n        [-0.1900,  0.8358],\n        [ 1.1384, -1.4987],\n        [-0.4850, -0.8085],\n        [-2.1479, -0.8616],\n        [ 0.9232,  0.0479],\n        [ 0.3618, -0.7924],\n        [-1.2253,  0.1714]]), tensor([[ 9.4372],\n        [ 0.8145],\n        [ 3.3749],\n        [ 0.9582],\n        [11.5770],\n        [ 5.9775],\n        [ 2.8450],\n        [ 5.8905],\n        [ 7.6133],\n        [ 1.1836]])]\n</code></pre>\n<h2 id=\"2-7-训练\"><a href=\"#2-7-训练\" class=\"headerlink\" title=\"2.7 训练\"></a>2.7 训练</h2><ul>\n<li>重复以下训练，知道完成：<ul>\n<li>net(x)生成预测并计算损失l（正向传播）</li>\n<li>backward()计算梯度（反向传播）</li>\n<li>优化器更新模型参数</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#1 超参数</span></span><br><span class=\"line\">num_epochs = <span class=\"number\">3</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#2 训练</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">    <span class=\"keyword\">for</span> X,y <span class=\"keyword\">in</span> data_iter:</span><br><span class=\"line\">        l = loss(net(X),y) <span class=\"comment\">#l是小批量X和y的损失</span></span><br><span class=\"line\">        trainer.zero_grad()</span><br><span class=\"line\">        l.backward()</span><br><span class=\"line\">        trainer.step() <span class=\"comment\">#更新模型参数</span></span><br><span class=\"line\">    l = loss(net(features),labels) <span class=\"comment\">#整个数据集的损失</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;epoch <span class=\"subst\">&#123;epoch + <span class=\"number\">1</span>&#125;</span>, loss <span class=\"subst\">&#123;l:f&#125;</span>&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#3 检验</span></span><br><span class=\"line\">w = net[<span class=\"number\">0</span>].weight.data</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;w的估计误差：&#x27;</span>, true_w - w.reshape(true_w.shape))</span><br><span class=\"line\">b = net[<span class=\"number\">0</span>].bias.data</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;b的估计误差：&#x27;</span>,true_b-b)</span><br></pre></td></tr></table></figure>\n\n<pre><code>epoch 1, loss 0.000270\nepoch 2, loss 0.000103\nepoch 3, loss 0.000103\nw的估计误差： tensor([-0.0003, -0.0005])\nb的估计误差： tensor([0.0007])\n</code></pre>"},{"title":"2.4 softmax回归","date":"2024-02-03T06:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 4 softmax回归\n## 4.1 分类问题\n- 独热编码（one-hot encoding）：$y\\in{(1,0,0),(0,1,0),(0,0,1)}$\n## 4.2 网络架构\n- 为了解决线性模型的分类问题，需要和输出一样多的仿射函数（affine function）。$$o_1=x_1w_{11}+x_2w_{12}+x_3w_{13}+x_4w_{14}+b1$$ $$o_1=x_1w_{21}+x_2w_{22}+x_3w_{23}+x_4w_{24}+b2$$ $$o_1=x_1w_{31}+x_2w_{32}+x_3w_{33}+x_4w_{34}+b3$$\n\n![$ \\mathfb{o}= \\mathfb{W} \\mathfb{x}+ \\mathfb{b}$](4img/1.png)\n![$ \\mathfb{o} = \\mathfb{W} \\mathfb{x} + \\mathfb{b}$](img/deeplearning/code/pytorch/2_linear_neural_network/4img/1.png)\n## 4.3 softmax全连接层的参数开销\n- $d$个输入和$q$个输出的全连接层，参数开销为$O(dq)$\n- 可以将成本减少到$O(\\frac{dq}{n})$,n可以灵活指定\n## 4.4 softmax运算\n- softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持可导的性质。\n$$\\hat{y}_ j= \\frac{exp(o_ j)}{ \\sum_ {i=1}^ qexp(o_ i)}$$\n- 尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。因此，softmax回归是一个线性模型（linear model）。\n## 4.5 小批量样本的矢量化\n- 批量：$\\mathbf{X}$；特征维度：$d$；批量大小：$n$；类别数：$q$；\n\n\n- 批量样本特征：$\\mathbf{X}\\in\\mathbb{R}^{n\\times d}$；权重：$\\mathbf{W}\\in\\mathbb{R}^{d\\times q}$；偏置：$\\mathbf{b}\\in\\mathbb{R}^{1\\times q}$ $$\\mathbf{O}=\\mathbf{XW}+\\mathbf{b}$$ $$\\hat{\\mathbf{Y}}=\\text{softmax}(\\mathbf{O})$$\n## 4.6 损失函数\n- 使用极大似然估计\n### 4.6.1 对数似然\n- softmax函数输出向量$\\hat{\\mathbf{Y}}$：x属于各个类别的概率分布（$\\hat{y}_1=P(y=猫|x),\\hat{y}_2=P(y=狗|x),\\hat{y}_3=P(y=鸡|x)$）\n- 1）根据最大似然估计，需要最大化观测数据的联合概率。$$P(\\mathbf{Y}|\\mathbf{X})=\\prod_{i=1}^nP(\\mathbf{y}^{(i)}|\\mathbf{x}^{(i)})$$\n    - $\\mathbf{y}^{(i)}|\\mathbf{x}^{(i)}$：对于样本i，特征向量为：$\\mathbf{x}^{(i)}$，标签向量为：$\\mathbf{y}^{(i)}$\n    - $y_j^{(i)}$：样本i的标签向量中属于类别j的概率\n- 2）相当于最小化负对数似然(损失函数)：$$-\\log P(\\mathbf{Y}|\\mathbf{X})=-\\sum_{i=1}^n\\log P(y^{(i)}|\\mathbf{x}^{(i)})=\\sum_{i=1}^nl(\\mathbf{y}^{(i)},\\mathbf{\\hat{y}}^{(i)})$$\n    - 损失函数为（交叉熵损失 cross-entropy loss）：$$l(\\mathbf{y},\\mathbf{\\hat{y}})=-\\sum_{j=1}^qy_j\\log\\hat{y}_j$$\n\n        由于y是一个长度为q的独热编码向量，所以除了一个项以外的所有项j都消失了。由于所有yˆj都是预测的概率，所以它们的对数永远不会大于0。因此，如果正确地预测实际标签，即如果实际标签P(y | x) = 1，则损失函数不能进一步最小化。注意，这往往是不可能的。例如，数据集中可能存在标签噪声（比如某些样本可能被误标），或输入特征没有足够的信息来完美地对每一个样本分类。？？？\n\n\n\n### 4.6.2 softmax及其导数\n- 对损失函数：$$ \\begin{aligned} l( \\mathbf{y}, \\mathbf{ \\hat{y}}) &= - \\sum_ {j=1}^ qy_ j \\log \\hat{y}_ j \\\\ &= - \\sum_ {j=1}^ qy_ j \\log \\frac{ \\exp(o_ j)}{ \\sum_ {i=1}^ q \\exp(o_ i)} \\\\ &= - \\sum_ {j=1}^ q(y_ j(o_ j- \\log \\sum_ {i=1}^ q \\exp(o_ i))) \\\\ &= \\sum_ {j=1}^ qy_ j \\log \\sum_ {i=1}^ q \\exp(o_ i)- \\sum_ {j=1}^ qy_ jo_ j \\\\ &= \\log \\sum_ {i=1}^ q \\exp(o_ i)- \\sum_ {j=1}^ qy_ jo_ j \\end{aligned}$$\n- 损失函数对$o_j$的导数(log以e为底)：$$\\frac{\\partial l(\\mathbf{y},\\mathbf{\\hat{y}})}{\\partial o_j}=\\frac{\\exp(o_j)}{\\sum_{i=1}^q\\exp(o_i)}-y_j=softmax(o)_j-y_j$$\n    - **这与我们在回归中看到的非常相似，其中梯度是观测值y和估计值yˆ之间的差异。这不是巧合，在任何指数族分布模型中对数似然的梯度正是由此得出的。这使梯度计算在实践中变得容易很多。**\n\n### 4.6.3 交叉熵损失\n- 对于标签$\\mathbf{y}$，我们可以使用与以前相同的表示形式。唯一的区别是，我们现在用一个概率向量表示，如(0.1, 0.2, 0.7)，而不是仅包含二元项的向量(0, 0, 1)。\n- 交叉熵损失（分类问题最常用）：$$l(\\mathbf{y},\\mathbf{\\hat{y}})=-\\sum_{j=1}^qy_j\\log\\hat{y}_j$$\n\n## 4.7 信息论基础\n- 信息论（information theory）涉及编码、解码、发送以及尽可能简洁地处理信息或数据。\n### 4.7.1 熵(entropy)\n$$H[P]=-\\sum_{j}-P(j)\\log P(j)$$\n- 量化数据中的信息内容\n- 为了对从分布$p$中取样的数据进行编码，至少需要$H[p]$个纳特（nat）对其进行编码。（纳特相当于比特，底数是$e$，1nat= $\\frac{1}{\\log(2)}$ bit）\n### 4.7.2 信息量\n- 压缩与预测：如果我们很容易预测下一个数据，那么这个数据就很容易压缩。\n- 如果我们不能完全预测每一个事件，那么我们会感到“惊异”。克劳德·香农决定用信息量$-\\log P(j)$来量化这种惊异度。在观察一个事件j时，并赋予它（主观）概率 $P(j)$。当我们赋予一个事件较低的概率时，我们的惊异会更大，该事件的信息量也就更大。\n- 熵：当分配的概率真正匹配数据生成过程时的信息量的期望。\n\n### 4.7.3 交叉熵\n- 如果把熵$H(p)$当作“知道真实概率的人所经历的惊异程度”，那么交叉熵从P到Q，记为H(P, Q)。我们可以把交叉熵想象为“主观概率为Q的观察者在看到根据概率P生成的数据时的预期惊异”。当P = Q时，交叉熵达到最低。在这种情况下，从P到Q的交叉熵是H(P, P) = H(P)。\n- 可以从两方面来考虑交叉熵分类目标：（i）最大化观测数据的似然；（ii）最小化传达标签所需的惊异。\n\n## 4.8 模型预测和评估\n- 使用预测概率最高的类别作为输出类别。如果预测与实际类别（标签）一致，则预测是正确的。\n- 用精度（accuracy）来评估模型的性能。精度=正确个数/总个数\n","source":"_posts/deeplearning/code/pytorch/2_linear_neural_network/4_softmax_regression.md","raw":"---\ntitle: 2.4 softmax回归\ndate: 2024-2-3 14:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 4 softmax回归\n## 4.1 分类问题\n- 独热编码（one-hot encoding）：$y\\in{(1,0,0),(0,1,0),(0,0,1)}$\n## 4.2 网络架构\n- 为了解决线性模型的分类问题，需要和输出一样多的仿射函数（affine function）。$$o_1=x_1w_{11}+x_2w_{12}+x_3w_{13}+x_4w_{14}+b1$$ $$o_1=x_1w_{21}+x_2w_{22}+x_3w_{23}+x_4w_{24}+b2$$ $$o_1=x_1w_{31}+x_2w_{32}+x_3w_{33}+x_4w_{34}+b3$$\n\n![$ \\mathfb{o}= \\mathfb{W} \\mathfb{x}+ \\mathfb{b}$](4img/1.png)\n![$ \\mathfb{o} = \\mathfb{W} \\mathfb{x} + \\mathfb{b}$](img/deeplearning/code/pytorch/2_linear_neural_network/4img/1.png)\n## 4.3 softmax全连接层的参数开销\n- $d$个输入和$q$个输出的全连接层，参数开销为$O(dq)$\n- 可以将成本减少到$O(\\frac{dq}{n})$,n可以灵活指定\n## 4.4 softmax运算\n- softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持可导的性质。\n$$\\hat{y}_ j= \\frac{exp(o_ j)}{ \\sum_ {i=1}^ qexp(o_ i)}$$\n- 尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。因此，softmax回归是一个线性模型（linear model）。\n## 4.5 小批量样本的矢量化\n- 批量：$\\mathbf{X}$；特征维度：$d$；批量大小：$n$；类别数：$q$；\n\n\n- 批量样本特征：$\\mathbf{X}\\in\\mathbb{R}^{n\\times d}$；权重：$\\mathbf{W}\\in\\mathbb{R}^{d\\times q}$；偏置：$\\mathbf{b}\\in\\mathbb{R}^{1\\times q}$ $$\\mathbf{O}=\\mathbf{XW}+\\mathbf{b}$$ $$\\hat{\\mathbf{Y}}=\\text{softmax}(\\mathbf{O})$$\n## 4.6 损失函数\n- 使用极大似然估计\n### 4.6.1 对数似然\n- softmax函数输出向量$\\hat{\\mathbf{Y}}$：x属于各个类别的概率分布（$\\hat{y}_1=P(y=猫|x),\\hat{y}_2=P(y=狗|x),\\hat{y}_3=P(y=鸡|x)$）\n- 1）根据最大似然估计，需要最大化观测数据的联合概率。$$P(\\mathbf{Y}|\\mathbf{X})=\\prod_{i=1}^nP(\\mathbf{y}^{(i)}|\\mathbf{x}^{(i)})$$\n    - $\\mathbf{y}^{(i)}|\\mathbf{x}^{(i)}$：对于样本i，特征向量为：$\\mathbf{x}^{(i)}$，标签向量为：$\\mathbf{y}^{(i)}$\n    - $y_j^{(i)}$：样本i的标签向量中属于类别j的概率\n- 2）相当于最小化负对数似然(损失函数)：$$-\\log P(\\mathbf{Y}|\\mathbf{X})=-\\sum_{i=1}^n\\log P(y^{(i)}|\\mathbf{x}^{(i)})=\\sum_{i=1}^nl(\\mathbf{y}^{(i)},\\mathbf{\\hat{y}}^{(i)})$$\n    - 损失函数为（交叉熵损失 cross-entropy loss）：$$l(\\mathbf{y},\\mathbf{\\hat{y}})=-\\sum_{j=1}^qy_j\\log\\hat{y}_j$$\n\n        由于y是一个长度为q的独热编码向量，所以除了一个项以外的所有项j都消失了。由于所有yˆj都是预测的概率，所以它们的对数永远不会大于0。因此，如果正确地预测实际标签，即如果实际标签P(y | x) = 1，则损失函数不能进一步最小化。注意，这往往是不可能的。例如，数据集中可能存在标签噪声（比如某些样本可能被误标），或输入特征没有足够的信息来完美地对每一个样本分类。？？？\n\n\n\n### 4.6.2 softmax及其导数\n- 对损失函数：$$ \\begin{aligned} l( \\mathbf{y}, \\mathbf{ \\hat{y}}) &= - \\sum_ {j=1}^ qy_ j \\log \\hat{y}_ j \\\\ &= - \\sum_ {j=1}^ qy_ j \\log \\frac{ \\exp(o_ j)}{ \\sum_ {i=1}^ q \\exp(o_ i)} \\\\ &= - \\sum_ {j=1}^ q(y_ j(o_ j- \\log \\sum_ {i=1}^ q \\exp(o_ i))) \\\\ &= \\sum_ {j=1}^ qy_ j \\log \\sum_ {i=1}^ q \\exp(o_ i)- \\sum_ {j=1}^ qy_ jo_ j \\\\ &= \\log \\sum_ {i=1}^ q \\exp(o_ i)- \\sum_ {j=1}^ qy_ jo_ j \\end{aligned}$$\n- 损失函数对$o_j$的导数(log以e为底)：$$\\frac{\\partial l(\\mathbf{y},\\mathbf{\\hat{y}})}{\\partial o_j}=\\frac{\\exp(o_j)}{\\sum_{i=1}^q\\exp(o_i)}-y_j=softmax(o)_j-y_j$$\n    - **这与我们在回归中看到的非常相似，其中梯度是观测值y和估计值yˆ之间的差异。这不是巧合，在任何指数族分布模型中对数似然的梯度正是由此得出的。这使梯度计算在实践中变得容易很多。**\n\n### 4.6.3 交叉熵损失\n- 对于标签$\\mathbf{y}$，我们可以使用与以前相同的表示形式。唯一的区别是，我们现在用一个概率向量表示，如(0.1, 0.2, 0.7)，而不是仅包含二元项的向量(0, 0, 1)。\n- 交叉熵损失（分类问题最常用）：$$l(\\mathbf{y},\\mathbf{\\hat{y}})=-\\sum_{j=1}^qy_j\\log\\hat{y}_j$$\n\n## 4.7 信息论基础\n- 信息论（information theory）涉及编码、解码、发送以及尽可能简洁地处理信息或数据。\n### 4.7.1 熵(entropy)\n$$H[P]=-\\sum_{j}-P(j)\\log P(j)$$\n- 量化数据中的信息内容\n- 为了对从分布$p$中取样的数据进行编码，至少需要$H[p]$个纳特（nat）对其进行编码。（纳特相当于比特，底数是$e$，1nat= $\\frac{1}{\\log(2)}$ bit）\n### 4.7.2 信息量\n- 压缩与预测：如果我们很容易预测下一个数据，那么这个数据就很容易压缩。\n- 如果我们不能完全预测每一个事件，那么我们会感到“惊异”。克劳德·香农决定用信息量$-\\log P(j)$来量化这种惊异度。在观察一个事件j时，并赋予它（主观）概率 $P(j)$。当我们赋予一个事件较低的概率时，我们的惊异会更大，该事件的信息量也就更大。\n- 熵：当分配的概率真正匹配数据生成过程时的信息量的期望。\n\n### 4.7.3 交叉熵\n- 如果把熵$H(p)$当作“知道真实概率的人所经历的惊异程度”，那么交叉熵从P到Q，记为H(P, Q)。我们可以把交叉熵想象为“主观概率为Q的观察者在看到根据概率P生成的数据时的预期惊异”。当P = Q时，交叉熵达到最低。在这种情况下，从P到Q的交叉熵是H(P, P) = H(P)。\n- 可以从两方面来考虑交叉熵分类目标：（i）最大化观测数据的似然；（ii）最小化传达标签所需的惊异。\n\n## 4.8 模型预测和评估\n- 使用预测概率最高的类别作为输出类别。如果预测与实际类别（标签）一致，则预测是正确的。\n- 用精度（accuracy）来评估模型的性能。精度=正确个数/总个数\n","slug":"deeplearning/code/pytorch/2_linear_neural_network/4_softmax_regression","published":1,"updated":"2024-05-02T08:54:59.596Z","_id":"clvuli5r1000hzsvw6a1wex7x","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"4-softmax回归\"><a href=\"#4-softmax回归\" class=\"headerlink\" title=\"4 softmax回归\"></a>4 softmax回归</h1><h2 id=\"4-1-分类问题\"><a href=\"#4-1-分类问题\" class=\"headerlink\" title=\"4.1 分类问题\"></a>4.1 分类问题</h2><ul>\n<li>独热编码（one-hot encoding）：$y\\in{(1,0,0),(0,1,0),(0,0,1)}$</li>\n</ul>\n<h2 id=\"4-2-网络架构\"><a href=\"#4-2-网络架构\" class=\"headerlink\" title=\"4.2 网络架构\"></a>4.2 网络架构</h2><ul>\n<li>为了解决线性模型的分类问题，需要和输出一样多的仿射函数（affine function）。$$o_1&#x3D;x_1w_{11}+x_2w_{12}+x_3w_{13}+x_4w_{14}+b1$$ $$o_1&#x3D;x_1w_{21}+x_2w_{22}+x_3w_{23}+x_4w_{24}+b2$$ $$o_1&#x3D;x_1w_{31}+x_2w_{32}+x_3w_{33}+x_4w_{34}+b3$$</li>\n</ul>\n<p><img src=\"/4img/1.png\" alt=\"$ \\mathfb{o}= \\mathfb{W} \\mathfb{x}+ \\mathfb{b}$\"><br><img src=\"/img/deeplearning/code/pytorch/2_linear_neural_network/4img/1.png\" alt=\"$ \\mathfb{o} = \\mathfb{W} \\mathfb{x} + \\mathfb{b}$\"></p>\n<h2 id=\"4-3-softmax全连接层的参数开销\"><a href=\"#4-3-softmax全连接层的参数开销\" class=\"headerlink\" title=\"4.3 softmax全连接层的参数开销\"></a>4.3 softmax全连接层的参数开销</h2><ul>\n<li>$d$个输入和$q$个输出的全连接层，参数开销为$O(dq)$</li>\n<li>可以将成本减少到$O(\\frac{dq}{n})$,n可以灵活指定</li>\n</ul>\n<h2 id=\"4-4-softmax运算\"><a href=\"#4-4-softmax运算\" class=\"headerlink\" title=\"4.4 softmax运算\"></a>4.4 softmax运算</h2><ul>\n<li>softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持可导的性质。<br>$$\\hat{y}_ j&#x3D; \\frac{exp(o_ j)}{ \\sum_ {i&#x3D;1}^ qexp(o_ i)}$$</li>\n<li>尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。因此，softmax回归是一个线性模型（linear model）。</li>\n</ul>\n<h2 id=\"4-5-小批量样本的矢量化\"><a href=\"#4-5-小批量样本的矢量化\" class=\"headerlink\" title=\"4.5 小批量样本的矢量化\"></a>4.5 小批量样本的矢量化</h2><ul>\n<li><p>批量：$\\mathbf{X}$；特征维度：$d$；批量大小：$n$；类别数：$q$；</p>\n</li>\n<li><p>批量样本特征：$\\mathbf{X}\\in\\mathbb{R}^{n\\times d}$；权重：$\\mathbf{W}\\in\\mathbb{R}^{d\\times q}$；偏置：$\\mathbf{b}\\in\\mathbb{R}^{1\\times q}$ $$\\mathbf{O}&#x3D;\\mathbf{XW}+\\mathbf{b}$$ $$\\hat{\\mathbf{Y}}&#x3D;\\text{softmax}(\\mathbf{O})$$</p>\n</li>\n</ul>\n<h2 id=\"4-6-损失函数\"><a href=\"#4-6-损失函数\" class=\"headerlink\" title=\"4.6 损失函数\"></a>4.6 损失函数</h2><ul>\n<li>使用极大似然估计</li>\n</ul>\n<h3 id=\"4-6-1-对数似然\"><a href=\"#4-6-1-对数似然\" class=\"headerlink\" title=\"4.6.1 对数似然\"></a>4.6.1 对数似然</h3><ul>\n<li>softmax函数输出向量$\\hat{\\mathbf{Y}}$：x属于各个类别的概率分布（$\\hat{y}_1&#x3D;P(y&#x3D;猫|x),\\hat{y}_2&#x3D;P(y&#x3D;狗|x),\\hat{y}_3&#x3D;P(y&#x3D;鸡|x)$）</li>\n<li>1）根据最大似然估计，需要最大化观测数据的联合概率。$$P(\\mathbf{Y}|\\mathbf{X})&#x3D;\\prod_{i&#x3D;1}^nP(\\mathbf{y}^{(i)}|\\mathbf{x}^{(i)})$$<ul>\n<li>$\\mathbf{y}^{(i)}|\\mathbf{x}^{(i)}$：对于样本i，特征向量为：$\\mathbf{x}^{(i)}$，标签向量为：$\\mathbf{y}^{(i)}$</li>\n<li>$y_j^{(i)}$：样本i的标签向量中属于类别j的概率</li>\n</ul>\n</li>\n<li>2）相当于最小化负对数似然(损失函数)：$$-\\log P(\\mathbf{Y}|\\mathbf{X})&#x3D;-\\sum_{i&#x3D;1}^n\\log P(y^{(i)}|\\mathbf{x}^{(i)})&#x3D;\\sum_{i&#x3D;1}^nl(\\mathbf{y}^{(i)},\\mathbf{\\hat{y}}^{(i)})$$<ul>\n<li><p>损失函数为（交叉熵损失 cross-entropy loss）：$$l(\\mathbf{y},\\mathbf{\\hat{y}})&#x3D;-\\sum_{j&#x3D;1}^qy_j\\log\\hat{y}_j$$</p>\n<p>  由于y是一个长度为q的独热编码向量，所以除了一个项以外的所有项j都消失了。由于所有yˆj都是预测的概率，所以它们的对数永远不会大于0。因此，如果正确地预测实际标签，即如果实际标签P(y | x) &#x3D; 1，则损失函数不能进一步最小化。注意，这往往是不可能的。例如，数据集中可能存在标签噪声（比如某些样本可能被误标），或输入特征没有足够的信息来完美地对每一个样本分类。？？？</p>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"4-6-2-softmax及其导数\"><a href=\"#4-6-2-softmax及其导数\" class=\"headerlink\" title=\"4.6.2 softmax及其导数\"></a>4.6.2 softmax及其导数</h3><ul>\n<li>对损失函数：$$ \\begin{aligned} l( \\mathbf{y}, \\mathbf{ \\hat{y}}) &amp;&#x3D; - \\sum_ {j&#x3D;1}^ qy_ j \\log \\hat{y}_ j \\ &amp;&#x3D; - \\sum_ {j&#x3D;1}^ qy_ j \\log \\frac{ \\exp(o_ j)}{ \\sum_ {i&#x3D;1}^ q \\exp(o_ i)} \\ &amp;&#x3D; - \\sum_ {j&#x3D;1}^ q(y_ j(o_ j- \\log \\sum_ {i&#x3D;1}^ q \\exp(o_ i))) \\ &amp;&#x3D; \\sum_ {j&#x3D;1}^ qy_ j \\log \\sum_ {i&#x3D;1}^ q \\exp(o_ i)- \\sum_ {j&#x3D;1}^ qy_ jo_ j \\ &amp;&#x3D; \\log \\sum_ {i&#x3D;1}^ q \\exp(o_ i)- \\sum_ {j&#x3D;1}^ qy_ jo_ j \\end{aligned}$$</li>\n<li>损失函数对$o_j$的导数(log以e为底)：$$\\frac{\\partial l(\\mathbf{y},\\mathbf{\\hat{y}})}{\\partial o_j}&#x3D;\\frac{\\exp(o_j)}{\\sum_{i&#x3D;1}^q\\exp(o_i)}-y_j&#x3D;softmax(o)_j-y_j$$<ul>\n<li><strong>这与我们在回归中看到的非常相似，其中梯度是观测值y和估计值yˆ之间的差异。这不是巧合，在任何指数族分布模型中对数似然的梯度正是由此得出的。这使梯度计算在实践中变得容易很多。</strong></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"4-6-3-交叉熵损失\"><a href=\"#4-6-3-交叉熵损失\" class=\"headerlink\" title=\"4.6.3 交叉熵损失\"></a>4.6.3 交叉熵损失</h3><ul>\n<li>对于标签$\\mathbf{y}$，我们可以使用与以前相同的表示形式。唯一的区别是，我们现在用一个概率向量表示，如(0.1, 0.2, 0.7)，而不是仅包含二元项的向量(0, 0, 1)。</li>\n<li>交叉熵损失（分类问题最常用）：$$l(\\mathbf{y},\\mathbf{\\hat{y}})&#x3D;-\\sum_{j&#x3D;1}^qy_j\\log\\hat{y}_j$$</li>\n</ul>\n<h2 id=\"4-7-信息论基础\"><a href=\"#4-7-信息论基础\" class=\"headerlink\" title=\"4.7 信息论基础\"></a>4.7 信息论基础</h2><ul>\n<li>信息论（information theory）涉及编码、解码、发送以及尽可能简洁地处理信息或数据。</li>\n</ul>\n<h3 id=\"4-7-1-熵-entropy\"><a href=\"#4-7-1-熵-entropy\" class=\"headerlink\" title=\"4.7.1 熵(entropy)\"></a>4.7.1 熵(entropy)</h3><p>$$H[P]&#x3D;-\\sum_{j}-P(j)\\log P(j)$$</p>\n<ul>\n<li>量化数据中的信息内容</li>\n<li>为了对从分布$p$中取样的数据进行编码，至少需要$H[p]$个纳特（nat）对其进行编码。（纳特相当于比特，底数是$e$，1nat&#x3D; $\\frac{1}{\\log(2)}$ bit）</li>\n</ul>\n<h3 id=\"4-7-2-信息量\"><a href=\"#4-7-2-信息量\" class=\"headerlink\" title=\"4.7.2 信息量\"></a>4.7.2 信息量</h3><ul>\n<li>压缩与预测：如果我们很容易预测下一个数据，那么这个数据就很容易压缩。</li>\n<li>如果我们不能完全预测每一个事件，那么我们会感到“惊异”。克劳德·香农决定用信息量$-\\log P(j)$来量化这种惊异度。在观察一个事件j时，并赋予它（主观）概率 $P(j)$。当我们赋予一个事件较低的概率时，我们的惊异会更大，该事件的信息量也就更大。</li>\n<li>熵：当分配的概率真正匹配数据生成过程时的信息量的期望。</li>\n</ul>\n<h3 id=\"4-7-3-交叉熵\"><a href=\"#4-7-3-交叉熵\" class=\"headerlink\" title=\"4.7.3 交叉熵\"></a>4.7.3 交叉熵</h3><ul>\n<li>如果把熵$H(p)$当作“知道真实概率的人所经历的惊异程度”，那么交叉熵从P到Q，记为H(P, Q)。我们可以把交叉熵想象为“主观概率为Q的观察者在看到根据概率P生成的数据时的预期惊异”。当P &#x3D; Q时，交叉熵达到最低。在这种情况下，从P到Q的交叉熵是H(P, P) &#x3D; H(P)。</li>\n<li>可以从两方面来考虑交叉熵分类目标：（i）最大化观测数据的似然；（ii）最小化传达标签所需的惊异。</li>\n</ul>\n<h2 id=\"4-8-模型预测和评估\"><a href=\"#4-8-模型预测和评估\" class=\"headerlink\" title=\"4.8 模型预测和评估\"></a>4.8 模型预测和评估</h2><ul>\n<li>使用预测概率最高的类别作为输出类别。如果预测与实际类别（标签）一致，则预测是正确的。</li>\n<li>用精度（accuracy）来评估模型的性能。精度&#x3D;正确个数&#x2F;总个数</li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"4-softmax回归\"><a href=\"#4-softmax回归\" class=\"headerlink\" title=\"4 softmax回归\"></a>4 softmax回归</h1><h2 id=\"4-1-分类问题\"><a href=\"#4-1-分类问题\" class=\"headerlink\" title=\"4.1 分类问题\"></a>4.1 分类问题</h2><ul>\n<li>独热编码（one-hot encoding）：$y\\in{(1,0,0),(0,1,0),(0,0,1)}$</li>\n</ul>\n<h2 id=\"4-2-网络架构\"><a href=\"#4-2-网络架构\" class=\"headerlink\" title=\"4.2 网络架构\"></a>4.2 网络架构</h2><ul>\n<li>为了解决线性模型的分类问题，需要和输出一样多的仿射函数（affine function）。$$o_1&#x3D;x_1w_{11}+x_2w_{12}+x_3w_{13}+x_4w_{14}+b1$$ $$o_1&#x3D;x_1w_{21}+x_2w_{22}+x_3w_{23}+x_4w_{24}+b2$$ $$o_1&#x3D;x_1w_{31}+x_2w_{32}+x_3w_{33}+x_4w_{34}+b3$$</li>\n</ul>\n<p><img src=\"/4img/1.png\" alt=\"$ \\mathfb{o}= \\mathfb{W} \\mathfb{x}+ \\mathfb{b}$\"><br><img src=\"/img/deeplearning/code/pytorch/2_linear_neural_network/4img/1.png\" alt=\"$ \\mathfb{o} = \\mathfb{W} \\mathfb{x} + \\mathfb{b}$\"></p>\n<h2 id=\"4-3-softmax全连接层的参数开销\"><a href=\"#4-3-softmax全连接层的参数开销\" class=\"headerlink\" title=\"4.3 softmax全连接层的参数开销\"></a>4.3 softmax全连接层的参数开销</h2><ul>\n<li>$d$个输入和$q$个输出的全连接层，参数开销为$O(dq)$</li>\n<li>可以将成本减少到$O(\\frac{dq}{n})$,n可以灵活指定</li>\n</ul>\n<h2 id=\"4-4-softmax运算\"><a href=\"#4-4-softmax运算\" class=\"headerlink\" title=\"4.4 softmax运算\"></a>4.4 softmax运算</h2><ul>\n<li>softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持可导的性质。<br>$$\\hat{y}_ j&#x3D; \\frac{exp(o_ j)}{ \\sum_ {i&#x3D;1}^ qexp(o_ i)}$$</li>\n<li>尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。因此，softmax回归是一个线性模型（linear model）。</li>\n</ul>\n<h2 id=\"4-5-小批量样本的矢量化\"><a href=\"#4-5-小批量样本的矢量化\" class=\"headerlink\" title=\"4.5 小批量样本的矢量化\"></a>4.5 小批量样本的矢量化</h2><ul>\n<li><p>批量：$\\mathbf{X}$；特征维度：$d$；批量大小：$n$；类别数：$q$；</p>\n</li>\n<li><p>批量样本特征：$\\mathbf{X}\\in\\mathbb{R}^{n\\times d}$；权重：$\\mathbf{W}\\in\\mathbb{R}^{d\\times q}$；偏置：$\\mathbf{b}\\in\\mathbb{R}^{1\\times q}$ $$\\mathbf{O}&#x3D;\\mathbf{XW}+\\mathbf{b}$$ $$\\hat{\\mathbf{Y}}&#x3D;\\text{softmax}(\\mathbf{O})$$</p>\n</li>\n</ul>\n<h2 id=\"4-6-损失函数\"><a href=\"#4-6-损失函数\" class=\"headerlink\" title=\"4.6 损失函数\"></a>4.6 损失函数</h2><ul>\n<li>使用极大似然估计</li>\n</ul>\n<h3 id=\"4-6-1-对数似然\"><a href=\"#4-6-1-对数似然\" class=\"headerlink\" title=\"4.6.1 对数似然\"></a>4.6.1 对数似然</h3><ul>\n<li>softmax函数输出向量$\\hat{\\mathbf{Y}}$：x属于各个类别的概率分布（$\\hat{y}_1&#x3D;P(y&#x3D;猫|x),\\hat{y}_2&#x3D;P(y&#x3D;狗|x),\\hat{y}_3&#x3D;P(y&#x3D;鸡|x)$）</li>\n<li>1）根据最大似然估计，需要最大化观测数据的联合概率。$$P(\\mathbf{Y}|\\mathbf{X})&#x3D;\\prod_{i&#x3D;1}^nP(\\mathbf{y}^{(i)}|\\mathbf{x}^{(i)})$$<ul>\n<li>$\\mathbf{y}^{(i)}|\\mathbf{x}^{(i)}$：对于样本i，特征向量为：$\\mathbf{x}^{(i)}$，标签向量为：$\\mathbf{y}^{(i)}$</li>\n<li>$y_j^{(i)}$：样本i的标签向量中属于类别j的概率</li>\n</ul>\n</li>\n<li>2）相当于最小化负对数似然(损失函数)：$$-\\log P(\\mathbf{Y}|\\mathbf{X})&#x3D;-\\sum_{i&#x3D;1}^n\\log P(y^{(i)}|\\mathbf{x}^{(i)})&#x3D;\\sum_{i&#x3D;1}^nl(\\mathbf{y}^{(i)},\\mathbf{\\hat{y}}^{(i)})$$<ul>\n<li><p>损失函数为（交叉熵损失 cross-entropy loss）：$$l(\\mathbf{y},\\mathbf{\\hat{y}})&#x3D;-\\sum_{j&#x3D;1}^qy_j\\log\\hat{y}_j$$</p>\n<p>  由于y是一个长度为q的独热编码向量，所以除了一个项以外的所有项j都消失了。由于所有yˆj都是预测的概率，所以它们的对数永远不会大于0。因此，如果正确地预测实际标签，即如果实际标签P(y | x) &#x3D; 1，则损失函数不能进一步最小化。注意，这往往是不可能的。例如，数据集中可能存在标签噪声（比如某些样本可能被误标），或输入特征没有足够的信息来完美地对每一个样本分类。？？？</p>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"4-6-2-softmax及其导数\"><a href=\"#4-6-2-softmax及其导数\" class=\"headerlink\" title=\"4.6.2 softmax及其导数\"></a>4.6.2 softmax及其导数</h3><ul>\n<li>对损失函数：$$ \\begin{aligned} l( \\mathbf{y}, \\mathbf{ \\hat{y}}) &amp;&#x3D; - \\sum_ {j&#x3D;1}^ qy_ j \\log \\hat{y}_ j \\ &amp;&#x3D; - \\sum_ {j&#x3D;1}^ qy_ j \\log \\frac{ \\exp(o_ j)}{ \\sum_ {i&#x3D;1}^ q \\exp(o_ i)} \\ &amp;&#x3D; - \\sum_ {j&#x3D;1}^ q(y_ j(o_ j- \\log \\sum_ {i&#x3D;1}^ q \\exp(o_ i))) \\ &amp;&#x3D; \\sum_ {j&#x3D;1}^ qy_ j \\log \\sum_ {i&#x3D;1}^ q \\exp(o_ i)- \\sum_ {j&#x3D;1}^ qy_ jo_ j \\ &amp;&#x3D; \\log \\sum_ {i&#x3D;1}^ q \\exp(o_ i)- \\sum_ {j&#x3D;1}^ qy_ jo_ j \\end{aligned}$$</li>\n<li>损失函数对$o_j$的导数(log以e为底)：$$\\frac{\\partial l(\\mathbf{y},\\mathbf{\\hat{y}})}{\\partial o_j}&#x3D;\\frac{\\exp(o_j)}{\\sum_{i&#x3D;1}^q\\exp(o_i)}-y_j&#x3D;softmax(o)_j-y_j$$<ul>\n<li><strong>这与我们在回归中看到的非常相似，其中梯度是观测值y和估计值yˆ之间的差异。这不是巧合，在任何指数族分布模型中对数似然的梯度正是由此得出的。这使梯度计算在实践中变得容易很多。</strong></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"4-6-3-交叉熵损失\"><a href=\"#4-6-3-交叉熵损失\" class=\"headerlink\" title=\"4.6.3 交叉熵损失\"></a>4.6.3 交叉熵损失</h3><ul>\n<li>对于标签$\\mathbf{y}$，我们可以使用与以前相同的表示形式。唯一的区别是，我们现在用一个概率向量表示，如(0.1, 0.2, 0.7)，而不是仅包含二元项的向量(0, 0, 1)。</li>\n<li>交叉熵损失（分类问题最常用）：$$l(\\mathbf{y},\\mathbf{\\hat{y}})&#x3D;-\\sum_{j&#x3D;1}^qy_j\\log\\hat{y}_j$$</li>\n</ul>\n<h2 id=\"4-7-信息论基础\"><a href=\"#4-7-信息论基础\" class=\"headerlink\" title=\"4.7 信息论基础\"></a>4.7 信息论基础</h2><ul>\n<li>信息论（information theory）涉及编码、解码、发送以及尽可能简洁地处理信息或数据。</li>\n</ul>\n<h3 id=\"4-7-1-熵-entropy\"><a href=\"#4-7-1-熵-entropy\" class=\"headerlink\" title=\"4.7.1 熵(entropy)\"></a>4.7.1 熵(entropy)</h3><p>$$H[P]&#x3D;-\\sum_{j}-P(j)\\log P(j)$$</p>\n<ul>\n<li>量化数据中的信息内容</li>\n<li>为了对从分布$p$中取样的数据进行编码，至少需要$H[p]$个纳特（nat）对其进行编码。（纳特相当于比特，底数是$e$，1nat&#x3D; $\\frac{1}{\\log(2)}$ bit）</li>\n</ul>\n<h3 id=\"4-7-2-信息量\"><a href=\"#4-7-2-信息量\" class=\"headerlink\" title=\"4.7.2 信息量\"></a>4.7.2 信息量</h3><ul>\n<li>压缩与预测：如果我们很容易预测下一个数据，那么这个数据就很容易压缩。</li>\n<li>如果我们不能完全预测每一个事件，那么我们会感到“惊异”。克劳德·香农决定用信息量$-\\log P(j)$来量化这种惊异度。在观察一个事件j时，并赋予它（主观）概率 $P(j)$。当我们赋予一个事件较低的概率时，我们的惊异会更大，该事件的信息量也就更大。</li>\n<li>熵：当分配的概率真正匹配数据生成过程时的信息量的期望。</li>\n</ul>\n<h3 id=\"4-7-3-交叉熵\"><a href=\"#4-7-3-交叉熵\" class=\"headerlink\" title=\"4.7.3 交叉熵\"></a>4.7.3 交叉熵</h3><ul>\n<li>如果把熵$H(p)$当作“知道真实概率的人所经历的惊异程度”，那么交叉熵从P到Q，记为H(P, Q)。我们可以把交叉熵想象为“主观概率为Q的观察者在看到根据概率P生成的数据时的预期惊异”。当P &#x3D; Q时，交叉熵达到最低。在这种情况下，从P到Q的交叉熵是H(P, P) &#x3D; H(P)。</li>\n<li>可以从两方面来考虑交叉熵分类目标：（i）最大化观测数据的似然；（ii）最小化传达标签所需的惊异。</li>\n</ul>\n<h2 id=\"4-8-模型预测和评估\"><a href=\"#4-8-模型预测和评估\" class=\"headerlink\" title=\"4.8 模型预测和评估\"></a>4.8 模型预测和评估</h2><ul>\n<li>使用预测概率最高的类别作为输出类别。如果预测与实际类别（标签）一致，则预测是正确的。</li>\n<li>用精度（accuracy）来评估模型的性能。精度&#x3D;正确个数&#x2F;总个数</li>\n</ul>"},{"title":"1.3 线性代数","date":"2024-02-01T06:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 3. 线性代数\n## 3.1 标量\n\n\n\n```python\nimport torch\n\nx=torch.tensor(3.0)\n```\n\n## 3.2 向量\n\n\n```python\nx=torch.arange(4)\n\n#1 取值\nprint(x[3])\n\n#2 长度\nprint(len(x))\nprint(x.shape)\n```\n\n    tensor(3)\n    4\n    torch.Size([4])\n    \n\n## 3.3 矩阵\n\n\n\n```python\nA=torch.arange(20).reshape(5,4) #矩阵用大写字母\nprint(A)\n\n#1 转置\nprint(A.T)\n```\n\n    tensor([[ 0,  1,  2,  3],\n            [ 4,  5,  6,  7],\n            [ 8,  9, 10, 11],\n            [12, 13, 14, 15],\n            [16, 17, 18, 19]])\n    tensor([[ 0,  4,  8, 12, 16],\n            [ 1,  5,  9, 13, 17],\n            [ 2,  6, 10, 14, 18],\n            [ 3,  7, 11, 15, 19]])\n    \n\n## 3.4 张量\n\n\n```python\nX= torch.arange(24).reshape(2,3,4)\nX\n```\n\n\n\n\n    tensor([[[ 0,  1,  2,  3],\n             [ 4,  5,  6,  7],\n             [ 8,  9, 10, 11]],\n    \n            [[12, 13, 14, 15],\n             [16, 17, 18, 19],\n             [20, 21, 22, 23]]])\n\n\n\n## 3.5 张量算法的基本性质\n\n\n```python\nA=torch.arange(20,dtype=torch.float32).reshape(5,4)\n\n#1 复制\nB=A.clone()\nprint(A==B)\n\n#2 Hadamard积⊙ (对应位置相乘)\nprint(A*B)\n\n#3 张量+标量（所有元素加上标量）\nprint(A+2)\n\n#4 元素个数\nprint(A.numel())\n```\n\n    tensor([[True, True, True, True],\n            [True, True, True, True],\n            [True, True, True, True],\n            [True, True, True, True],\n            [True, True, True, True]])\n    tensor([[  0.,   1.,   4.,   9.],\n            [ 16.,  25.,  36.,  49.],\n            [ 64.,  81., 100., 121.],\n            [144., 169., 196., 225.],\n            [256., 289., 324., 361.]])\n    tensor([[ 2.,  3.,  4.,  5.],\n            [ 6.,  7.,  8.,  9.],\n            [10., 11., 12., 13.],\n            [14., 15., 16., 17.],\n            [18., 19., 20., 21.]])\n    20\n    \n\n## 3.6 降维\n\n\n```python\nX= torch.arange(4,dtype=torch.float32).reshape(2,2)\n\n#1 按轴求和\nprint(X.sum(axis=0)) #压缩掉第0维\n\n#2 按轴求平均\nprint(X.mean(axis=0))\n\n#3 非降维求和\nsum_X=X.sum(axis=0,keepdims=True)\nprint(sum_X) #还是二维，只是第0维的长度为1\n\n#4 沿某个轴计算A元素的累积总和\nprint(A.cumsum(axis=0))\n```\n\n    tensor([2., 4.])\n    tensor([1., 2.])\n    tensor([[2., 4.]])\n    tensor([[ 0.,  1.,  2.,  3.],\n            [ 4.,  6.,  8., 10.],\n            [12., 15., 18., 21.],\n            [24., 28., 32., 36.],\n            [40., 45., 50., 55.]])\n    \n\n## 3.7 点积\n- $$<x,y>= x^Ty= \\Sigma^{d}_{i=1}x_iy_i$$\n\n\n```python\nx=y=torch.arange(4,dtype=torch.float32)\nprint(x,y,torch.dot(x,y),sep='\\n')\n```\n\n    tensor([0., 1., 2., 3.])\n    tensor([0., 1., 2., 3.])\n    tensor(14.)\n    \n\n## 3.8 矩阵-向量积\n\n\n```python\nprint(A)\nprint(x)\ntorch.mv(A,x) #输出是【5，1】但是1被压缩\n```\n\n    tensor([[ 0.,  1.,  2.,  3.],\n            [ 4.,  5.,  6.,  7.],\n            [ 8.,  9., 10., 11.],\n            [12., 13., 14., 15.],\n            [16., 17., 18., 19.]])\n    tensor([0., 1., 2., 3.])\n    \n\n\n\n\n    tensor([ 14.,  38.,  62.,  86., 110.])\n\n\n\n## 3.9 矩阵-矩阵乘法\n\n\n\n```python\nprint(A)\nB=torch.ones(4,3)\ntorch.mm(A,B)\n```\n\n    tensor([[ 0.,  1.,  2.,  3.],\n            [ 4.,  5.,  6.,  7.],\n            [ 8.,  9., 10., 11.],\n            [12., 13., 14., 15.],\n            [16., 17., 18., 19.]])\n    \n\n\n\n\n    tensor([[ 6.,  6.,  6.],\n            [22., 22., 22.],\n            [38., 38., 38.],\n            [54., 54., 54.],\n            [70., 70., 70.]])\n\n\n\n## 3.10 范数\n- 表示一个向量有多大。这里考虑的大小概念不涉及维度，而是分量的大小。\n### 3.10.1 向量范数\n- 将向量x映射到标量的函数$f$。\n- 向量范数必须满足以下性质：\n    - 1）按常数因子α缩放向量的所有元素，其范数也会按相同常数因子的绝对值缩放：\n    $$f(\\alpha x)=|\\alpha|f(x)$$\n    - 2）三角不等式：\n    $$f(x+y)\\leq f(x)+f(y)$$\n    - 3）非负性：\n    $$f(x)\\geq 0$$\n    - 4）范数最小为0，当且仅当向量全由0组成：\n    $$\\forall i,[x]_i=0 \\Leftrightarrow f(x)=0 $$\n\n### 3.10.2 L2范数\n- 平方和的平方根：\n$$ ||x||=||x||_ 2 = \\sqrt{ \\sum \\limits_ {i=1}^ {n}x_ i^ 2}$$\n\n\n```python\nu=torch.tensor([3.0,-4.0])\ntorch.norm(u) #范数\n```\n\n\n\n\n    tensor(5.)\n\n\n\n### 3.10.3 L1范数\n- 绝对值之和\n$$ ||x||_ 1 = \\sum \\limits_ {i=1}^ {n}|x_ i|$$\n- 与L2范数相比，L1范数受异常值的影响较小\n\n\n```python\ntorch.abs(u).sum() #L1范数\n```\n\n\n\n\n    tensor(7.)\n\n\n\n### 3.10.4 Lp范数\n$$||x||_ p = ( \\sum \\limits_ {i=1}^ {n}|x_ i|^ p)^ { \\frac{1}{p}}$$\n\n### 3.10.5 Frobenius范数\n- 矩阵L2范数\n$$||X||_ F = \\sqrt{ \\sum \\limits_ {i=1}^ {m} \\sum \\limits_ {j=1}^ {n}x_ {ij}^ 2}$$\n\n\n```python\ntorch.norm(torch.ones((4,9)))\n```\n\n\n\n\n    tensor(6.)\n\n\n\n- 目标，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数。\n## 3.11 练习\n\n\n```python\na=torch.tensor([[1,2,3],[4,5,6]])\nb=a.sum(axis=0)\nc=a.sum(axis=1)\nprint(a.shape,b.shape,c.shape)\n\nprint(a+b) #b->(2)->(1,2)->(2,3)\nprint(a+c) #c->(3)->(1,3)->不匹配\n```\n\n    torch.Size([2, 3]) torch.Size([3]) torch.Size([2])\n    tensor([[ 6,  9, 12],\n            [ 9, 12, 15]])\n    \n\n\n    ---------------------------------------------------------------------------\n\n    RuntimeError                              Traceback (most recent call last)\n\n    Cell In[37], line 7\n          4 print(a.shape,b.shape,c.shape)\n          6 print(a+b) #b->(2)->(1,2)->(2,3)\n    ----> 7 print(a+c) #c->(3)->(1,3)->不匹配\n    \n\n    RuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1\n\n","source":"_posts/deeplearning/code/pytorch/1_prepare/3_linearalgebra.md","raw":"---\ntitle: 1.3 线性代数\ndate: 2024-2-1 14:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 3. 线性代数\n## 3.1 标量\n\n\n\n```python\nimport torch\n\nx=torch.tensor(3.0)\n```\n\n## 3.2 向量\n\n\n```python\nx=torch.arange(4)\n\n#1 取值\nprint(x[3])\n\n#2 长度\nprint(len(x))\nprint(x.shape)\n```\n\n    tensor(3)\n    4\n    torch.Size([4])\n    \n\n## 3.3 矩阵\n\n\n\n```python\nA=torch.arange(20).reshape(5,4) #矩阵用大写字母\nprint(A)\n\n#1 转置\nprint(A.T)\n```\n\n    tensor([[ 0,  1,  2,  3],\n            [ 4,  5,  6,  7],\n            [ 8,  9, 10, 11],\n            [12, 13, 14, 15],\n            [16, 17, 18, 19]])\n    tensor([[ 0,  4,  8, 12, 16],\n            [ 1,  5,  9, 13, 17],\n            [ 2,  6, 10, 14, 18],\n            [ 3,  7, 11, 15, 19]])\n    \n\n## 3.4 张量\n\n\n```python\nX= torch.arange(24).reshape(2,3,4)\nX\n```\n\n\n\n\n    tensor([[[ 0,  1,  2,  3],\n             [ 4,  5,  6,  7],\n             [ 8,  9, 10, 11]],\n    \n            [[12, 13, 14, 15],\n             [16, 17, 18, 19],\n             [20, 21, 22, 23]]])\n\n\n\n## 3.5 张量算法的基本性质\n\n\n```python\nA=torch.arange(20,dtype=torch.float32).reshape(5,4)\n\n#1 复制\nB=A.clone()\nprint(A==B)\n\n#2 Hadamard积⊙ (对应位置相乘)\nprint(A*B)\n\n#3 张量+标量（所有元素加上标量）\nprint(A+2)\n\n#4 元素个数\nprint(A.numel())\n```\n\n    tensor([[True, True, True, True],\n            [True, True, True, True],\n            [True, True, True, True],\n            [True, True, True, True],\n            [True, True, True, True]])\n    tensor([[  0.,   1.,   4.,   9.],\n            [ 16.,  25.,  36.,  49.],\n            [ 64.,  81., 100., 121.],\n            [144., 169., 196., 225.],\n            [256., 289., 324., 361.]])\n    tensor([[ 2.,  3.,  4.,  5.],\n            [ 6.,  7.,  8.,  9.],\n            [10., 11., 12., 13.],\n            [14., 15., 16., 17.],\n            [18., 19., 20., 21.]])\n    20\n    \n\n## 3.6 降维\n\n\n```python\nX= torch.arange(4,dtype=torch.float32).reshape(2,2)\n\n#1 按轴求和\nprint(X.sum(axis=0)) #压缩掉第0维\n\n#2 按轴求平均\nprint(X.mean(axis=0))\n\n#3 非降维求和\nsum_X=X.sum(axis=0,keepdims=True)\nprint(sum_X) #还是二维，只是第0维的长度为1\n\n#4 沿某个轴计算A元素的累积总和\nprint(A.cumsum(axis=0))\n```\n\n    tensor([2., 4.])\n    tensor([1., 2.])\n    tensor([[2., 4.]])\n    tensor([[ 0.,  1.,  2.,  3.],\n            [ 4.,  6.,  8., 10.],\n            [12., 15., 18., 21.],\n            [24., 28., 32., 36.],\n            [40., 45., 50., 55.]])\n    \n\n## 3.7 点积\n- $$<x,y>= x^Ty= \\Sigma^{d}_{i=1}x_iy_i$$\n\n\n```python\nx=y=torch.arange(4,dtype=torch.float32)\nprint(x,y,torch.dot(x,y),sep='\\n')\n```\n\n    tensor([0., 1., 2., 3.])\n    tensor([0., 1., 2., 3.])\n    tensor(14.)\n    \n\n## 3.8 矩阵-向量积\n\n\n```python\nprint(A)\nprint(x)\ntorch.mv(A,x) #输出是【5，1】但是1被压缩\n```\n\n    tensor([[ 0.,  1.,  2.,  3.],\n            [ 4.,  5.,  6.,  7.],\n            [ 8.,  9., 10., 11.],\n            [12., 13., 14., 15.],\n            [16., 17., 18., 19.]])\n    tensor([0., 1., 2., 3.])\n    \n\n\n\n\n    tensor([ 14.,  38.,  62.,  86., 110.])\n\n\n\n## 3.9 矩阵-矩阵乘法\n\n\n\n```python\nprint(A)\nB=torch.ones(4,3)\ntorch.mm(A,B)\n```\n\n    tensor([[ 0.,  1.,  2.,  3.],\n            [ 4.,  5.,  6.,  7.],\n            [ 8.,  9., 10., 11.],\n            [12., 13., 14., 15.],\n            [16., 17., 18., 19.]])\n    \n\n\n\n\n    tensor([[ 6.,  6.,  6.],\n            [22., 22., 22.],\n            [38., 38., 38.],\n            [54., 54., 54.],\n            [70., 70., 70.]])\n\n\n\n## 3.10 范数\n- 表示一个向量有多大。这里考虑的大小概念不涉及维度，而是分量的大小。\n### 3.10.1 向量范数\n- 将向量x映射到标量的函数$f$。\n- 向量范数必须满足以下性质：\n    - 1）按常数因子α缩放向量的所有元素，其范数也会按相同常数因子的绝对值缩放：\n    $$f(\\alpha x)=|\\alpha|f(x)$$\n    - 2）三角不等式：\n    $$f(x+y)\\leq f(x)+f(y)$$\n    - 3）非负性：\n    $$f(x)\\geq 0$$\n    - 4）范数最小为0，当且仅当向量全由0组成：\n    $$\\forall i,[x]_i=0 \\Leftrightarrow f(x)=0 $$\n\n### 3.10.2 L2范数\n- 平方和的平方根：\n$$ ||x||=||x||_ 2 = \\sqrt{ \\sum \\limits_ {i=1}^ {n}x_ i^ 2}$$\n\n\n```python\nu=torch.tensor([3.0,-4.0])\ntorch.norm(u) #范数\n```\n\n\n\n\n    tensor(5.)\n\n\n\n### 3.10.3 L1范数\n- 绝对值之和\n$$ ||x||_ 1 = \\sum \\limits_ {i=1}^ {n}|x_ i|$$\n- 与L2范数相比，L1范数受异常值的影响较小\n\n\n```python\ntorch.abs(u).sum() #L1范数\n```\n\n\n\n\n    tensor(7.)\n\n\n\n### 3.10.4 Lp范数\n$$||x||_ p = ( \\sum \\limits_ {i=1}^ {n}|x_ i|^ p)^ { \\frac{1}{p}}$$\n\n### 3.10.5 Frobenius范数\n- 矩阵L2范数\n$$||X||_ F = \\sqrt{ \\sum \\limits_ {i=1}^ {m} \\sum \\limits_ {j=1}^ {n}x_ {ij}^ 2}$$\n\n\n```python\ntorch.norm(torch.ones((4,9)))\n```\n\n\n\n\n    tensor(6.)\n\n\n\n- 目标，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数。\n## 3.11 练习\n\n\n```python\na=torch.tensor([[1,2,3],[4,5,6]])\nb=a.sum(axis=0)\nc=a.sum(axis=1)\nprint(a.shape,b.shape,c.shape)\n\nprint(a+b) #b->(2)->(1,2)->(2,3)\nprint(a+c) #c->(3)->(1,3)->不匹配\n```\n\n    torch.Size([2, 3]) torch.Size([3]) torch.Size([2])\n    tensor([[ 6,  9, 12],\n            [ 9, 12, 15]])\n    \n\n\n    ---------------------------------------------------------------------------\n\n    RuntimeError                              Traceback (most recent call last)\n\n    Cell In[37], line 7\n          4 print(a.shape,b.shape,c.shape)\n          6 print(a+b) #b->(2)->(1,2)->(2,3)\n    ----> 7 print(a+c) #c->(3)->(1,3)->不匹配\n    \n\n    RuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1\n\n","slug":"deeplearning/code/pytorch/1_prepare/3_linearalgebra","published":1,"updated":"2024-05-02T08:56:15.385Z","_id":"clvuli5sv000wzsvwa9xncfth","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"3-线性代数\"><a href=\"#3-线性代数\" class=\"headerlink\" title=\"3. 线性代数\"></a>3. 线性代数</h1><h2 id=\"3-1-标量\"><a href=\"#3-1-标量\" class=\"headerlink\" title=\"3.1 标量\"></a>3.1 标量</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"></span><br><span class=\"line\">x=torch.tensor(<span class=\"number\">3.0</span>)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"3-2-向量\"><a href=\"#3-2-向量\" class=\"headerlink\" title=\"3.2 向量\"></a>3.2 向量</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=torch.arange(<span class=\"number\">4</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#1 取值</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(x[<span class=\"number\">3</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#2 长度</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">len</span>(x))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.shape)</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor(3)\n4\ntorch.Size([4])\n</code></pre>\n<h2 id=\"3-3-矩阵\"><a href=\"#3-3-矩阵\" class=\"headerlink\" title=\"3.3 矩阵\"></a>3.3 矩阵</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A=torch.arange(<span class=\"number\">20</span>).reshape(<span class=\"number\">5</span>,<span class=\"number\">4</span>) <span class=\"comment\">#矩阵用大写字母</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(A)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#1 转置</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(A.T)</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11],\n        [12, 13, 14, 15],\n        [16, 17, 18, 19]])\ntensor([[ 0,  4,  8, 12, 16],\n        [ 1,  5,  9, 13, 17],\n        [ 2,  6, 10, 14, 18],\n        [ 3,  7, 11, 15, 19]])\n</code></pre>\n<h2 id=\"3-4-张量\"><a href=\"#3-4-张量\" class=\"headerlink\" title=\"3.4 张量\"></a>3.4 张量</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X= torch.arange(<span class=\"number\">24</span>).reshape(<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\">X</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>tensor([[[ 0,  1,  2,  3],\n         [ 4,  5,  6,  7],\n         [ 8,  9, 10, 11]],\n\n        [[12, 13, 14, 15],\n         [16, 17, 18, 19],\n         [20, 21, 22, 23]]])\n</code></pre>\n<h2 id=\"3-5-张量算法的基本性质\"><a href=\"#3-5-张量算法的基本性质\" class=\"headerlink\" title=\"3.5 张量算法的基本性质\"></a>3.5 张量算法的基本性质</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A=torch.arange(<span class=\"number\">20</span>,dtype=torch.float32).reshape(<span class=\"number\">5</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#1 复制</span></span><br><span class=\"line\">B=A.clone()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(A==B)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#2 Hadamard积⊙ (对应位置相乘)</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(A*B)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#3 张量+标量（所有元素加上标量）</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(A+<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#4 元素个数</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(A.numel())</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])\ntensor([[  0.,   1.,   4.,   9.],\n        [ 16.,  25.,  36.,  49.],\n        [ 64.,  81., 100., 121.],\n        [144., 169., 196., 225.],\n        [256., 289., 324., 361.]])\ntensor([[ 2.,  3.,  4.,  5.],\n        [ 6.,  7.,  8.,  9.],\n        [10., 11., 12., 13.],\n        [14., 15., 16., 17.],\n        [18., 19., 20., 21.]])\n20\n</code></pre>\n<h2 id=\"3-6-降维\"><a href=\"#3-6-降维\" class=\"headerlink\" title=\"3.6 降维\"></a>3.6 降维</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X= torch.arange(<span class=\"number\">4</span>,dtype=torch.float32).reshape(<span class=\"number\">2</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#1 按轴求和</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(X.<span class=\"built_in\">sum</span>(axis=<span class=\"number\">0</span>)) <span class=\"comment\">#压缩掉第0维</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#2 按轴求平均</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(X.mean(axis=<span class=\"number\">0</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#3 非降维求和</span></span><br><span class=\"line\">sum_X=X.<span class=\"built_in\">sum</span>(axis=<span class=\"number\">0</span>,keepdims=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(sum_X) <span class=\"comment\">#还是二维，只是第0维的长度为1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#4 沿某个轴计算A元素的累积总和</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(A.cumsum(axis=<span class=\"number\">0</span>))</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([2., 4.])\ntensor([1., 2.])\ntensor([[2., 4.]])\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  6.,  8., 10.],\n        [12., 15., 18., 21.],\n        [24., 28., 32., 36.],\n        [40., 45., 50., 55.]])\n</code></pre>\n<h2 id=\"3-7-点积\"><a href=\"#3-7-点积\" class=\"headerlink\" title=\"3.7 点积\"></a>3.7 点积</h2><ul>\n<li>$$&lt;x,y&gt;&#x3D; x^Ty&#x3D; \\Sigma^{d}_{i&#x3D;1}x_iy_i$$</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=y=torch.arange(<span class=\"number\">4</span>,dtype=torch.float32)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x,y,torch.dot(x,y),sep=<span class=\"string\">&#x27;\\n&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([0., 1., 2., 3.])\ntensor([0., 1., 2., 3.])\ntensor(14.)\n</code></pre>\n<h2 id=\"3-8-矩阵-向量积\"><a href=\"#3-8-矩阵-向量积\" class=\"headerlink\" title=\"3.8 矩阵-向量积\"></a>3.8 矩阵-向量积</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(A)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x)</span><br><span class=\"line\">torch.mv(A,x) <span class=\"comment\">#输出是【5，1】但是1被压缩</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.],\n        [16., 17., 18., 19.]])\ntensor([0., 1., 2., 3.])\n\n\n\n\n\ntensor([ 14.,  38.,  62.,  86., 110.])\n</code></pre>\n<h2 id=\"3-9-矩阵-矩阵乘法\"><a href=\"#3-9-矩阵-矩阵乘法\" class=\"headerlink\" title=\"3.9 矩阵-矩阵乘法\"></a>3.9 矩阵-矩阵乘法</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(A)</span><br><span class=\"line\">B=torch.ones(<span class=\"number\">4</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">torch.mm(A,B)</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.],\n        [16., 17., 18., 19.]])\n\n\n\n\n\ntensor([[ 6.,  6.,  6.],\n        [22., 22., 22.],\n        [38., 38., 38.],\n        [54., 54., 54.],\n        [70., 70., 70.]])\n</code></pre>\n<h2 id=\"3-10-范数\"><a href=\"#3-10-范数\" class=\"headerlink\" title=\"3.10 范数\"></a>3.10 范数</h2><ul>\n<li>表示一个向量有多大。这里考虑的大小概念不涉及维度，而是分量的大小。</li>\n</ul>\n<h3 id=\"3-10-1-向量范数\"><a href=\"#3-10-1-向量范数\" class=\"headerlink\" title=\"3.10.1 向量范数\"></a>3.10.1 向量范数</h3><ul>\n<li>将向量x映射到标量的函数$f$。</li>\n<li>向量范数必须满足以下性质：<ul>\n<li>1）按常数因子α缩放向量的所有元素，其范数也会按相同常数因子的绝对值缩放：<br>  $$f(\\alpha x)&#x3D;|\\alpha|f(x)$$</li>\n<li>2）三角不等式：<br>  $$f(x+y)\\leq f(x)+f(y)$$</li>\n<li>3）非负性：<br>  $$f(x)\\geq 0$$</li>\n<li>4）范数最小为0，当且仅当向量全由0组成：<br>  $$\\forall i,[x]_i&#x3D;0 \\Leftrightarrow f(x)&#x3D;0 $$</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"3-10-2-L2范数\"><a href=\"#3-10-2-L2范数\" class=\"headerlink\" title=\"3.10.2 L2范数\"></a>3.10.2 L2范数</h3><ul>\n<li>平方和的平方根：<br>$$ ||x||&#x3D;||x||_ 2 &#x3D; \\sqrt{ \\sum \\limits_ {i&#x3D;1}^ {n}x_ i^ 2}$$</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">u=torch.tensor([<span class=\"number\">3.0</span>,-<span class=\"number\">4.0</span>])</span><br><span class=\"line\">torch.norm(u) <span class=\"comment\">#范数</span></span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>tensor(5.)\n</code></pre>\n<h3 id=\"3-10-3-L1范数\"><a href=\"#3-10-3-L1范数\" class=\"headerlink\" title=\"3.10.3 L1范数\"></a>3.10.3 L1范数</h3><ul>\n<li>绝对值之和<br>$$ ||x||_ 1 &#x3D; \\sum \\limits_ {i&#x3D;1}^ {n}|x_ i|$$</li>\n<li>与L2范数相比，L1范数受异常值的影响较小</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.<span class=\"built_in\">abs</span>(u).<span class=\"built_in\">sum</span>() <span class=\"comment\">#L1范数</span></span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>tensor(7.)\n</code></pre>\n<h3 id=\"3-10-4-Lp范数\"><a href=\"#3-10-4-Lp范数\" class=\"headerlink\" title=\"3.10.4 Lp范数\"></a>3.10.4 Lp范数</h3><p>$$||x||_ p &#x3D; ( \\sum \\limits_ {i&#x3D;1}^ {n}|x_ i|^ p)^ { \\frac{1}{p}}$$</p>\n<h3 id=\"3-10-5-Frobenius范数\"><a href=\"#3-10-5-Frobenius范数\" class=\"headerlink\" title=\"3.10.5 Frobenius范数\"></a>3.10.5 Frobenius范数</h3><ul>\n<li>矩阵L2范数<br>$$||X||_ F &#x3D; \\sqrt{ \\sum \\limits_ {i&#x3D;1}^ {m} \\sum \\limits_ {j&#x3D;1}^ {n}x_ {ij}^ 2}$$</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.norm(torch.ones((<span class=\"number\">4</span>,<span class=\"number\">9</span>)))</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>tensor(6.)\n</code></pre>\n<ul>\n<li>目标，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数。</li>\n</ul>\n<h2 id=\"3-11-练习\"><a href=\"#3-11-练习\" class=\"headerlink\" title=\"3.11 练习\"></a>3.11 练习</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=torch.tensor([[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>],[<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>]])</span><br><span class=\"line\">b=a.<span class=\"built_in\">sum</span>(axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">c=a.<span class=\"built_in\">sum</span>(axis=<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(a.shape,b.shape,c.shape)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(a+b) <span class=\"comment\">#b-&gt;(2)-&gt;(1,2)-&gt;(2,3)</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(a+c) <span class=\"comment\">#c-&gt;(3)-&gt;(1,3)-&gt;不匹配</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>torch.Size([2, 3]) torch.Size([3]) torch.Size([2])\ntensor([[ 6,  9, 12],\n        [ 9, 12, 15]])\n\n\n\n---------------------------------------------------------------------------\n\nRuntimeError                              Traceback (most recent call last)\n\nCell In[37], line 7\n      4 print(a.shape,b.shape,c.shape)\n      6 print(a+b) #b-&gt;(2)-&gt;(1,2)-&gt;(2,3)\n----&gt; 7 print(a+c) #c-&gt;(3)-&gt;(1,3)-&gt;不匹配\n\n\nRuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1\n</code></pre>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"3-线性代数\"><a href=\"#3-线性代数\" class=\"headerlink\" title=\"3. 线性代数\"></a>3. 线性代数</h1><h2 id=\"3-1-标量\"><a href=\"#3-1-标量\" class=\"headerlink\" title=\"3.1 标量\"></a>3.1 标量</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"></span><br><span class=\"line\">x=torch.tensor(<span class=\"number\">3.0</span>)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"3-2-向量\"><a href=\"#3-2-向量\" class=\"headerlink\" title=\"3.2 向量\"></a>3.2 向量</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=torch.arange(<span class=\"number\">4</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#1 取值</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(x[<span class=\"number\">3</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#2 长度</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">len</span>(x))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.shape)</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor(3)\n4\ntorch.Size([4])\n</code></pre>\n<h2 id=\"3-3-矩阵\"><a href=\"#3-3-矩阵\" class=\"headerlink\" title=\"3.3 矩阵\"></a>3.3 矩阵</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A=torch.arange(<span class=\"number\">20</span>).reshape(<span class=\"number\">5</span>,<span class=\"number\">4</span>) <span class=\"comment\">#矩阵用大写字母</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(A)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#1 转置</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(A.T)</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11],\n        [12, 13, 14, 15],\n        [16, 17, 18, 19]])\ntensor([[ 0,  4,  8, 12, 16],\n        [ 1,  5,  9, 13, 17],\n        [ 2,  6, 10, 14, 18],\n        [ 3,  7, 11, 15, 19]])\n</code></pre>\n<h2 id=\"3-4-张量\"><a href=\"#3-4-张量\" class=\"headerlink\" title=\"3.4 张量\"></a>3.4 张量</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X= torch.arange(<span class=\"number\">24</span>).reshape(<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\">X</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>tensor([[[ 0,  1,  2,  3],\n         [ 4,  5,  6,  7],\n         [ 8,  9, 10, 11]],\n\n        [[12, 13, 14, 15],\n         [16, 17, 18, 19],\n         [20, 21, 22, 23]]])\n</code></pre>\n<h2 id=\"3-5-张量算法的基本性质\"><a href=\"#3-5-张量算法的基本性质\" class=\"headerlink\" title=\"3.5 张量算法的基本性质\"></a>3.5 张量算法的基本性质</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A=torch.arange(<span class=\"number\">20</span>,dtype=torch.float32).reshape(<span class=\"number\">5</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#1 复制</span></span><br><span class=\"line\">B=A.clone()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(A==B)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#2 Hadamard积⊙ (对应位置相乘)</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(A*B)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#3 张量+标量（所有元素加上标量）</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(A+<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#4 元素个数</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(A.numel())</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])\ntensor([[  0.,   1.,   4.,   9.],\n        [ 16.,  25.,  36.,  49.],\n        [ 64.,  81., 100., 121.],\n        [144., 169., 196., 225.],\n        [256., 289., 324., 361.]])\ntensor([[ 2.,  3.,  4.,  5.],\n        [ 6.,  7.,  8.,  9.],\n        [10., 11., 12., 13.],\n        [14., 15., 16., 17.],\n        [18., 19., 20., 21.]])\n20\n</code></pre>\n<h2 id=\"3-6-降维\"><a href=\"#3-6-降维\" class=\"headerlink\" title=\"3.6 降维\"></a>3.6 降维</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X= torch.arange(<span class=\"number\">4</span>,dtype=torch.float32).reshape(<span class=\"number\">2</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#1 按轴求和</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(X.<span class=\"built_in\">sum</span>(axis=<span class=\"number\">0</span>)) <span class=\"comment\">#压缩掉第0维</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#2 按轴求平均</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(X.mean(axis=<span class=\"number\">0</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#3 非降维求和</span></span><br><span class=\"line\">sum_X=X.<span class=\"built_in\">sum</span>(axis=<span class=\"number\">0</span>,keepdims=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(sum_X) <span class=\"comment\">#还是二维，只是第0维的长度为1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#4 沿某个轴计算A元素的累积总和</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(A.cumsum(axis=<span class=\"number\">0</span>))</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([2., 4.])\ntensor([1., 2.])\ntensor([[2., 4.]])\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  6.,  8., 10.],\n        [12., 15., 18., 21.],\n        [24., 28., 32., 36.],\n        [40., 45., 50., 55.]])\n</code></pre>\n<h2 id=\"3-7-点积\"><a href=\"#3-7-点积\" class=\"headerlink\" title=\"3.7 点积\"></a>3.7 点积</h2><ul>\n<li>$$&lt;x,y&gt;&#x3D; x^Ty&#x3D; \\Sigma^{d}_{i&#x3D;1}x_iy_i$$</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x=y=torch.arange(<span class=\"number\">4</span>,dtype=torch.float32)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x,y,torch.dot(x,y),sep=<span class=\"string\">&#x27;\\n&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([0., 1., 2., 3.])\ntensor([0., 1., 2., 3.])\ntensor(14.)\n</code></pre>\n<h2 id=\"3-8-矩阵-向量积\"><a href=\"#3-8-矩阵-向量积\" class=\"headerlink\" title=\"3.8 矩阵-向量积\"></a>3.8 矩阵-向量积</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(A)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x)</span><br><span class=\"line\">torch.mv(A,x) <span class=\"comment\">#输出是【5，1】但是1被压缩</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.],\n        [16., 17., 18., 19.]])\ntensor([0., 1., 2., 3.])\n\n\n\n\n\ntensor([ 14.,  38.,  62.,  86., 110.])\n</code></pre>\n<h2 id=\"3-9-矩阵-矩阵乘法\"><a href=\"#3-9-矩阵-矩阵乘法\" class=\"headerlink\" title=\"3.9 矩阵-矩阵乘法\"></a>3.9 矩阵-矩阵乘法</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(A)</span><br><span class=\"line\">B=torch.ones(<span class=\"number\">4</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">torch.mm(A,B)</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.],\n        [16., 17., 18., 19.]])\n\n\n\n\n\ntensor([[ 6.,  6.,  6.],\n        [22., 22., 22.],\n        [38., 38., 38.],\n        [54., 54., 54.],\n        [70., 70., 70.]])\n</code></pre>\n<h2 id=\"3-10-范数\"><a href=\"#3-10-范数\" class=\"headerlink\" title=\"3.10 范数\"></a>3.10 范数</h2><ul>\n<li>表示一个向量有多大。这里考虑的大小概念不涉及维度，而是分量的大小。</li>\n</ul>\n<h3 id=\"3-10-1-向量范数\"><a href=\"#3-10-1-向量范数\" class=\"headerlink\" title=\"3.10.1 向量范数\"></a>3.10.1 向量范数</h3><ul>\n<li>将向量x映射到标量的函数$f$。</li>\n<li>向量范数必须满足以下性质：<ul>\n<li>1）按常数因子α缩放向量的所有元素，其范数也会按相同常数因子的绝对值缩放：<br>  $$f(\\alpha x)&#x3D;|\\alpha|f(x)$$</li>\n<li>2）三角不等式：<br>  $$f(x+y)\\leq f(x)+f(y)$$</li>\n<li>3）非负性：<br>  $$f(x)\\geq 0$$</li>\n<li>4）范数最小为0，当且仅当向量全由0组成：<br>  $$\\forall i,[x]_i&#x3D;0 \\Leftrightarrow f(x)&#x3D;0 $$</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"3-10-2-L2范数\"><a href=\"#3-10-2-L2范数\" class=\"headerlink\" title=\"3.10.2 L2范数\"></a>3.10.2 L2范数</h3><ul>\n<li>平方和的平方根：<br>$$ ||x||&#x3D;||x||_ 2 &#x3D; \\sqrt{ \\sum \\limits_ {i&#x3D;1}^ {n}x_ i^ 2}$$</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">u=torch.tensor([<span class=\"number\">3.0</span>,-<span class=\"number\">4.0</span>])</span><br><span class=\"line\">torch.norm(u) <span class=\"comment\">#范数</span></span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>tensor(5.)\n</code></pre>\n<h3 id=\"3-10-3-L1范数\"><a href=\"#3-10-3-L1范数\" class=\"headerlink\" title=\"3.10.3 L1范数\"></a>3.10.3 L1范数</h3><ul>\n<li>绝对值之和<br>$$ ||x||_ 1 &#x3D; \\sum \\limits_ {i&#x3D;1}^ {n}|x_ i|$$</li>\n<li>与L2范数相比，L1范数受异常值的影响较小</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.<span class=\"built_in\">abs</span>(u).<span class=\"built_in\">sum</span>() <span class=\"comment\">#L1范数</span></span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>tensor(7.)\n</code></pre>\n<h3 id=\"3-10-4-Lp范数\"><a href=\"#3-10-4-Lp范数\" class=\"headerlink\" title=\"3.10.4 Lp范数\"></a>3.10.4 Lp范数</h3><p>$$||x||_ p &#x3D; ( \\sum \\limits_ {i&#x3D;1}^ {n}|x_ i|^ p)^ { \\frac{1}{p}}$$</p>\n<h3 id=\"3-10-5-Frobenius范数\"><a href=\"#3-10-5-Frobenius范数\" class=\"headerlink\" title=\"3.10.5 Frobenius范数\"></a>3.10.5 Frobenius范数</h3><ul>\n<li>矩阵L2范数<br>$$||X||_ F &#x3D; \\sqrt{ \\sum \\limits_ {i&#x3D;1}^ {m} \\sum \\limits_ {j&#x3D;1}^ {n}x_ {ij}^ 2}$$</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.norm(torch.ones((<span class=\"number\">4</span>,<span class=\"number\">9</span>)))</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>tensor(6.)\n</code></pre>\n<ul>\n<li>目标，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数。</li>\n</ul>\n<h2 id=\"3-11-练习\"><a href=\"#3-11-练习\" class=\"headerlink\" title=\"3.11 练习\"></a>3.11 练习</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=torch.tensor([[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>],[<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>]])</span><br><span class=\"line\">b=a.<span class=\"built_in\">sum</span>(axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">c=a.<span class=\"built_in\">sum</span>(axis=<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(a.shape,b.shape,c.shape)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(a+b) <span class=\"comment\">#b-&gt;(2)-&gt;(1,2)-&gt;(2,3)</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(a+c) <span class=\"comment\">#c-&gt;(3)-&gt;(1,3)-&gt;不匹配</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>torch.Size([2, 3]) torch.Size([3]) torch.Size([2])\ntensor([[ 6,  9, 12],\n        [ 9, 12, 15]])\n\n\n\n---------------------------------------------------------------------------\n\nRuntimeError                              Traceback (most recent call last)\n\nCell In[37], line 7\n      4 print(a.shape,b.shape,c.shape)\n      6 print(a+b) #b-&gt;(2)-&gt;(1,2)-&gt;(2,3)\n----&gt; 7 print(a+c) #c-&gt;(3)-&gt;(1,3)-&gt;不匹配\n\n\nRuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1\n</code></pre>"},{"title":"1.4 微积分","date":"2024-02-01T06:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 4. 微积分\n- 拟合模型的任务分解为两个关键问题：\n    - 优化（optimization）：用模型拟合观测数据的过程；\n    - 泛化（generalization）：数学原理和实践者的智慧，能够指导我们生成出有效性超出用于训练的数据集本身的模型。\n## 4.1 导数和微分\n- 通常选择对于模型参数可微的损失函数。简而言之，对于每个参数，如果我们把这个参数增加或减少一个无穷小的量，可以知道损失会以多快的速度增加或减少。\n- 导数：$f'(x)=\\lim\\limits_{h\\rightarrow 0}\\frac{f(x+h)-f(x)}{h}=y^\\prime=\\frac{dy}{dx}=\\frac{df}{dx}=\\frac{d}{dx}f(x)=Df(x)=D_x f(x)$\n    - 1)常数相乘法则：$(Cf)^\\prime=C(f^\\prime)$\n    - 2)加法法则：$(f+g)^\\prime=f^\\prime+g^\\prime$   \n    - 3)乘法法则：$(fg)^\\prime=f^\\prime g+fg^\\prime$\n    - 4)除法法则：$(\\frac{f}{g})^\\prime=\\frac{f^\\prime g-fg^\\prime}{g^2}$ \n- 微分：若$f^\\prime(a)$存在，则称$f$在$a$处可微\n\n\n\n```python\n#将图形嵌入到Notebook中 %matplotlib inline\n\nimport os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\" #这两行可以解决崩溃的错误\nfrom IPython.display import set_matplotlib_formats\nimport numpy as np\nimport matplotlib.pyplot as plt\n#from matplotlib_inline import backend_inline\nfrom d2l import torch as d2l\n\n#1 定义一个函数f\ndef f(x):\n    return 3 * x ** 2 - 4 * x\n\n#2 定义f'\ndef numerical_lim(f, x, h):\n    return (f(x + h) - f(x)) / h\n    \n#一些函数\ndef use_svg_display(): #@save\n    \"\"\"使用svg格式在Jupyter中显示绘图\"\"\"\n    set_matplotlib_formats('svg')\ndef set_figsize(figsize=(3.5, 2.5)): #@save\n    \"\"\"设置matplotlib的图表大小\"\"\"\n    use_svg_display()\n    d2l.plt.rcParams['figure.figsize'] = figsize\n#@save\ndef set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n    \"\"\"设置matplotlib的轴\"\"\"\n    axes.set_xlabel(xlabel)\n    axes.set_ylabel(ylabel)\n    axes.set_xscale(xscale)\n    axes.set_yscale(yscale)\n    axes.set_xlim(xlim)\n    axes.set_ylim(ylim)\n    if legend:\n        axes.legend(legend)\n    axes.grid()\n#@save\ndef plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None,ylim=None, xscale='linear', yscale='linear',fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):\n    \"\"\"绘制数据点\"\"\"\n    if legend is None:\n        legend = []\n    set_figsize(figsize)\n    axes = axes if axes else d2l.plt.gca()\n    # 如果X有一个轴，输出True\n\n    def has_one_axis(X):\n        return (hasattr(X, \"ndim\") and X.ndim == 1 or isinstance(X, list) and not hasattr(X[0], \"__len__\"))\n    if has_one_axis(X):\n        X = [X]\n    if Y is None:\n        X, Y = [[]] * len(X), X\n    elif has_one_axis(Y):\n        Y = [Y]\n    if len(X) != len(Y):\n        X = X * len(Y)\n    axes.cla()\n    for x, y, fmt in zip(X, Y, fmts):\n        if len(x):\n            axes.plot(x, y, fmt)\n        else:\n            axes.plot(y, fmt)\n    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n\n#3 绘制切线\nx = np.arange(0, 3, 0.1)\nplot(x, [f(x), 2 * x - 3], 'x', 'f(x)', legend=['f(x)', 'Tangent line (x=1)'])\nplt.show()\n```\n\n    C:\\Users\\123\\AppData\\Local\\Temp\\ipykernel_13552\\2373785819.py:22: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n      set_matplotlib_formats('svg')\n    \n\n\n    \n![svg](4_calculus_files/4_calculus_1_1.svg)\n![](img/deeplearning/code/pytorch/1_prepare/4_calculus_1_1.svg)\n    \n\n\n## 4.2 偏导数\n- 偏导数：$f(x_1,x_2,...,x_n)$关于$x_i$的偏导数：$\\frac{\\partial f}{\\partial x_i}=\\lim\\limits_{h\\rightarrow 0}\\frac{f(x_1,...,x_i+h,...,x_n)-f(x_1,...,x_i,...,x_n)}{h}$\n\n## 4.3 梯度\n- 梯度：$\\nabla_x f(x)=[\\frac{\\partial f(x)}{\\partial x_1},\\frac{\\partial f(x)}{\\partial x_2},...,\\frac{\\partial f(x)}{\\partial x_n}]^T$\n- 一些性质：\n    - $\\forall A\\in R^{m\\times n},\\nabla_x(Ax)=A^T$\n    - $\\forall A\\in R^{m\\times n},\\nabla_x(x^TA)=A$\n    - $\\forall A\\in R^{m\\times n},\\nabla_x(x^TAx)=(A+A^T)x$\n    - $\\nabla_x||x||_F^2=\\nabla_x(x^Tx)=2x$\n\n## 4.4 链式法则\n- $y=f(u_1,u_2,...,u_m),u_i=g_i(x_1,x_2,...,x_n),则\\frac{\\partial y}{\\partial x_i}=\\sum\\limits_{j=1}^m\\frac{\\partial y}{\\partial u_j}\\frac{\\partial u_j}{\\partial x_i}$\n\n## 练习\n- 绘制函数$y=X^3-\\frac{1}{X}$在x=1处的图像\n\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef f(x):\n    return x**3 - 1/x\nx=np.arange(0,3,0.1)\nplt.plot(x,f(x))\nplt.plot(x,3*x**2+1/x**2-4)\nplt.show()\n```\n\n    C:\\Users\\123\\AppData\\Local\\Temp\\ipykernel_13552\\2157169762.py:5: RuntimeWarning: divide by zero encountered in divide\n      return x**3 - 1/x\n    C:\\Users\\123\\AppData\\Local\\Temp\\ipykernel_13552\\2157169762.py:8: RuntimeWarning: divide by zero encountered in divide\n      plt.plot(x,3*x**2+1/x**2-4)\n    \n\n\n    \n![svg](4_calculus_files/4_calculus_3_1.svg)\n![](img/deeplearning/code/pytorch/1_prepare/4_calculus_3_1.svg)\n    \n\n","source":"_posts/deeplearning/code/pytorch/1_prepare/4_calculus.md","raw":"---\ntitle: 1.4 微积分\ndate: 2024-2-1 14:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 4. 微积分\n- 拟合模型的任务分解为两个关键问题：\n    - 优化（optimization）：用模型拟合观测数据的过程；\n    - 泛化（generalization）：数学原理和实践者的智慧，能够指导我们生成出有效性超出用于训练的数据集本身的模型。\n## 4.1 导数和微分\n- 通常选择对于模型参数可微的损失函数。简而言之，对于每个参数，如果我们把这个参数增加或减少一个无穷小的量，可以知道损失会以多快的速度增加或减少。\n- 导数：$f'(x)=\\lim\\limits_{h\\rightarrow 0}\\frac{f(x+h)-f(x)}{h}=y^\\prime=\\frac{dy}{dx}=\\frac{df}{dx}=\\frac{d}{dx}f(x)=Df(x)=D_x f(x)$\n    - 1)常数相乘法则：$(Cf)^\\prime=C(f^\\prime)$\n    - 2)加法法则：$(f+g)^\\prime=f^\\prime+g^\\prime$   \n    - 3)乘法法则：$(fg)^\\prime=f^\\prime g+fg^\\prime$\n    - 4)除法法则：$(\\frac{f}{g})^\\prime=\\frac{f^\\prime g-fg^\\prime}{g^2}$ \n- 微分：若$f^\\prime(a)$存在，则称$f$在$a$处可微\n\n\n\n```python\n#将图形嵌入到Notebook中 %matplotlib inline\n\nimport os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\" #这两行可以解决崩溃的错误\nfrom IPython.display import set_matplotlib_formats\nimport numpy as np\nimport matplotlib.pyplot as plt\n#from matplotlib_inline import backend_inline\nfrom d2l import torch as d2l\n\n#1 定义一个函数f\ndef f(x):\n    return 3 * x ** 2 - 4 * x\n\n#2 定义f'\ndef numerical_lim(f, x, h):\n    return (f(x + h) - f(x)) / h\n    \n#一些函数\ndef use_svg_display(): #@save\n    \"\"\"使用svg格式在Jupyter中显示绘图\"\"\"\n    set_matplotlib_formats('svg')\ndef set_figsize(figsize=(3.5, 2.5)): #@save\n    \"\"\"设置matplotlib的图表大小\"\"\"\n    use_svg_display()\n    d2l.plt.rcParams['figure.figsize'] = figsize\n#@save\ndef set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n    \"\"\"设置matplotlib的轴\"\"\"\n    axes.set_xlabel(xlabel)\n    axes.set_ylabel(ylabel)\n    axes.set_xscale(xscale)\n    axes.set_yscale(yscale)\n    axes.set_xlim(xlim)\n    axes.set_ylim(ylim)\n    if legend:\n        axes.legend(legend)\n    axes.grid()\n#@save\ndef plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None,ylim=None, xscale='linear', yscale='linear',fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):\n    \"\"\"绘制数据点\"\"\"\n    if legend is None:\n        legend = []\n    set_figsize(figsize)\n    axes = axes if axes else d2l.plt.gca()\n    # 如果X有一个轴，输出True\n\n    def has_one_axis(X):\n        return (hasattr(X, \"ndim\") and X.ndim == 1 or isinstance(X, list) and not hasattr(X[0], \"__len__\"))\n    if has_one_axis(X):\n        X = [X]\n    if Y is None:\n        X, Y = [[]] * len(X), X\n    elif has_one_axis(Y):\n        Y = [Y]\n    if len(X) != len(Y):\n        X = X * len(Y)\n    axes.cla()\n    for x, y, fmt in zip(X, Y, fmts):\n        if len(x):\n            axes.plot(x, y, fmt)\n        else:\n            axes.plot(y, fmt)\n    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n\n#3 绘制切线\nx = np.arange(0, 3, 0.1)\nplot(x, [f(x), 2 * x - 3], 'x', 'f(x)', legend=['f(x)', 'Tangent line (x=1)'])\nplt.show()\n```\n\n    C:\\Users\\123\\AppData\\Local\\Temp\\ipykernel_13552\\2373785819.py:22: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n      set_matplotlib_formats('svg')\n    \n\n\n    \n![svg](4_calculus_files/4_calculus_1_1.svg)\n![](img/deeplearning/code/pytorch/1_prepare/4_calculus_1_1.svg)\n    \n\n\n## 4.2 偏导数\n- 偏导数：$f(x_1,x_2,...,x_n)$关于$x_i$的偏导数：$\\frac{\\partial f}{\\partial x_i}=\\lim\\limits_{h\\rightarrow 0}\\frac{f(x_1,...,x_i+h,...,x_n)-f(x_1,...,x_i,...,x_n)}{h}$\n\n## 4.3 梯度\n- 梯度：$\\nabla_x f(x)=[\\frac{\\partial f(x)}{\\partial x_1},\\frac{\\partial f(x)}{\\partial x_2},...,\\frac{\\partial f(x)}{\\partial x_n}]^T$\n- 一些性质：\n    - $\\forall A\\in R^{m\\times n},\\nabla_x(Ax)=A^T$\n    - $\\forall A\\in R^{m\\times n},\\nabla_x(x^TA)=A$\n    - $\\forall A\\in R^{m\\times n},\\nabla_x(x^TAx)=(A+A^T)x$\n    - $\\nabla_x||x||_F^2=\\nabla_x(x^Tx)=2x$\n\n## 4.4 链式法则\n- $y=f(u_1,u_2,...,u_m),u_i=g_i(x_1,x_2,...,x_n),则\\frac{\\partial y}{\\partial x_i}=\\sum\\limits_{j=1}^m\\frac{\\partial y}{\\partial u_j}\\frac{\\partial u_j}{\\partial x_i}$\n\n## 练习\n- 绘制函数$y=X^3-\\frac{1}{X}$在x=1处的图像\n\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef f(x):\n    return x**3 - 1/x\nx=np.arange(0,3,0.1)\nplt.plot(x,f(x))\nplt.plot(x,3*x**2+1/x**2-4)\nplt.show()\n```\n\n    C:\\Users\\123\\AppData\\Local\\Temp\\ipykernel_13552\\2157169762.py:5: RuntimeWarning: divide by zero encountered in divide\n      return x**3 - 1/x\n    C:\\Users\\123\\AppData\\Local\\Temp\\ipykernel_13552\\2157169762.py:8: RuntimeWarning: divide by zero encountered in divide\n      plt.plot(x,3*x**2+1/x**2-4)\n    \n\n\n    \n![svg](4_calculus_files/4_calculus_3_1.svg)\n![](img/deeplearning/code/pytorch/1_prepare/4_calculus_3_1.svg)\n    \n\n","slug":"deeplearning/code/pytorch/1_prepare/4_calculus","published":1,"updated":"2024-05-02T08:56:28.462Z","_id":"clvuli5vy0010zsvw6s0yhr6g","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"4-微积分\"><a href=\"#4-微积分\" class=\"headerlink\" title=\"4. 微积分\"></a>4. 微积分</h1><ul>\n<li>拟合模型的任务分解为两个关键问题：<ul>\n<li>优化（optimization）：用模型拟合观测数据的过程；</li>\n<li>泛化（generalization）：数学原理和实践者的智慧，能够指导我们生成出有效性超出用于训练的数据集本身的模型。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"4-1-导数和微分\"><a href=\"#4-1-导数和微分\" class=\"headerlink\" title=\"4.1 导数和微分\"></a>4.1 导数和微分</h2><ul>\n<li>通常选择对于模型参数可微的损失函数。简而言之，对于每个参数，如果我们把这个参数增加或减少一个无穷小的量，可以知道损失会以多快的速度增加或减少。</li>\n<li>导数：$f’(x)&#x3D;\\lim\\limits_{h\\rightarrow 0}\\frac{f(x+h)-f(x)}{h}&#x3D;y^\\prime&#x3D;\\frac{dy}{dx}&#x3D;\\frac{df}{dx}&#x3D;\\frac{d}{dx}f(x)&#x3D;Df(x)&#x3D;D_x f(x)$<ul>\n<li>1)常数相乘法则：$(Cf)^\\prime&#x3D;C(f^\\prime)$</li>\n<li>2)加法法则：$(f+g)^\\prime&#x3D;f^\\prime+g^\\prime$   </li>\n<li>3)乘法法则：$(fg)^\\prime&#x3D;f^\\prime g+fg^\\prime$</li>\n<li>4)除法法则：$(\\frac{f}{g})^\\prime&#x3D;\\frac{f^\\prime g-fg^\\prime}{g^2}$</li>\n</ul>\n</li>\n<li>微分：若$f^\\prime(a)$存在，则称$f$在$a$处可微</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#将图形嵌入到Notebook中 %matplotlib inline</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>]=<span class=\"string\">&quot;TRUE&quot;</span> <span class=\"comment\">#这两行可以解决崩溃的错误</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> IPython.display <span class=\"keyword\">import</span> set_matplotlib_formats</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"comment\">#from matplotlib_inline import backend_inline</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#1 定义一个函数f</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">f</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">3</span> * x ** <span class=\"number\">2</span> - <span class=\"number\">4</span> * x</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#2 定义f&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">numerical_lim</span>(<span class=\"params\">f, x, h</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> (f(x + h) - f(x)) / h</span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"comment\">#一些函数</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">use_svg_display</span>(): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;使用svg格式在Jupyter中显示绘图&quot;&quot;&quot;</span></span><br><span class=\"line\">    set_matplotlib_formats(<span class=\"string\">&#x27;svg&#x27;</span>)</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">set_figsize</span>(<span class=\"params\">figsize=(<span class=\"params\"><span class=\"number\">3.5</span>, <span class=\"number\">2.5</span></span>)</span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;设置matplotlib的图表大小&quot;&quot;&quot;</span></span><br><span class=\"line\">    use_svg_display()</span><br><span class=\"line\">    d2l.plt.rcParams[<span class=\"string\">&#x27;figure.figsize&#x27;</span>] = figsize</span><br><span class=\"line\"><span class=\"comment\">#@save</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">set_axes</span>(<span class=\"params\">axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;设置matplotlib的轴&quot;&quot;&quot;</span></span><br><span class=\"line\">    axes.set_xlabel(xlabel)</span><br><span class=\"line\">    axes.set_ylabel(ylabel)</span><br><span class=\"line\">    axes.set_xscale(xscale)</span><br><span class=\"line\">    axes.set_yscale(yscale)</span><br><span class=\"line\">    axes.set_xlim(xlim)</span><br><span class=\"line\">    axes.set_ylim(ylim)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> legend:</span><br><span class=\"line\">        axes.legend(legend)</span><br><span class=\"line\">    axes.grid()</span><br><span class=\"line\"><span class=\"comment\">#@save</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">plot</span>(<span class=\"params\">X, Y=<span class=\"literal\">None</span>, xlabel=<span class=\"literal\">None</span>, ylabel=<span class=\"literal\">None</span>, legend=<span class=\"literal\">None</span>, xlim=<span class=\"literal\">None</span>,ylim=<span class=\"literal\">None</span>, xscale=<span class=\"string\">&#x27;linear&#x27;</span>, yscale=<span class=\"string\">&#x27;linear&#x27;</span>,fmts=(<span class=\"params\"><span class=\"string\">&#x27;-&#x27;</span>, <span class=\"string\">&#x27;m--&#x27;</span>, <span class=\"string\">&#x27;g-.&#x27;</span>, <span class=\"string\">&#x27;r:&#x27;</span></span>), figsize=(<span class=\"params\"><span class=\"number\">3.5</span>, <span class=\"number\">2.5</span></span>), axes=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;绘制数据点&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> legend <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        legend = []</span><br><span class=\"line\">    set_figsize(figsize)</span><br><span class=\"line\">    axes = axes <span class=\"keyword\">if</span> axes <span class=\"keyword\">else</span> d2l.plt.gca()</span><br><span class=\"line\">    <span class=\"comment\"># 如果X有一个轴，输出True</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">has_one_axis</span>(<span class=\"params\">X</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> (<span class=\"built_in\">hasattr</span>(X, <span class=\"string\">&quot;ndim&quot;</span>) <span class=\"keyword\">and</span> X.ndim == <span class=\"number\">1</span> <span class=\"keyword\">or</span> <span class=\"built_in\">isinstance</span>(X, <span class=\"built_in\">list</span>) <span class=\"keyword\">and</span> <span class=\"keyword\">not</span> <span class=\"built_in\">hasattr</span>(X[<span class=\"number\">0</span>], <span class=\"string\">&quot;__len__&quot;</span>))</span><br><span class=\"line\">    <span class=\"keyword\">if</span> has_one_axis(X):</span><br><span class=\"line\">        X = [X]</span><br><span class=\"line\">    <span class=\"keyword\">if</span> Y <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        X, Y = [[]] * <span class=\"built_in\">len</span>(X), X</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> has_one_axis(Y):</span><br><span class=\"line\">        Y = [Y]</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(X) != <span class=\"built_in\">len</span>(Y):</span><br><span class=\"line\">        X = X * <span class=\"built_in\">len</span>(Y)</span><br><span class=\"line\">    axes.cla()</span><br><span class=\"line\">    <span class=\"keyword\">for</span> x, y, fmt <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(X, Y, fmts):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(x):</span><br><span class=\"line\">            axes.plot(x, y, fmt)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            axes.plot(y, fmt)</span><br><span class=\"line\">    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#3 绘制切线</span></span><br><span class=\"line\">x = np.arange(<span class=\"number\">0</span>, <span class=\"number\">3</span>, <span class=\"number\">0.1</span>)</span><br><span class=\"line\">plot(x, [f(x), <span class=\"number\">2</span> * x - <span class=\"number\">3</span>], <span class=\"string\">&#x27;x&#x27;</span>, <span class=\"string\">&#x27;f(x)&#x27;</span>, legend=[<span class=\"string\">&#x27;f(x)&#x27;</span>, <span class=\"string\">&#x27;Tangent line (x=1)&#x27;</span>])</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n<pre><code>C:\\Users\\123\\AppData\\Local\\Temp\\ipykernel_13552\\2373785819.py:22: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n  set_matplotlib_formats(&#39;svg&#39;)\n</code></pre>\n<p><img src=\"/4_calculus_files/4_calculus_1_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/1_prepare/4_calculus_1_1.svg\"></p>\n<h2 id=\"4-2-偏导数\"><a href=\"#4-2-偏导数\" class=\"headerlink\" title=\"4.2 偏导数\"></a>4.2 偏导数</h2><ul>\n<li>偏导数：$f(x_1,x_2,…,x_n)$关于$x_i$的偏导数：$\\frac{\\partial f}{\\partial x_i}&#x3D;\\lim\\limits_{h\\rightarrow 0}\\frac{f(x_1,…,x_i+h,…,x_n)-f(x_1,…,x_i,…,x_n)}{h}$</li>\n</ul>\n<h2 id=\"4-3-梯度\"><a href=\"#4-3-梯度\" class=\"headerlink\" title=\"4.3 梯度\"></a>4.3 梯度</h2><ul>\n<li>梯度：$\\nabla_x f(x)&#x3D;[\\frac{\\partial f(x)}{\\partial x_1},\\frac{\\partial f(x)}{\\partial x_2},…,\\frac{\\partial f(x)}{\\partial x_n}]^T$</li>\n<li>一些性质：<ul>\n<li>$\\forall A\\in R^{m\\times n},\\nabla_x(Ax)&#x3D;A^T$</li>\n<li>$\\forall A\\in R^{m\\times n},\\nabla_x(x^TA)&#x3D;A$</li>\n<li>$\\forall A\\in R^{m\\times n},\\nabla_x(x^TAx)&#x3D;(A+A^T)x$</li>\n<li>$\\nabla_x||x||_F^2&#x3D;\\nabla_x(x^Tx)&#x3D;2x$</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"4-4-链式法则\"><a href=\"#4-4-链式法则\" class=\"headerlink\" title=\"4.4 链式法则\"></a>4.4 链式法则</h2><ul>\n<li>$y&#x3D;f(u_1,u_2,…,u_m),u_i&#x3D;g_i(x_1,x_2,…,x_n),则\\frac{\\partial y}{\\partial x_i}&#x3D;\\sum\\limits_{j&#x3D;1}^m\\frac{\\partial y}{\\partial u_j}\\frac{\\partial u_j}{\\partial x_i}$</li>\n</ul>\n<h2 id=\"练习\"><a href=\"#练习\" class=\"headerlink\" title=\"练习\"></a>练习</h2><ul>\n<li>绘制函数$y&#x3D;X^3-\\frac{1}{X}$在x&#x3D;1处的图像</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">f</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> x**<span class=\"number\">3</span> - <span class=\"number\">1</span>/x</span><br><span class=\"line\">x=np.arange(<span class=\"number\">0</span>,<span class=\"number\">3</span>,<span class=\"number\">0.1</span>)</span><br><span class=\"line\">plt.plot(x,f(x))</span><br><span class=\"line\">plt.plot(x,<span class=\"number\">3</span>*x**<span class=\"number\">2</span>+<span class=\"number\">1</span>/x**<span class=\"number\">2</span>-<span class=\"number\">4</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n<pre><code>C:\\Users\\123\\AppData\\Local\\Temp\\ipykernel_13552\\2157169762.py:5: RuntimeWarning: divide by zero encountered in divide\n  return x**3 - 1/x\nC:\\Users\\123\\AppData\\Local\\Temp\\ipykernel_13552\\2157169762.py:8: RuntimeWarning: divide by zero encountered in divide\n  plt.plot(x,3*x**2+1/x**2-4)\n</code></pre>\n<p><img src=\"/4_calculus_files/4_calculus_3_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/1_prepare/4_calculus_3_1.svg\"></p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"4-微积分\"><a href=\"#4-微积分\" class=\"headerlink\" title=\"4. 微积分\"></a>4. 微积分</h1><ul>\n<li>拟合模型的任务分解为两个关键问题：<ul>\n<li>优化（optimization）：用模型拟合观测数据的过程；</li>\n<li>泛化（generalization）：数学原理和实践者的智慧，能够指导我们生成出有效性超出用于训练的数据集本身的模型。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"4-1-导数和微分\"><a href=\"#4-1-导数和微分\" class=\"headerlink\" title=\"4.1 导数和微分\"></a>4.1 导数和微分</h2><ul>\n<li>通常选择对于模型参数可微的损失函数。简而言之，对于每个参数，如果我们把这个参数增加或减少一个无穷小的量，可以知道损失会以多快的速度增加或减少。</li>\n<li>导数：$f’(x)&#x3D;\\lim\\limits_{h\\rightarrow 0}\\frac{f(x+h)-f(x)}{h}&#x3D;y^\\prime&#x3D;\\frac{dy}{dx}&#x3D;\\frac{df}{dx}&#x3D;\\frac{d}{dx}f(x)&#x3D;Df(x)&#x3D;D_x f(x)$<ul>\n<li>1)常数相乘法则：$(Cf)^\\prime&#x3D;C(f^\\prime)$</li>\n<li>2)加法法则：$(f+g)^\\prime&#x3D;f^\\prime+g^\\prime$   </li>\n<li>3)乘法法则：$(fg)^\\prime&#x3D;f^\\prime g+fg^\\prime$</li>\n<li>4)除法法则：$(\\frac{f}{g})^\\prime&#x3D;\\frac{f^\\prime g-fg^\\prime}{g^2}$</li>\n</ul>\n</li>\n<li>微分：若$f^\\prime(a)$存在，则称$f$在$a$处可微</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#将图形嵌入到Notebook中 %matplotlib inline</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>]=<span class=\"string\">&quot;TRUE&quot;</span> <span class=\"comment\">#这两行可以解决崩溃的错误</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> IPython.display <span class=\"keyword\">import</span> set_matplotlib_formats</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"comment\">#from matplotlib_inline import backend_inline</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#1 定义一个函数f</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">f</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">3</span> * x ** <span class=\"number\">2</span> - <span class=\"number\">4</span> * x</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#2 定义f&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">numerical_lim</span>(<span class=\"params\">f, x, h</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> (f(x + h) - f(x)) / h</span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"comment\">#一些函数</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">use_svg_display</span>(): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;使用svg格式在Jupyter中显示绘图&quot;&quot;&quot;</span></span><br><span class=\"line\">    set_matplotlib_formats(<span class=\"string\">&#x27;svg&#x27;</span>)</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">set_figsize</span>(<span class=\"params\">figsize=(<span class=\"params\"><span class=\"number\">3.5</span>, <span class=\"number\">2.5</span></span>)</span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;设置matplotlib的图表大小&quot;&quot;&quot;</span></span><br><span class=\"line\">    use_svg_display()</span><br><span class=\"line\">    d2l.plt.rcParams[<span class=\"string\">&#x27;figure.figsize&#x27;</span>] = figsize</span><br><span class=\"line\"><span class=\"comment\">#@save</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">set_axes</span>(<span class=\"params\">axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;设置matplotlib的轴&quot;&quot;&quot;</span></span><br><span class=\"line\">    axes.set_xlabel(xlabel)</span><br><span class=\"line\">    axes.set_ylabel(ylabel)</span><br><span class=\"line\">    axes.set_xscale(xscale)</span><br><span class=\"line\">    axes.set_yscale(yscale)</span><br><span class=\"line\">    axes.set_xlim(xlim)</span><br><span class=\"line\">    axes.set_ylim(ylim)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> legend:</span><br><span class=\"line\">        axes.legend(legend)</span><br><span class=\"line\">    axes.grid()</span><br><span class=\"line\"><span class=\"comment\">#@save</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">plot</span>(<span class=\"params\">X, Y=<span class=\"literal\">None</span>, xlabel=<span class=\"literal\">None</span>, ylabel=<span class=\"literal\">None</span>, legend=<span class=\"literal\">None</span>, xlim=<span class=\"literal\">None</span>,ylim=<span class=\"literal\">None</span>, xscale=<span class=\"string\">&#x27;linear&#x27;</span>, yscale=<span class=\"string\">&#x27;linear&#x27;</span>,fmts=(<span class=\"params\"><span class=\"string\">&#x27;-&#x27;</span>, <span class=\"string\">&#x27;m--&#x27;</span>, <span class=\"string\">&#x27;g-.&#x27;</span>, <span class=\"string\">&#x27;r:&#x27;</span></span>), figsize=(<span class=\"params\"><span class=\"number\">3.5</span>, <span class=\"number\">2.5</span></span>), axes=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;绘制数据点&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> legend <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        legend = []</span><br><span class=\"line\">    set_figsize(figsize)</span><br><span class=\"line\">    axes = axes <span class=\"keyword\">if</span> axes <span class=\"keyword\">else</span> d2l.plt.gca()</span><br><span class=\"line\">    <span class=\"comment\"># 如果X有一个轴，输出True</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">has_one_axis</span>(<span class=\"params\">X</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> (<span class=\"built_in\">hasattr</span>(X, <span class=\"string\">&quot;ndim&quot;</span>) <span class=\"keyword\">and</span> X.ndim == <span class=\"number\">1</span> <span class=\"keyword\">or</span> <span class=\"built_in\">isinstance</span>(X, <span class=\"built_in\">list</span>) <span class=\"keyword\">and</span> <span class=\"keyword\">not</span> <span class=\"built_in\">hasattr</span>(X[<span class=\"number\">0</span>], <span class=\"string\">&quot;__len__&quot;</span>))</span><br><span class=\"line\">    <span class=\"keyword\">if</span> has_one_axis(X):</span><br><span class=\"line\">        X = [X]</span><br><span class=\"line\">    <span class=\"keyword\">if</span> Y <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        X, Y = [[]] * <span class=\"built_in\">len</span>(X), X</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> has_one_axis(Y):</span><br><span class=\"line\">        Y = [Y]</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(X) != <span class=\"built_in\">len</span>(Y):</span><br><span class=\"line\">        X = X * <span class=\"built_in\">len</span>(Y)</span><br><span class=\"line\">    axes.cla()</span><br><span class=\"line\">    <span class=\"keyword\">for</span> x, y, fmt <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(X, Y, fmts):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(x):</span><br><span class=\"line\">            axes.plot(x, y, fmt)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            axes.plot(y, fmt)</span><br><span class=\"line\">    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#3 绘制切线</span></span><br><span class=\"line\">x = np.arange(<span class=\"number\">0</span>, <span class=\"number\">3</span>, <span class=\"number\">0.1</span>)</span><br><span class=\"line\">plot(x, [f(x), <span class=\"number\">2</span> * x - <span class=\"number\">3</span>], <span class=\"string\">&#x27;x&#x27;</span>, <span class=\"string\">&#x27;f(x)&#x27;</span>, legend=[<span class=\"string\">&#x27;f(x)&#x27;</span>, <span class=\"string\">&#x27;Tangent line (x=1)&#x27;</span>])</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n<pre><code>C:\\Users\\123\\AppData\\Local\\Temp\\ipykernel_13552\\2373785819.py:22: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n  set_matplotlib_formats(&#39;svg&#39;)\n</code></pre>\n<p><img src=\"/4_calculus_files/4_calculus_1_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/1_prepare/4_calculus_1_1.svg\"></p>\n<h2 id=\"4-2-偏导数\"><a href=\"#4-2-偏导数\" class=\"headerlink\" title=\"4.2 偏导数\"></a>4.2 偏导数</h2><ul>\n<li>偏导数：$f(x_1,x_2,…,x_n)$关于$x_i$的偏导数：$\\frac{\\partial f}{\\partial x_i}&#x3D;\\lim\\limits_{h\\rightarrow 0}\\frac{f(x_1,…,x_i+h,…,x_n)-f(x_1,…,x_i,…,x_n)}{h}$</li>\n</ul>\n<h2 id=\"4-3-梯度\"><a href=\"#4-3-梯度\" class=\"headerlink\" title=\"4.3 梯度\"></a>4.3 梯度</h2><ul>\n<li>梯度：$\\nabla_x f(x)&#x3D;[\\frac{\\partial f(x)}{\\partial x_1},\\frac{\\partial f(x)}{\\partial x_2},…,\\frac{\\partial f(x)}{\\partial x_n}]^T$</li>\n<li>一些性质：<ul>\n<li>$\\forall A\\in R^{m\\times n},\\nabla_x(Ax)&#x3D;A^T$</li>\n<li>$\\forall A\\in R^{m\\times n},\\nabla_x(x^TA)&#x3D;A$</li>\n<li>$\\forall A\\in R^{m\\times n},\\nabla_x(x^TAx)&#x3D;(A+A^T)x$</li>\n<li>$\\nabla_x||x||_F^2&#x3D;\\nabla_x(x^Tx)&#x3D;2x$</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"4-4-链式法则\"><a href=\"#4-4-链式法则\" class=\"headerlink\" title=\"4.4 链式法则\"></a>4.4 链式法则</h2><ul>\n<li>$y&#x3D;f(u_1,u_2,…,u_m),u_i&#x3D;g_i(x_1,x_2,…,x_n),则\\frac{\\partial y}{\\partial x_i}&#x3D;\\sum\\limits_{j&#x3D;1}^m\\frac{\\partial y}{\\partial u_j}\\frac{\\partial u_j}{\\partial x_i}$</li>\n</ul>\n<h2 id=\"练习\"><a href=\"#练习\" class=\"headerlink\" title=\"练习\"></a>练习</h2><ul>\n<li>绘制函数$y&#x3D;X^3-\\frac{1}{X}$在x&#x3D;1处的图像</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">f</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> x**<span class=\"number\">3</span> - <span class=\"number\">1</span>/x</span><br><span class=\"line\">x=np.arange(<span class=\"number\">0</span>,<span class=\"number\">3</span>,<span class=\"number\">0.1</span>)</span><br><span class=\"line\">plt.plot(x,f(x))</span><br><span class=\"line\">plt.plot(x,<span class=\"number\">3</span>*x**<span class=\"number\">2</span>+<span class=\"number\">1</span>/x**<span class=\"number\">2</span>-<span class=\"number\">4</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n<pre><code>C:\\Users\\123\\AppData\\Local\\Temp\\ipykernel_13552\\2157169762.py:5: RuntimeWarning: divide by zero encountered in divide\n  return x**3 - 1/x\nC:\\Users\\123\\AppData\\Local\\Temp\\ipykernel_13552\\2157169762.py:8: RuntimeWarning: divide by zero encountered in divide\n  plt.plot(x,3*x**2+1/x**2-4)\n</code></pre>\n<p><img src=\"/4_calculus_files/4_calculus_3_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/1_prepare/4_calculus_3_1.svg\"></p>"},{"title":"1.6 概率","date":"2024-02-02T06:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 6. 概率\n## 6.1 基本概率论\n### 6.1.1 一点代码\n- 笼统来说，可以把分布（distribution）看作对事件的概率分配\n\n\n```python\n#一下两行代码可选，有时候会出现崩溃\nimport os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\nimport torch\nfrom torch.distributions import multinomial\nfrom d2l import torch as d2l\nimport matplotlib.pyplot as plt\n\n#1 抽样\nfair_probs = torch.ones([6])/6 #六个数，每个数的概率都是1/6\ns = multinomial.Multinomial(99999, fair_probs).sample() #抽样99999次,根据概率分布抽样，抽中的数字对应的位置为+1，其他位置为0\nprint(s)\n#分组抽样\ncounts = multinomial.Multinomial(10,fair_probs).sample((500,)) #500组实验，每组抽样10个样本\ncum_counts = counts.cumsum(dim=0) #累加\nestimates = cum_counts/cum_counts.sum(dim=1,keepdims=True) #计算频率,第一组10个样本，计算频率；第二组10+10个，计算频率\n\n#2 绘图 越到后面，采样越多，估计的概率越接近真实概率\nd2l.set_figsize((6,4.5))\nfor i in range(6):\n    d2l.plt.plot(estimates[:,i].numpy(),label=(\"P(die=\" + str(i+1) + \")\"))\nd2l.plt.axhline(y=0.167, color='black', linestyle='dashed')\nd2l.plt.gca().set_xlabel('Groups of experiments')\nd2l.plt.gca().set_ylabel('Estimated probability')\nd2l.plt.legend()\n```\n\n    tensor([16569., 16678., 16493., 16636., 16712., 16911.])\n    \n\n\n\n\n    <matplotlib.legend.Legend at 0x16dc26d9460>\n\n\n\n\n    \n![svg](6_probability_files/6_probability_1_2.svg)\n![](img/deeplearning/code/pytorch/1_prepare/6_probability_files/6_probability_1_2.svg)\n    \n\n\n### 6.1.2 概率论公理\n- 非负性：对于任意事件A，P(A) >= 0\n- 规范性：P(Ω) = 1\n- 可列可加性：对于任意两个不相容事件A和B，P(A∪B) = P(A) + P(B)\n### 6.1.3 随机变量\n## 6.2 处理多个随机变量\n### 6.2.1 联合概率\n- $P(A,B) = P(A|B)P(B)$\n### 6.2.2 条件概率\n- $P(A|B) = \\frac{P(A,B)}{P(B)}$\n### 6.2.3 贝叶斯定理\n- $P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$\n### 6.2.4 边缘化\n- $P(A) = \\sum\\limits_{B}P(A,B)$\n- 边际化结果的概率或分布称为边际概率（marginal probability）或边际分布（marginal distribution）\n### 6.2.5 独立性\n- $A \\perp B \\Rightarrow P(A,B) = P(A)P(B)$\n- $A \\perp B|C \\Rightarrow P(A,B|C) = P(A|C)P(B|C)$\n## 6.3 期望和方差\n- 期望：$E_{x\\sim P}[f(x)] = \\sum\\limits_{x}P(x)f(x)$\n- 方差：$Var(f(x)) = E[(f(x) - E[f(x)])^2] = E[f(x)^2] - E[f(x)]^2$\n","source":"_posts/deeplearning/code/pytorch/1_prepare/6_probability.md","raw":"---\ntitle: 1.6 概率\ndate: 2024-2-2 14:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 6. 概率\n## 6.1 基本概率论\n### 6.1.1 一点代码\n- 笼统来说，可以把分布（distribution）看作对事件的概率分配\n\n\n```python\n#一下两行代码可选，有时候会出现崩溃\nimport os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\nimport torch\nfrom torch.distributions import multinomial\nfrom d2l import torch as d2l\nimport matplotlib.pyplot as plt\n\n#1 抽样\nfair_probs = torch.ones([6])/6 #六个数，每个数的概率都是1/6\ns = multinomial.Multinomial(99999, fair_probs).sample() #抽样99999次,根据概率分布抽样，抽中的数字对应的位置为+1，其他位置为0\nprint(s)\n#分组抽样\ncounts = multinomial.Multinomial(10,fair_probs).sample((500,)) #500组实验，每组抽样10个样本\ncum_counts = counts.cumsum(dim=0) #累加\nestimates = cum_counts/cum_counts.sum(dim=1,keepdims=True) #计算频率,第一组10个样本，计算频率；第二组10+10个，计算频率\n\n#2 绘图 越到后面，采样越多，估计的概率越接近真实概率\nd2l.set_figsize((6,4.5))\nfor i in range(6):\n    d2l.plt.plot(estimates[:,i].numpy(),label=(\"P(die=\" + str(i+1) + \")\"))\nd2l.plt.axhline(y=0.167, color='black', linestyle='dashed')\nd2l.plt.gca().set_xlabel('Groups of experiments')\nd2l.plt.gca().set_ylabel('Estimated probability')\nd2l.plt.legend()\n```\n\n    tensor([16569., 16678., 16493., 16636., 16712., 16911.])\n    \n\n\n\n\n    <matplotlib.legend.Legend at 0x16dc26d9460>\n\n\n\n\n    \n![svg](6_probability_files/6_probability_1_2.svg)\n![](img/deeplearning/code/pytorch/1_prepare/6_probability_files/6_probability_1_2.svg)\n    \n\n\n### 6.1.2 概率论公理\n- 非负性：对于任意事件A，P(A) >= 0\n- 规范性：P(Ω) = 1\n- 可列可加性：对于任意两个不相容事件A和B，P(A∪B) = P(A) + P(B)\n### 6.1.3 随机变量\n## 6.2 处理多个随机变量\n### 6.2.1 联合概率\n- $P(A,B) = P(A|B)P(B)$\n### 6.2.2 条件概率\n- $P(A|B) = \\frac{P(A,B)}{P(B)}$\n### 6.2.3 贝叶斯定理\n- $P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$\n### 6.2.4 边缘化\n- $P(A) = \\sum\\limits_{B}P(A,B)$\n- 边际化结果的概率或分布称为边际概率（marginal probability）或边际分布（marginal distribution）\n### 6.2.5 独立性\n- $A \\perp B \\Rightarrow P(A,B) = P(A)P(B)$\n- $A \\perp B|C \\Rightarrow P(A,B|C) = P(A|C)P(B|C)$\n## 6.3 期望和方差\n- 期望：$E_{x\\sim P}[f(x)] = \\sum\\limits_{x}P(x)f(x)$\n- 方差：$Var(f(x)) = E[(f(x) - E[f(x)])^2] = E[f(x)^2] - E[f(x)]^2$\n","slug":"deeplearning/code/pytorch/1_prepare/6_probability","published":1,"updated":"2024-05-02T08:56:48.243Z","_id":"clvuli5zc0014zsvwb31setbq","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"6-概率\"><a href=\"#6-概率\" class=\"headerlink\" title=\"6. 概率\"></a>6. 概率</h1><h2 id=\"6-1-基本概率论\"><a href=\"#6-1-基本概率论\" class=\"headerlink\" title=\"6.1 基本概率论\"></a>6.1 基本概率论</h2><h3 id=\"6-1-1-一点代码\"><a href=\"#6-1-1-一点代码\" class=\"headerlink\" title=\"6.1.1 一点代码\"></a>6.1.1 一点代码</h3><ul>\n<li>笼统来说，可以把分布（distribution）看作对事件的概率分配</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#一下两行代码可选，有时候会出现崩溃</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>]=<span class=\"string\">&quot;TRUE&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.distributions <span class=\"keyword\">import</span> multinomial</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#1 抽样</span></span><br><span class=\"line\">fair_probs = torch.ones([<span class=\"number\">6</span>])/<span class=\"number\">6</span> <span class=\"comment\">#六个数，每个数的概率都是1/6</span></span><br><span class=\"line\">s = multinomial.Multinomial(<span class=\"number\">99999</span>, fair_probs).sample() <span class=\"comment\">#抽样99999次,根据概率分布抽样，抽中的数字对应的位置为+1，其他位置为0</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(s)</span><br><span class=\"line\"><span class=\"comment\">#分组抽样</span></span><br><span class=\"line\">counts = multinomial.Multinomial(<span class=\"number\">10</span>,fair_probs).sample((<span class=\"number\">500</span>,)) <span class=\"comment\">#500组实验，每组抽样10个样本</span></span><br><span class=\"line\">cum_counts = counts.cumsum(dim=<span class=\"number\">0</span>) <span class=\"comment\">#累加</span></span><br><span class=\"line\">estimates = cum_counts/cum_counts.<span class=\"built_in\">sum</span>(dim=<span class=\"number\">1</span>,keepdims=<span class=\"literal\">True</span>) <span class=\"comment\">#计算频率,第一组10个样本，计算频率；第二组10+10个，计算频率</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#2 绘图 越到后面，采样越多，估计的概率越接近真实概率</span></span><br><span class=\"line\">d2l.set_figsize((<span class=\"number\">6</span>,<span class=\"number\">4.5</span>))</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">6</span>):</span><br><span class=\"line\">    d2l.plt.plot(estimates[:,i].numpy(),label=(<span class=\"string\">&quot;P(die=&quot;</span> + <span class=\"built_in\">str</span>(i+<span class=\"number\">1</span>) + <span class=\"string\">&quot;)&quot;</span>))</span><br><span class=\"line\">d2l.plt.axhline(y=<span class=\"number\">0.167</span>, color=<span class=\"string\">&#x27;black&#x27;</span>, linestyle=<span class=\"string\">&#x27;dashed&#x27;</span>)</span><br><span class=\"line\">d2l.plt.gca().set_xlabel(<span class=\"string\">&#x27;Groups of experiments&#x27;</span>)</span><br><span class=\"line\">d2l.plt.gca().set_ylabel(<span class=\"string\">&#x27;Estimated probability&#x27;</span>)</span><br><span class=\"line\">d2l.plt.legend()</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([16569., 16678., 16493., 16636., 16712., 16911.])\n\n\n\n\n\n&lt;matplotlib.legend.Legend at 0x16dc26d9460&gt;\n</code></pre>\n<p><img src=\"/6_probability_files/6_probability_1_2.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/1_prepare/6_probability_files/6_probability_1_2.svg\"></p>\n<h3 id=\"6-1-2-概率论公理\"><a href=\"#6-1-2-概率论公理\" class=\"headerlink\" title=\"6.1.2 概率论公理\"></a>6.1.2 概率论公理</h3><ul>\n<li>非负性：对于任意事件A，P(A) &gt;&#x3D; 0</li>\n<li>规范性：P(Ω) &#x3D; 1</li>\n<li>可列可加性：对于任意两个不相容事件A和B，P(A∪B) &#x3D; P(A) + P(B)</li>\n</ul>\n<h3 id=\"6-1-3-随机变量\"><a href=\"#6-1-3-随机变量\" class=\"headerlink\" title=\"6.1.3 随机变量\"></a>6.1.3 随机变量</h3><h2 id=\"6-2-处理多个随机变量\"><a href=\"#6-2-处理多个随机变量\" class=\"headerlink\" title=\"6.2 处理多个随机变量\"></a>6.2 处理多个随机变量</h2><h3 id=\"6-2-1-联合概率\"><a href=\"#6-2-1-联合概率\" class=\"headerlink\" title=\"6.2.1 联合概率\"></a>6.2.1 联合概率</h3><ul>\n<li>$P(A,B) &#x3D; P(A|B)P(B)$</li>\n</ul>\n<h3 id=\"6-2-2-条件概率\"><a href=\"#6-2-2-条件概率\" class=\"headerlink\" title=\"6.2.2 条件概率\"></a>6.2.2 条件概率</h3><ul>\n<li>$P(A|B) &#x3D; \\frac{P(A,B)}{P(B)}$</li>\n</ul>\n<h3 id=\"6-2-3-贝叶斯定理\"><a href=\"#6-2-3-贝叶斯定理\" class=\"headerlink\" title=\"6.2.3 贝叶斯定理\"></a>6.2.3 贝叶斯定理</h3><ul>\n<li>$P(A|B) &#x3D; \\frac{P(B|A)P(A)}{P(B)}$</li>\n</ul>\n<h3 id=\"6-2-4-边缘化\"><a href=\"#6-2-4-边缘化\" class=\"headerlink\" title=\"6.2.4 边缘化\"></a>6.2.4 边缘化</h3><ul>\n<li>$P(A) &#x3D; \\sum\\limits_{B}P(A,B)$</li>\n<li>边际化结果的概率或分布称为边际概率（marginal probability）或边际分布（marginal distribution）</li>\n</ul>\n<h3 id=\"6-2-5-独立性\"><a href=\"#6-2-5-独立性\" class=\"headerlink\" title=\"6.2.5 独立性\"></a>6.2.5 独立性</h3><ul>\n<li>$A \\perp B \\Rightarrow P(A,B) &#x3D; P(A)P(B)$</li>\n<li>$A \\perp B|C \\Rightarrow P(A,B|C) &#x3D; P(A|C)P(B|C)$</li>\n</ul>\n<h2 id=\"6-3-期望和方差\"><a href=\"#6-3-期望和方差\" class=\"headerlink\" title=\"6.3 期望和方差\"></a>6.3 期望和方差</h2><ul>\n<li>期望：$E_{x\\sim P}[f(x)] &#x3D; \\sum\\limits_{x}P(x)f(x)$</li>\n<li>方差：$Var(f(x)) &#x3D; E[(f(x) - E[f(x)])^2] &#x3D; E[f(x)^2] - E[f(x)]^2$</li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"6-概率\"><a href=\"#6-概率\" class=\"headerlink\" title=\"6. 概率\"></a>6. 概率</h1><h2 id=\"6-1-基本概率论\"><a href=\"#6-1-基本概率论\" class=\"headerlink\" title=\"6.1 基本概率论\"></a>6.1 基本概率论</h2><h3 id=\"6-1-1-一点代码\"><a href=\"#6-1-1-一点代码\" class=\"headerlink\" title=\"6.1.1 一点代码\"></a>6.1.1 一点代码</h3><ul>\n<li>笼统来说，可以把分布（distribution）看作对事件的概率分配</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#一下两行代码可选，有时候会出现崩溃</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>]=<span class=\"string\">&quot;TRUE&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.distributions <span class=\"keyword\">import</span> multinomial</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#1 抽样</span></span><br><span class=\"line\">fair_probs = torch.ones([<span class=\"number\">6</span>])/<span class=\"number\">6</span> <span class=\"comment\">#六个数，每个数的概率都是1/6</span></span><br><span class=\"line\">s = multinomial.Multinomial(<span class=\"number\">99999</span>, fair_probs).sample() <span class=\"comment\">#抽样99999次,根据概率分布抽样，抽中的数字对应的位置为+1，其他位置为0</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(s)</span><br><span class=\"line\"><span class=\"comment\">#分组抽样</span></span><br><span class=\"line\">counts = multinomial.Multinomial(<span class=\"number\">10</span>,fair_probs).sample((<span class=\"number\">500</span>,)) <span class=\"comment\">#500组实验，每组抽样10个样本</span></span><br><span class=\"line\">cum_counts = counts.cumsum(dim=<span class=\"number\">0</span>) <span class=\"comment\">#累加</span></span><br><span class=\"line\">estimates = cum_counts/cum_counts.<span class=\"built_in\">sum</span>(dim=<span class=\"number\">1</span>,keepdims=<span class=\"literal\">True</span>) <span class=\"comment\">#计算频率,第一组10个样本，计算频率；第二组10+10个，计算频率</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#2 绘图 越到后面，采样越多，估计的概率越接近真实概率</span></span><br><span class=\"line\">d2l.set_figsize((<span class=\"number\">6</span>,<span class=\"number\">4.5</span>))</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">6</span>):</span><br><span class=\"line\">    d2l.plt.plot(estimates[:,i].numpy(),label=(<span class=\"string\">&quot;P(die=&quot;</span> + <span class=\"built_in\">str</span>(i+<span class=\"number\">1</span>) + <span class=\"string\">&quot;)&quot;</span>))</span><br><span class=\"line\">d2l.plt.axhline(y=<span class=\"number\">0.167</span>, color=<span class=\"string\">&#x27;black&#x27;</span>, linestyle=<span class=\"string\">&#x27;dashed&#x27;</span>)</span><br><span class=\"line\">d2l.plt.gca().set_xlabel(<span class=\"string\">&#x27;Groups of experiments&#x27;</span>)</span><br><span class=\"line\">d2l.plt.gca().set_ylabel(<span class=\"string\">&#x27;Estimated probability&#x27;</span>)</span><br><span class=\"line\">d2l.plt.legend()</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([16569., 16678., 16493., 16636., 16712., 16911.])\n\n\n\n\n\n&lt;matplotlib.legend.Legend at 0x16dc26d9460&gt;\n</code></pre>\n<p><img src=\"/6_probability_files/6_probability_1_2.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/1_prepare/6_probability_files/6_probability_1_2.svg\"></p>\n<h3 id=\"6-1-2-概率论公理\"><a href=\"#6-1-2-概率论公理\" class=\"headerlink\" title=\"6.1.2 概率论公理\"></a>6.1.2 概率论公理</h3><ul>\n<li>非负性：对于任意事件A，P(A) &gt;&#x3D; 0</li>\n<li>规范性：P(Ω) &#x3D; 1</li>\n<li>可列可加性：对于任意两个不相容事件A和B，P(A∪B) &#x3D; P(A) + P(B)</li>\n</ul>\n<h3 id=\"6-1-3-随机变量\"><a href=\"#6-1-3-随机变量\" class=\"headerlink\" title=\"6.1.3 随机变量\"></a>6.1.3 随机变量</h3><h2 id=\"6-2-处理多个随机变量\"><a href=\"#6-2-处理多个随机变量\" class=\"headerlink\" title=\"6.2 处理多个随机变量\"></a>6.2 处理多个随机变量</h2><h3 id=\"6-2-1-联合概率\"><a href=\"#6-2-1-联合概率\" class=\"headerlink\" title=\"6.2.1 联合概率\"></a>6.2.1 联合概率</h3><ul>\n<li>$P(A,B) &#x3D; P(A|B)P(B)$</li>\n</ul>\n<h3 id=\"6-2-2-条件概率\"><a href=\"#6-2-2-条件概率\" class=\"headerlink\" title=\"6.2.2 条件概率\"></a>6.2.2 条件概率</h3><ul>\n<li>$P(A|B) &#x3D; \\frac{P(A,B)}{P(B)}$</li>\n</ul>\n<h3 id=\"6-2-3-贝叶斯定理\"><a href=\"#6-2-3-贝叶斯定理\" class=\"headerlink\" title=\"6.2.3 贝叶斯定理\"></a>6.2.3 贝叶斯定理</h3><ul>\n<li>$P(A|B) &#x3D; \\frac{P(B|A)P(A)}{P(B)}$</li>\n</ul>\n<h3 id=\"6-2-4-边缘化\"><a href=\"#6-2-4-边缘化\" class=\"headerlink\" title=\"6.2.4 边缘化\"></a>6.2.4 边缘化</h3><ul>\n<li>$P(A) &#x3D; \\sum\\limits_{B}P(A,B)$</li>\n<li>边际化结果的概率或分布称为边际概率（marginal probability）或边际分布（marginal distribution）</li>\n</ul>\n<h3 id=\"6-2-5-独立性\"><a href=\"#6-2-5-独立性\" class=\"headerlink\" title=\"6.2.5 独立性\"></a>6.2.5 独立性</h3><ul>\n<li>$A \\perp B \\Rightarrow P(A,B) &#x3D; P(A)P(B)$</li>\n<li>$A \\perp B|C \\Rightarrow P(A,B|C) &#x3D; P(A|C)P(B|C)$</li>\n</ul>\n<h2 id=\"6-3-期望和方差\"><a href=\"#6-3-期望和方差\" class=\"headerlink\" title=\"6.3 期望和方差\"></a>6.3 期望和方差</h2><ul>\n<li>期望：$E_{x\\sim P}[f(x)] &#x3D; \\sum\\limits_{x}P(x)f(x)$</li>\n<li>方差：$Var(f(x)) &#x3D; E[(f(x) - E[f(x)])^2] &#x3D; E[f(x)^2] - E[f(x)]^2$</li>\n</ul>"},{"title":"1.5 自动微分","date":"2024-02-01T06:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 5. 自动微分\n- 深度学习框架通过自动计算导数，即自动微分（automatic differentiation）来加快求导。根据设计好的模型，系统会构建一个计算图（computational graph），来跟踪计算是哪些数据通过哪些操作组合起来产生输出。自动微分使系统能够随后反向传播梯度。这里，反向传播（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。\n## 5.1 \n- 对函数$y=2\\mathbf{x}^{\\top}\\mathbf{x}$关于列向量$\\mathbf{x}$求导\n\n\n```python\nimport torch\n#1 创建变量x\nx=torch.arange(4.0)\ny=2*torch.dot(x,x)\nprint(x)\n\n#2 存储梯度 （一个标量函数关于向量x的梯度是向量，并且与x具有相同的形状。）\nx.requires_grad_(True) #等价于x=torch.arange(4.0,requires_grad=True)\nprint(x)\n\n#3 计算y\ny=2*torch.dot(x,x) #点积\nprint(y)\n\n#4 反向传播计算梯度\nprint(x.grad) #默认为None\ny.backward() #反向传播\nprint(x.grad) #梯度为4x\n\n#5 阻止跟踪\ny=x.sum() #y=6\ny.backward()\nprint(x.grad) #x的梯度会累加\nx.grad.zero_() #清除梯度后就不会累加了\ny.backward()\nprint(x.grad)\n```\n\n    tensor([0., 1., 2., 3.])\n    tensor([0., 1., 2., 3.], requires_grad=True)\n    tensor(28., grad_fn=<MulBackward0>)\n    None\n    tensor([ 0.,  4.,  8., 12.])\n    tensor([ 1.,  5.,  9., 13.])\n    tensor([1., 1., 1., 1.])\n    \n\n## 5.2 非标量变量的反向传播\n\n\n\n```python\n# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。\n# 本例只想求偏导数的和，所以传递一个1的梯度是合适的\nx.grad.zero_()\ny=x*x #这里y是向量\ny.sum().backward() #这里y.sum()是标量,所以可以调用backward，相当于y.backward(torch.ones(len(x)))\nprint(x.grad)\n\n```\n\n    tensor([0., 2., 4., 6.])\n    \n\n## 5.3 分离计算\n- 将某些计算移动到记录的计算图之外。（作为常数处理）\n    - 假设$y=f(x), z=g(x,y)$。我们想计算z关于x的梯度，但由于某种原因，希望将y视为一个常数，并且只考虑到x在y被计算后发挥的作用。\n\n\n```python\nx.grad.zero_()\ny=x*x\nu=y.detach() #分u相当于一个常数，不需要求梯度\nz=u*x\nz.sum().backward()\nprint(x.grad) #u\n\nx.grad.zero_()\nz=y*x\nz.sum().backward()\nprint(x.grad) #3x^2\n```\n\n    tensor([0., 1., 4., 9.])\n    tensor([ 0.,  3., 12., 27.])\n    \n\n## 5.4 python控制流的梯度计算\n\n\n```python\ndef f(a):\n    b=a*2\n    while b.norm()<1000: #经过while后b=ka\n        b=b*2\n    if b.sum()>0:\n        c=b #c=kb=ka\n    else:\n        c=100*b #c=100kb=100ka\n    return c #ka\na=torch.randn(size=(),requires_grad=True)\nD=f(a) #\nD.backward()\na.grad==D/a #梯度就是k=D/a\n```\n\n\n\n\n    tensor(True)\n\n\n","source":"_posts/deeplearning/code/pytorch/1_prepare/5_autodifferential.md","raw":"---\ntitle: 1.5 自动微分\ndate: 2024-2-1 14:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 5. 自动微分\n- 深度学习框架通过自动计算导数，即自动微分（automatic differentiation）来加快求导。根据设计好的模型，系统会构建一个计算图（computational graph），来跟踪计算是哪些数据通过哪些操作组合起来产生输出。自动微分使系统能够随后反向传播梯度。这里，反向传播（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。\n## 5.1 \n- 对函数$y=2\\mathbf{x}^{\\top}\\mathbf{x}$关于列向量$\\mathbf{x}$求导\n\n\n```python\nimport torch\n#1 创建变量x\nx=torch.arange(4.0)\ny=2*torch.dot(x,x)\nprint(x)\n\n#2 存储梯度 （一个标量函数关于向量x的梯度是向量，并且与x具有相同的形状。）\nx.requires_grad_(True) #等价于x=torch.arange(4.0,requires_grad=True)\nprint(x)\n\n#3 计算y\ny=2*torch.dot(x,x) #点积\nprint(y)\n\n#4 反向传播计算梯度\nprint(x.grad) #默认为None\ny.backward() #反向传播\nprint(x.grad) #梯度为4x\n\n#5 阻止跟踪\ny=x.sum() #y=6\ny.backward()\nprint(x.grad) #x的梯度会累加\nx.grad.zero_() #清除梯度后就不会累加了\ny.backward()\nprint(x.grad)\n```\n\n    tensor([0., 1., 2., 3.])\n    tensor([0., 1., 2., 3.], requires_grad=True)\n    tensor(28., grad_fn=<MulBackward0>)\n    None\n    tensor([ 0.,  4.,  8., 12.])\n    tensor([ 1.,  5.,  9., 13.])\n    tensor([1., 1., 1., 1.])\n    \n\n## 5.2 非标量变量的反向传播\n\n\n\n```python\n# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。\n# 本例只想求偏导数的和，所以传递一个1的梯度是合适的\nx.grad.zero_()\ny=x*x #这里y是向量\ny.sum().backward() #这里y.sum()是标量,所以可以调用backward，相当于y.backward(torch.ones(len(x)))\nprint(x.grad)\n\n```\n\n    tensor([0., 2., 4., 6.])\n    \n\n## 5.3 分离计算\n- 将某些计算移动到记录的计算图之外。（作为常数处理）\n    - 假设$y=f(x), z=g(x,y)$。我们想计算z关于x的梯度，但由于某种原因，希望将y视为一个常数，并且只考虑到x在y被计算后发挥的作用。\n\n\n```python\nx.grad.zero_()\ny=x*x\nu=y.detach() #分u相当于一个常数，不需要求梯度\nz=u*x\nz.sum().backward()\nprint(x.grad) #u\n\nx.grad.zero_()\nz=y*x\nz.sum().backward()\nprint(x.grad) #3x^2\n```\n\n    tensor([0., 1., 4., 9.])\n    tensor([ 0.,  3., 12., 27.])\n    \n\n## 5.4 python控制流的梯度计算\n\n\n```python\ndef f(a):\n    b=a*2\n    while b.norm()<1000: #经过while后b=ka\n        b=b*2\n    if b.sum()>0:\n        c=b #c=kb=ka\n    else:\n        c=100*b #c=100kb=100ka\n    return c #ka\na=torch.randn(size=(),requires_grad=True)\nD=f(a) #\nD.backward()\na.grad==D/a #梯度就是k=D/a\n```\n\n\n\n\n    tensor(True)\n\n\n","slug":"deeplearning/code/pytorch/1_prepare/5_autodifferential","published":1,"updated":"2024-05-02T08:56:38.169Z","_id":"clvuli5zd0015zsvw74e5co72","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"5-自动微分\"><a href=\"#5-自动微分\" class=\"headerlink\" title=\"5. 自动微分\"></a>5. 自动微分</h1><ul>\n<li>深度学习框架通过自动计算导数，即自动微分（automatic differentiation）来加快求导。根据设计好的模型，系统会构建一个计算图（computational graph），来跟踪计算是哪些数据通过哪些操作组合起来产生输出。自动微分使系统能够随后反向传播梯度。这里，反向传播（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。</li>\n</ul>\n<h2 id=\"5-1\"><a href=\"#5-1\" class=\"headerlink\" title=\"5.1\"></a>5.1</h2><ul>\n<li>对函数$y&#x3D;2\\mathbf{x}^{\\top}\\mathbf{x}$关于列向量$\\mathbf{x}$求导</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"comment\">#1 创建变量x</span></span><br><span class=\"line\">x=torch.arange(<span class=\"number\">4.0</span>)</span><br><span class=\"line\">y=<span class=\"number\">2</span>*torch.dot(x,x)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#2 存储梯度 （一个标量函数关于向量x的梯度是向量，并且与x具有相同的形状。）</span></span><br><span class=\"line\">x.requires_grad_(<span class=\"literal\">True</span>) <span class=\"comment\">#等价于x=torch.arange(4.0,requires_grad=True)</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(x)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#3 计算y</span></span><br><span class=\"line\">y=<span class=\"number\">2</span>*torch.dot(x,x) <span class=\"comment\">#点积</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(y)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#4 反向传播计算梯度</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.grad) <span class=\"comment\">#默认为None</span></span><br><span class=\"line\">y.backward() <span class=\"comment\">#反向传播</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.grad) <span class=\"comment\">#梯度为4x</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#5 阻止跟踪</span></span><br><span class=\"line\">y=x.<span class=\"built_in\">sum</span>() <span class=\"comment\">#y=6</span></span><br><span class=\"line\">y.backward()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.grad) <span class=\"comment\">#x的梯度会累加</span></span><br><span class=\"line\">x.grad.zero_() <span class=\"comment\">#清除梯度后就不会累加了</span></span><br><span class=\"line\">y.backward()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.grad)</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([0., 1., 2., 3.])\ntensor([0., 1., 2., 3.], requires_grad=True)\ntensor(28., grad_fn=&lt;MulBackward0&gt;)\nNone\ntensor([ 0.,  4.,  8., 12.])\ntensor([ 1.,  5.,  9., 13.])\ntensor([1., 1., 1., 1.])\n</code></pre>\n<h2 id=\"5-2-非标量变量的反向传播\"><a href=\"#5-2-非标量变量的反向传播\" class=\"headerlink\" title=\"5.2 非标量变量的反向传播\"></a>5.2 非标量变量的反向传播</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。</span></span><br><span class=\"line\"><span class=\"comment\"># 本例只想求偏导数的和，所以传递一个1的梯度是合适的</span></span><br><span class=\"line\">x.grad.zero_()</span><br><span class=\"line\">y=x*x <span class=\"comment\">#这里y是向量</span></span><br><span class=\"line\">y.<span class=\"built_in\">sum</span>().backward() <span class=\"comment\">#这里y.sum()是标量,所以可以调用backward，相当于y.backward(torch.ones(len(x)))</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.grad)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([0., 2., 4., 6.])\n</code></pre>\n<h2 id=\"5-3-分离计算\"><a href=\"#5-3-分离计算\" class=\"headerlink\" title=\"5.3 分离计算\"></a>5.3 分离计算</h2><ul>\n<li>将某些计算移动到记录的计算图之外。（作为常数处理）<ul>\n<li>假设$y&#x3D;f(x), z&#x3D;g(x,y)$。我们想计算z关于x的梯度，但由于某种原因，希望将y视为一个常数，并且只考虑到x在y被计算后发挥的作用。</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x.grad.zero_()</span><br><span class=\"line\">y=x*x</span><br><span class=\"line\">u=y.detach() <span class=\"comment\">#分u相当于一个常数，不需要求梯度</span></span><br><span class=\"line\">z=u*x</span><br><span class=\"line\">z.<span class=\"built_in\">sum</span>().backward()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.grad) <span class=\"comment\">#u</span></span><br><span class=\"line\"></span><br><span class=\"line\">x.grad.zero_()</span><br><span class=\"line\">z=y*x</span><br><span class=\"line\">z.<span class=\"built_in\">sum</span>().backward()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.grad) <span class=\"comment\">#3x^2</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([0., 1., 4., 9.])\ntensor([ 0.,  3., 12., 27.])\n</code></pre>\n<h2 id=\"5-4-python控制流的梯度计算\"><a href=\"#5-4-python控制流的梯度计算\" class=\"headerlink\" title=\"5.4 python控制流的梯度计算\"></a>5.4 python控制流的梯度计算</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">f</span>(<span class=\"params\">a</span>):</span><br><span class=\"line\">    b=a*<span class=\"number\">2</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> b.norm()&lt;<span class=\"number\">1000</span>: <span class=\"comment\">#经过while后b=ka</span></span><br><span class=\"line\">        b=b*<span class=\"number\">2</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> b.<span class=\"built_in\">sum</span>()&gt;<span class=\"number\">0</span>:</span><br><span class=\"line\">        c=b <span class=\"comment\">#c=kb=ka</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        c=<span class=\"number\">100</span>*b <span class=\"comment\">#c=100kb=100ka</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> c <span class=\"comment\">#ka</span></span><br><span class=\"line\">a=torch.randn(size=(),requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">D=f(a) <span class=\"comment\">#</span></span><br><span class=\"line\">D.backward()</span><br><span class=\"line\">a.grad==D/a <span class=\"comment\">#梯度就是k=D/a</span></span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>tensor(True)\n</code></pre>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"5-自动微分\"><a href=\"#5-自动微分\" class=\"headerlink\" title=\"5. 自动微分\"></a>5. 自动微分</h1><ul>\n<li>深度学习框架通过自动计算导数，即自动微分（automatic differentiation）来加快求导。根据设计好的模型，系统会构建一个计算图（computational graph），来跟踪计算是哪些数据通过哪些操作组合起来产生输出。自动微分使系统能够随后反向传播梯度。这里，反向传播（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。</li>\n</ul>\n<h2 id=\"5-1\"><a href=\"#5-1\" class=\"headerlink\" title=\"5.1\"></a>5.1</h2><ul>\n<li>对函数$y&#x3D;2\\mathbf{x}^{\\top}\\mathbf{x}$关于列向量$\\mathbf{x}$求导</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"comment\">#1 创建变量x</span></span><br><span class=\"line\">x=torch.arange(<span class=\"number\">4.0</span>)</span><br><span class=\"line\">y=<span class=\"number\">2</span>*torch.dot(x,x)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#2 存储梯度 （一个标量函数关于向量x的梯度是向量，并且与x具有相同的形状。）</span></span><br><span class=\"line\">x.requires_grad_(<span class=\"literal\">True</span>) <span class=\"comment\">#等价于x=torch.arange(4.0,requires_grad=True)</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(x)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#3 计算y</span></span><br><span class=\"line\">y=<span class=\"number\">2</span>*torch.dot(x,x) <span class=\"comment\">#点积</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(y)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#4 反向传播计算梯度</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.grad) <span class=\"comment\">#默认为None</span></span><br><span class=\"line\">y.backward() <span class=\"comment\">#反向传播</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.grad) <span class=\"comment\">#梯度为4x</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#5 阻止跟踪</span></span><br><span class=\"line\">y=x.<span class=\"built_in\">sum</span>() <span class=\"comment\">#y=6</span></span><br><span class=\"line\">y.backward()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.grad) <span class=\"comment\">#x的梯度会累加</span></span><br><span class=\"line\">x.grad.zero_() <span class=\"comment\">#清除梯度后就不会累加了</span></span><br><span class=\"line\">y.backward()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.grad)</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([0., 1., 2., 3.])\ntensor([0., 1., 2., 3.], requires_grad=True)\ntensor(28., grad_fn=&lt;MulBackward0&gt;)\nNone\ntensor([ 0.,  4.,  8., 12.])\ntensor([ 1.,  5.,  9., 13.])\ntensor([1., 1., 1., 1.])\n</code></pre>\n<h2 id=\"5-2-非标量变量的反向传播\"><a href=\"#5-2-非标量变量的反向传播\" class=\"headerlink\" title=\"5.2 非标量变量的反向传播\"></a>5.2 非标量变量的反向传播</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。</span></span><br><span class=\"line\"><span class=\"comment\"># 本例只想求偏导数的和，所以传递一个1的梯度是合适的</span></span><br><span class=\"line\">x.grad.zero_()</span><br><span class=\"line\">y=x*x <span class=\"comment\">#这里y是向量</span></span><br><span class=\"line\">y.<span class=\"built_in\">sum</span>().backward() <span class=\"comment\">#这里y.sum()是标量,所以可以调用backward，相当于y.backward(torch.ones(len(x)))</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.grad)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([0., 2., 4., 6.])\n</code></pre>\n<h2 id=\"5-3-分离计算\"><a href=\"#5-3-分离计算\" class=\"headerlink\" title=\"5.3 分离计算\"></a>5.3 分离计算</h2><ul>\n<li>将某些计算移动到记录的计算图之外。（作为常数处理）<ul>\n<li>假设$y&#x3D;f(x), z&#x3D;g(x,y)$。我们想计算z关于x的梯度，但由于某种原因，希望将y视为一个常数，并且只考虑到x在y被计算后发挥的作用。</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x.grad.zero_()</span><br><span class=\"line\">y=x*x</span><br><span class=\"line\">u=y.detach() <span class=\"comment\">#分u相当于一个常数，不需要求梯度</span></span><br><span class=\"line\">z=u*x</span><br><span class=\"line\">z.<span class=\"built_in\">sum</span>().backward()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.grad) <span class=\"comment\">#u</span></span><br><span class=\"line\"></span><br><span class=\"line\">x.grad.zero_()</span><br><span class=\"line\">z=y*x</span><br><span class=\"line\">z.<span class=\"built_in\">sum</span>().backward()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.grad) <span class=\"comment\">#3x^2</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([0., 1., 4., 9.])\ntensor([ 0.,  3., 12., 27.])\n</code></pre>\n<h2 id=\"5-4-python控制流的梯度计算\"><a href=\"#5-4-python控制流的梯度计算\" class=\"headerlink\" title=\"5.4 python控制流的梯度计算\"></a>5.4 python控制流的梯度计算</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">f</span>(<span class=\"params\">a</span>):</span><br><span class=\"line\">    b=a*<span class=\"number\">2</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> b.norm()&lt;<span class=\"number\">1000</span>: <span class=\"comment\">#经过while后b=ka</span></span><br><span class=\"line\">        b=b*<span class=\"number\">2</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> b.<span class=\"built_in\">sum</span>()&gt;<span class=\"number\">0</span>:</span><br><span class=\"line\">        c=b <span class=\"comment\">#c=kb=ka</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        c=<span class=\"number\">100</span>*b <span class=\"comment\">#c=100kb=100ka</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> c <span class=\"comment\">#ka</span></span><br><span class=\"line\">a=torch.randn(size=(),requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">D=f(a) <span class=\"comment\">#</span></span><br><span class=\"line\">D.backward()</span><br><span class=\"line\">a.grad==D/a <span class=\"comment\">#梯度就是k=D/a</span></span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>tensor(True)\n</code></pre>"},{"title":"1.7 查阅文档","date":"2024-02-02T06:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 7. 查阅文档\n## 7.1 查找模块中的所有函数和类\n- dir()\n\n\n```python\nimport torch\nprint(dir(torch.distributions))\n```\n\n    ['AbsTransform', 'AffineTransform', 'Bernoulli', 'Beta', 'Binomial', 'CatTransform', 'Categorical', 'Cauchy', 'Chi2', 'ComposeTransform', 'ContinuousBernoulli', 'CorrCholeskyTransform', 'CumulativeDistributionTransform', 'Dirichlet', 'Distribution', 'ExpTransform', 'Exponential', 'ExponentialFamily', 'FisherSnedecor', 'Gamma', 'Geometric', 'Gumbel', 'HalfCauchy', 'HalfNormal', 'Independent', 'IndependentTransform', 'Kumaraswamy', 'LKJCholesky', 'Laplace', 'LogNormal', 'LogisticNormal', 'LowRankMultivariateNormal', 'LowerCholeskyTransform', 'MixtureSameFamily', 'Multinomial', 'MultivariateNormal', 'NegativeBinomial', 'Normal', 'OneHotCategorical', 'OneHotCategoricalStraightThrough', 'Pareto', 'Poisson', 'PowerTransform', 'RelaxedBernoulli', 'RelaxedOneHotCategorical', 'ReshapeTransform', 'SigmoidTransform', 'SoftmaxTransform', 'SoftplusTransform', 'StackTransform', 'StickBreakingTransform', 'StudentT', 'TanhTransform', 'Transform', 'TransformedDistribution', 'Uniform', 'VonMises', 'Weibull', 'Wishart', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'bernoulli', 'beta', 'biject_to', 'binomial', 'categorical', 'cauchy', 'chi2', 'constraint_registry', 'constraints', 'continuous_bernoulli', 'dirichlet', 'distribution', 'exp_family', 'exponential', 'fishersnedecor', 'gamma', 'geometric', 'gumbel', 'half_cauchy', 'half_normal', 'identity_transform', 'independent', 'kl', 'kl_divergence', 'kumaraswamy', 'laplace', 'lkj_cholesky', 'log_normal', 'logistic_normal', 'lowrank_multivariate_normal', 'mixture_same_family', 'multinomial', 'multivariate_normal', 'negative_binomial', 'normal', 'one_hot_categorical', 'pareto', 'poisson', 'register_kl', 'relaxed_bernoulli', 'relaxed_categorical', 'studentT', 'transform_to', 'transformed_distribution', 'transforms', 'uniform', 'utils', 'von_mises', 'weibull', 'wishart']\n    \n\n## 7.2 查找特定函数和类的使用\n- help()\n\n\n```python\nhelp(torch.ones)\n```\n\n    Help on built-in function ones in module torch:\n    \n    ones(...)\n        ones(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n        \n        Returns a tensor filled with the scalar value `1`, with the shape defined\n        by the variable argument :attr:`size`.\n        \n        Args:\n            size (int...): a sequence of integers defining the shape of the output tensor.\n                Can be a variable number of arguments or a collection like a list or tuple.\n        \n        Keyword arguments:\n            out (Tensor, optional): the output tensor.\n            dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n                Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).\n            layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n                Default: ``torch.strided``.\n            device (:class:`torch.device`, optional): the desired device of returned tensor.\n                Default: if ``None``, uses the current device for the default tensor type\n                (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n                for CPU tensor types and the current CUDA device for CUDA tensor types.\n            requires_grad (bool, optional): If autograd should record operations on the\n                returned tensor. Default: ``False``.\n        \n        Example::\n        \n            >>> torch.ones(2, 3)\n            tensor([[ 1.,  1.,  1.],\n                    [ 1.,  1.,  1.]])\n        \n            >>> torch.ones(5)\n            tensor([ 1.,  1.,  1.,  1.,  1.])\n    \n    \n\n\n```python\nlist?\n```\n\n    \u001b[1;31mInit signature:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m/\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n    \u001b[1;31mDocstring:\u001b[0m     \n    Built-in mutable sequence.\n    \n    If no argument is given, the constructor creates a new empty list.\n    The argument must be an iterable if specified.\n    \u001b[1;31mType:\u001b[0m           type\n    \u001b[1;31mSubclasses:\u001b[0m     _HashedSeq, StackSummary, _Threads, ConvertingList, DeferredConfigList, _ymd, SList, ParamSpec, _ConcatenateGenericAlias, _ImmutableLineList, ...\n","source":"_posts/deeplearning/code/pytorch/1_prepare/7_api.md","raw":"---\ntitle: 1.7 查阅文档\ndate: 2024-2-2 14:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 7. 查阅文档\n## 7.1 查找模块中的所有函数和类\n- dir()\n\n\n```python\nimport torch\nprint(dir(torch.distributions))\n```\n\n    ['AbsTransform', 'AffineTransform', 'Bernoulli', 'Beta', 'Binomial', 'CatTransform', 'Categorical', 'Cauchy', 'Chi2', 'ComposeTransform', 'ContinuousBernoulli', 'CorrCholeskyTransform', 'CumulativeDistributionTransform', 'Dirichlet', 'Distribution', 'ExpTransform', 'Exponential', 'ExponentialFamily', 'FisherSnedecor', 'Gamma', 'Geometric', 'Gumbel', 'HalfCauchy', 'HalfNormal', 'Independent', 'IndependentTransform', 'Kumaraswamy', 'LKJCholesky', 'Laplace', 'LogNormal', 'LogisticNormal', 'LowRankMultivariateNormal', 'LowerCholeskyTransform', 'MixtureSameFamily', 'Multinomial', 'MultivariateNormal', 'NegativeBinomial', 'Normal', 'OneHotCategorical', 'OneHotCategoricalStraightThrough', 'Pareto', 'Poisson', 'PowerTransform', 'RelaxedBernoulli', 'RelaxedOneHotCategorical', 'ReshapeTransform', 'SigmoidTransform', 'SoftmaxTransform', 'SoftplusTransform', 'StackTransform', 'StickBreakingTransform', 'StudentT', 'TanhTransform', 'Transform', 'TransformedDistribution', 'Uniform', 'VonMises', 'Weibull', 'Wishart', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'bernoulli', 'beta', 'biject_to', 'binomial', 'categorical', 'cauchy', 'chi2', 'constraint_registry', 'constraints', 'continuous_bernoulli', 'dirichlet', 'distribution', 'exp_family', 'exponential', 'fishersnedecor', 'gamma', 'geometric', 'gumbel', 'half_cauchy', 'half_normal', 'identity_transform', 'independent', 'kl', 'kl_divergence', 'kumaraswamy', 'laplace', 'lkj_cholesky', 'log_normal', 'logistic_normal', 'lowrank_multivariate_normal', 'mixture_same_family', 'multinomial', 'multivariate_normal', 'negative_binomial', 'normal', 'one_hot_categorical', 'pareto', 'poisson', 'register_kl', 'relaxed_bernoulli', 'relaxed_categorical', 'studentT', 'transform_to', 'transformed_distribution', 'transforms', 'uniform', 'utils', 'von_mises', 'weibull', 'wishart']\n    \n\n## 7.2 查找特定函数和类的使用\n- help()\n\n\n```python\nhelp(torch.ones)\n```\n\n    Help on built-in function ones in module torch:\n    \n    ones(...)\n        ones(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n        \n        Returns a tensor filled with the scalar value `1`, with the shape defined\n        by the variable argument :attr:`size`.\n        \n        Args:\n            size (int...): a sequence of integers defining the shape of the output tensor.\n                Can be a variable number of arguments or a collection like a list or tuple.\n        \n        Keyword arguments:\n            out (Tensor, optional): the output tensor.\n            dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n                Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).\n            layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n                Default: ``torch.strided``.\n            device (:class:`torch.device`, optional): the desired device of returned tensor.\n                Default: if ``None``, uses the current device for the default tensor type\n                (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n                for CPU tensor types and the current CUDA device for CUDA tensor types.\n            requires_grad (bool, optional): If autograd should record operations on the\n                returned tensor. Default: ``False``.\n        \n        Example::\n        \n            >>> torch.ones(2, 3)\n            tensor([[ 1.,  1.,  1.],\n                    [ 1.,  1.,  1.]])\n        \n            >>> torch.ones(5)\n            tensor([ 1.,  1.,  1.,  1.,  1.])\n    \n    \n\n\n```python\nlist?\n```\n\n    \u001b[1;31mInit signature:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m/\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n    \u001b[1;31mDocstring:\u001b[0m     \n    Built-in mutable sequence.\n    \n    If no argument is given, the constructor creates a new empty list.\n    The argument must be an iterable if specified.\n    \u001b[1;31mType:\u001b[0m           type\n    \u001b[1;31mSubclasses:\u001b[0m     _HashedSeq, StackSummary, _Threads, ConvertingList, DeferredConfigList, _ymd, SList, ParamSpec, _ConcatenateGenericAlias, _ImmutableLineList, ...\n","slug":"deeplearning/code/pytorch/1_prepare/7_api","published":1,"updated":"2024-05-02T08:56:59.931Z","_id":"clvuli5ze0017zsvwevac35jc","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"7-查阅文档\"><a href=\"#7-查阅文档\" class=\"headerlink\" title=\"7. 查阅文档\"></a>7. 查阅文档</h1><h2 id=\"7-1-查找模块中的所有函数和类\"><a href=\"#7-1-查找模块中的所有函数和类\" class=\"headerlink\" title=\"7.1 查找模块中的所有函数和类\"></a>7.1 查找模块中的所有函数和类</h2><ul>\n<li>dir()</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">dir</span>(torch.distributions))</span><br></pre></td></tr></table></figure>\n\n<pre><code>[&#39;AbsTransform&#39;, &#39;AffineTransform&#39;, &#39;Bernoulli&#39;, &#39;Beta&#39;, &#39;Binomial&#39;, &#39;CatTransform&#39;, &#39;Categorical&#39;, &#39;Cauchy&#39;, &#39;Chi2&#39;, &#39;ComposeTransform&#39;, &#39;ContinuousBernoulli&#39;, &#39;CorrCholeskyTransform&#39;, &#39;CumulativeDistributionTransform&#39;, &#39;Dirichlet&#39;, &#39;Distribution&#39;, &#39;ExpTransform&#39;, &#39;Exponential&#39;, &#39;ExponentialFamily&#39;, &#39;FisherSnedecor&#39;, &#39;Gamma&#39;, &#39;Geometric&#39;, &#39;Gumbel&#39;, &#39;HalfCauchy&#39;, &#39;HalfNormal&#39;, &#39;Independent&#39;, &#39;IndependentTransform&#39;, &#39;Kumaraswamy&#39;, &#39;LKJCholesky&#39;, &#39;Laplace&#39;, &#39;LogNormal&#39;, &#39;LogisticNormal&#39;, &#39;LowRankMultivariateNormal&#39;, &#39;LowerCholeskyTransform&#39;, &#39;MixtureSameFamily&#39;, &#39;Multinomial&#39;, &#39;MultivariateNormal&#39;, &#39;NegativeBinomial&#39;, &#39;Normal&#39;, &#39;OneHotCategorical&#39;, &#39;OneHotCategoricalStraightThrough&#39;, &#39;Pareto&#39;, &#39;Poisson&#39;, &#39;PowerTransform&#39;, &#39;RelaxedBernoulli&#39;, &#39;RelaxedOneHotCategorical&#39;, &#39;ReshapeTransform&#39;, &#39;SigmoidTransform&#39;, &#39;SoftmaxTransform&#39;, &#39;SoftplusTransform&#39;, &#39;StackTransform&#39;, &#39;StickBreakingTransform&#39;, &#39;StudentT&#39;, &#39;TanhTransform&#39;, &#39;Transform&#39;, &#39;TransformedDistribution&#39;, &#39;Uniform&#39;, &#39;VonMises&#39;, &#39;Weibull&#39;, &#39;Wishart&#39;, &#39;__all__&#39;, &#39;__builtins__&#39;, &#39;__cached__&#39;, &#39;__doc__&#39;, &#39;__file__&#39;, &#39;__loader__&#39;, &#39;__name__&#39;, &#39;__package__&#39;, &#39;__path__&#39;, &#39;__spec__&#39;, &#39;bernoulli&#39;, &#39;beta&#39;, &#39;biject_to&#39;, &#39;binomial&#39;, &#39;categorical&#39;, &#39;cauchy&#39;, &#39;chi2&#39;, &#39;constraint_registry&#39;, &#39;constraints&#39;, &#39;continuous_bernoulli&#39;, &#39;dirichlet&#39;, &#39;distribution&#39;, &#39;exp_family&#39;, &#39;exponential&#39;, &#39;fishersnedecor&#39;, &#39;gamma&#39;, &#39;geometric&#39;, &#39;gumbel&#39;, &#39;half_cauchy&#39;, &#39;half_normal&#39;, &#39;identity_transform&#39;, &#39;independent&#39;, &#39;kl&#39;, &#39;kl_divergence&#39;, &#39;kumaraswamy&#39;, &#39;laplace&#39;, &#39;lkj_cholesky&#39;, &#39;log_normal&#39;, &#39;logistic_normal&#39;, &#39;lowrank_multivariate_normal&#39;, &#39;mixture_same_family&#39;, &#39;multinomial&#39;, &#39;multivariate_normal&#39;, &#39;negative_binomial&#39;, &#39;normal&#39;, &#39;one_hot_categorical&#39;, &#39;pareto&#39;, &#39;poisson&#39;, &#39;register_kl&#39;, &#39;relaxed_bernoulli&#39;, &#39;relaxed_categorical&#39;, &#39;studentT&#39;, &#39;transform_to&#39;, &#39;transformed_distribution&#39;, &#39;transforms&#39;, &#39;uniform&#39;, &#39;utils&#39;, &#39;von_mises&#39;, &#39;weibull&#39;, &#39;wishart&#39;]\n</code></pre>\n<h2 id=\"7-2-查找特定函数和类的使用\"><a href=\"#7-2-查找特定函数和类的使用\" class=\"headerlink\" title=\"7.2 查找特定函数和类的使用\"></a>7.2 查找特定函数和类的使用</h2><ul>\n<li>help()</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">help</span>(torch.ones)</span><br></pre></td></tr></table></figure>\n\n<pre><code>Help on built-in function ones in module torch:\n\nones(...)\n    ones(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor\n    \n    Returns a tensor filled with the scalar value `1`, with the shape defined\n    by the variable argument :attr:`size`.\n    \n    Args:\n        size (int...): a sequence of integers defining the shape of the output tensor.\n            Can be a variable number of arguments or a collection like a list or tuple.\n    \n    Keyword arguments:\n        out (Tensor, optional): the output tensor.\n        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n            Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).\n        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n            Default: ``torch.strided``.\n        device (:class:`torch.device`, optional): the desired device of returned tensor.\n            Default: if ``None``, uses the current device for the default tensor type\n            (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n            for CPU tensor types and the current CUDA device for CUDA tensor types.\n        requires_grad (bool, optional): If autograd should record operations on the\n            returned tensor. Default: ``False``.\n    \n    Example::\n    \n        &gt;&gt;&gt; torch.ones(2, 3)\n        tensor([[ 1.,  1.,  1.],\n                [ 1.,  1.,  1.]])\n    \n        &gt;&gt;&gt; torch.ones(5)\n        tensor([ 1.,  1.,  1.,  1.,  1.])\n</code></pre>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">list</span>?</span><br></pre></td></tr></table></figure>\n\n<pre><code>\u001b[1;31mInit signature:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m/\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;31mDocstring:\u001b[0m     \nBuilt-in mutable sequence.\n\nIf no argument is given, the constructor creates a new empty list.\nThe argument must be an iterable if specified.\n\u001b[1;31mType:\u001b[0m           type\n\u001b[1;31mSubclasses:\u001b[0m     _HashedSeq, StackSummary, _Threads, ConvertingList, DeferredConfigList, _ymd, SList, ParamSpec, _ConcatenateGenericAlias, _ImmutableLineList, ...\n</code></pre>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"7-查阅文档\"><a href=\"#7-查阅文档\" class=\"headerlink\" title=\"7. 查阅文档\"></a>7. 查阅文档</h1><h2 id=\"7-1-查找模块中的所有函数和类\"><a href=\"#7-1-查找模块中的所有函数和类\" class=\"headerlink\" title=\"7.1 查找模块中的所有函数和类\"></a>7.1 查找模块中的所有函数和类</h2><ul>\n<li>dir()</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">dir</span>(torch.distributions))</span><br></pre></td></tr></table></figure>\n\n<pre><code>[&#39;AbsTransform&#39;, &#39;AffineTransform&#39;, &#39;Bernoulli&#39;, &#39;Beta&#39;, &#39;Binomial&#39;, &#39;CatTransform&#39;, &#39;Categorical&#39;, &#39;Cauchy&#39;, &#39;Chi2&#39;, &#39;ComposeTransform&#39;, &#39;ContinuousBernoulli&#39;, &#39;CorrCholeskyTransform&#39;, &#39;CumulativeDistributionTransform&#39;, &#39;Dirichlet&#39;, &#39;Distribution&#39;, &#39;ExpTransform&#39;, &#39;Exponential&#39;, &#39;ExponentialFamily&#39;, &#39;FisherSnedecor&#39;, &#39;Gamma&#39;, &#39;Geometric&#39;, &#39;Gumbel&#39;, &#39;HalfCauchy&#39;, &#39;HalfNormal&#39;, &#39;Independent&#39;, &#39;IndependentTransform&#39;, &#39;Kumaraswamy&#39;, &#39;LKJCholesky&#39;, &#39;Laplace&#39;, &#39;LogNormal&#39;, &#39;LogisticNormal&#39;, &#39;LowRankMultivariateNormal&#39;, &#39;LowerCholeskyTransform&#39;, &#39;MixtureSameFamily&#39;, &#39;Multinomial&#39;, &#39;MultivariateNormal&#39;, &#39;NegativeBinomial&#39;, &#39;Normal&#39;, &#39;OneHotCategorical&#39;, &#39;OneHotCategoricalStraightThrough&#39;, &#39;Pareto&#39;, &#39;Poisson&#39;, &#39;PowerTransform&#39;, &#39;RelaxedBernoulli&#39;, &#39;RelaxedOneHotCategorical&#39;, &#39;ReshapeTransform&#39;, &#39;SigmoidTransform&#39;, &#39;SoftmaxTransform&#39;, &#39;SoftplusTransform&#39;, &#39;StackTransform&#39;, &#39;StickBreakingTransform&#39;, &#39;StudentT&#39;, &#39;TanhTransform&#39;, &#39;Transform&#39;, &#39;TransformedDistribution&#39;, &#39;Uniform&#39;, &#39;VonMises&#39;, &#39;Weibull&#39;, &#39;Wishart&#39;, &#39;__all__&#39;, &#39;__builtins__&#39;, &#39;__cached__&#39;, &#39;__doc__&#39;, &#39;__file__&#39;, &#39;__loader__&#39;, &#39;__name__&#39;, &#39;__package__&#39;, &#39;__path__&#39;, &#39;__spec__&#39;, &#39;bernoulli&#39;, &#39;beta&#39;, &#39;biject_to&#39;, &#39;binomial&#39;, &#39;categorical&#39;, &#39;cauchy&#39;, &#39;chi2&#39;, &#39;constraint_registry&#39;, &#39;constraints&#39;, &#39;continuous_bernoulli&#39;, &#39;dirichlet&#39;, &#39;distribution&#39;, &#39;exp_family&#39;, &#39;exponential&#39;, &#39;fishersnedecor&#39;, &#39;gamma&#39;, &#39;geometric&#39;, &#39;gumbel&#39;, &#39;half_cauchy&#39;, &#39;half_normal&#39;, &#39;identity_transform&#39;, &#39;independent&#39;, &#39;kl&#39;, &#39;kl_divergence&#39;, &#39;kumaraswamy&#39;, &#39;laplace&#39;, &#39;lkj_cholesky&#39;, &#39;log_normal&#39;, &#39;logistic_normal&#39;, &#39;lowrank_multivariate_normal&#39;, &#39;mixture_same_family&#39;, &#39;multinomial&#39;, &#39;multivariate_normal&#39;, &#39;negative_binomial&#39;, &#39;normal&#39;, &#39;one_hot_categorical&#39;, &#39;pareto&#39;, &#39;poisson&#39;, &#39;register_kl&#39;, &#39;relaxed_bernoulli&#39;, &#39;relaxed_categorical&#39;, &#39;studentT&#39;, &#39;transform_to&#39;, &#39;transformed_distribution&#39;, &#39;transforms&#39;, &#39;uniform&#39;, &#39;utils&#39;, &#39;von_mises&#39;, &#39;weibull&#39;, &#39;wishart&#39;]\n</code></pre>\n<h2 id=\"7-2-查找特定函数和类的使用\"><a href=\"#7-2-查找特定函数和类的使用\" class=\"headerlink\" title=\"7.2 查找特定函数和类的使用\"></a>7.2 查找特定函数和类的使用</h2><ul>\n<li>help()</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">help</span>(torch.ones)</span><br></pre></td></tr></table></figure>\n\n<pre><code>Help on built-in function ones in module torch:\n\nones(...)\n    ones(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor\n    \n    Returns a tensor filled with the scalar value `1`, with the shape defined\n    by the variable argument :attr:`size`.\n    \n    Args:\n        size (int...): a sequence of integers defining the shape of the output tensor.\n            Can be a variable number of arguments or a collection like a list or tuple.\n    \n    Keyword arguments:\n        out (Tensor, optional): the output tensor.\n        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n            Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).\n        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n            Default: ``torch.strided``.\n        device (:class:`torch.device`, optional): the desired device of returned tensor.\n            Default: if ``None``, uses the current device for the default tensor type\n            (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n            for CPU tensor types and the current CUDA device for CUDA tensor types.\n        requires_grad (bool, optional): If autograd should record operations on the\n            returned tensor. Default: ``False``.\n    \n    Example::\n    \n        &gt;&gt;&gt; torch.ones(2, 3)\n        tensor([[ 1.,  1.,  1.],\n                [ 1.,  1.,  1.]])\n    \n        &gt;&gt;&gt; torch.ones(5)\n        tensor([ 1.,  1.,  1.,  1.,  1.])\n</code></pre>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">list</span>?</span><br></pre></td></tr></table></figure>\n\n<pre><code>\u001b[1;31mInit signature:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m/\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;31mDocstring:\u001b[0m     \nBuilt-in mutable sequence.\n\nIf no argument is given, the constructor creates a new empty list.\nThe argument must be an iterable if specified.\n\u001b[1;31mType:\u001b[0m           type\n\u001b[1;31mSubclasses:\u001b[0m     _HashedSeq, StackSummary, _Threads, ConvertingList, DeferredConfigList, _ymd, SList, ParamSpec, _ConcatenateGenericAlias, _ImmutableLineList, ...\n</code></pre>"},{"title":"2.6 softmax回归从零实现","date":"2024-02-04T06:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 6 softmax回归从零实现\n\n\n```python\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\nimport torch\nfrom IPython import display\nfrom d2l import torch as d2l\n\nbatch_size = 256\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n```\n\n## 6.1 初始化模型参数\n\n\n```python\nnum_inputs = 784\nnum_outputs = 10\n\nW = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)\nb = torch.zeros(num_outputs, requires_grad=True)\n```\n\n## 6.2 定义softmax操作\n$$ softmax(\\mathbf{X})_{ij} = \\frac{ \\exp(\\mathbf{X} _{ij}) } { \\sum_k \\exp(\\mathbf{X} _{ik}) } $$\n\n\n- 1）对每个项求幂\n- 2）对每一行（样本）求和，得到每个样本的规范化常数\n- 3）对每一行（样本）各个项除以该常数，确保和为1\n- 分母或规范化常数，有时也称为配分函数（其对数称为对数‐配分函数）。\n\n\n```python\n# 定义softmax操作\ndef softmax(X): #矩阵中的非常大或非常小的元素可能造成数值上溢或下溢，但我们没有采取措施来防止这点。\n    X_exp = torch.exp(X)\n    partition = X_exp.sum(1, keepdim=True)\n    return X_exp / partition # 这里应用了广播机制\n```\n\n## 6.3 定义模型\n\n\n```python\n# 定义模型\ndef net(X):\n    return softmax(torch.matmul(X.reshape(-1, W.shape[0]), W) + b)\n```\n\n## 6.4 定义损失函数\n$$l(y_i,y_i')=-\\sum_{j=1}^{q}y_{j}log(\\hat{y}_{j})$$\n\n\n```python\n# 定义损失函数\ndef cross_entropy(y_hat, y): #根据交叉熵公式，yi是一个（0001）的向量，所以对于一个样本，他的交叉熵是：真实类别对应的那个概率求对数\n    return -torch.log(y_hat[range(len(y_hat)), y])\n```\n\n## 6.5 分类精度\n\n\n\n```python\n# 计算正确预测的数量\ndef accuracy(y_hat, y): #@save\n    \"\"\"计算预测正确的数量。\"\"\"\n    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n        y_hat = y_hat.argmax(axis=1) #argmax(axis=1)返回矩阵每行最大值的索引\n    cmp = y_hat.type(y.dtype) == y #返回一个bool矩阵\n    return float(cmp.type(y.dtype).sum())\n# 评估模型的准确率\ndef evaluate_accuracy(net, data_iter): #@save\n    \"\"\"计算在指定数据集上模型的精度。\"\"\"\n    if isinstance(net, torch.nn.Module):\n        net.eval()  # 将模型设置为评估模式\n    metric = Accumulator(2)  # 正确预测数、预测总数\n    with torch.no_grad():\n        for X, y in data_iter:\n            metric.add(accuracy(net(X), y), y.numel()) #y.numel()返回y中元素的个数\n    return metric[0] / metric[1]\n# 累加器\nclass Accumulator:  #@save\n    \"\"\"在`n`个变量上累加。\"\"\"\n    def __init__(self, n):\n        self.data = [0.0] * n\n    def add(self, *args):\n        self.data = [a + float(b) for a, b in zip(self.data, args)]\n    def reset(self):\n        self.data = [0.0] * len(self.data)\n    def __getitem__(self, idx):\n        return self.data[idx]\n\n# 由于随机初始化，因此最开始的精度只有大概10%\nprint(evaluate_accuracy(net, test_iter))\n```\n\n    0.1141\n\n\n## 6.6 训练\n\n\n\n```python\n#1 训练一个epoch\ndef train_epoch_ch3(net, train_iter, loss, updater):  #@save\n    \"\"\"训练模型一个迭代周期\"\"\"\n    # 将模型设置为训练模式\n    if isinstance(net, torch.nn.Module):\n        net.train()\n    # 训练损失总和、训练准确度总和、样本数\n    metric = Accumulator(3)\n    for X, y in train_iter:\n        # 计算梯度并更新参数\n        y_hat = net(X)\n        l = loss(y_hat, y)\n        if isinstance(updater, torch.optim.Optimizer):\n            # 使用PyTorch内置的优化器和损失函数\n            updater.zero_grad()\n            l.mean.backward() #l.mean()返回l中所有元素的均值\n            updater.step()\n        else:\n            # 使用定制的优化器和损失函数\n            l.sum().backward() #l.sum()返回l中所有元素的和\n            updater(X.shape[0])\n        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel()) #记录损失、准确率、样本数\n    # 返回训练损失和训练准确率\n    return metric[0] / metric[2], metric[1] / metric[2] #因为损失做了sum，所以要除以样本数\n\n#画图程序\nclass Animator: #@save\n    \"\"\"在动画中绘制数据\"\"\"\n    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n                 ylim=None, xscale='linear', yscale='linear',\n                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n                 figsize=(3.5, 2.5)):\n        # 增量地绘制多条线\n        if legend is None:\n            legend = []\n        d2l.use_svg_display()\n        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)\n        if nrows * ncols == 1:\n            self.axes = [self.axes,]\n        # 使用lambda函数捕获参数\n        self.config_axes = lambda: d2l.set_axes(self.axes[\n            0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n        self.X, self.Y, self.fmts = None, None, fmts\n    def add(self,x,y):\n        # 向图表中添加多个数据点\n        if not hasattr(y, \"__len__\"):\n            y = [y]\n        n = len(y)\n        if not hasattr(x, \"__len__\"):\n            x = [x] * n\n        if not self.X:\n            self.X = [[] for _ in range(n)]\n        if not self.Y:\n            self.Y = [[] for _ in range(n)]\n        for i, (a, b) in enumerate(zip(x, y)):\n            if a is not None and b is not None:\n                self.X[i].append(a)\n                self.Y[i].append(b)\n        self.axes[0].cla()\n        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n            self.axes[0].plot(x, y, fmt)\n        self.config_axes()\n        display.display(self.fig)\n        display.clear_output(wait=True)\n\n#2 训练函数\ndef train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):  #@save\n    \"\"\"训练模型\"\"\"\n    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],\n                        legend=['train loss', 'train acc', 'test acc'])\n    for epoch in range(num_epochs):\n        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)\n        test_acc = evaluate_accuracy(net, test_iter)\n        animator.add(epoch + 1, train_metrics + (test_acc,)) #记录epoch,训练损失、训练准确率、测试准确率\n    train_loss, train_acc = train_metrics\n    assert train_loss < 0.5, train_loss #如果train_loss大于0.5，说明模型训练失败\n    assert train_acc <= 1 and train_acc > 0.7, train_acc\n    assert test_acc <= 1 and test_acc > 0.7, test_acc\n\n#3 优化器\nlr = 0.1\ndef updater(batch_size):\n    return d2l.sgd([W, b], lr, batch_size)\n\n#4 训练\nnum_epochs = 10\ntrain_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)\n```\n\n\n​    \n![svg](6_softmax_realize_files/6_softmax_realize_13_0.svg)\n![](img/deeplearning/code/pytorch/2_linear_neural_network/6_softmax_realize_files/6_softmax_realize_13_0.svg)\n​    \n\n\n# 6.7 预测\n\n\n\n```python\ndef predict_ch3(net, test_iter, n=6): #@save\n    \"\"\"预测标签\"\"\"\n    for X, y in test_iter:\n        break\n    trues = d2l.get_fashion_mnist_labels(y)\n    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=1))\n    titles = [true + '\\n' + pred for true, pred in zip(trues, preds)]\n    d2l.show_images(X[0:n].reshape((n, 28, 28)), 1, n, titles=titles[0:n])\npredict_ch3(net, test_iter)\n```\n\n\n​    \n![svg](6_softmax_realize_files/6_softmax_realize_15_0.svg)\n![](img/deeplearning/code/pytorch/2_linear_neural_network/6_softmax_realize_files/6_softmax_realize_15_0.svg)\n​    \n\n","source":"_posts/deeplearning/code/pytorch/2_linear_neural_network/6_softmax_realize.md","raw":"---\ntitle: 2.6 softmax回归从零实现\ndate: 2024-2-4 14:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 6 softmax回归从零实现\n\n\n```python\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\nimport torch\nfrom IPython import display\nfrom d2l import torch as d2l\n\nbatch_size = 256\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n```\n\n## 6.1 初始化模型参数\n\n\n```python\nnum_inputs = 784\nnum_outputs = 10\n\nW = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)\nb = torch.zeros(num_outputs, requires_grad=True)\n```\n\n## 6.2 定义softmax操作\n$$ softmax(\\mathbf{X})_{ij} = \\frac{ \\exp(\\mathbf{X} _{ij}) } { \\sum_k \\exp(\\mathbf{X} _{ik}) } $$\n\n\n- 1）对每个项求幂\n- 2）对每一行（样本）求和，得到每个样本的规范化常数\n- 3）对每一行（样本）各个项除以该常数，确保和为1\n- 分母或规范化常数，有时也称为配分函数（其对数称为对数‐配分函数）。\n\n\n```python\n# 定义softmax操作\ndef softmax(X): #矩阵中的非常大或非常小的元素可能造成数值上溢或下溢，但我们没有采取措施来防止这点。\n    X_exp = torch.exp(X)\n    partition = X_exp.sum(1, keepdim=True)\n    return X_exp / partition # 这里应用了广播机制\n```\n\n## 6.3 定义模型\n\n\n```python\n# 定义模型\ndef net(X):\n    return softmax(torch.matmul(X.reshape(-1, W.shape[0]), W) + b)\n```\n\n## 6.4 定义损失函数\n$$l(y_i,y_i')=-\\sum_{j=1}^{q}y_{j}log(\\hat{y}_{j})$$\n\n\n```python\n# 定义损失函数\ndef cross_entropy(y_hat, y): #根据交叉熵公式，yi是一个（0001）的向量，所以对于一个样本，他的交叉熵是：真实类别对应的那个概率求对数\n    return -torch.log(y_hat[range(len(y_hat)), y])\n```\n\n## 6.5 分类精度\n\n\n\n```python\n# 计算正确预测的数量\ndef accuracy(y_hat, y): #@save\n    \"\"\"计算预测正确的数量。\"\"\"\n    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n        y_hat = y_hat.argmax(axis=1) #argmax(axis=1)返回矩阵每行最大值的索引\n    cmp = y_hat.type(y.dtype) == y #返回一个bool矩阵\n    return float(cmp.type(y.dtype).sum())\n# 评估模型的准确率\ndef evaluate_accuracy(net, data_iter): #@save\n    \"\"\"计算在指定数据集上模型的精度。\"\"\"\n    if isinstance(net, torch.nn.Module):\n        net.eval()  # 将模型设置为评估模式\n    metric = Accumulator(2)  # 正确预测数、预测总数\n    with torch.no_grad():\n        for X, y in data_iter:\n            metric.add(accuracy(net(X), y), y.numel()) #y.numel()返回y中元素的个数\n    return metric[0] / metric[1]\n# 累加器\nclass Accumulator:  #@save\n    \"\"\"在`n`个变量上累加。\"\"\"\n    def __init__(self, n):\n        self.data = [0.0] * n\n    def add(self, *args):\n        self.data = [a + float(b) for a, b in zip(self.data, args)]\n    def reset(self):\n        self.data = [0.0] * len(self.data)\n    def __getitem__(self, idx):\n        return self.data[idx]\n\n# 由于随机初始化，因此最开始的精度只有大概10%\nprint(evaluate_accuracy(net, test_iter))\n```\n\n    0.1141\n\n\n## 6.6 训练\n\n\n\n```python\n#1 训练一个epoch\ndef train_epoch_ch3(net, train_iter, loss, updater):  #@save\n    \"\"\"训练模型一个迭代周期\"\"\"\n    # 将模型设置为训练模式\n    if isinstance(net, torch.nn.Module):\n        net.train()\n    # 训练损失总和、训练准确度总和、样本数\n    metric = Accumulator(3)\n    for X, y in train_iter:\n        # 计算梯度并更新参数\n        y_hat = net(X)\n        l = loss(y_hat, y)\n        if isinstance(updater, torch.optim.Optimizer):\n            # 使用PyTorch内置的优化器和损失函数\n            updater.zero_grad()\n            l.mean.backward() #l.mean()返回l中所有元素的均值\n            updater.step()\n        else:\n            # 使用定制的优化器和损失函数\n            l.sum().backward() #l.sum()返回l中所有元素的和\n            updater(X.shape[0])\n        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel()) #记录损失、准确率、样本数\n    # 返回训练损失和训练准确率\n    return metric[0] / metric[2], metric[1] / metric[2] #因为损失做了sum，所以要除以样本数\n\n#画图程序\nclass Animator: #@save\n    \"\"\"在动画中绘制数据\"\"\"\n    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n                 ylim=None, xscale='linear', yscale='linear',\n                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n                 figsize=(3.5, 2.5)):\n        # 增量地绘制多条线\n        if legend is None:\n            legend = []\n        d2l.use_svg_display()\n        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)\n        if nrows * ncols == 1:\n            self.axes = [self.axes,]\n        # 使用lambda函数捕获参数\n        self.config_axes = lambda: d2l.set_axes(self.axes[\n            0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n        self.X, self.Y, self.fmts = None, None, fmts\n    def add(self,x,y):\n        # 向图表中添加多个数据点\n        if not hasattr(y, \"__len__\"):\n            y = [y]\n        n = len(y)\n        if not hasattr(x, \"__len__\"):\n            x = [x] * n\n        if not self.X:\n            self.X = [[] for _ in range(n)]\n        if not self.Y:\n            self.Y = [[] for _ in range(n)]\n        for i, (a, b) in enumerate(zip(x, y)):\n            if a is not None and b is not None:\n                self.X[i].append(a)\n                self.Y[i].append(b)\n        self.axes[0].cla()\n        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n            self.axes[0].plot(x, y, fmt)\n        self.config_axes()\n        display.display(self.fig)\n        display.clear_output(wait=True)\n\n#2 训练函数\ndef train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):  #@save\n    \"\"\"训练模型\"\"\"\n    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],\n                        legend=['train loss', 'train acc', 'test acc'])\n    for epoch in range(num_epochs):\n        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)\n        test_acc = evaluate_accuracy(net, test_iter)\n        animator.add(epoch + 1, train_metrics + (test_acc,)) #记录epoch,训练损失、训练准确率、测试准确率\n    train_loss, train_acc = train_metrics\n    assert train_loss < 0.5, train_loss #如果train_loss大于0.5，说明模型训练失败\n    assert train_acc <= 1 and train_acc > 0.7, train_acc\n    assert test_acc <= 1 and test_acc > 0.7, test_acc\n\n#3 优化器\nlr = 0.1\ndef updater(batch_size):\n    return d2l.sgd([W, b], lr, batch_size)\n\n#4 训练\nnum_epochs = 10\ntrain_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)\n```\n\n\n​    \n![svg](6_softmax_realize_files/6_softmax_realize_13_0.svg)\n![](img/deeplearning/code/pytorch/2_linear_neural_network/6_softmax_realize_files/6_softmax_realize_13_0.svg)\n​    \n\n\n# 6.7 预测\n\n\n\n```python\ndef predict_ch3(net, test_iter, n=6): #@save\n    \"\"\"预测标签\"\"\"\n    for X, y in test_iter:\n        break\n    trues = d2l.get_fashion_mnist_labels(y)\n    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=1))\n    titles = [true + '\\n' + pred for true, pred in zip(trues, preds)]\n    d2l.show_images(X[0:n].reshape((n, 28, 28)), 1, n, titles=titles[0:n])\npredict_ch3(net, test_iter)\n```\n\n\n​    \n![svg](6_softmax_realize_files/6_softmax_realize_15_0.svg)\n![](img/deeplearning/code/pytorch/2_linear_neural_network/6_softmax_realize_files/6_softmax_realize_15_0.svg)\n​    \n\n","slug":"deeplearning/code/pytorch/2_linear_neural_network/6_softmax_realize","published":1,"updated":"2024-05-02T08:55:18.917Z","_id":"clvuli60d001gzsvw8h9dgxao","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"6-softmax回归从零实现\"><a href=\"#6-softmax回归从零实现\" class=\"headerlink\" title=\"6 softmax回归从零实现\"></a>6 softmax回归从零实现</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>] = <span class=\"string\">&#x27;True&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> IPython <span class=\"keyword\">import</span> display</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">batch_size = <span class=\"number\">256</span></span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"6-1-初始化模型参数\"><a href=\"#6-1-初始化模型参数\" class=\"headerlink\" title=\"6.1 初始化模型参数\"></a>6.1 初始化模型参数</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_inputs = <span class=\"number\">784</span></span><br><span class=\"line\">num_outputs = <span class=\"number\">10</span></span><br><span class=\"line\"></span><br><span class=\"line\">W = torch.normal(<span class=\"number\">0</span>, <span class=\"number\">0.01</span>, size=(num_inputs, num_outputs), requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">b = torch.zeros(num_outputs, requires_grad=<span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"6-2-定义softmax操作\"><a href=\"#6-2-定义softmax操作\" class=\"headerlink\" title=\"6.2 定义softmax操作\"></a>6.2 定义softmax操作</h2><p>$$ softmax(\\mathbf{X})_{ij} &#x3D; \\frac{ \\exp(\\mathbf{X} _{ij}) } { \\sum_k \\exp(\\mathbf{X} _{ik}) } $$</p>\n<ul>\n<li>1）对每个项求幂</li>\n<li>2）对每一行（样本）求和，得到每个样本的规范化常数</li>\n<li>3）对每一行（样本）各个项除以该常数，确保和为1</li>\n<li>分母或规范化常数，有时也称为配分函数（其对数称为对数‐配分函数）。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 定义softmax操作</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">softmax</span>(<span class=\"params\">X</span>): <span class=\"comment\">#矩阵中的非常大或非常小的元素可能造成数值上溢或下溢，但我们没有采取措施来防止这点。</span></span><br><span class=\"line\">    X_exp = torch.exp(X)</span><br><span class=\"line\">    partition = X_exp.<span class=\"built_in\">sum</span>(<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> X_exp / partition <span class=\"comment\"># 这里应用了广播机制</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"6-3-定义模型\"><a href=\"#6-3-定义模型\" class=\"headerlink\" title=\"6.3 定义模型\"></a>6.3 定义模型</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 定义模型</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">net</span>(<span class=\"params\">X</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> softmax(torch.matmul(X.reshape(-<span class=\"number\">1</span>, W.shape[<span class=\"number\">0</span>]), W) + b)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"6-4-定义损失函数\"><a href=\"#6-4-定义损失函数\" class=\"headerlink\" title=\"6.4 定义损失函数\"></a>6.4 定义损失函数</h2><p>$$l(y_i,y_i’)&#x3D;-\\sum_{j&#x3D;1}^{q}y_{j}log(\\hat{y}_{j})$$</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 定义损失函数</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">cross_entropy</span>(<span class=\"params\">y_hat, y</span>): <span class=\"comment\">#根据交叉熵公式，yi是一个（0001）的向量，所以对于一个样本，他的交叉熵是：真实类别对应的那个概率求对数</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> -torch.log(y_hat[<span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(y_hat)), y])</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"6-5-分类精度\"><a href=\"#6-5-分类精度\" class=\"headerlink\" title=\"6.5 分类精度\"></a>6.5 分类精度</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 计算正确预测的数量</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">accuracy</span>(<span class=\"params\">y_hat, y</span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;计算预测正确的数量。&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(y_hat.shape) &gt; <span class=\"number\">1</span> <span class=\"keyword\">and</span> y_hat.shape[<span class=\"number\">1</span>] &gt; <span class=\"number\">1</span>:</span><br><span class=\"line\">        y_hat = y_hat.argmax(axis=<span class=\"number\">1</span>) <span class=\"comment\">#argmax(axis=1)返回矩阵每行最大值的索引</span></span><br><span class=\"line\">    cmp = y_hat.<span class=\"built_in\">type</span>(y.dtype) == y <span class=\"comment\">#返回一个bool矩阵</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">float</span>(cmp.<span class=\"built_in\">type</span>(y.dtype).<span class=\"built_in\">sum</span>())</span><br><span class=\"line\"><span class=\"comment\"># 评估模型的准确率</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">evaluate_accuracy</span>(<span class=\"params\">net, data_iter</span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;计算在指定数据集上模型的精度。&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(net, torch.nn.Module):</span><br><span class=\"line\">        net.<span class=\"built_in\">eval</span>()  <span class=\"comment\"># 将模型设置为评估模式</span></span><br><span class=\"line\">    metric = Accumulator(<span class=\"number\">2</span>)  <span class=\"comment\"># 正确预测数、预测总数</span></span><br><span class=\"line\">    <span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">        <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> data_iter:</span><br><span class=\"line\">            metric.add(accuracy(net(X), y), y.numel()) <span class=\"comment\">#y.numel()返回y中元素的个数</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> metric[<span class=\"number\">0</span>] / metric[<span class=\"number\">1</span>]</span><br><span class=\"line\"><span class=\"comment\"># 累加器</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Accumulator</span>:  <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;在`n`个变量上累加。&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, n</span>):</span><br><span class=\"line\">        self.data = [<span class=\"number\">0.0</span>] * n</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">add</span>(<span class=\"params\">self, *args</span>):</span><br><span class=\"line\">        self.data = [a + <span class=\"built_in\">float</span>(b) <span class=\"keyword\">for</span> a, b <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(self.data, args)]</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">reset</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        self.data = [<span class=\"number\">0.0</span>] * <span class=\"built_in\">len</span>(self.data)</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__getitem__</span>(<span class=\"params\">self, idx</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.data[idx]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 由于随机初始化，因此最开始的精度只有大概10%</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(evaluate_accuracy(net, test_iter))</span><br></pre></td></tr></table></figure>\n\n<pre><code>0.1141\n</code></pre>\n<h2 id=\"6-6-训练\"><a href=\"#6-6-训练\" class=\"headerlink\" title=\"6.6 训练\"></a>6.6 训练</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#1 训练一个epoch</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train_epoch_ch3</span>(<span class=\"params\">net, train_iter, loss, updater</span>):  <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;训练模型一个迭代周期&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"comment\"># 将模型设置为训练模式</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(net, torch.nn.Module):</span><br><span class=\"line\">        net.train()</span><br><span class=\"line\">    <span class=\"comment\"># 训练损失总和、训练准确度总和、样本数</span></span><br><span class=\"line\">    metric = Accumulator(<span class=\"number\">3</span>)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> train_iter:</span><br><span class=\"line\">        <span class=\"comment\"># 计算梯度并更新参数</span></span><br><span class=\"line\">        y_hat = net(X)</span><br><span class=\"line\">        l = loss(y_hat, y)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(updater, torch.optim.Optimizer):</span><br><span class=\"line\">            <span class=\"comment\"># 使用PyTorch内置的优化器和损失函数</span></span><br><span class=\"line\">            updater.zero_grad()</span><br><span class=\"line\">            l.mean.backward() <span class=\"comment\">#l.mean()返回l中所有元素的均值</span></span><br><span class=\"line\">            updater.step()</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"comment\"># 使用定制的优化器和损失函数</span></span><br><span class=\"line\">            l.<span class=\"built_in\">sum</span>().backward() <span class=\"comment\">#l.sum()返回l中所有元素的和</span></span><br><span class=\"line\">            updater(X.shape[<span class=\"number\">0</span>])</span><br><span class=\"line\">        metric.add(<span class=\"built_in\">float</span>(l.<span class=\"built_in\">sum</span>()), accuracy(y_hat, y), y.numel()) <span class=\"comment\">#记录损失、准确率、样本数</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回训练损失和训练准确率</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> metric[<span class=\"number\">0</span>] / metric[<span class=\"number\">2</span>], metric[<span class=\"number\">1</span>] / metric[<span class=\"number\">2</span>] <span class=\"comment\">#因为损失做了sum，所以要除以样本数</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#画图程序</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Animator</span>: <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;在动画中绘制数据&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, xlabel=<span class=\"literal\">None</span>, ylabel=<span class=\"literal\">None</span>, legend=<span class=\"literal\">None</span>, xlim=<span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">                 ylim=<span class=\"literal\">None</span>, xscale=<span class=\"string\">&#x27;linear&#x27;</span>, yscale=<span class=\"string\">&#x27;linear&#x27;</span>,</span></span><br><span class=\"line\"><span class=\"params\">                 fmts=(<span class=\"params\"><span class=\"string\">&#x27;-&#x27;</span>, <span class=\"string\">&#x27;m--&#x27;</span>, <span class=\"string\">&#x27;g-.&#x27;</span>, <span class=\"string\">&#x27;r:&#x27;</span></span>), nrows=<span class=\"number\">1</span>, ncols=<span class=\"number\">1</span>,</span></span><br><span class=\"line\"><span class=\"params\">                 figsize=(<span class=\"params\"><span class=\"number\">3.5</span>, <span class=\"number\">2.5</span></span>)</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 增量地绘制多条线</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> legend <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            legend = []</span><br><span class=\"line\">        d2l.use_svg_display()</span><br><span class=\"line\">        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> nrows * ncols == <span class=\"number\">1</span>:</span><br><span class=\"line\">            self.axes = [self.axes,]</span><br><span class=\"line\">        <span class=\"comment\"># 使用lambda函数捕获参数</span></span><br><span class=\"line\">        self.config_axes = <span class=\"keyword\">lambda</span>: d2l.set_axes(self.axes[</span><br><span class=\"line\">            <span class=\"number\">0</span>], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)</span><br><span class=\"line\">        self.X, self.Y, self.fmts = <span class=\"literal\">None</span>, <span class=\"literal\">None</span>, fmts</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">add</span>(<span class=\"params\">self,x,y</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 向图表中添加多个数据点</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> <span class=\"built_in\">hasattr</span>(y, <span class=\"string\">&quot;__len__&quot;</span>):</span><br><span class=\"line\">            y = [y]</span><br><span class=\"line\">        n = <span class=\"built_in\">len</span>(y)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> <span class=\"built_in\">hasattr</span>(x, <span class=\"string\">&quot;__len__&quot;</span>):</span><br><span class=\"line\">            x = [x] * n</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> self.X:</span><br><span class=\"line\">            self.X = [[] <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(n)]</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> self.Y:</span><br><span class=\"line\">            self.Y = [[] <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(n)]</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i, (a, b) <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(<span class=\"built_in\">zip</span>(x, y)):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> a <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span> <span class=\"keyword\">and</span> b <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">                self.X[i].append(a)</span><br><span class=\"line\">                self.Y[i].append(b)</span><br><span class=\"line\">        self.axes[<span class=\"number\">0</span>].cla()</span><br><span class=\"line\">        <span class=\"keyword\">for</span> x, y, fmt <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(self.X, self.Y, self.fmts):</span><br><span class=\"line\">            self.axes[<span class=\"number\">0</span>].plot(x, y, fmt)</span><br><span class=\"line\">        self.config_axes()</span><br><span class=\"line\">        display.display(self.fig)</span><br><span class=\"line\">        display.clear_output(wait=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#2 训练函数</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train_ch3</span>(<span class=\"params\">net, train_iter, test_iter, loss, num_epochs, updater</span>):  <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;训练模型&quot;&quot;&quot;</span></span><br><span class=\"line\">    animator = Animator(xlabel=<span class=\"string\">&#x27;epoch&#x27;</span>, xlim=[<span class=\"number\">1</span>, num_epochs], ylim=[<span class=\"number\">0.3</span>, <span class=\"number\">0.9</span>],</span><br><span class=\"line\">                        legend=[<span class=\"string\">&#x27;train loss&#x27;</span>, <span class=\"string\">&#x27;train acc&#x27;</span>, <span class=\"string\">&#x27;test acc&#x27;</span>])</span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)</span><br><span class=\"line\">        test_acc = evaluate_accuracy(net, test_iter)</span><br><span class=\"line\">        animator.add(epoch + <span class=\"number\">1</span>, train_metrics + (test_acc,)) <span class=\"comment\">#记录epoch,训练损失、训练准确率、测试准确率</span></span><br><span class=\"line\">    train_loss, train_acc = train_metrics</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> train_loss &lt; <span class=\"number\">0.5</span>, train_loss <span class=\"comment\">#如果train_loss大于0.5，说明模型训练失败</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> train_acc &lt;= <span class=\"number\">1</span> <span class=\"keyword\">and</span> train_acc &gt; <span class=\"number\">0.7</span>, train_acc</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> test_acc &lt;= <span class=\"number\">1</span> <span class=\"keyword\">and</span> test_acc &gt; <span class=\"number\">0.7</span>, test_acc</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#3 优化器</span></span><br><span class=\"line\">lr = <span class=\"number\">0.1</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">updater</span>(<span class=\"params\">batch_size</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> d2l.sgd([W, b], lr, batch_size)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#4 训练</span></span><br><span class=\"line\">num_epochs = <span class=\"number\">10</span></span><br><span class=\"line\">train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)</span><br></pre></td></tr></table></figure>\n\n\n<p>​<br><img src=\"/6_softmax_realize_files/6_softmax_realize_13_0.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/2_linear_neural_network/6_softmax_realize_files/6_softmax_realize_13_0.svg\"><br>​    </p>\n<h1 id=\"6-7-预测\"><a href=\"#6-7-预测\" class=\"headerlink\" title=\"6.7 预测\"></a>6.7 预测</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">predict_ch3</span>(<span class=\"params\">net, test_iter, n=<span class=\"number\">6</span></span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;预测标签&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> test_iter:</span><br><span class=\"line\">        <span class=\"keyword\">break</span></span><br><span class=\"line\">    trues = d2l.get_fashion_mnist_labels(y)</span><br><span class=\"line\">    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=<span class=\"number\">1</span>))</span><br><span class=\"line\">    titles = [true + <span class=\"string\">&#x27;\\n&#x27;</span> + pred <span class=\"keyword\">for</span> true, pred <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(trues, preds)]</span><br><span class=\"line\">    d2l.show_images(X[<span class=\"number\">0</span>:n].reshape((n, <span class=\"number\">28</span>, <span class=\"number\">28</span>)), <span class=\"number\">1</span>, n, titles=titles[<span class=\"number\">0</span>:n])</span><br><span class=\"line\">predict_ch3(net, test_iter)</span><br></pre></td></tr></table></figure>\n\n\n<p>​<br><img src=\"/6_softmax_realize_files/6_softmax_realize_15_0.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/2_linear_neural_network/6_softmax_realize_files/6_softmax_realize_15_0.svg\"><br>​    </p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"6-softmax回归从零实现\"><a href=\"#6-softmax回归从零实现\" class=\"headerlink\" title=\"6 softmax回归从零实现\"></a>6 softmax回归从零实现</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>] = <span class=\"string\">&#x27;True&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> IPython <span class=\"keyword\">import</span> display</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">batch_size = <span class=\"number\">256</span></span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"6-1-初始化模型参数\"><a href=\"#6-1-初始化模型参数\" class=\"headerlink\" title=\"6.1 初始化模型参数\"></a>6.1 初始化模型参数</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_inputs = <span class=\"number\">784</span></span><br><span class=\"line\">num_outputs = <span class=\"number\">10</span></span><br><span class=\"line\"></span><br><span class=\"line\">W = torch.normal(<span class=\"number\">0</span>, <span class=\"number\">0.01</span>, size=(num_inputs, num_outputs), requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">b = torch.zeros(num_outputs, requires_grad=<span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"6-2-定义softmax操作\"><a href=\"#6-2-定义softmax操作\" class=\"headerlink\" title=\"6.2 定义softmax操作\"></a>6.2 定义softmax操作</h2><p>$$ softmax(\\mathbf{X})_{ij} &#x3D; \\frac{ \\exp(\\mathbf{X} _{ij}) } { \\sum_k \\exp(\\mathbf{X} _{ik}) } $$</p>\n<ul>\n<li>1）对每个项求幂</li>\n<li>2）对每一行（样本）求和，得到每个样本的规范化常数</li>\n<li>3）对每一行（样本）各个项除以该常数，确保和为1</li>\n<li>分母或规范化常数，有时也称为配分函数（其对数称为对数‐配分函数）。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 定义softmax操作</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">softmax</span>(<span class=\"params\">X</span>): <span class=\"comment\">#矩阵中的非常大或非常小的元素可能造成数值上溢或下溢，但我们没有采取措施来防止这点。</span></span><br><span class=\"line\">    X_exp = torch.exp(X)</span><br><span class=\"line\">    partition = X_exp.<span class=\"built_in\">sum</span>(<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> X_exp / partition <span class=\"comment\"># 这里应用了广播机制</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"6-3-定义模型\"><a href=\"#6-3-定义模型\" class=\"headerlink\" title=\"6.3 定义模型\"></a>6.3 定义模型</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 定义模型</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">net</span>(<span class=\"params\">X</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> softmax(torch.matmul(X.reshape(-<span class=\"number\">1</span>, W.shape[<span class=\"number\">0</span>]), W) + b)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"6-4-定义损失函数\"><a href=\"#6-4-定义损失函数\" class=\"headerlink\" title=\"6.4 定义损失函数\"></a>6.4 定义损失函数</h2><p>$$l(y_i,y_i’)&#x3D;-\\sum_{j&#x3D;1}^{q}y_{j}log(\\hat{y}_{j})$$</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 定义损失函数</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">cross_entropy</span>(<span class=\"params\">y_hat, y</span>): <span class=\"comment\">#根据交叉熵公式，yi是一个（0001）的向量，所以对于一个样本，他的交叉熵是：真实类别对应的那个概率求对数</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> -torch.log(y_hat[<span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(y_hat)), y])</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"6-5-分类精度\"><a href=\"#6-5-分类精度\" class=\"headerlink\" title=\"6.5 分类精度\"></a>6.5 分类精度</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 计算正确预测的数量</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">accuracy</span>(<span class=\"params\">y_hat, y</span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;计算预测正确的数量。&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(y_hat.shape) &gt; <span class=\"number\">1</span> <span class=\"keyword\">and</span> y_hat.shape[<span class=\"number\">1</span>] &gt; <span class=\"number\">1</span>:</span><br><span class=\"line\">        y_hat = y_hat.argmax(axis=<span class=\"number\">1</span>) <span class=\"comment\">#argmax(axis=1)返回矩阵每行最大值的索引</span></span><br><span class=\"line\">    cmp = y_hat.<span class=\"built_in\">type</span>(y.dtype) == y <span class=\"comment\">#返回一个bool矩阵</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">float</span>(cmp.<span class=\"built_in\">type</span>(y.dtype).<span class=\"built_in\">sum</span>())</span><br><span class=\"line\"><span class=\"comment\"># 评估模型的准确率</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">evaluate_accuracy</span>(<span class=\"params\">net, data_iter</span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;计算在指定数据集上模型的精度。&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(net, torch.nn.Module):</span><br><span class=\"line\">        net.<span class=\"built_in\">eval</span>()  <span class=\"comment\"># 将模型设置为评估模式</span></span><br><span class=\"line\">    metric = Accumulator(<span class=\"number\">2</span>)  <span class=\"comment\"># 正确预测数、预测总数</span></span><br><span class=\"line\">    <span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">        <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> data_iter:</span><br><span class=\"line\">            metric.add(accuracy(net(X), y), y.numel()) <span class=\"comment\">#y.numel()返回y中元素的个数</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> metric[<span class=\"number\">0</span>] / metric[<span class=\"number\">1</span>]</span><br><span class=\"line\"><span class=\"comment\"># 累加器</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Accumulator</span>:  <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;在`n`个变量上累加。&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, n</span>):</span><br><span class=\"line\">        self.data = [<span class=\"number\">0.0</span>] * n</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">add</span>(<span class=\"params\">self, *args</span>):</span><br><span class=\"line\">        self.data = [a + <span class=\"built_in\">float</span>(b) <span class=\"keyword\">for</span> a, b <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(self.data, args)]</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">reset</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        self.data = [<span class=\"number\">0.0</span>] * <span class=\"built_in\">len</span>(self.data)</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__getitem__</span>(<span class=\"params\">self, idx</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.data[idx]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 由于随机初始化，因此最开始的精度只有大概10%</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(evaluate_accuracy(net, test_iter))</span><br></pre></td></tr></table></figure>\n\n<pre><code>0.1141\n</code></pre>\n<h2 id=\"6-6-训练\"><a href=\"#6-6-训练\" class=\"headerlink\" title=\"6.6 训练\"></a>6.6 训练</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#1 训练一个epoch</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train_epoch_ch3</span>(<span class=\"params\">net, train_iter, loss, updater</span>):  <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;训练模型一个迭代周期&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"comment\"># 将模型设置为训练模式</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(net, torch.nn.Module):</span><br><span class=\"line\">        net.train()</span><br><span class=\"line\">    <span class=\"comment\"># 训练损失总和、训练准确度总和、样本数</span></span><br><span class=\"line\">    metric = Accumulator(<span class=\"number\">3</span>)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> train_iter:</span><br><span class=\"line\">        <span class=\"comment\"># 计算梯度并更新参数</span></span><br><span class=\"line\">        y_hat = net(X)</span><br><span class=\"line\">        l = loss(y_hat, y)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(updater, torch.optim.Optimizer):</span><br><span class=\"line\">            <span class=\"comment\"># 使用PyTorch内置的优化器和损失函数</span></span><br><span class=\"line\">            updater.zero_grad()</span><br><span class=\"line\">            l.mean.backward() <span class=\"comment\">#l.mean()返回l中所有元素的均值</span></span><br><span class=\"line\">            updater.step()</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"comment\"># 使用定制的优化器和损失函数</span></span><br><span class=\"line\">            l.<span class=\"built_in\">sum</span>().backward() <span class=\"comment\">#l.sum()返回l中所有元素的和</span></span><br><span class=\"line\">            updater(X.shape[<span class=\"number\">0</span>])</span><br><span class=\"line\">        metric.add(<span class=\"built_in\">float</span>(l.<span class=\"built_in\">sum</span>()), accuracy(y_hat, y), y.numel()) <span class=\"comment\">#记录损失、准确率、样本数</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回训练损失和训练准确率</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> metric[<span class=\"number\">0</span>] / metric[<span class=\"number\">2</span>], metric[<span class=\"number\">1</span>] / metric[<span class=\"number\">2</span>] <span class=\"comment\">#因为损失做了sum，所以要除以样本数</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#画图程序</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Animator</span>: <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;在动画中绘制数据&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, xlabel=<span class=\"literal\">None</span>, ylabel=<span class=\"literal\">None</span>, legend=<span class=\"literal\">None</span>, xlim=<span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">                 ylim=<span class=\"literal\">None</span>, xscale=<span class=\"string\">&#x27;linear&#x27;</span>, yscale=<span class=\"string\">&#x27;linear&#x27;</span>,</span></span><br><span class=\"line\"><span class=\"params\">                 fmts=(<span class=\"params\"><span class=\"string\">&#x27;-&#x27;</span>, <span class=\"string\">&#x27;m--&#x27;</span>, <span class=\"string\">&#x27;g-.&#x27;</span>, <span class=\"string\">&#x27;r:&#x27;</span></span>), nrows=<span class=\"number\">1</span>, ncols=<span class=\"number\">1</span>,</span></span><br><span class=\"line\"><span class=\"params\">                 figsize=(<span class=\"params\"><span class=\"number\">3.5</span>, <span class=\"number\">2.5</span></span>)</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 增量地绘制多条线</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> legend <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            legend = []</span><br><span class=\"line\">        d2l.use_svg_display()</span><br><span class=\"line\">        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> nrows * ncols == <span class=\"number\">1</span>:</span><br><span class=\"line\">            self.axes = [self.axes,]</span><br><span class=\"line\">        <span class=\"comment\"># 使用lambda函数捕获参数</span></span><br><span class=\"line\">        self.config_axes = <span class=\"keyword\">lambda</span>: d2l.set_axes(self.axes[</span><br><span class=\"line\">            <span class=\"number\">0</span>], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)</span><br><span class=\"line\">        self.X, self.Y, self.fmts = <span class=\"literal\">None</span>, <span class=\"literal\">None</span>, fmts</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">add</span>(<span class=\"params\">self,x,y</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 向图表中添加多个数据点</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> <span class=\"built_in\">hasattr</span>(y, <span class=\"string\">&quot;__len__&quot;</span>):</span><br><span class=\"line\">            y = [y]</span><br><span class=\"line\">        n = <span class=\"built_in\">len</span>(y)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> <span class=\"built_in\">hasattr</span>(x, <span class=\"string\">&quot;__len__&quot;</span>):</span><br><span class=\"line\">            x = [x] * n</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> self.X:</span><br><span class=\"line\">            self.X = [[] <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(n)]</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> self.Y:</span><br><span class=\"line\">            self.Y = [[] <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(n)]</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i, (a, b) <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(<span class=\"built_in\">zip</span>(x, y)):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> a <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span> <span class=\"keyword\">and</span> b <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">                self.X[i].append(a)</span><br><span class=\"line\">                self.Y[i].append(b)</span><br><span class=\"line\">        self.axes[<span class=\"number\">0</span>].cla()</span><br><span class=\"line\">        <span class=\"keyword\">for</span> x, y, fmt <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(self.X, self.Y, self.fmts):</span><br><span class=\"line\">            self.axes[<span class=\"number\">0</span>].plot(x, y, fmt)</span><br><span class=\"line\">        self.config_axes()</span><br><span class=\"line\">        display.display(self.fig)</span><br><span class=\"line\">        display.clear_output(wait=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#2 训练函数</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train_ch3</span>(<span class=\"params\">net, train_iter, test_iter, loss, num_epochs, updater</span>):  <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;训练模型&quot;&quot;&quot;</span></span><br><span class=\"line\">    animator = Animator(xlabel=<span class=\"string\">&#x27;epoch&#x27;</span>, xlim=[<span class=\"number\">1</span>, num_epochs], ylim=[<span class=\"number\">0.3</span>, <span class=\"number\">0.9</span>],</span><br><span class=\"line\">                        legend=[<span class=\"string\">&#x27;train loss&#x27;</span>, <span class=\"string\">&#x27;train acc&#x27;</span>, <span class=\"string\">&#x27;test acc&#x27;</span>])</span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)</span><br><span class=\"line\">        test_acc = evaluate_accuracy(net, test_iter)</span><br><span class=\"line\">        animator.add(epoch + <span class=\"number\">1</span>, train_metrics + (test_acc,)) <span class=\"comment\">#记录epoch,训练损失、训练准确率、测试准确率</span></span><br><span class=\"line\">    train_loss, train_acc = train_metrics</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> train_loss &lt; <span class=\"number\">0.5</span>, train_loss <span class=\"comment\">#如果train_loss大于0.5，说明模型训练失败</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> train_acc &lt;= <span class=\"number\">1</span> <span class=\"keyword\">and</span> train_acc &gt; <span class=\"number\">0.7</span>, train_acc</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> test_acc &lt;= <span class=\"number\">1</span> <span class=\"keyword\">and</span> test_acc &gt; <span class=\"number\">0.7</span>, test_acc</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#3 优化器</span></span><br><span class=\"line\">lr = <span class=\"number\">0.1</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">updater</span>(<span class=\"params\">batch_size</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> d2l.sgd([W, b], lr, batch_size)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#4 训练</span></span><br><span class=\"line\">num_epochs = <span class=\"number\">10</span></span><br><span class=\"line\">train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)</span><br></pre></td></tr></table></figure>\n\n\n<p>​<br><img src=\"/6_softmax_realize_files/6_softmax_realize_13_0.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/2_linear_neural_network/6_softmax_realize_files/6_softmax_realize_13_0.svg\"><br>​    </p>\n<h1 id=\"6-7-预测\"><a href=\"#6-7-预测\" class=\"headerlink\" title=\"6.7 预测\"></a>6.7 预测</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">predict_ch3</span>(<span class=\"params\">net, test_iter, n=<span class=\"number\">6</span></span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;预测标签&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> test_iter:</span><br><span class=\"line\">        <span class=\"keyword\">break</span></span><br><span class=\"line\">    trues = d2l.get_fashion_mnist_labels(y)</span><br><span class=\"line\">    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=<span class=\"number\">1</span>))</span><br><span class=\"line\">    titles = [true + <span class=\"string\">&#x27;\\n&#x27;</span> + pred <span class=\"keyword\">for</span> true, pred <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(trues, preds)]</span><br><span class=\"line\">    d2l.show_images(X[<span class=\"number\">0</span>:n].reshape((n, <span class=\"number\">28</span>, <span class=\"number\">28</span>)), <span class=\"number\">1</span>, n, titles=titles[<span class=\"number\">0</span>:n])</span><br><span class=\"line\">predict_ch3(net, test_iter)</span><br></pre></td></tr></table></figure>\n\n\n<p>​<br><img src=\"/6_softmax_realize_files/6_softmax_realize_15_0.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/2_linear_neural_network/6_softmax_realize_files/6_softmax_realize_15_0.svg\"><br>​    </p>"},{"title":"2.7 softmax回归简洁实现","date":"2024-02-04T06:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 7 softmax回归简洁实现\n\n\n```python\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK']='True'\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\nbatch_size = 256\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n```\n\n## 7.1 初始化模型参数\n\n\n\n```python\n#1 初始化模型参数\n# pytorch不会隐式地调整输入的形状，因此我们在线性层前定义展平层来调整网络输入的形状\nnet = nn.Sequential(nn.Flatten(), nn.Linear(28*28, 10))\n\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        nn.init.normal_(m.weight, std=0.01)\n\nnet.apply(init_weights)\n```\n\n\n\n\n    Sequential(\n      (0): Flatten(start_dim=1, end_dim=-1)\n      (1): Linear(in_features=784, out_features=10, bias=True)\n    )\n\n\n\n## 7.2 优化softmax计算\n- 为了防止溢出，我们先将每个元素减去该行的最大值（每个$o_k$按常数进行移动不会改变softmax的输出）。$$\\begin{aligned} \\hat{y}_ j &= \\frac{\\exp(o_ j)}{ \\sum_{i=1}^ q \\exp(o_ i)} \\\\ &= \\frac{ \\exp(o_ j - \\max(o)) \\exp( \\max(o))}{ \\sum_ {i=1}^ q \\exp(o_ i - \\max(o)) \\exp( \\max(o))} \\\\ &= \\frac{ \\exp(o_ j - \\max(o))}{ \\sum_ {i=1}^ q \\exp(o_ i - \\max(o))} \\end{aligned}$$\n- 有些oj − max(ok)具有较大的负值。由于精度受限，exp(oj − max(ok))将有接近零的值，即下溢（underflow）。这些值可能会四舍五入为零，使$\\hat{y}_ j$为零，并且使得$log(\\hat{y}_ j )$的值为-inf。反向传播几步后，我们可能会发现自己面对一屏幕可怕的nan结果。\n- 通过将softmax和交叉熵结合在一起，可以避免反向传播过程中可能会困扰我们的数值稳定性问题。避免计算$\\exp{o_j-\\max(o_k)}$,因为log与exp抵消了\n $$\\begin{aligned} log( \\hat{y}_ j) &= log \\left( \\frac{ \\exp(o_ j - \\max(o_ k))}{ \\sum_{k=1}^ q \\exp(o_ k - \\max(o_ k))} \\right) \\\\ &= log( \\exp(o_ j -  \\max(o_ k))) - log( \\sum_ {k=1}^ q \\exp(o_ k -  \\max(o_ k))) \\\\ &= o_ j - \\max(o_ k) - log( \\sum_ {k=1}^ q \\exp(o_ k - \\max(o_ k))) \\end{aligned}$$\n\n\n\n```python\n#2 损失函数\nloss = nn.CrossEntropyLoss(reduction='none') # reduction='none'表示返回每个样本的损失\n\n#3 优化算法\ntrainer = torch.optim.SGD(net.parameters(), lr=0.1)\n\n#4 训练\nnum_epochs = 10\nd2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\n```\n\n\n    \n![svg](7_softmax_realize_simple_files/7_softmax_realize_simple_5_0.svg)\n![](img/deeplearning/code/pytorch/2_linear_neural_network/7_softmax_realize_simple_files/7_softmax_realize_simple_5_0.svg)\n    \n\n","source":"_posts/deeplearning/code/pytorch/2_linear_neural_network/7_softmax_realize_simple.md","raw":"---\ntitle: 2.7 softmax回归简洁实现\ndate: 2024-2-4 14:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 7 softmax回归简洁实现\n\n\n```python\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK']='True'\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\nbatch_size = 256\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n```\n\n## 7.1 初始化模型参数\n\n\n\n```python\n#1 初始化模型参数\n# pytorch不会隐式地调整输入的形状，因此我们在线性层前定义展平层来调整网络输入的形状\nnet = nn.Sequential(nn.Flatten(), nn.Linear(28*28, 10))\n\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        nn.init.normal_(m.weight, std=0.01)\n\nnet.apply(init_weights)\n```\n\n\n\n\n    Sequential(\n      (0): Flatten(start_dim=1, end_dim=-1)\n      (1): Linear(in_features=784, out_features=10, bias=True)\n    )\n\n\n\n## 7.2 优化softmax计算\n- 为了防止溢出，我们先将每个元素减去该行的最大值（每个$o_k$按常数进行移动不会改变softmax的输出）。$$\\begin{aligned} \\hat{y}_ j &= \\frac{\\exp(o_ j)}{ \\sum_{i=1}^ q \\exp(o_ i)} \\\\ &= \\frac{ \\exp(o_ j - \\max(o)) \\exp( \\max(o))}{ \\sum_ {i=1}^ q \\exp(o_ i - \\max(o)) \\exp( \\max(o))} \\\\ &= \\frac{ \\exp(o_ j - \\max(o))}{ \\sum_ {i=1}^ q \\exp(o_ i - \\max(o))} \\end{aligned}$$\n- 有些oj − max(ok)具有较大的负值。由于精度受限，exp(oj − max(ok))将有接近零的值，即下溢（underflow）。这些值可能会四舍五入为零，使$\\hat{y}_ j$为零，并且使得$log(\\hat{y}_ j )$的值为-inf。反向传播几步后，我们可能会发现自己面对一屏幕可怕的nan结果。\n- 通过将softmax和交叉熵结合在一起，可以避免反向传播过程中可能会困扰我们的数值稳定性问题。避免计算$\\exp{o_j-\\max(o_k)}$,因为log与exp抵消了\n $$\\begin{aligned} log( \\hat{y}_ j) &= log \\left( \\frac{ \\exp(o_ j - \\max(o_ k))}{ \\sum_{k=1}^ q \\exp(o_ k - \\max(o_ k))} \\right) \\\\ &= log( \\exp(o_ j -  \\max(o_ k))) - log( \\sum_ {k=1}^ q \\exp(o_ k -  \\max(o_ k))) \\\\ &= o_ j - \\max(o_ k) - log( \\sum_ {k=1}^ q \\exp(o_ k - \\max(o_ k))) \\end{aligned}$$\n\n\n\n```python\n#2 损失函数\nloss = nn.CrossEntropyLoss(reduction='none') # reduction='none'表示返回每个样本的损失\n\n#3 优化算法\ntrainer = torch.optim.SGD(net.parameters(), lr=0.1)\n\n#4 训练\nnum_epochs = 10\nd2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\n```\n\n\n    \n![svg](7_softmax_realize_simple_files/7_softmax_realize_simple_5_0.svg)\n![](img/deeplearning/code/pytorch/2_linear_neural_network/7_softmax_realize_simple_files/7_softmax_realize_simple_5_0.svg)\n    \n\n","slug":"deeplearning/code/pytorch/2_linear_neural_network/7_softmax_realize_simple","published":1,"updated":"2024-05-02T08:55:28.336Z","_id":"clvuli60e001hzsvw5ydaf8j7","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"7-softmax回归简洁实现\"><a href=\"#7-softmax回归简洁实现\" class=\"headerlink\" title=\"7 softmax回归简洁实现\"></a>7 softmax回归简洁实现</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>]=<span class=\"string\">&#x27;True&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">batch_size = <span class=\"number\">256</span></span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"7-1-初始化模型参数\"><a href=\"#7-1-初始化模型参数\" class=\"headerlink\" title=\"7.1 初始化模型参数\"></a>7.1 初始化模型参数</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#1 初始化模型参数</span></span><br><span class=\"line\"><span class=\"comment\"># pytorch不会隐式地调整输入的形状，因此我们在线性层前定义展平层来调整网络输入的形状</span></span><br><span class=\"line\">net = nn.Sequential(nn.Flatten(), nn.Linear(<span class=\"number\">28</span>*<span class=\"number\">28</span>, <span class=\"number\">10</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_weights</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear:</span><br><span class=\"line\">        nn.init.normal_(m.weight, std=<span class=\"number\">0.01</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">net.apply(init_weights)</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>Sequential(\n  (0): Flatten(start_dim=1, end_dim=-1)\n  (1): Linear(in_features=784, out_features=10, bias=True)\n)\n</code></pre>\n<h2 id=\"7-2-优化softmax计算\"><a href=\"#7-2-优化softmax计算\" class=\"headerlink\" title=\"7.2 优化softmax计算\"></a>7.2 优化softmax计算</h2><ul>\n<li>为了防止溢出，我们先将每个元素减去该行的最大值（每个$o_k$按常数进行移动不会改变softmax的输出）。$$\\begin{aligned} \\hat{y}_ j &amp;&#x3D; \\frac{\\exp(o_ j)}{ \\sum_{i&#x3D;1}^ q \\exp(o_ i)} \\ &amp;&#x3D; \\frac{ \\exp(o_ j - \\max(o)) \\exp( \\max(o))}{ \\sum_ {i&#x3D;1}^ q \\exp(o_ i - \\max(o)) \\exp( \\max(o))} \\ &amp;&#x3D; \\frac{ \\exp(o_ j - \\max(o))}{ \\sum_ {i&#x3D;1}^ q \\exp(o_ i - \\max(o))} \\end{aligned}$$</li>\n<li>有些oj − max(ok)具有较大的负值。由于精度受限，exp(oj − max(ok))将有接近零的值，即下溢（underflow）。这些值可能会四舍五入为零，使$\\hat{y}_ j$为零，并且使得$log(\\hat{y}_ j )$的值为-inf。反向传播几步后，我们可能会发现自己面对一屏幕可怕的nan结果。</li>\n<li>通过将softmax和交叉熵结合在一起，可以避免反向传播过程中可能会困扰我们的数值稳定性问题。避免计算$\\exp{o_j-\\max(o_k)}$,因为log与exp抵消了<br> $$\\begin{aligned} log( \\hat{y}_ j) &amp;&#x3D; log \\left( \\frac{ \\exp(o_ j - \\max(o_ k))}{ \\sum_{k&#x3D;1}^ q \\exp(o_ k - \\max(o_ k))} \\right) \\ &amp;&#x3D; log( \\exp(o_ j -  \\max(o_ k))) - log( \\sum_ {k&#x3D;1}^ q \\exp(o_ k -  \\max(o_ k))) \\ &amp;&#x3D; o_ j - \\max(o_ k) - log( \\sum_ {k&#x3D;1}^ q \\exp(o_ k - \\max(o_ k))) \\end{aligned}$$</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#2 损失函数</span></span><br><span class=\"line\">loss = nn.CrossEntropyLoss(reduction=<span class=\"string\">&#x27;none&#x27;</span>) <span class=\"comment\"># reduction=&#x27;none&#x27;表示返回每个样本的损失</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#3 优化算法</span></span><br><span class=\"line\">trainer = torch.optim.SGD(net.parameters(), lr=<span class=\"number\">0.1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#4 训练</span></span><br><span class=\"line\">num_epochs = <span class=\"number\">10</span></span><br><span class=\"line\">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/7_softmax_realize_simple_files/7_softmax_realize_simple_5_0.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/2_linear_neural_network/7_softmax_realize_simple_files/7_softmax_realize_simple_5_0.svg\"></p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"7-softmax回归简洁实现\"><a href=\"#7-softmax回归简洁实现\" class=\"headerlink\" title=\"7 softmax回归简洁实现\"></a>7 softmax回归简洁实现</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>]=<span class=\"string\">&#x27;True&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">batch_size = <span class=\"number\">256</span></span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"7-1-初始化模型参数\"><a href=\"#7-1-初始化模型参数\" class=\"headerlink\" title=\"7.1 初始化模型参数\"></a>7.1 初始化模型参数</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#1 初始化模型参数</span></span><br><span class=\"line\"><span class=\"comment\"># pytorch不会隐式地调整输入的形状，因此我们在线性层前定义展平层来调整网络输入的形状</span></span><br><span class=\"line\">net = nn.Sequential(nn.Flatten(), nn.Linear(<span class=\"number\">28</span>*<span class=\"number\">28</span>, <span class=\"number\">10</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_weights</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear:</span><br><span class=\"line\">        nn.init.normal_(m.weight, std=<span class=\"number\">0.01</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">net.apply(init_weights)</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>Sequential(\n  (0): Flatten(start_dim=1, end_dim=-1)\n  (1): Linear(in_features=784, out_features=10, bias=True)\n)\n</code></pre>\n<h2 id=\"7-2-优化softmax计算\"><a href=\"#7-2-优化softmax计算\" class=\"headerlink\" title=\"7.2 优化softmax计算\"></a>7.2 优化softmax计算</h2><ul>\n<li>为了防止溢出，我们先将每个元素减去该行的最大值（每个$o_k$按常数进行移动不会改变softmax的输出）。$$\\begin{aligned} \\hat{y}_ j &amp;&#x3D; \\frac{\\exp(o_ j)}{ \\sum_{i&#x3D;1}^ q \\exp(o_ i)} \\ &amp;&#x3D; \\frac{ \\exp(o_ j - \\max(o)) \\exp( \\max(o))}{ \\sum_ {i&#x3D;1}^ q \\exp(o_ i - \\max(o)) \\exp( \\max(o))} \\ &amp;&#x3D; \\frac{ \\exp(o_ j - \\max(o))}{ \\sum_ {i&#x3D;1}^ q \\exp(o_ i - \\max(o))} \\end{aligned}$$</li>\n<li>有些oj − max(ok)具有较大的负值。由于精度受限，exp(oj − max(ok))将有接近零的值，即下溢（underflow）。这些值可能会四舍五入为零，使$\\hat{y}_ j$为零，并且使得$log(\\hat{y}_ j )$的值为-inf。反向传播几步后，我们可能会发现自己面对一屏幕可怕的nan结果。</li>\n<li>通过将softmax和交叉熵结合在一起，可以避免反向传播过程中可能会困扰我们的数值稳定性问题。避免计算$\\exp{o_j-\\max(o_k)}$,因为log与exp抵消了<br> $$\\begin{aligned} log( \\hat{y}_ j) &amp;&#x3D; log \\left( \\frac{ \\exp(o_ j - \\max(o_ k))}{ \\sum_{k&#x3D;1}^ q \\exp(o_ k - \\max(o_ k))} \\right) \\ &amp;&#x3D; log( \\exp(o_ j -  \\max(o_ k))) - log( \\sum_ {k&#x3D;1}^ q \\exp(o_ k -  \\max(o_ k))) \\ &amp;&#x3D; o_ j - \\max(o_ k) - log( \\sum_ {k&#x3D;1}^ q \\exp(o_ k - \\max(o_ k))) \\end{aligned}$$</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#2 损失函数</span></span><br><span class=\"line\">loss = nn.CrossEntropyLoss(reduction=<span class=\"string\">&#x27;none&#x27;</span>) <span class=\"comment\"># reduction=&#x27;none&#x27;表示返回每个样本的损失</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#3 优化算法</span></span><br><span class=\"line\">trainer = torch.optim.SGD(net.parameters(), lr=<span class=\"number\">0.1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#4 训练</span></span><br><span class=\"line\">num_epochs = <span class=\"number\">10</span></span><br><span class=\"line\">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/7_softmax_realize_simple_files/7_softmax_realize_simple_5_0.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/2_linear_neural_network/7_softmax_realize_simple_files/7_softmax_realize_simple_5_0.svg\"></p>"},{"title":"4.2 参数管理","date":"2024-04-19T06:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 2 参数管理\n- 访问参数，用于调试、诊断和可视化\n- 参数初始化\n- 在不同模型组件间共享参数\n\n\n```python\n'''单隐藏层的多层感知机'''\nimport torch\nfrom torch import nn\n\nnet = nn.Sequential(nn.Linear(4,8), nn.ReLU(), nn.Linear(8,1))\nX = torch.rand(size=(2,4))\nprint(net(X))\n```\n\n    tensor([[0.2326],\n            [0.2463]], grad_fn=<AddmmBackward0>)\n    \n\n## 2.1 参数访问\n- 对于Sequential类模型，通过索引访问参数\n\n\n```python\nprint(net[2].state_dict())\n#输出的这个层包含两个参数，分别是权重和偏置。\n```\n\n    OrderedDict([('weight', tensor([[ 0.0830, -0.2212, -0.3532,  0.2318,  0.2688, -0.1233, -0.2256, -0.1163]])), ('bias', tensor([0.1769]))])\n    \n\n### 2.1.1 目标参数\n- 提取第三层的偏置\n\n\n```python\nparam = net[2].bias\n\nprint(param)\nprint(type(param)) #类型\nprint(param.data) #数据\n```\n\n    Parameter containing:\n    tensor([0.1769], requires_grad=True)\n    <class 'torch.nn.parameter.Parameter'>\n    tensor([0.1769])\n    \n\n- 参数是复合对象，包含值、梯度和额外信息\n\n\n```python\n# 访问每个参数的梯度\nprint(net[2].weight.grad == None)\n```\n\n    True\n    \n\n- 访问所有参数\n\n\n```python\n# 访问第一个全连接层的参数\nprint(*[(name, param.shape) for name, param in net[0].named_parameters()])\n# 访问所有层\nprint(*[(name, param.shape) for name, param in net.named_parameters()])\n# 访问一个参数\nprint(net.state_dict()['2.bias'].data)\n```\n\n    ('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n    ('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n    tensor([0.1769])\n    \n\n- 从嵌套块收集参数\n\n\n```python\ndef block1():\n    return nn.Sequential(\n        nn.Linear(4,8),nn.ReLU(),\n        nn.Linear(8,4), nn.ReLU()\n    )\ndef block2():\n    net = nn.Sequential()\n    for i in range(4):\n        net.add_module(f'block{i}',block1()) #增加一个模块\n    return net\n\nrgnet = nn.Sequential(block2(), nn.Linear(4,1))\nprint(rgnet(X))\n# 输出网络\nprint(rgnet)\n# 访问第一个主要的模块中第二个子模块第一层的偏置\nprint(rgnet[0][1][0].bias.data)\n```\n\n    tensor([[-0.2233],\n            [-0.2233]], grad_fn=<AddmmBackward0>)\n    Sequential(\n      (0): Sequential(\n        (block0): Sequential(\n          (0): Linear(in_features=4, out_features=8, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=8, out_features=4, bias=True)\n          (3): ReLU()\n        )\n        (block1): Sequential(\n          (0): Linear(in_features=4, out_features=8, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=8, out_features=4, bias=True)\n          (3): ReLU()\n        )\n        (block2): Sequential(\n          (0): Linear(in_features=4, out_features=8, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=8, out_features=4, bias=True)\n          (3): ReLU()\n        )\n        (block3): Sequential(\n          (0): Linear(in_features=4, out_features=8, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=8, out_features=4, bias=True)\n          (3): ReLU()\n        )\n      )\n      (1): Linear(in_features=4, out_features=1, bias=True)\n    )\n    tensor([ 0.4794,  0.4585,  0.0098,  0.1719, -0.2016, -0.3174, -0.4974,  0.4953])\n    \n\n## 2.2 参数初始化\n- 默认情况下，pytorch会根据一个均匀地初始化权重和偏置\n- pytorch的nn.init模块提供了多种初始化方法\n### 2.2.1 内置初始化\n- 权重初始化标准差为0.01的高斯随机变量，偏置设置为0\n\n\n\n```python\ndef init_normal(m):\n    if type(m) == nn.Linear:\n        nn.init.normal_(m.weight, mean = 0, std=0.01)\n        nn.init.zeros_(m.bias)\nnet.apply(init_normal)\nprint(net[0].weight.data[0], net[0].bias.data[0])\n```\n\n    tensor([ 0.0058, -0.0130, -0.0114,  0.0044]) tensor(0.)\n    \n\n- 初始化为常熟\n\n\n```python\ndef init_constant(m):\n    if type(m) == nn.Linear:\n        nn.init.constant_(m.weight,1)\n        nn.init.zeros_(m.bias)\nnet.apply(init_constant)\nprint(net[0].weight.data[0],net[0].bias.data[0])\n```\n\n    tensor([1., 1., 1., 1.]) tensor(0.)\n    \n\n- 不同块使用不同初始化方法：用Xavier初始化方法初始化第一层，第三层初始化为常熟42\n\n\n```python\ndef init_xavier(m):\n    if type(m) == nn.Linear:\n        nn.init.xavier_uniform_(m.weight)\ndef init_42(m):\n    if type(m) == nn.Linear:\n        nn.init.constant_(m.weight, 42)\nnet[0].apply(init_xavier)\nnet[2].apply(init_42)\nprint(net[0].weight.data[0])\nprint(net[2].weight.data)\n```\n\n    tensor([ 0.2578, -0.6072, -0.2665,  0.4973])\n    tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n    \n\n### 2.2.2 自定义初始化\n$$ \\omega = \\begin{cases} U(5,10) &可能性 \\frac{1}{4} \\\\ 0 &可能性 \\frac{1}{2} \\\\ U(-10,-5) &可能性 \\frac{1}{4} \\\\ \\end{cases} $$\n\n\n```python\ndef my_init(m):\n    if type(m) == nn.Linear:\n        print(\"Init\", *[(name, param.shape)\n                        for name, param in m.named_parameters()][0])\n        nn.init.uniform_(m.weight, -10,10) #参数-> [-10,10]\n        m.weight.data *= m.weight.data.abs() >= 5 # 参数abs>=5的概率=1/2 ， 1/2的概率=0，1/2的概率在两个分布\nnet.apply(my_init)\nprint(net[0].weight[:2])\n# 直接设置参数\nnet[0].weight.data[:] += 1\nnet[0].weight.data[0,0] = 42\nprint(net[0].weight.data[0])\n```\n\n    Init weight torch.Size([8, 4])\n    Init weight torch.Size([1, 8])\n    tensor([[ 8.4257,  0.0000, -7.9400, -0.0000],\n            [ 0.0000,  8.8472,  5.5845, -0.0000]], grad_fn=<SliceBackward0>)\n    tensor([42.0000,  1.0000, -6.9400,  1.0000])\n    \n\n## 2.3 参数绑定\n- 有时我们希望在多个层之间共享参数\n\n\n```python\n# 定义一个共享层\nshared = nn.Linear(8,8)\nnet = nn.Sequential(nn.Linear(4,8), nn.ReLU(),\n                    shared, nn.ReLU(),\n                    shared, nn.ReLU(),\n                    nn.Linear(8,1))\nnet(X)\n# 检查参数是否相同\nprint(net[2].weight.data[0] == net[4].weight.data[0])\nnet[2].weight.data[0,0] = 100\n# 确保它们是同一个对象，而不只是具有相同的值\nprint(net[2].weight.data[0] == net[4].weight.data[0])\n```\n\n    tensor([True, True, True, True, True, True, True, True])\n    tensor([True, True, True, True, True, True, True, True])\n    \n\n- 当参数绑定时，梯度会发生什么情况：由于模型参数包含梯度，因此在反向传播期间第二个隐藏层（net[2]）和第三个隐藏层（net[5]）的梯度会加在一起\n","source":"_posts/deeplearning/code/pytorch/4_calculate/2_paramter.md","raw":"---\ntitle: 4.2 参数管理\ndate: 2024-4-19 14:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 2 参数管理\n- 访问参数，用于调试、诊断和可视化\n- 参数初始化\n- 在不同模型组件间共享参数\n\n\n```python\n'''单隐藏层的多层感知机'''\nimport torch\nfrom torch import nn\n\nnet = nn.Sequential(nn.Linear(4,8), nn.ReLU(), nn.Linear(8,1))\nX = torch.rand(size=(2,4))\nprint(net(X))\n```\n\n    tensor([[0.2326],\n            [0.2463]], grad_fn=<AddmmBackward0>)\n    \n\n## 2.1 参数访问\n- 对于Sequential类模型，通过索引访问参数\n\n\n```python\nprint(net[2].state_dict())\n#输出的这个层包含两个参数，分别是权重和偏置。\n```\n\n    OrderedDict([('weight', tensor([[ 0.0830, -0.2212, -0.3532,  0.2318,  0.2688, -0.1233, -0.2256, -0.1163]])), ('bias', tensor([0.1769]))])\n    \n\n### 2.1.1 目标参数\n- 提取第三层的偏置\n\n\n```python\nparam = net[2].bias\n\nprint(param)\nprint(type(param)) #类型\nprint(param.data) #数据\n```\n\n    Parameter containing:\n    tensor([0.1769], requires_grad=True)\n    <class 'torch.nn.parameter.Parameter'>\n    tensor([0.1769])\n    \n\n- 参数是复合对象，包含值、梯度和额外信息\n\n\n```python\n# 访问每个参数的梯度\nprint(net[2].weight.grad == None)\n```\n\n    True\n    \n\n- 访问所有参数\n\n\n```python\n# 访问第一个全连接层的参数\nprint(*[(name, param.shape) for name, param in net[0].named_parameters()])\n# 访问所有层\nprint(*[(name, param.shape) for name, param in net.named_parameters()])\n# 访问一个参数\nprint(net.state_dict()['2.bias'].data)\n```\n\n    ('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n    ('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n    tensor([0.1769])\n    \n\n- 从嵌套块收集参数\n\n\n```python\ndef block1():\n    return nn.Sequential(\n        nn.Linear(4,8),nn.ReLU(),\n        nn.Linear(8,4), nn.ReLU()\n    )\ndef block2():\n    net = nn.Sequential()\n    for i in range(4):\n        net.add_module(f'block{i}',block1()) #增加一个模块\n    return net\n\nrgnet = nn.Sequential(block2(), nn.Linear(4,1))\nprint(rgnet(X))\n# 输出网络\nprint(rgnet)\n# 访问第一个主要的模块中第二个子模块第一层的偏置\nprint(rgnet[0][1][0].bias.data)\n```\n\n    tensor([[-0.2233],\n            [-0.2233]], grad_fn=<AddmmBackward0>)\n    Sequential(\n      (0): Sequential(\n        (block0): Sequential(\n          (0): Linear(in_features=4, out_features=8, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=8, out_features=4, bias=True)\n          (3): ReLU()\n        )\n        (block1): Sequential(\n          (0): Linear(in_features=4, out_features=8, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=8, out_features=4, bias=True)\n          (3): ReLU()\n        )\n        (block2): Sequential(\n          (0): Linear(in_features=4, out_features=8, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=8, out_features=4, bias=True)\n          (3): ReLU()\n        )\n        (block3): Sequential(\n          (0): Linear(in_features=4, out_features=8, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=8, out_features=4, bias=True)\n          (3): ReLU()\n        )\n      )\n      (1): Linear(in_features=4, out_features=1, bias=True)\n    )\n    tensor([ 0.4794,  0.4585,  0.0098,  0.1719, -0.2016, -0.3174, -0.4974,  0.4953])\n    \n\n## 2.2 参数初始化\n- 默认情况下，pytorch会根据一个均匀地初始化权重和偏置\n- pytorch的nn.init模块提供了多种初始化方法\n### 2.2.1 内置初始化\n- 权重初始化标准差为0.01的高斯随机变量，偏置设置为0\n\n\n\n```python\ndef init_normal(m):\n    if type(m) == nn.Linear:\n        nn.init.normal_(m.weight, mean = 0, std=0.01)\n        nn.init.zeros_(m.bias)\nnet.apply(init_normal)\nprint(net[0].weight.data[0], net[0].bias.data[0])\n```\n\n    tensor([ 0.0058, -0.0130, -0.0114,  0.0044]) tensor(0.)\n    \n\n- 初始化为常熟\n\n\n```python\ndef init_constant(m):\n    if type(m) == nn.Linear:\n        nn.init.constant_(m.weight,1)\n        nn.init.zeros_(m.bias)\nnet.apply(init_constant)\nprint(net[0].weight.data[0],net[0].bias.data[0])\n```\n\n    tensor([1., 1., 1., 1.]) tensor(0.)\n    \n\n- 不同块使用不同初始化方法：用Xavier初始化方法初始化第一层，第三层初始化为常熟42\n\n\n```python\ndef init_xavier(m):\n    if type(m) == nn.Linear:\n        nn.init.xavier_uniform_(m.weight)\ndef init_42(m):\n    if type(m) == nn.Linear:\n        nn.init.constant_(m.weight, 42)\nnet[0].apply(init_xavier)\nnet[2].apply(init_42)\nprint(net[0].weight.data[0])\nprint(net[2].weight.data)\n```\n\n    tensor([ 0.2578, -0.6072, -0.2665,  0.4973])\n    tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n    \n\n### 2.2.2 自定义初始化\n$$ \\omega = \\begin{cases} U(5,10) &可能性 \\frac{1}{4} \\\\ 0 &可能性 \\frac{1}{2} \\\\ U(-10,-5) &可能性 \\frac{1}{4} \\\\ \\end{cases} $$\n\n\n```python\ndef my_init(m):\n    if type(m) == nn.Linear:\n        print(\"Init\", *[(name, param.shape)\n                        for name, param in m.named_parameters()][0])\n        nn.init.uniform_(m.weight, -10,10) #参数-> [-10,10]\n        m.weight.data *= m.weight.data.abs() >= 5 # 参数abs>=5的概率=1/2 ， 1/2的概率=0，1/2的概率在两个分布\nnet.apply(my_init)\nprint(net[0].weight[:2])\n# 直接设置参数\nnet[0].weight.data[:] += 1\nnet[0].weight.data[0,0] = 42\nprint(net[0].weight.data[0])\n```\n\n    Init weight torch.Size([8, 4])\n    Init weight torch.Size([1, 8])\n    tensor([[ 8.4257,  0.0000, -7.9400, -0.0000],\n            [ 0.0000,  8.8472,  5.5845, -0.0000]], grad_fn=<SliceBackward0>)\n    tensor([42.0000,  1.0000, -6.9400,  1.0000])\n    \n\n## 2.3 参数绑定\n- 有时我们希望在多个层之间共享参数\n\n\n```python\n# 定义一个共享层\nshared = nn.Linear(8,8)\nnet = nn.Sequential(nn.Linear(4,8), nn.ReLU(),\n                    shared, nn.ReLU(),\n                    shared, nn.ReLU(),\n                    nn.Linear(8,1))\nnet(X)\n# 检查参数是否相同\nprint(net[2].weight.data[0] == net[4].weight.data[0])\nnet[2].weight.data[0,0] = 100\n# 确保它们是同一个对象，而不只是具有相同的值\nprint(net[2].weight.data[0] == net[4].weight.data[0])\n```\n\n    tensor([True, True, True, True, True, True, True, True])\n    tensor([True, True, True, True, True, True, True, True])\n    \n\n- 当参数绑定时，梯度会发生什么情况：由于模型参数包含梯度，因此在反向传播期间第二个隐藏层（net[2]）和第三个隐藏层（net[5]）的梯度会加在一起\n","slug":"deeplearning/code/pytorch/4_calculate/2_paramter","published":1,"updated":"2024-05-02T08:51:59.011Z","comments":1,"layout":"post","photos":[],"_id":"clvuli66n001ozsvwbw6kanwy","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"2-参数管理\"><a href=\"#2-参数管理\" class=\"headerlink\" title=\"2 参数管理\"></a>2 参数管理</h1><ul>\n<li>访问参数，用于调试、诊断和可视化</li>\n<li>参数初始化</li>\n<li>在不同模型组件间共享参数</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;单隐藏层的多层感知机&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\">net = nn.Sequential(nn.Linear(<span class=\"number\">4</span>,<span class=\"number\">8</span>), nn.ReLU(), nn.Linear(<span class=\"number\">8</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">X = torch.rand(size=(<span class=\"number\">2</span>,<span class=\"number\">4</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net(X))</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[0.2326],\n        [0.2463]], grad_fn=&lt;AddmmBackward0&gt;)\n</code></pre>\n<h2 id=\"2-1-参数访问\"><a href=\"#2-1-参数访问\" class=\"headerlink\" title=\"2.1 参数访问\"></a>2.1 参数访问</h2><ul>\n<li>对于Sequential类模型，通过索引访问参数</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(net[<span class=\"number\">2</span>].state_dict())</span><br><span class=\"line\"><span class=\"comment\">#输出的这个层包含两个参数，分别是权重和偏置。</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>OrderedDict([(&#39;weight&#39;, tensor([[ 0.0830, -0.2212, -0.3532,  0.2318,  0.2688, -0.1233, -0.2256, -0.1163]])), (&#39;bias&#39;, tensor([0.1769]))])\n</code></pre>\n<h3 id=\"2-1-1-目标参数\"><a href=\"#2-1-1-目标参数\" class=\"headerlink\" title=\"2.1.1 目标参数\"></a>2.1.1 目标参数</h3><ul>\n<li>提取第三层的偏置</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">param = net[<span class=\"number\">2</span>].bias</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(param)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">type</span>(param)) <span class=\"comment\">#类型</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(param.data) <span class=\"comment\">#数据</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>Parameter containing:\ntensor([0.1769], requires_grad=True)\n&lt;class &#39;torch.nn.parameter.Parameter&#39;&gt;\ntensor([0.1769])\n</code></pre>\n<ul>\n<li>参数是复合对象，包含值、梯度和额外信息</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 访问每个参数的梯度</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(net[<span class=\"number\">2</span>].weight.grad == <span class=\"literal\">None</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>True\n</code></pre>\n<ul>\n<li>访问所有参数</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 访问第一个全连接层的参数</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(*[(name, param.shape) <span class=\"keyword\">for</span> name, param <span class=\"keyword\">in</span> net[<span class=\"number\">0</span>].named_parameters()])</span><br><span class=\"line\"><span class=\"comment\"># 访问所有层</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(*[(name, param.shape) <span class=\"keyword\">for</span> name, param <span class=\"keyword\">in</span> net.named_parameters()])</span><br><span class=\"line\"><span class=\"comment\"># 访问一个参数</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(net.state_dict()[<span class=\"string\">&#x27;2.bias&#x27;</span>].data)</span><br></pre></td></tr></table></figure>\n\n<pre><code>(&#39;weight&#39;, torch.Size([8, 4])) (&#39;bias&#39;, torch.Size([8]))\n(&#39;0.weight&#39;, torch.Size([8, 4])) (&#39;0.bias&#39;, torch.Size([8])) (&#39;2.weight&#39;, torch.Size([1, 8])) (&#39;2.bias&#39;, torch.Size([1]))\ntensor([0.1769])\n</code></pre>\n<ul>\n<li>从嵌套块收集参数</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">block1</span>():</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nn.Sequential(</span><br><span class=\"line\">        nn.Linear(<span class=\"number\">4</span>,<span class=\"number\">8</span>),nn.ReLU(),</span><br><span class=\"line\">        nn.Linear(<span class=\"number\">8</span>,<span class=\"number\">4</span>), nn.ReLU()</span><br><span class=\"line\">    )</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">block2</span>():</span><br><span class=\"line\">    net = nn.Sequential()</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">4</span>):</span><br><span class=\"line\">        net.add_module(<span class=\"string\">f&#x27;block<span class=\"subst\">&#123;i&#125;</span>&#x27;</span>,block1()) <span class=\"comment\">#增加一个模块</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> net</span><br><span class=\"line\"></span><br><span class=\"line\">rgnet = nn.Sequential(block2(), nn.Linear(<span class=\"number\">4</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(rgnet(X))</span><br><span class=\"line\"><span class=\"comment\"># 输出网络</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(rgnet)</span><br><span class=\"line\"><span class=\"comment\"># 访问第一个主要的模块中第二个子模块第一层的偏置</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(rgnet[<span class=\"number\">0</span>][<span class=\"number\">1</span>][<span class=\"number\">0</span>].bias.data)</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[-0.2233],\n        [-0.2233]], grad_fn=&lt;AddmmBackward0&gt;)\nSequential(\n  (0): Sequential(\n    (block0): Sequential(\n      (0): Linear(in_features=4, out_features=8, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=8, out_features=4, bias=True)\n      (3): ReLU()\n    )\n    (block1): Sequential(\n      (0): Linear(in_features=4, out_features=8, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=8, out_features=4, bias=True)\n      (3): ReLU()\n    )\n    (block2): Sequential(\n      (0): Linear(in_features=4, out_features=8, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=8, out_features=4, bias=True)\n      (3): ReLU()\n    )\n    (block3): Sequential(\n      (0): Linear(in_features=4, out_features=8, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=8, out_features=4, bias=True)\n      (3): ReLU()\n    )\n  )\n  (1): Linear(in_features=4, out_features=1, bias=True)\n)\ntensor([ 0.4794,  0.4585,  0.0098,  0.1719, -0.2016, -0.3174, -0.4974,  0.4953])\n</code></pre>\n<h2 id=\"2-2-参数初始化\"><a href=\"#2-2-参数初始化\" class=\"headerlink\" title=\"2.2 参数初始化\"></a>2.2 参数初始化</h2><ul>\n<li>默认情况下，pytorch会根据一个均匀地初始化权重和偏置</li>\n<li>pytorch的nn.init模块提供了多种初始化方法</li>\n</ul>\n<h3 id=\"2-2-1-内置初始化\"><a href=\"#2-2-1-内置初始化\" class=\"headerlink\" title=\"2.2.1 内置初始化\"></a>2.2.1 内置初始化</h3><ul>\n<li>权重初始化标准差为0.01的高斯随机变量，偏置设置为0</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_normal</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear:</span><br><span class=\"line\">        nn.init.normal_(m.weight, mean = <span class=\"number\">0</span>, std=<span class=\"number\">0.01</span>)</span><br><span class=\"line\">        nn.init.zeros_(m.bias)</span><br><span class=\"line\">net.apply(init_normal)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net[<span class=\"number\">0</span>].weight.data[<span class=\"number\">0</span>], net[<span class=\"number\">0</span>].bias.data[<span class=\"number\">0</span>])</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([ 0.0058, -0.0130, -0.0114,  0.0044]) tensor(0.)\n</code></pre>\n<ul>\n<li>初始化为常熟</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_constant</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear:</span><br><span class=\"line\">        nn.init.constant_(m.weight,<span class=\"number\">1</span>)</span><br><span class=\"line\">        nn.init.zeros_(m.bias)</span><br><span class=\"line\">net.apply(init_constant)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net[<span class=\"number\">0</span>].weight.data[<span class=\"number\">0</span>],net[<span class=\"number\">0</span>].bias.data[<span class=\"number\">0</span>])</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([1., 1., 1., 1.]) tensor(0.)\n</code></pre>\n<ul>\n<li>不同块使用不同初始化方法：用Xavier初始化方法初始化第一层，第三层初始化为常熟42</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_xavier</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear:</span><br><span class=\"line\">        nn.init.xavier_uniform_(m.weight)</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_42</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear:</span><br><span class=\"line\">        nn.init.constant_(m.weight, <span class=\"number\">42</span>)</span><br><span class=\"line\">net[<span class=\"number\">0</span>].apply(init_xavier)</span><br><span class=\"line\">net[<span class=\"number\">2</span>].apply(init_42)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net[<span class=\"number\">0</span>].weight.data[<span class=\"number\">0</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net[<span class=\"number\">2</span>].weight.data)</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([ 0.2578, -0.6072, -0.2665,  0.4973])\ntensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n</code></pre>\n<h3 id=\"2-2-2-自定义初始化\"><a href=\"#2-2-2-自定义初始化\" class=\"headerlink\" title=\"2.2.2 自定义初始化\"></a>2.2.2 自定义初始化</h3><p>$$ \\omega &#x3D; \\begin{cases} U(5,10) &amp;可能性 \\frac{1}{4} \\ 0 &amp;可能性 \\frac{1}{2} \\ U(-10,-5) &amp;可能性 \\frac{1}{4} \\ \\end{cases} $$</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">my_init</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&quot;Init&quot;</span>, *[(name, param.shape)</span><br><span class=\"line\">                        <span class=\"keyword\">for</span> name, param <span class=\"keyword\">in</span> m.named_parameters()][<span class=\"number\">0</span>])</span><br><span class=\"line\">        nn.init.uniform_(m.weight, -<span class=\"number\">10</span>,<span class=\"number\">10</span>) <span class=\"comment\">#参数-&gt; [-10,10]</span></span><br><span class=\"line\">        m.weight.data *= m.weight.data.<span class=\"built_in\">abs</span>() &gt;= <span class=\"number\">5</span> <span class=\"comment\"># 参数abs&gt;=5的概率=1/2 ， 1/2的概率=0，1/2的概率在两个分布</span></span><br><span class=\"line\">net.apply(my_init)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net[<span class=\"number\">0</span>].weight[:<span class=\"number\">2</span>])</span><br><span class=\"line\"><span class=\"comment\"># 直接设置参数</span></span><br><span class=\"line\">net[<span class=\"number\">0</span>].weight.data[:] += <span class=\"number\">1</span></span><br><span class=\"line\">net[<span class=\"number\">0</span>].weight.data[<span class=\"number\">0</span>,<span class=\"number\">0</span>] = <span class=\"number\">42</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(net[<span class=\"number\">0</span>].weight.data[<span class=\"number\">0</span>])</span><br></pre></td></tr></table></figure>\n\n<pre><code>Init weight torch.Size([8, 4])\nInit weight torch.Size([1, 8])\ntensor([[ 8.4257,  0.0000, -7.9400, -0.0000],\n        [ 0.0000,  8.8472,  5.5845, -0.0000]], grad_fn=&lt;SliceBackward0&gt;)\ntensor([42.0000,  1.0000, -6.9400,  1.0000])\n</code></pre>\n<h2 id=\"2-3-参数绑定\"><a href=\"#2-3-参数绑定\" class=\"headerlink\" title=\"2.3 参数绑定\"></a>2.3 参数绑定</h2><ul>\n<li>有时我们希望在多个层之间共享参数</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 定义一个共享层</span></span><br><span class=\"line\">shared = nn.Linear(<span class=\"number\">8</span>,<span class=\"number\">8</span>)</span><br><span class=\"line\">net = nn.Sequential(nn.Linear(<span class=\"number\">4</span>,<span class=\"number\">8</span>), nn.ReLU(),</span><br><span class=\"line\">                    shared, nn.ReLU(),</span><br><span class=\"line\">                    shared, nn.ReLU(),</span><br><span class=\"line\">                    nn.Linear(<span class=\"number\">8</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">net(X)</span><br><span class=\"line\"><span class=\"comment\"># 检查参数是否相同</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(net[<span class=\"number\">2</span>].weight.data[<span class=\"number\">0</span>] == net[<span class=\"number\">4</span>].weight.data[<span class=\"number\">0</span>])</span><br><span class=\"line\">net[<span class=\"number\">2</span>].weight.data[<span class=\"number\">0</span>,<span class=\"number\">0</span>] = <span class=\"number\">100</span></span><br><span class=\"line\"><span class=\"comment\"># 确保它们是同一个对象，而不只是具有相同的值</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(net[<span class=\"number\">2</span>].weight.data[<span class=\"number\">0</span>] == net[<span class=\"number\">4</span>].weight.data[<span class=\"number\">0</span>])</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([True, True, True, True, True, True, True, True])\ntensor([True, True, True, True, True, True, True, True])\n</code></pre>\n<ul>\n<li>当参数绑定时，梯度会发生什么情况：由于模型参数包含梯度，因此在反向传播期间第二个隐藏层（net[2]）和第三个隐藏层（net[5]）的梯度会加在一起</li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"2-参数管理\"><a href=\"#2-参数管理\" class=\"headerlink\" title=\"2 参数管理\"></a>2 参数管理</h1><ul>\n<li>访问参数，用于调试、诊断和可视化</li>\n<li>参数初始化</li>\n<li>在不同模型组件间共享参数</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;单隐藏层的多层感知机&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\">net = nn.Sequential(nn.Linear(<span class=\"number\">4</span>,<span class=\"number\">8</span>), nn.ReLU(), nn.Linear(<span class=\"number\">8</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">X = torch.rand(size=(<span class=\"number\">2</span>,<span class=\"number\">4</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net(X))</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[0.2326],\n        [0.2463]], grad_fn=&lt;AddmmBackward0&gt;)\n</code></pre>\n<h2 id=\"2-1-参数访问\"><a href=\"#2-1-参数访问\" class=\"headerlink\" title=\"2.1 参数访问\"></a>2.1 参数访问</h2><ul>\n<li>对于Sequential类模型，通过索引访问参数</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(net[<span class=\"number\">2</span>].state_dict())</span><br><span class=\"line\"><span class=\"comment\">#输出的这个层包含两个参数，分别是权重和偏置。</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>OrderedDict([(&#39;weight&#39;, tensor([[ 0.0830, -0.2212, -0.3532,  0.2318,  0.2688, -0.1233, -0.2256, -0.1163]])), (&#39;bias&#39;, tensor([0.1769]))])\n</code></pre>\n<h3 id=\"2-1-1-目标参数\"><a href=\"#2-1-1-目标参数\" class=\"headerlink\" title=\"2.1.1 目标参数\"></a>2.1.1 目标参数</h3><ul>\n<li>提取第三层的偏置</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">param = net[<span class=\"number\">2</span>].bias</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(param)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">type</span>(param)) <span class=\"comment\">#类型</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(param.data) <span class=\"comment\">#数据</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>Parameter containing:\ntensor([0.1769], requires_grad=True)\n&lt;class &#39;torch.nn.parameter.Parameter&#39;&gt;\ntensor([0.1769])\n</code></pre>\n<ul>\n<li>参数是复合对象，包含值、梯度和额外信息</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 访问每个参数的梯度</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(net[<span class=\"number\">2</span>].weight.grad == <span class=\"literal\">None</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>True\n</code></pre>\n<ul>\n<li>访问所有参数</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 访问第一个全连接层的参数</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(*[(name, param.shape) <span class=\"keyword\">for</span> name, param <span class=\"keyword\">in</span> net[<span class=\"number\">0</span>].named_parameters()])</span><br><span class=\"line\"><span class=\"comment\"># 访问所有层</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(*[(name, param.shape) <span class=\"keyword\">for</span> name, param <span class=\"keyword\">in</span> net.named_parameters()])</span><br><span class=\"line\"><span class=\"comment\"># 访问一个参数</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(net.state_dict()[<span class=\"string\">&#x27;2.bias&#x27;</span>].data)</span><br></pre></td></tr></table></figure>\n\n<pre><code>(&#39;weight&#39;, torch.Size([8, 4])) (&#39;bias&#39;, torch.Size([8]))\n(&#39;0.weight&#39;, torch.Size([8, 4])) (&#39;0.bias&#39;, torch.Size([8])) (&#39;2.weight&#39;, torch.Size([1, 8])) (&#39;2.bias&#39;, torch.Size([1]))\ntensor([0.1769])\n</code></pre>\n<ul>\n<li>从嵌套块收集参数</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">block1</span>():</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nn.Sequential(</span><br><span class=\"line\">        nn.Linear(<span class=\"number\">4</span>,<span class=\"number\">8</span>),nn.ReLU(),</span><br><span class=\"line\">        nn.Linear(<span class=\"number\">8</span>,<span class=\"number\">4</span>), nn.ReLU()</span><br><span class=\"line\">    )</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">block2</span>():</span><br><span class=\"line\">    net = nn.Sequential()</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">4</span>):</span><br><span class=\"line\">        net.add_module(<span class=\"string\">f&#x27;block<span class=\"subst\">&#123;i&#125;</span>&#x27;</span>,block1()) <span class=\"comment\">#增加一个模块</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> net</span><br><span class=\"line\"></span><br><span class=\"line\">rgnet = nn.Sequential(block2(), nn.Linear(<span class=\"number\">4</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(rgnet(X))</span><br><span class=\"line\"><span class=\"comment\"># 输出网络</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(rgnet)</span><br><span class=\"line\"><span class=\"comment\"># 访问第一个主要的模块中第二个子模块第一层的偏置</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(rgnet[<span class=\"number\">0</span>][<span class=\"number\">1</span>][<span class=\"number\">0</span>].bias.data)</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[-0.2233],\n        [-0.2233]], grad_fn=&lt;AddmmBackward0&gt;)\nSequential(\n  (0): Sequential(\n    (block0): Sequential(\n      (0): Linear(in_features=4, out_features=8, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=8, out_features=4, bias=True)\n      (3): ReLU()\n    )\n    (block1): Sequential(\n      (0): Linear(in_features=4, out_features=8, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=8, out_features=4, bias=True)\n      (3): ReLU()\n    )\n    (block2): Sequential(\n      (0): Linear(in_features=4, out_features=8, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=8, out_features=4, bias=True)\n      (3): ReLU()\n    )\n    (block3): Sequential(\n      (0): Linear(in_features=4, out_features=8, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=8, out_features=4, bias=True)\n      (3): ReLU()\n    )\n  )\n  (1): Linear(in_features=4, out_features=1, bias=True)\n)\ntensor([ 0.4794,  0.4585,  0.0098,  0.1719, -0.2016, -0.3174, -0.4974,  0.4953])\n</code></pre>\n<h2 id=\"2-2-参数初始化\"><a href=\"#2-2-参数初始化\" class=\"headerlink\" title=\"2.2 参数初始化\"></a>2.2 参数初始化</h2><ul>\n<li>默认情况下，pytorch会根据一个均匀地初始化权重和偏置</li>\n<li>pytorch的nn.init模块提供了多种初始化方法</li>\n</ul>\n<h3 id=\"2-2-1-内置初始化\"><a href=\"#2-2-1-内置初始化\" class=\"headerlink\" title=\"2.2.1 内置初始化\"></a>2.2.1 内置初始化</h3><ul>\n<li>权重初始化标准差为0.01的高斯随机变量，偏置设置为0</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_normal</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear:</span><br><span class=\"line\">        nn.init.normal_(m.weight, mean = <span class=\"number\">0</span>, std=<span class=\"number\">0.01</span>)</span><br><span class=\"line\">        nn.init.zeros_(m.bias)</span><br><span class=\"line\">net.apply(init_normal)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net[<span class=\"number\">0</span>].weight.data[<span class=\"number\">0</span>], net[<span class=\"number\">0</span>].bias.data[<span class=\"number\">0</span>])</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([ 0.0058, -0.0130, -0.0114,  0.0044]) tensor(0.)\n</code></pre>\n<ul>\n<li>初始化为常熟</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_constant</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear:</span><br><span class=\"line\">        nn.init.constant_(m.weight,<span class=\"number\">1</span>)</span><br><span class=\"line\">        nn.init.zeros_(m.bias)</span><br><span class=\"line\">net.apply(init_constant)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net[<span class=\"number\">0</span>].weight.data[<span class=\"number\">0</span>],net[<span class=\"number\">0</span>].bias.data[<span class=\"number\">0</span>])</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([1., 1., 1., 1.]) tensor(0.)\n</code></pre>\n<ul>\n<li>不同块使用不同初始化方法：用Xavier初始化方法初始化第一层，第三层初始化为常熟42</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_xavier</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear:</span><br><span class=\"line\">        nn.init.xavier_uniform_(m.weight)</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_42</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear:</span><br><span class=\"line\">        nn.init.constant_(m.weight, <span class=\"number\">42</span>)</span><br><span class=\"line\">net[<span class=\"number\">0</span>].apply(init_xavier)</span><br><span class=\"line\">net[<span class=\"number\">2</span>].apply(init_42)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net[<span class=\"number\">0</span>].weight.data[<span class=\"number\">0</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net[<span class=\"number\">2</span>].weight.data)</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([ 0.2578, -0.6072, -0.2665,  0.4973])\ntensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n</code></pre>\n<h3 id=\"2-2-2-自定义初始化\"><a href=\"#2-2-2-自定义初始化\" class=\"headerlink\" title=\"2.2.2 自定义初始化\"></a>2.2.2 自定义初始化</h3><p>$$ \\omega &#x3D; \\begin{cases} U(5,10) &amp;可能性 \\frac{1}{4} \\ 0 &amp;可能性 \\frac{1}{2} \\ U(-10,-5) &amp;可能性 \\frac{1}{4} \\ \\end{cases} $$</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">my_init</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&quot;Init&quot;</span>, *[(name, param.shape)</span><br><span class=\"line\">                        <span class=\"keyword\">for</span> name, param <span class=\"keyword\">in</span> m.named_parameters()][<span class=\"number\">0</span>])</span><br><span class=\"line\">        nn.init.uniform_(m.weight, -<span class=\"number\">10</span>,<span class=\"number\">10</span>) <span class=\"comment\">#参数-&gt; [-10,10]</span></span><br><span class=\"line\">        m.weight.data *= m.weight.data.<span class=\"built_in\">abs</span>() &gt;= <span class=\"number\">5</span> <span class=\"comment\"># 参数abs&gt;=5的概率=1/2 ， 1/2的概率=0，1/2的概率在两个分布</span></span><br><span class=\"line\">net.apply(my_init)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net[<span class=\"number\">0</span>].weight[:<span class=\"number\">2</span>])</span><br><span class=\"line\"><span class=\"comment\"># 直接设置参数</span></span><br><span class=\"line\">net[<span class=\"number\">0</span>].weight.data[:] += <span class=\"number\">1</span></span><br><span class=\"line\">net[<span class=\"number\">0</span>].weight.data[<span class=\"number\">0</span>,<span class=\"number\">0</span>] = <span class=\"number\">42</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(net[<span class=\"number\">0</span>].weight.data[<span class=\"number\">0</span>])</span><br></pre></td></tr></table></figure>\n\n<pre><code>Init weight torch.Size([8, 4])\nInit weight torch.Size([1, 8])\ntensor([[ 8.4257,  0.0000, -7.9400, -0.0000],\n        [ 0.0000,  8.8472,  5.5845, -0.0000]], grad_fn=&lt;SliceBackward0&gt;)\ntensor([42.0000,  1.0000, -6.9400,  1.0000])\n</code></pre>\n<h2 id=\"2-3-参数绑定\"><a href=\"#2-3-参数绑定\" class=\"headerlink\" title=\"2.3 参数绑定\"></a>2.3 参数绑定</h2><ul>\n<li>有时我们希望在多个层之间共享参数</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 定义一个共享层</span></span><br><span class=\"line\">shared = nn.Linear(<span class=\"number\">8</span>,<span class=\"number\">8</span>)</span><br><span class=\"line\">net = nn.Sequential(nn.Linear(<span class=\"number\">4</span>,<span class=\"number\">8</span>), nn.ReLU(),</span><br><span class=\"line\">                    shared, nn.ReLU(),</span><br><span class=\"line\">                    shared, nn.ReLU(),</span><br><span class=\"line\">                    nn.Linear(<span class=\"number\">8</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">net(X)</span><br><span class=\"line\"><span class=\"comment\"># 检查参数是否相同</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(net[<span class=\"number\">2</span>].weight.data[<span class=\"number\">0</span>] == net[<span class=\"number\">4</span>].weight.data[<span class=\"number\">0</span>])</span><br><span class=\"line\">net[<span class=\"number\">2</span>].weight.data[<span class=\"number\">0</span>,<span class=\"number\">0</span>] = <span class=\"number\">100</span></span><br><span class=\"line\"><span class=\"comment\"># 确保它们是同一个对象，而不只是具有相同的值</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(net[<span class=\"number\">2</span>].weight.data[<span class=\"number\">0</span>] == net[<span class=\"number\">4</span>].weight.data[<span class=\"number\">0</span>])</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([True, True, True, True, True, True, True, True])\ntensor([True, True, True, True, True, True, True, True])\n</code></pre>\n<ul>\n<li>当参数绑定时，梯度会发生什么情况：由于模型参数包含梯度，因此在反向传播期间第二个隐藏层（net[2]）和第三个隐藏层（net[5]）的梯度会加在一起</li>\n</ul>"},{"title":"4.1 层和块","date":"2024-04-18T06:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 1 层和块\n- 单个神经网络：\n    - 一个输入\n    - 标量输出\n    - 一组相关参数，这些参数可以通过学习而优化\n- 层：\n    - 一组输入\n    - 一组输出\n    - 一组可调参数\n- 从编程的角度看，块由类来表示。通常需要定义一个将输入转换成输出的forward函数，并且必须存储任何必须的参数。\n- 定义一个网络：256个单元和ReLU的全连接隐藏层，10个隐藏单元且不带激活函数的全连接输出层\n    - nn.Sequential是一种特殊的module，表示一个块，维护了一个由module组成的有序列表\n    - net(x)相当于net.__call__(x)\n\n\n```python\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nnet= nn.Sequential( #一种特殊的module，表示一个块，维护了一个由module组成的有序列表\n    nn.Linear(20,256),\n    nn.ReLU(), \n    nn.Linear(256,10))\n\nX = torch.rand(2,20)\nprint(net(X))\n```\n\n    tensor([[ 0.0302, -0.1207, -0.0915,  0.0522, -0.3685,  0.0474,  0.0665, -0.0055,\n             -0.1445, -0.1954],\n            [ 0.0275, -0.1345,  0.0009,  0.0987, -0.4689,  0.0405,  0.0012, -0.0243,\n             -0.2954, -0.1414]], grad_fn=<AddmmBackward0>)\n    \n\n## 1.1 自定义块\n- 每个块必须提供的功能：\n    - 数据输入forward函数得到输出\n    - 计算输出关于输入的梯度，通过backward函数\n    - 存储和访问前向传播计算所需要的参数\n    - 初始化模型参数\n\n\n```python\nclass MLP(nn.Module):\n    # 用模型参数声明层。声明两个全连接层\n    def __init__(self):\n        super().__init__() #Module的构造函数进行必要的初始化\n        self.hidden = nn.Linear(20,256) #隐藏层\n        self.out = nn.Linear(256,10) #输出层\n\n    # 前行传播，如何根据输入x返回所需的模型输出\n    def forward(self,X):\n        return self.out(F.relu(self.hidden(X)))\n    \nnet = MLP()\nprint(net(X))\n```\n\n    tensor([[ 0.2163,  0.0644, -0.1128, -0.2947,  0.0708,  0.0907, -0.0680, -0.0381,\n             -0.0843,  0.0921],\n            [ 0.2196,  0.0087,  0.0257, -0.1403, -0.0191,  0.0435, -0.1980,  0.0350,\n              0.0158,  0.0848]], grad_fn=<AddmmBackward0>)\n    \n\n## 1.2 顺序块\n- 构建自己简化的Sequential类需要\n    - 将block逐个追加到列表中\n    - forward函数中，将输入按顺序传递\n\n\n```python\nclass MySequential(nn.Module):\n    def __init__(self, *args) -> None:\n        super().__init__()\n        for idx, module in enumerate(args):\n            # module是Module子类的一个实例\n            # _modules中，_module的类型是OrderedDict\n            #为啥每个Module都有一个_modules属性，为啥不用python列表？\n            # _modules优点：在模块的参数初始化过程中，系统知道在_modules字典中查找需要初始化参数的子块。\n            self._modules[str(idx)] = module\n    def forward(self, X):\n        for block in self._modules.values():\n            X = block(X)\n        return X\n    \nnet = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256,10))\nprint(net(X))\n```\n\n    tensor([[-0.1585, -0.0420,  0.3813, -0.3592, -0.0889,  0.0362, -0.0543, -0.0557,\n             -0.2022,  0.0183],\n            [-0.0438, -0.1248,  0.5774, -0.3087, -0.0576, -0.0479,  0.0954, -0.2362,\n             -0.2333, -0.1394]], grad_fn=<AddmmBackward0>)\n    \n\n## 1.3 在前向传播函数中执行代码\n- 有时我们希望合并既不是上一层的结果也不是可更新参数的项，成为常数参数。\n\n\n```python\nclass FixedHiddenMLP(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        # 不计算梯度的随机权重参数，因此其在训练期间保持不变\n        self.rand_weight = torch.rand((20,20), requires_grad=False)\n        self.linear = nn.Linear(20,20)\n\n    def forward(self, X):\n        X = self.linear(X)\n        # 使用创建的常量参数以及relu和mm函数\n        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n        # 复用全连接层。相当于两个全连接成共享参数\n        X = self.linear(X)\n        # 控制流：\n        while X.abs().sum() > 1: #l1范数\n            X/=2\n        return X.sum()\nnet = FixedHiddenMLP()\nprint(net(X))\n```\n\n    tensor(-0.2985, grad_fn=<SumBackward0>)\n    \n\n## 1.4 效率\n我们在一个高性能的深度学习库中进行了大量的字典查找、代码执行和许多其他的Python代码。Python的问题全局解释器锁是众所周知的。在深度学习环境中，我们担心速度极快的GPU可能要等到CPU运行Python代码后才能运行另一个作业。\n","source":"_posts/deeplearning/code/pytorch/4_calculate/1_block.md","raw":"---\ntitle: 4.1 层和块\ndate: 2024-4-18 14:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 1 层和块\n- 单个神经网络：\n    - 一个输入\n    - 标量输出\n    - 一组相关参数，这些参数可以通过学习而优化\n- 层：\n    - 一组输入\n    - 一组输出\n    - 一组可调参数\n- 从编程的角度看，块由类来表示。通常需要定义一个将输入转换成输出的forward函数，并且必须存储任何必须的参数。\n- 定义一个网络：256个单元和ReLU的全连接隐藏层，10个隐藏单元且不带激活函数的全连接输出层\n    - nn.Sequential是一种特殊的module，表示一个块，维护了一个由module组成的有序列表\n    - net(x)相当于net.__call__(x)\n\n\n```python\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nnet= nn.Sequential( #一种特殊的module，表示一个块，维护了一个由module组成的有序列表\n    nn.Linear(20,256),\n    nn.ReLU(), \n    nn.Linear(256,10))\n\nX = torch.rand(2,20)\nprint(net(X))\n```\n\n    tensor([[ 0.0302, -0.1207, -0.0915,  0.0522, -0.3685,  0.0474,  0.0665, -0.0055,\n             -0.1445, -0.1954],\n            [ 0.0275, -0.1345,  0.0009,  0.0987, -0.4689,  0.0405,  0.0012, -0.0243,\n             -0.2954, -0.1414]], grad_fn=<AddmmBackward0>)\n    \n\n## 1.1 自定义块\n- 每个块必须提供的功能：\n    - 数据输入forward函数得到输出\n    - 计算输出关于输入的梯度，通过backward函数\n    - 存储和访问前向传播计算所需要的参数\n    - 初始化模型参数\n\n\n```python\nclass MLP(nn.Module):\n    # 用模型参数声明层。声明两个全连接层\n    def __init__(self):\n        super().__init__() #Module的构造函数进行必要的初始化\n        self.hidden = nn.Linear(20,256) #隐藏层\n        self.out = nn.Linear(256,10) #输出层\n\n    # 前行传播，如何根据输入x返回所需的模型输出\n    def forward(self,X):\n        return self.out(F.relu(self.hidden(X)))\n    \nnet = MLP()\nprint(net(X))\n```\n\n    tensor([[ 0.2163,  0.0644, -0.1128, -0.2947,  0.0708,  0.0907, -0.0680, -0.0381,\n             -0.0843,  0.0921],\n            [ 0.2196,  0.0087,  0.0257, -0.1403, -0.0191,  0.0435, -0.1980,  0.0350,\n              0.0158,  0.0848]], grad_fn=<AddmmBackward0>)\n    \n\n## 1.2 顺序块\n- 构建自己简化的Sequential类需要\n    - 将block逐个追加到列表中\n    - forward函数中，将输入按顺序传递\n\n\n```python\nclass MySequential(nn.Module):\n    def __init__(self, *args) -> None:\n        super().__init__()\n        for idx, module in enumerate(args):\n            # module是Module子类的一个实例\n            # _modules中，_module的类型是OrderedDict\n            #为啥每个Module都有一个_modules属性，为啥不用python列表？\n            # _modules优点：在模块的参数初始化过程中，系统知道在_modules字典中查找需要初始化参数的子块。\n            self._modules[str(idx)] = module\n    def forward(self, X):\n        for block in self._modules.values():\n            X = block(X)\n        return X\n    \nnet = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256,10))\nprint(net(X))\n```\n\n    tensor([[-0.1585, -0.0420,  0.3813, -0.3592, -0.0889,  0.0362, -0.0543, -0.0557,\n             -0.2022,  0.0183],\n            [-0.0438, -0.1248,  0.5774, -0.3087, -0.0576, -0.0479,  0.0954, -0.2362,\n             -0.2333, -0.1394]], grad_fn=<AddmmBackward0>)\n    \n\n## 1.3 在前向传播函数中执行代码\n- 有时我们希望合并既不是上一层的结果也不是可更新参数的项，成为常数参数。\n\n\n```python\nclass FixedHiddenMLP(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        # 不计算梯度的随机权重参数，因此其在训练期间保持不变\n        self.rand_weight = torch.rand((20,20), requires_grad=False)\n        self.linear = nn.Linear(20,20)\n\n    def forward(self, X):\n        X = self.linear(X)\n        # 使用创建的常量参数以及relu和mm函数\n        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n        # 复用全连接层。相当于两个全连接成共享参数\n        X = self.linear(X)\n        # 控制流：\n        while X.abs().sum() > 1: #l1范数\n            X/=2\n        return X.sum()\nnet = FixedHiddenMLP()\nprint(net(X))\n```\n\n    tensor(-0.2985, grad_fn=<SumBackward0>)\n    \n\n## 1.4 效率\n我们在一个高性能的深度学习库中进行了大量的字典查找、代码执行和许多其他的Python代码。Python的问题全局解释器锁是众所周知的。在深度学习环境中，我们担心速度极快的GPU可能要等到CPU运行Python代码后才能运行另一个作业。\n","slug":"deeplearning/code/pytorch/4_calculate/1_block","published":1,"updated":"2024-05-02T08:51:50.742Z","comments":1,"layout":"post","photos":[],"_id":"clvuli66o001pzsvw2hg1azdc","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"1-层和块\"><a href=\"#1-层和块\" class=\"headerlink\" title=\"1 层和块\"></a>1 层和块</h1><ul>\n<li>单个神经网络：<ul>\n<li>一个输入</li>\n<li>标量输出</li>\n<li>一组相关参数，这些参数可以通过学习而优化</li>\n</ul>\n</li>\n<li>层：<ul>\n<li>一组输入</li>\n<li>一组输出</li>\n<li>一组可调参数</li>\n</ul>\n</li>\n<li>从编程的角度看，块由类来表示。通常需要定义一个将输入转换成输出的forward函数，并且必须存储任何必须的参数。</li>\n<li>定义一个网络：256个单元和ReLU的全连接隐藏层，10个隐藏单元且不带激活函数的全连接输出层<ul>\n<li>nn.Sequential是一种特殊的module，表示一个块，维护了一个由module组成的有序列表</li>\n<li>net(x)相当于net.<strong>call</strong>(x)</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.nn <span class=\"keyword\">import</span> functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"></span><br><span class=\"line\">net= nn.Sequential( <span class=\"comment\">#一种特殊的module，表示一个块，维护了一个由module组成的有序列表</span></span><br><span class=\"line\">    nn.Linear(<span class=\"number\">20</span>,<span class=\"number\">256</span>),</span><br><span class=\"line\">    nn.ReLU(), </span><br><span class=\"line\">    nn.Linear(<span class=\"number\">256</span>,<span class=\"number\">10</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">X = torch.rand(<span class=\"number\">2</span>,<span class=\"number\">20</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net(X))</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[ 0.0302, -0.1207, -0.0915,  0.0522, -0.3685,  0.0474,  0.0665, -0.0055,\n         -0.1445, -0.1954],\n        [ 0.0275, -0.1345,  0.0009,  0.0987, -0.4689,  0.0405,  0.0012, -0.0243,\n         -0.2954, -0.1414]], grad_fn=&lt;AddmmBackward0&gt;)\n</code></pre>\n<h2 id=\"1-1-自定义块\"><a href=\"#1-1-自定义块\" class=\"headerlink\" title=\"1.1 自定义块\"></a>1.1 自定义块</h2><ul>\n<li>每个块必须提供的功能：<ul>\n<li>数据输入forward函数得到输出</li>\n<li>计算输出关于输入的梯度，通过backward函数</li>\n<li>存储和访问前向传播计算所需要的参数</li>\n<li>初始化模型参数</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MLP</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"comment\"># 用模型参数声明层。声明两个全连接层</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__() <span class=\"comment\">#Module的构造函数进行必要的初始化</span></span><br><span class=\"line\">        self.hidden = nn.Linear(<span class=\"number\">20</span>,<span class=\"number\">256</span>) <span class=\"comment\">#隐藏层</span></span><br><span class=\"line\">        self.out = nn.Linear(<span class=\"number\">256</span>,<span class=\"number\">10</span>) <span class=\"comment\">#输出层</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 前行传播，如何根据输入x返回所需的模型输出</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self,X</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.out(F.relu(self.hidden(X)))</span><br><span class=\"line\">    </span><br><span class=\"line\">net = MLP()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net(X))</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[ 0.2163,  0.0644, -0.1128, -0.2947,  0.0708,  0.0907, -0.0680, -0.0381,\n         -0.0843,  0.0921],\n        [ 0.2196,  0.0087,  0.0257, -0.1403, -0.0191,  0.0435, -0.1980,  0.0350,\n          0.0158,  0.0848]], grad_fn=&lt;AddmmBackward0&gt;)\n</code></pre>\n<h2 id=\"1-2-顺序块\"><a href=\"#1-2-顺序块\" class=\"headerlink\" title=\"1.2 顺序块\"></a>1.2 顺序块</h2><ul>\n<li>构建自己简化的Sequential类需要<ul>\n<li>将block逐个追加到列表中</li>\n<li>forward函数中，将输入按顺序传递</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MySequential</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, *args</span>) -&gt; <span class=\"literal\">None</span>:</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"keyword\">for</span> idx, module <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(args):</span><br><span class=\"line\">            <span class=\"comment\"># module是Module子类的一个实例</span></span><br><span class=\"line\">            <span class=\"comment\"># _modules中，_module的类型是OrderedDict</span></span><br><span class=\"line\">            <span class=\"comment\">#为啥每个Module都有一个_modules属性，为啥不用python列表？</span></span><br><span class=\"line\">            <span class=\"comment\"># _modules优点：在模块的参数初始化过程中，系统知道在_modules字典中查找需要初始化参数的子块。</span></span><br><span class=\"line\">            self._modules[<span class=\"built_in\">str</span>(idx)] = module</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> block <span class=\"keyword\">in</span> self._modules.values():</span><br><span class=\"line\">            X = block(X)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> X</span><br><span class=\"line\">    </span><br><span class=\"line\">net = MySequential(nn.Linear(<span class=\"number\">20</span>, <span class=\"number\">256</span>), nn.ReLU(), nn.Linear(<span class=\"number\">256</span>,<span class=\"number\">10</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net(X))</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[-0.1585, -0.0420,  0.3813, -0.3592, -0.0889,  0.0362, -0.0543, -0.0557,\n         -0.2022,  0.0183],\n        [-0.0438, -0.1248,  0.5774, -0.3087, -0.0576, -0.0479,  0.0954, -0.2362,\n         -0.2333, -0.1394]], grad_fn=&lt;AddmmBackward0&gt;)\n</code></pre>\n<h2 id=\"1-3-在前向传播函数中执行代码\"><a href=\"#1-3-在前向传播函数中执行代码\" class=\"headerlink\" title=\"1.3 在前向传播函数中执行代码\"></a>1.3 在前向传播函数中执行代码</h2><ul>\n<li>有时我们希望合并既不是上一层的结果也不是可更新参数的项，成为常数参数。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">FixedHiddenMLP</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>) -&gt; <span class=\"literal\">None</span>:</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"comment\"># 不计算梯度的随机权重参数，因此其在训练期间保持不变</span></span><br><span class=\"line\">        self.rand_weight = torch.rand((<span class=\"number\">20</span>,<span class=\"number\">20</span>), requires_grad=<span class=\"literal\">False</span>)</span><br><span class=\"line\">        self.linear = nn.Linear(<span class=\"number\">20</span>,<span class=\"number\">20</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        X = self.linear(X)</span><br><span class=\"line\">        <span class=\"comment\"># 使用创建的常量参数以及relu和mm函数</span></span><br><span class=\"line\">        X = F.relu(torch.mm(X, self.rand_weight) + <span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"comment\"># 复用全连接层。相当于两个全连接成共享参数</span></span><br><span class=\"line\">        X = self.linear(X)</span><br><span class=\"line\">        <span class=\"comment\"># 控制流：</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> X.<span class=\"built_in\">abs</span>().<span class=\"built_in\">sum</span>() &gt; <span class=\"number\">1</span>: <span class=\"comment\">#l1范数</span></span><br><span class=\"line\">            X/=<span class=\"number\">2</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> X.<span class=\"built_in\">sum</span>()</span><br><span class=\"line\">net = FixedHiddenMLP()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net(X))</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor(-0.2985, grad_fn=&lt;SumBackward0&gt;)\n</code></pre>\n<h2 id=\"1-4-效率\"><a href=\"#1-4-效率\" class=\"headerlink\" title=\"1.4 效率\"></a>1.4 效率</h2><p>我们在一个高性能的深度学习库中进行了大量的字典查找、代码执行和许多其他的Python代码。Python的问题全局解释器锁是众所周知的。在深度学习环境中，我们担心速度极快的GPU可能要等到CPU运行Python代码后才能运行另一个作业。</p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"1-层和块\"><a href=\"#1-层和块\" class=\"headerlink\" title=\"1 层和块\"></a>1 层和块</h1><ul>\n<li>单个神经网络：<ul>\n<li>一个输入</li>\n<li>标量输出</li>\n<li>一组相关参数，这些参数可以通过学习而优化</li>\n</ul>\n</li>\n<li>层：<ul>\n<li>一组输入</li>\n<li>一组输出</li>\n<li>一组可调参数</li>\n</ul>\n</li>\n<li>从编程的角度看，块由类来表示。通常需要定义一个将输入转换成输出的forward函数，并且必须存储任何必须的参数。</li>\n<li>定义一个网络：256个单元和ReLU的全连接隐藏层，10个隐藏单元且不带激活函数的全连接输出层<ul>\n<li>nn.Sequential是一种特殊的module，表示一个块，维护了一个由module组成的有序列表</li>\n<li>net(x)相当于net.<strong>call</strong>(x)</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.nn <span class=\"keyword\">import</span> functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"></span><br><span class=\"line\">net= nn.Sequential( <span class=\"comment\">#一种特殊的module，表示一个块，维护了一个由module组成的有序列表</span></span><br><span class=\"line\">    nn.Linear(<span class=\"number\">20</span>,<span class=\"number\">256</span>),</span><br><span class=\"line\">    nn.ReLU(), </span><br><span class=\"line\">    nn.Linear(<span class=\"number\">256</span>,<span class=\"number\">10</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">X = torch.rand(<span class=\"number\">2</span>,<span class=\"number\">20</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net(X))</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[ 0.0302, -0.1207, -0.0915,  0.0522, -0.3685,  0.0474,  0.0665, -0.0055,\n         -0.1445, -0.1954],\n        [ 0.0275, -0.1345,  0.0009,  0.0987, -0.4689,  0.0405,  0.0012, -0.0243,\n         -0.2954, -0.1414]], grad_fn=&lt;AddmmBackward0&gt;)\n</code></pre>\n<h2 id=\"1-1-自定义块\"><a href=\"#1-1-自定义块\" class=\"headerlink\" title=\"1.1 自定义块\"></a>1.1 自定义块</h2><ul>\n<li>每个块必须提供的功能：<ul>\n<li>数据输入forward函数得到输出</li>\n<li>计算输出关于输入的梯度，通过backward函数</li>\n<li>存储和访问前向传播计算所需要的参数</li>\n<li>初始化模型参数</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MLP</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"comment\"># 用模型参数声明层。声明两个全连接层</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__() <span class=\"comment\">#Module的构造函数进行必要的初始化</span></span><br><span class=\"line\">        self.hidden = nn.Linear(<span class=\"number\">20</span>,<span class=\"number\">256</span>) <span class=\"comment\">#隐藏层</span></span><br><span class=\"line\">        self.out = nn.Linear(<span class=\"number\">256</span>,<span class=\"number\">10</span>) <span class=\"comment\">#输出层</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 前行传播，如何根据输入x返回所需的模型输出</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self,X</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.out(F.relu(self.hidden(X)))</span><br><span class=\"line\">    </span><br><span class=\"line\">net = MLP()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net(X))</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[ 0.2163,  0.0644, -0.1128, -0.2947,  0.0708,  0.0907, -0.0680, -0.0381,\n         -0.0843,  0.0921],\n        [ 0.2196,  0.0087,  0.0257, -0.1403, -0.0191,  0.0435, -0.1980,  0.0350,\n          0.0158,  0.0848]], grad_fn=&lt;AddmmBackward0&gt;)\n</code></pre>\n<h2 id=\"1-2-顺序块\"><a href=\"#1-2-顺序块\" class=\"headerlink\" title=\"1.2 顺序块\"></a>1.2 顺序块</h2><ul>\n<li>构建自己简化的Sequential类需要<ul>\n<li>将block逐个追加到列表中</li>\n<li>forward函数中，将输入按顺序传递</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MySequential</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, *args</span>) -&gt; <span class=\"literal\">None</span>:</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"keyword\">for</span> idx, module <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(args):</span><br><span class=\"line\">            <span class=\"comment\"># module是Module子类的一个实例</span></span><br><span class=\"line\">            <span class=\"comment\"># _modules中，_module的类型是OrderedDict</span></span><br><span class=\"line\">            <span class=\"comment\">#为啥每个Module都有一个_modules属性，为啥不用python列表？</span></span><br><span class=\"line\">            <span class=\"comment\"># _modules优点：在模块的参数初始化过程中，系统知道在_modules字典中查找需要初始化参数的子块。</span></span><br><span class=\"line\">            self._modules[<span class=\"built_in\">str</span>(idx)] = module</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> block <span class=\"keyword\">in</span> self._modules.values():</span><br><span class=\"line\">            X = block(X)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> X</span><br><span class=\"line\">    </span><br><span class=\"line\">net = MySequential(nn.Linear(<span class=\"number\">20</span>, <span class=\"number\">256</span>), nn.ReLU(), nn.Linear(<span class=\"number\">256</span>,<span class=\"number\">10</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net(X))</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[-0.1585, -0.0420,  0.3813, -0.3592, -0.0889,  0.0362, -0.0543, -0.0557,\n         -0.2022,  0.0183],\n        [-0.0438, -0.1248,  0.5774, -0.3087, -0.0576, -0.0479,  0.0954, -0.2362,\n         -0.2333, -0.1394]], grad_fn=&lt;AddmmBackward0&gt;)\n</code></pre>\n<h2 id=\"1-3-在前向传播函数中执行代码\"><a href=\"#1-3-在前向传播函数中执行代码\" class=\"headerlink\" title=\"1.3 在前向传播函数中执行代码\"></a>1.3 在前向传播函数中执行代码</h2><ul>\n<li>有时我们希望合并既不是上一层的结果也不是可更新参数的项，成为常数参数。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">FixedHiddenMLP</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>) -&gt; <span class=\"literal\">None</span>:</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"comment\"># 不计算梯度的随机权重参数，因此其在训练期间保持不变</span></span><br><span class=\"line\">        self.rand_weight = torch.rand((<span class=\"number\">20</span>,<span class=\"number\">20</span>), requires_grad=<span class=\"literal\">False</span>)</span><br><span class=\"line\">        self.linear = nn.Linear(<span class=\"number\">20</span>,<span class=\"number\">20</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        X = self.linear(X)</span><br><span class=\"line\">        <span class=\"comment\"># 使用创建的常量参数以及relu和mm函数</span></span><br><span class=\"line\">        X = F.relu(torch.mm(X, self.rand_weight) + <span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"comment\"># 复用全连接层。相当于两个全连接成共享参数</span></span><br><span class=\"line\">        X = self.linear(X)</span><br><span class=\"line\">        <span class=\"comment\"># 控制流：</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> X.<span class=\"built_in\">abs</span>().<span class=\"built_in\">sum</span>() &gt; <span class=\"number\">1</span>: <span class=\"comment\">#l1范数</span></span><br><span class=\"line\">            X/=<span class=\"number\">2</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> X.<span class=\"built_in\">sum</span>()</span><br><span class=\"line\">net = FixedHiddenMLP()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net(X))</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor(-0.2985, grad_fn=&lt;SumBackward0&gt;)\n</code></pre>\n<h2 id=\"1-4-效率\"><a href=\"#1-4-效率\" class=\"headerlink\" title=\"1.4 效率\"></a>1.4 效率</h2><p>我们在一个高性能的深度学习库中进行了大量的字典查找、代码执行和许多其他的Python代码。Python的问题全局解释器锁是众所周知的。在深度学习环境中，我们担心速度极快的GPU可能要等到CPU运行Python代码后才能运行另一个作业。</p>"},{"title":"4.3 延后初始化","date":"2024-04-20T06:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 3 延后初始化\n- 在上一节：\n    - 定义了网络架构，但没有指定输入维度\n    - 添加层时没有指定前一层的输出维度\n    - 在初始化参数时，没有足够的信息来确定模型应该包含多少参数\n- 框架的延后初始化（defers initialization）：知道数据第一次通过模型传递时，框架才会动态地推断出每个层的大小\n## 3.1 实例化网络\n。。。\n","source":"_posts/deeplearning/code/pytorch/4_calculate/3_laterinit.md","raw":"---\ntitle: 4.3 延后初始化\ndate: 2024-4-20 14:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 3 延后初始化\n- 在上一节：\n    - 定义了网络架构，但没有指定输入维度\n    - 添加层时没有指定前一层的输出维度\n    - 在初始化参数时，没有足够的信息来确定模型应该包含多少参数\n- 框架的延后初始化（defers initialization）：知道数据第一次通过模型传递时，框架才会动态地推断出每个层的大小\n## 3.1 实例化网络\n。。。\n","slug":"deeplearning/code/pytorch/4_calculate/3_laterinit","published":1,"updated":"2024-05-02T08:52:07.122Z","comments":1,"layout":"post","photos":[],"_id":"clvuli66o001rzsvwgqpsf2my","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"3-延后初始化\"><a href=\"#3-延后初始化\" class=\"headerlink\" title=\"3 延后初始化\"></a>3 延后初始化</h1><ul>\n<li>在上一节：<ul>\n<li>定义了网络架构，但没有指定输入维度</li>\n<li>添加层时没有指定前一层的输出维度</li>\n<li>在初始化参数时，没有足够的信息来确定模型应该包含多少参数</li>\n</ul>\n</li>\n<li>框架的延后初始化（defers initialization）：知道数据第一次通过模型传递时，框架才会动态地推断出每个层的大小</li>\n</ul>\n<h2 id=\"3-1-实例化网络\"><a href=\"#3-1-实例化网络\" class=\"headerlink\" title=\"3.1 实例化网络\"></a>3.1 实例化网络</h2><p>。。。</p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"3-延后初始化\"><a href=\"#3-延后初始化\" class=\"headerlink\" title=\"3 延后初始化\"></a>3 延后初始化</h1><ul>\n<li>在上一节：<ul>\n<li>定义了网络架构，但没有指定输入维度</li>\n<li>添加层时没有指定前一层的输出维度</li>\n<li>在初始化参数时，没有足够的信息来确定模型应该包含多少参数</li>\n</ul>\n</li>\n<li>框架的延后初始化（defers initialization）：知道数据第一次通过模型传递时，框架才会动态地推断出每个层的大小</li>\n</ul>\n<h2 id=\"3-1-实例化网络\"><a href=\"#3-1-实例化网络\" class=\"headerlink\" title=\"3.1 实例化网络\"></a>3.1 实例化网络</h2><p>。。。</p>"},{"title":"4.5 读写文件","date":"2024-04-20T08:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 5 读写文件\n## 5.1 加载和保存张量\n\n\n\n```python\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n# 保存一个张量\nx = torch.arange(4)\ntorch.save(x,'x-file') # 保存，二进制文件\nx2 = torch.load('x-file') #加载\nprint(x2)\n\n# 保存一个张量列表\ny = torch.zeros(4)\ntorch.save([x,y], 'x-files')\nx2, y2 = torch.load('x-files')\nprint(x2,y2)\n\n# 保存字符串-张量字典\nmydict = {'x':x, 'y':y}\ntorch.save(mydict, 'mydict')\nmydict2 = torch.load('mydict')\nprint(mydict2)\n```\n\n    tensor([0, 1, 2, 3])\n    tensor([0, 1, 2, 3]) tensor([0., 0., 0., 0.])\n    {'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}\n    \n\n## 5.2 加载和保存模型参数\n\n\n```python\nclass MLP(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.hidden = nn.Linear(20, 256)\n        self.output = nn.Linear(256, 10)\n    def forward(self, x):\n        return self.output(F.relu(self.hidden(x)))\nnet = MLP()\nX = torch.randn(size=(2,20))\nY = net(X)\n\n#保存模型\ntorch.save(net.state_dict(), 'mlp.params')\n\n#加载模型\nclone = MLP()\nclone.load_state_dict(torch.load('mlp.params'))\nprint(clone.eval())\n\n#验证加载后的模型是否与初始模型相等\nY_clone = clone(X)\nprint(Y_clone == Y)\n```\n\n    MLP(\n      (hidden): Linear(in_features=20, out_features=256, bias=True)\n      (output): Linear(in_features=256, out_features=10, bias=True)\n    )\n    tensor([[True, True, True, True, True, True, True, True, True, True],\n            [True, True, True, True, True, True, True, True, True, True]])\n    \n","source":"_posts/deeplearning/code/pytorch/4_calculate/5_file.md","raw":"---\ntitle: 4.5 读写文件\ndate: 2024-4-20 16:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 5 读写文件\n## 5.1 加载和保存张量\n\n\n\n```python\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n# 保存一个张量\nx = torch.arange(4)\ntorch.save(x,'x-file') # 保存，二进制文件\nx2 = torch.load('x-file') #加载\nprint(x2)\n\n# 保存一个张量列表\ny = torch.zeros(4)\ntorch.save([x,y], 'x-files')\nx2, y2 = torch.load('x-files')\nprint(x2,y2)\n\n# 保存字符串-张量字典\nmydict = {'x':x, 'y':y}\ntorch.save(mydict, 'mydict')\nmydict2 = torch.load('mydict')\nprint(mydict2)\n```\n\n    tensor([0, 1, 2, 3])\n    tensor([0, 1, 2, 3]) tensor([0., 0., 0., 0.])\n    {'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}\n    \n\n## 5.2 加载和保存模型参数\n\n\n```python\nclass MLP(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.hidden = nn.Linear(20, 256)\n        self.output = nn.Linear(256, 10)\n    def forward(self, x):\n        return self.output(F.relu(self.hidden(x)))\nnet = MLP()\nX = torch.randn(size=(2,20))\nY = net(X)\n\n#保存模型\ntorch.save(net.state_dict(), 'mlp.params')\n\n#加载模型\nclone = MLP()\nclone.load_state_dict(torch.load('mlp.params'))\nprint(clone.eval())\n\n#验证加载后的模型是否与初始模型相等\nY_clone = clone(X)\nprint(Y_clone == Y)\n```\n\n    MLP(\n      (hidden): Linear(in_features=20, out_features=256, bias=True)\n      (output): Linear(in_features=256, out_features=10, bias=True)\n    )\n    tensor([[True, True, True, True, True, True, True, True, True, True],\n            [True, True, True, True, True, True, True, True, True, True]])\n    \n","slug":"deeplearning/code/pytorch/4_calculate/5_file","published":1,"updated":"2024-05-02T08:52:23.379Z","comments":1,"layout":"post","photos":[],"_id":"clvuli66p001tzsvwdh4s5r7i","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"5-读写文件\"><a href=\"#5-读写文件\" class=\"headerlink\" title=\"5 读写文件\"></a>5 读写文件</h1><h2 id=\"5-1-加载和保存张量\"><a href=\"#5-1-加载和保存张量\" class=\"headerlink\" title=\"5.1 加载和保存张量\"></a>5.1 加载和保存张量</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.nn <span class=\"keyword\">import</span> functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 保存一个张量</span></span><br><span class=\"line\">x = torch.arange(<span class=\"number\">4</span>)</span><br><span class=\"line\">torch.save(x,<span class=\"string\">&#x27;x-file&#x27;</span>) <span class=\"comment\"># 保存，二进制文件</span></span><br><span class=\"line\">x2 = torch.load(<span class=\"string\">&#x27;x-file&#x27;</span>) <span class=\"comment\">#加载</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(x2)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 保存一个张量列表</span></span><br><span class=\"line\">y = torch.zeros(<span class=\"number\">4</span>)</span><br><span class=\"line\">torch.save([x,y], <span class=\"string\">&#x27;x-files&#x27;</span>)</span><br><span class=\"line\">x2, y2 = torch.load(<span class=\"string\">&#x27;x-files&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x2,y2)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 保存字符串-张量字典</span></span><br><span class=\"line\">mydict = &#123;<span class=\"string\">&#x27;x&#x27;</span>:x, <span class=\"string\">&#x27;y&#x27;</span>:y&#125;</span><br><span class=\"line\">torch.save(mydict, <span class=\"string\">&#x27;mydict&#x27;</span>)</span><br><span class=\"line\">mydict2 = torch.load(<span class=\"string\">&#x27;mydict&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(mydict2)</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([0, 1, 2, 3])\ntensor([0, 1, 2, 3]) tensor([0., 0., 0., 0.])\n&#123;&#39;x&#39;: tensor([0, 1, 2, 3]), &#39;y&#39;: tensor([0., 0., 0., 0.])&#125;\n</code></pre>\n<h2 id=\"5-2-加载和保存模型参数\"><a href=\"#5-2-加载和保存模型参数\" class=\"headerlink\" title=\"5.2 加载和保存模型参数\"></a>5.2 加载和保存模型参数</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MLP</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>) -&gt; <span class=\"literal\">None</span>:</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        self.hidden = nn.Linear(<span class=\"number\">20</span>, <span class=\"number\">256</span>)</span><br><span class=\"line\">        self.output = nn.Linear(<span class=\"number\">256</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.output(F.relu(self.hidden(x)))</span><br><span class=\"line\">net = MLP()</span><br><span class=\"line\">X = torch.randn(size=(<span class=\"number\">2</span>,<span class=\"number\">20</span>))</span><br><span class=\"line\">Y = net(X)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#保存模型</span></span><br><span class=\"line\">torch.save(net.state_dict(), <span class=\"string\">&#x27;mlp.params&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#加载模型</span></span><br><span class=\"line\">clone = MLP()</span><br><span class=\"line\">clone.load_state_dict(torch.load(<span class=\"string\">&#x27;mlp.params&#x27;</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(clone.<span class=\"built_in\">eval</span>())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#验证加载后的模型是否与初始模型相等</span></span><br><span class=\"line\">Y_clone = clone(X)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(Y_clone == Y)</span><br></pre></td></tr></table></figure>\n\n<pre><code>MLP(\n  (hidden): Linear(in_features=20, out_features=256, bias=True)\n  (output): Linear(in_features=256, out_features=10, bias=True)\n)\ntensor([[True, True, True, True, True, True, True, True, True, True],\n        [True, True, True, True, True, True, True, True, True, True]])\n</code></pre>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"5-读写文件\"><a href=\"#5-读写文件\" class=\"headerlink\" title=\"5 读写文件\"></a>5 读写文件</h1><h2 id=\"5-1-加载和保存张量\"><a href=\"#5-1-加载和保存张量\" class=\"headerlink\" title=\"5.1 加载和保存张量\"></a>5.1 加载和保存张量</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.nn <span class=\"keyword\">import</span> functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 保存一个张量</span></span><br><span class=\"line\">x = torch.arange(<span class=\"number\">4</span>)</span><br><span class=\"line\">torch.save(x,<span class=\"string\">&#x27;x-file&#x27;</span>) <span class=\"comment\"># 保存，二进制文件</span></span><br><span class=\"line\">x2 = torch.load(<span class=\"string\">&#x27;x-file&#x27;</span>) <span class=\"comment\">#加载</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(x2)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 保存一个张量列表</span></span><br><span class=\"line\">y = torch.zeros(<span class=\"number\">4</span>)</span><br><span class=\"line\">torch.save([x,y], <span class=\"string\">&#x27;x-files&#x27;</span>)</span><br><span class=\"line\">x2, y2 = torch.load(<span class=\"string\">&#x27;x-files&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x2,y2)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 保存字符串-张量字典</span></span><br><span class=\"line\">mydict = &#123;<span class=\"string\">&#x27;x&#x27;</span>:x, <span class=\"string\">&#x27;y&#x27;</span>:y&#125;</span><br><span class=\"line\">torch.save(mydict, <span class=\"string\">&#x27;mydict&#x27;</span>)</span><br><span class=\"line\">mydict2 = torch.load(<span class=\"string\">&#x27;mydict&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(mydict2)</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([0, 1, 2, 3])\ntensor([0, 1, 2, 3]) tensor([0., 0., 0., 0.])\n&#123;&#39;x&#39;: tensor([0, 1, 2, 3]), &#39;y&#39;: tensor([0., 0., 0., 0.])&#125;\n</code></pre>\n<h2 id=\"5-2-加载和保存模型参数\"><a href=\"#5-2-加载和保存模型参数\" class=\"headerlink\" title=\"5.2 加载和保存模型参数\"></a>5.2 加载和保存模型参数</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MLP</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>) -&gt; <span class=\"literal\">None</span>:</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        self.hidden = nn.Linear(<span class=\"number\">20</span>, <span class=\"number\">256</span>)</span><br><span class=\"line\">        self.output = nn.Linear(<span class=\"number\">256</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.output(F.relu(self.hidden(x)))</span><br><span class=\"line\">net = MLP()</span><br><span class=\"line\">X = torch.randn(size=(<span class=\"number\">2</span>,<span class=\"number\">20</span>))</span><br><span class=\"line\">Y = net(X)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#保存模型</span></span><br><span class=\"line\">torch.save(net.state_dict(), <span class=\"string\">&#x27;mlp.params&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#加载模型</span></span><br><span class=\"line\">clone = MLP()</span><br><span class=\"line\">clone.load_state_dict(torch.load(<span class=\"string\">&#x27;mlp.params&#x27;</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(clone.<span class=\"built_in\">eval</span>())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#验证加载后的模型是否与初始模型相等</span></span><br><span class=\"line\">Y_clone = clone(X)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(Y_clone == Y)</span><br></pre></td></tr></table></figure>\n\n<pre><code>MLP(\n  (hidden): Linear(in_features=20, out_features=256, bias=True)\n  (output): Linear(in_features=256, out_features=10, bias=True)\n)\ntensor([[True, True, True, True, True, True, True, True, True, True],\n        [True, True, True, True, True, True, True, True, True, True]])\n</code></pre>"},{"title":"4.4 自定义层","date":"2024-04-20T06:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 4 自定义层\n## 4.1 不带参数的层\n- 创建一个CenteredLayer，接受一个输入，输出内容是输入减去输入的均值\n\n\n```python\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nclass CenteredLayer(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n    def forward(self, X):\n        return X-X.mean()\nlayer = CenteredLayer()\nlayer(torch.FloatTensor([1,2,3,4,5]))\n```\n\n\n\n\n    tensor([-2., -1.,  0.,  1.,  2.])\n\n\n\n- 将层作为组件合并到更复杂的模型中\n\n\n```python\nnet = nn.Sequential(nn.Linear(8,128), CenteredLayer())\n\n'''向网络发送随机数据，检查均值是否为0'''\nY = net(torch.rand(4,8))\nprint(Y.mean())\n```\n\n    tensor(2.0955e-09, grad_fn=<MeanBackward0>)\n    \n\n## 4.2 带参数的层\n- 使用内置函数来创建参数，这些函数可以：管理访问、初始化、共享、保存、加载参数\n- 自定义全连接层\n\n\n```python\nclass MyLinear(nn.Module):\n    def __init__(self, in_units, units):\n        super().__init__()\n        self.weight = nn.Parameter(torch.randn(in_units,units))\n        self.bias = nn.Parameter(torch.randn(units,))\n    def forward(self, X):\n        linear = torch.matmul(X, self.weight.data) + self.bias.data\n        return F.relu(linear)\nlinear = MyLinear(5,3)\nprint(linear.weight) #访问模型参数\nprint(linear(torch.rand(2,5))) #前向传播\n```\n\n    Parameter containing:\n    tensor([[-1.2740, -0.5589, -0.0577],\n            [ 0.2042, -2.4343, -1.4043],\n            [ 2.3896,  0.5617, -0.1255],\n            [-0.7957, -0.7547, -0.5688],\n            [-1.3496,  0.6484,  0.9639]], requires_grad=True)\n    tensor([[0., 0., 0.],\n            [0., 0., 0.]])\n    \n\n- 用自定义层构建模型\n\n\n```python\nnet = nn.Sequential(MyLinear(64,8), MyLinear(8,1))\nprint(net(torch.rand(2,64)))\n```\n\n    tensor([[ 5.7095],\n            [10.4373]])\n    \n","source":"_posts/deeplearning/code/pytorch/4_calculate/4_selflayer.md","raw":"---\ntitle: 4.4 自定义层\ndate: 2024-4-20 14:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 4 自定义层\n## 4.1 不带参数的层\n- 创建一个CenteredLayer，接受一个输入，输出内容是输入减去输入的均值\n\n\n```python\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nclass CenteredLayer(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n    def forward(self, X):\n        return X-X.mean()\nlayer = CenteredLayer()\nlayer(torch.FloatTensor([1,2,3,4,5]))\n```\n\n\n\n\n    tensor([-2., -1.,  0.,  1.,  2.])\n\n\n\n- 将层作为组件合并到更复杂的模型中\n\n\n```python\nnet = nn.Sequential(nn.Linear(8,128), CenteredLayer())\n\n'''向网络发送随机数据，检查均值是否为0'''\nY = net(torch.rand(4,8))\nprint(Y.mean())\n```\n\n    tensor(2.0955e-09, grad_fn=<MeanBackward0>)\n    \n\n## 4.2 带参数的层\n- 使用内置函数来创建参数，这些函数可以：管理访问、初始化、共享、保存、加载参数\n- 自定义全连接层\n\n\n```python\nclass MyLinear(nn.Module):\n    def __init__(self, in_units, units):\n        super().__init__()\n        self.weight = nn.Parameter(torch.randn(in_units,units))\n        self.bias = nn.Parameter(torch.randn(units,))\n    def forward(self, X):\n        linear = torch.matmul(X, self.weight.data) + self.bias.data\n        return F.relu(linear)\nlinear = MyLinear(5,3)\nprint(linear.weight) #访问模型参数\nprint(linear(torch.rand(2,5))) #前向传播\n```\n\n    Parameter containing:\n    tensor([[-1.2740, -0.5589, -0.0577],\n            [ 0.2042, -2.4343, -1.4043],\n            [ 2.3896,  0.5617, -0.1255],\n            [-0.7957, -0.7547, -0.5688],\n            [-1.3496,  0.6484,  0.9639]], requires_grad=True)\n    tensor([[0., 0., 0.],\n            [0., 0., 0.]])\n    \n\n- 用自定义层构建模型\n\n\n```python\nnet = nn.Sequential(MyLinear(64,8), MyLinear(8,1))\nprint(net(torch.rand(2,64)))\n```\n\n    tensor([[ 5.7095],\n            [10.4373]])\n    \n","slug":"deeplearning/code/pytorch/4_calculate/4_selflayer","published":1,"updated":"2024-05-02T08:52:14.896Z","comments":1,"layout":"post","photos":[],"_id":"clvuli66q001wzsvw62c4dag9","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"4-自定义层\"><a href=\"#4-自定义层\" class=\"headerlink\" title=\"4 自定义层\"></a>4 自定义层</h1><h2 id=\"4-1-不带参数的层\"><a href=\"#4-1-不带参数的层\" class=\"headerlink\" title=\"4.1 不带参数的层\"></a>4.1 不带参数的层</h2><ul>\n<li>创建一个CenteredLayer，接受一个输入，输出内容是输入减去输入的均值</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">CenteredLayer</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>) -&gt; <span class=\"literal\">None</span>:</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> X-X.mean()</span><br><span class=\"line\">layer = CenteredLayer()</span><br><span class=\"line\">layer(torch.FloatTensor([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>]))</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>tensor([-2., -1.,  0.,  1.,  2.])\n</code></pre>\n<ul>\n<li>将层作为组件合并到更复杂的模型中</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net = nn.Sequential(nn.Linear(<span class=\"number\">8</span>,<span class=\"number\">128</span>), CenteredLayer())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;向网络发送随机数据，检查均值是否为0&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">Y = net(torch.rand(<span class=\"number\">4</span>,<span class=\"number\">8</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(Y.mean())</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor(2.0955e-09, grad_fn=&lt;MeanBackward0&gt;)\n</code></pre>\n<h2 id=\"4-2-带参数的层\"><a href=\"#4-2-带参数的层\" class=\"headerlink\" title=\"4.2 带参数的层\"></a>4.2 带参数的层</h2><ul>\n<li>使用内置函数来创建参数，这些函数可以：管理访问、初始化、共享、保存、加载参数</li>\n<li>自定义全连接层</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MyLinear</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, in_units, units</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        self.weight = nn.Parameter(torch.randn(in_units,units))</span><br><span class=\"line\">        self.bias = nn.Parameter(torch.randn(units,))</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        linear = torch.matmul(X, self.weight.data) + self.bias.data</span><br><span class=\"line\">        <span class=\"keyword\">return</span> F.relu(linear)</span><br><span class=\"line\">linear = MyLinear(<span class=\"number\">5</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(linear.weight) <span class=\"comment\">#访问模型参数</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(linear(torch.rand(<span class=\"number\">2</span>,<span class=\"number\">5</span>))) <span class=\"comment\">#前向传播</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>Parameter containing:\ntensor([[-1.2740, -0.5589, -0.0577],\n        [ 0.2042, -2.4343, -1.4043],\n        [ 2.3896,  0.5617, -0.1255],\n        [-0.7957, -0.7547, -0.5688],\n        [-1.3496,  0.6484,  0.9639]], requires_grad=True)\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\n</code></pre>\n<ul>\n<li>用自定义层构建模型</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net = nn.Sequential(MyLinear(<span class=\"number\">64</span>,<span class=\"number\">8</span>), MyLinear(<span class=\"number\">8</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net(torch.rand(<span class=\"number\">2</span>,<span class=\"number\">64</span>)))</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[ 5.7095],\n        [10.4373]])\n</code></pre>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"4-自定义层\"><a href=\"#4-自定义层\" class=\"headerlink\" title=\"4 自定义层\"></a>4 自定义层</h1><h2 id=\"4-1-不带参数的层\"><a href=\"#4-1-不带参数的层\" class=\"headerlink\" title=\"4.1 不带参数的层\"></a>4.1 不带参数的层</h2><ul>\n<li>创建一个CenteredLayer，接受一个输入，输出内容是输入减去输入的均值</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">CenteredLayer</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>) -&gt; <span class=\"literal\">None</span>:</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> X-X.mean()</span><br><span class=\"line\">layer = CenteredLayer()</span><br><span class=\"line\">layer(torch.FloatTensor([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>]))</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>tensor([-2., -1.,  0.,  1.,  2.])\n</code></pre>\n<ul>\n<li>将层作为组件合并到更复杂的模型中</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net = nn.Sequential(nn.Linear(<span class=\"number\">8</span>,<span class=\"number\">128</span>), CenteredLayer())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;向网络发送随机数据，检查均值是否为0&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">Y = net(torch.rand(<span class=\"number\">4</span>,<span class=\"number\">8</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(Y.mean())</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor(2.0955e-09, grad_fn=&lt;MeanBackward0&gt;)\n</code></pre>\n<h2 id=\"4-2-带参数的层\"><a href=\"#4-2-带参数的层\" class=\"headerlink\" title=\"4.2 带参数的层\"></a>4.2 带参数的层</h2><ul>\n<li>使用内置函数来创建参数，这些函数可以：管理访问、初始化、共享、保存、加载参数</li>\n<li>自定义全连接层</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MyLinear</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, in_units, units</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        self.weight = nn.Parameter(torch.randn(in_units,units))</span><br><span class=\"line\">        self.bias = nn.Parameter(torch.randn(units,))</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        linear = torch.matmul(X, self.weight.data) + self.bias.data</span><br><span class=\"line\">        <span class=\"keyword\">return</span> F.relu(linear)</span><br><span class=\"line\">linear = MyLinear(<span class=\"number\">5</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(linear.weight) <span class=\"comment\">#访问模型参数</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(linear(torch.rand(<span class=\"number\">2</span>,<span class=\"number\">5</span>))) <span class=\"comment\">#前向传播</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>Parameter containing:\ntensor([[-1.2740, -0.5589, -0.0577],\n        [ 0.2042, -2.4343, -1.4043],\n        [ 2.3896,  0.5617, -0.1255],\n        [-0.7957, -0.7547, -0.5688],\n        [-1.3496,  0.6484,  0.9639]], requires_grad=True)\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\n</code></pre>\n<ul>\n<li>用自定义层构建模型</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net = nn.Sequential(MyLinear(<span class=\"number\">64</span>,<span class=\"number\">8</span>), MyLinear(<span class=\"number\">8</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net(torch.rand(<span class=\"number\">2</span>,<span class=\"number\">64</span>)))</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[ 5.7095],\n        [10.4373]])\n</code></pre>"},{"title":"4.6 GPU","date":"2024-04-20T09:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 6 GPU\n- 查看显卡信息\n\n\n```python\n!nvidia-smi\n```\n\n- pytorch中，每个数组都有一个设备，称为：环境。默认情况下，所有变量和相关的计算都分配给CPU。\n## 6.1 计算设备\n\n\n\n```python\nimport torch\nfrom torch import nn\n\ntorch.device('cpu'), torch.device('cuda'), torch.device('cuda:1')\n#查询可用GPU数量\nprint(torch.cuda.device_count())\n```\n\n    0\n    \n\n- 选择GPU或CPU\n\n\n```python\ndef try_gpu(i=0): #@save\n    '''如果存在，返回GPU(i),否则cpu()'''\n    if torch.cuda.device_count() >= i+1:\n        return torch.device(f'cuda:{i}')\n    return torch.device('cpu')\ndef try_all_gpus(): #@save\n    '''返回所有可用GPU，无则cpu'''\n    devices = [\n        torch.device(f'cuda:{i}')\n        for i in range(torch.cuda.device_count())\n               ]\n    return devices if devices else[torch.device('cpu')]\n\nprint(try_gpu())\nprint(try_gpu(10))\nprint(try_all_gpus())\n```\n\n    cpu\n    cpu\n    [device(type='cpu')]\n    \n\n## 6.2 张量与GPU\n- 查看张量所在设备\n\n\n```python\nx = torch.tensor([1,2,3])\nprint(x.device)\n```\n\n    cpu\n    \n\n### 6.2.1 存储在GPU上\n\n\n```python\nX = torch.ones(2,3,device=try_gpu())\nprint(X)\n```\n\n    tensor([[1., 1., 1.],\n            [1., 1., 1.]])\n    \n\n### 6.2.2 复制\n- 两个变量处于不同的GPU上，不能直接计算，需要将其中一个变量复制到另一个GPU上。\n- 如果z在GPU0上，z.cuda(0)不会复制\n\n\n```python\nZ = X.cuda(0) #等价于X.to(torch.device('cuda:0'))\n```\n\n### 6.2.3 旁注\n- 多GPU运行时，数据传输比计算慢得多\n- 一次执行几个操作比代码中散步的多个单操作快得多\n- 打印张量或将张量转换为numpy格式时，如果数据不在内存中，框架会先将其复制到内存中，这会导致额外的传输开销。\n## 6.3 神经网络与GPU\n- 神经网络模型也可以指定设备\n\n\n```python\nnet = nn.Sequential(nn.Linear(3,1))\nnet = net.to(device=try_gpu())\n```\n","source":"_posts/deeplearning/code/pytorch/4_calculate/6_GPU.md","raw":"---\ntitle: 4.6 GPU\ndate: 2024-4-20 17:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 6 GPU\n- 查看显卡信息\n\n\n```python\n!nvidia-smi\n```\n\n- pytorch中，每个数组都有一个设备，称为：环境。默认情况下，所有变量和相关的计算都分配给CPU。\n## 6.1 计算设备\n\n\n\n```python\nimport torch\nfrom torch import nn\n\ntorch.device('cpu'), torch.device('cuda'), torch.device('cuda:1')\n#查询可用GPU数量\nprint(torch.cuda.device_count())\n```\n\n    0\n    \n\n- 选择GPU或CPU\n\n\n```python\ndef try_gpu(i=0): #@save\n    '''如果存在，返回GPU(i),否则cpu()'''\n    if torch.cuda.device_count() >= i+1:\n        return torch.device(f'cuda:{i}')\n    return torch.device('cpu')\ndef try_all_gpus(): #@save\n    '''返回所有可用GPU，无则cpu'''\n    devices = [\n        torch.device(f'cuda:{i}')\n        for i in range(torch.cuda.device_count())\n               ]\n    return devices if devices else[torch.device('cpu')]\n\nprint(try_gpu())\nprint(try_gpu(10))\nprint(try_all_gpus())\n```\n\n    cpu\n    cpu\n    [device(type='cpu')]\n    \n\n## 6.2 张量与GPU\n- 查看张量所在设备\n\n\n```python\nx = torch.tensor([1,2,3])\nprint(x.device)\n```\n\n    cpu\n    \n\n### 6.2.1 存储在GPU上\n\n\n```python\nX = torch.ones(2,3,device=try_gpu())\nprint(X)\n```\n\n    tensor([[1., 1., 1.],\n            [1., 1., 1.]])\n    \n\n### 6.2.2 复制\n- 两个变量处于不同的GPU上，不能直接计算，需要将其中一个变量复制到另一个GPU上。\n- 如果z在GPU0上，z.cuda(0)不会复制\n\n\n```python\nZ = X.cuda(0) #等价于X.to(torch.device('cuda:0'))\n```\n\n### 6.2.3 旁注\n- 多GPU运行时，数据传输比计算慢得多\n- 一次执行几个操作比代码中散步的多个单操作快得多\n- 打印张量或将张量转换为numpy格式时，如果数据不在内存中，框架会先将其复制到内存中，这会导致额外的传输开销。\n## 6.3 神经网络与GPU\n- 神经网络模型也可以指定设备\n\n\n```python\nnet = nn.Sequential(nn.Linear(3,1))\nnet = net.to(device=try_gpu())\n```\n","slug":"deeplearning/code/pytorch/4_calculate/6_GPU","published":1,"updated":"2024-05-02T08:52:32.314Z","comments":1,"layout":"post","photos":[],"_id":"clvuli66r001zzsvweltc9e4r","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"6-GPU\"><a href=\"#6-GPU\" class=\"headerlink\" title=\"6 GPU\"></a>6 GPU</h1><ul>\n<li>查看显卡信息</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">!nvidia-smi</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>pytorch中，每个数组都有一个设备，称为：环境。默认情况下，所有变量和相关的计算都分配给CPU。</li>\n</ul>\n<h2 id=\"6-1-计算设备\"><a href=\"#6-1-计算设备\" class=\"headerlink\" title=\"6.1 计算设备\"></a>6.1 计算设备</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\">torch.device(<span class=\"string\">&#x27;cpu&#x27;</span>), torch.device(<span class=\"string\">&#x27;cuda&#x27;</span>), torch.device(<span class=\"string\">&#x27;cuda:1&#x27;</span>)</span><br><span class=\"line\"><span class=\"comment\">#查询可用GPU数量</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.cuda.device_count())</span><br></pre></td></tr></table></figure>\n\n<pre><code>0\n</code></pre>\n<ul>\n<li>选择GPU或CPU</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">try_gpu</span>(<span class=\"params\">i=<span class=\"number\">0</span></span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;如果存在，返回GPU(i),否则cpu()&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> torch.cuda.device_count() &gt;= i+<span class=\"number\">1</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> torch.device(<span class=\"string\">f&#x27;cuda:<span class=\"subst\">&#123;i&#125;</span>&#x27;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.device(<span class=\"string\">&#x27;cpu&#x27;</span>)</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">try_all_gpus</span>(): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;返回所有可用GPU，无则cpu&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    devices = [</span><br><span class=\"line\">        torch.device(<span class=\"string\">f&#x27;cuda:<span class=\"subst\">&#123;i&#125;</span>&#x27;</span>)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(torch.cuda.device_count())</span><br><span class=\"line\">               ]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> devices <span class=\"keyword\">if</span> devices <span class=\"keyword\">else</span>[torch.device(<span class=\"string\">&#x27;cpu&#x27;</span>)]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(try_gpu())</span><br><span class=\"line\"><span class=\"built_in\">print</span>(try_gpu(<span class=\"number\">10</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(try_all_gpus())</span><br></pre></td></tr></table></figure>\n\n<pre><code>cpu\ncpu\n[device(type=&#39;cpu&#39;)]\n</code></pre>\n<h2 id=\"6-2-张量与GPU\"><a href=\"#6-2-张量与GPU\" class=\"headerlink\" title=\"6.2 张量与GPU\"></a>6.2 张量与GPU</h2><ul>\n<li>查看张量所在设备</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.tensor([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.device)</span><br></pre></td></tr></table></figure>\n\n<pre><code>cpu\n</code></pre>\n<h3 id=\"6-2-1-存储在GPU上\"><a href=\"#6-2-1-存储在GPU上\" class=\"headerlink\" title=\"6.2.1 存储在GPU上\"></a>6.2.1 存储在GPU上</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = torch.ones(<span class=\"number\">2</span>,<span class=\"number\">3</span>,device=try_gpu())</span><br><span class=\"line\"><span class=\"built_in\">print</span>(X)</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[1., 1., 1.],\n        [1., 1., 1.]])\n</code></pre>\n<h3 id=\"6-2-2-复制\"><a href=\"#6-2-2-复制\" class=\"headerlink\" title=\"6.2.2 复制\"></a>6.2.2 复制</h3><ul>\n<li>两个变量处于不同的GPU上，不能直接计算，需要将其中一个变量复制到另一个GPU上。</li>\n<li>如果z在GPU0上，z.cuda(0)不会复制</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Z = X.cuda(<span class=\"number\">0</span>) <span class=\"comment\">#等价于X.to(torch.device(&#x27;cuda:0&#x27;))</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"6-2-3-旁注\"><a href=\"#6-2-3-旁注\" class=\"headerlink\" title=\"6.2.3 旁注\"></a>6.2.3 旁注</h3><ul>\n<li>多GPU运行时，数据传输比计算慢得多</li>\n<li>一次执行几个操作比代码中散步的多个单操作快得多</li>\n<li>打印张量或将张量转换为numpy格式时，如果数据不在内存中，框架会先将其复制到内存中，这会导致额外的传输开销。</li>\n</ul>\n<h2 id=\"6-3-神经网络与GPU\"><a href=\"#6-3-神经网络与GPU\" class=\"headerlink\" title=\"6.3 神经网络与GPU\"></a>6.3 神经网络与GPU</h2><ul>\n<li>神经网络模型也可以指定设备</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net = nn.Sequential(nn.Linear(<span class=\"number\">3</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">net = net.to(device=try_gpu())</span><br></pre></td></tr></table></figure>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"6-GPU\"><a href=\"#6-GPU\" class=\"headerlink\" title=\"6 GPU\"></a>6 GPU</h1><ul>\n<li>查看显卡信息</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">!nvidia-smi</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>pytorch中，每个数组都有一个设备，称为：环境。默认情况下，所有变量和相关的计算都分配给CPU。</li>\n</ul>\n<h2 id=\"6-1-计算设备\"><a href=\"#6-1-计算设备\" class=\"headerlink\" title=\"6.1 计算设备\"></a>6.1 计算设备</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\">torch.device(<span class=\"string\">&#x27;cpu&#x27;</span>), torch.device(<span class=\"string\">&#x27;cuda&#x27;</span>), torch.device(<span class=\"string\">&#x27;cuda:1&#x27;</span>)</span><br><span class=\"line\"><span class=\"comment\">#查询可用GPU数量</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.cuda.device_count())</span><br></pre></td></tr></table></figure>\n\n<pre><code>0\n</code></pre>\n<ul>\n<li>选择GPU或CPU</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">try_gpu</span>(<span class=\"params\">i=<span class=\"number\">0</span></span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;如果存在，返回GPU(i),否则cpu()&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> torch.cuda.device_count() &gt;= i+<span class=\"number\">1</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> torch.device(<span class=\"string\">f&#x27;cuda:<span class=\"subst\">&#123;i&#125;</span>&#x27;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.device(<span class=\"string\">&#x27;cpu&#x27;</span>)</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">try_all_gpus</span>(): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;返回所有可用GPU，无则cpu&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    devices = [</span><br><span class=\"line\">        torch.device(<span class=\"string\">f&#x27;cuda:<span class=\"subst\">&#123;i&#125;</span>&#x27;</span>)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(torch.cuda.device_count())</span><br><span class=\"line\">               ]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> devices <span class=\"keyword\">if</span> devices <span class=\"keyword\">else</span>[torch.device(<span class=\"string\">&#x27;cpu&#x27;</span>)]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(try_gpu())</span><br><span class=\"line\"><span class=\"built_in\">print</span>(try_gpu(<span class=\"number\">10</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(try_all_gpus())</span><br></pre></td></tr></table></figure>\n\n<pre><code>cpu\ncpu\n[device(type=&#39;cpu&#39;)]\n</code></pre>\n<h2 id=\"6-2-张量与GPU\"><a href=\"#6-2-张量与GPU\" class=\"headerlink\" title=\"6.2 张量与GPU\"></a>6.2 张量与GPU</h2><ul>\n<li>查看张量所在设备</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.tensor([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.device)</span><br></pre></td></tr></table></figure>\n\n<pre><code>cpu\n</code></pre>\n<h3 id=\"6-2-1-存储在GPU上\"><a href=\"#6-2-1-存储在GPU上\" class=\"headerlink\" title=\"6.2.1 存储在GPU上\"></a>6.2.1 存储在GPU上</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = torch.ones(<span class=\"number\">2</span>,<span class=\"number\">3</span>,device=try_gpu())</span><br><span class=\"line\"><span class=\"built_in\">print</span>(X)</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[1., 1., 1.],\n        [1., 1., 1.]])\n</code></pre>\n<h3 id=\"6-2-2-复制\"><a href=\"#6-2-2-复制\" class=\"headerlink\" title=\"6.2.2 复制\"></a>6.2.2 复制</h3><ul>\n<li>两个变量处于不同的GPU上，不能直接计算，需要将其中一个变量复制到另一个GPU上。</li>\n<li>如果z在GPU0上，z.cuda(0)不会复制</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Z = X.cuda(<span class=\"number\">0</span>) <span class=\"comment\">#等价于X.to(torch.device(&#x27;cuda:0&#x27;))</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"6-2-3-旁注\"><a href=\"#6-2-3-旁注\" class=\"headerlink\" title=\"6.2.3 旁注\"></a>6.2.3 旁注</h3><ul>\n<li>多GPU运行时，数据传输比计算慢得多</li>\n<li>一次执行几个操作比代码中散步的多个单操作快得多</li>\n<li>打印张量或将张量转换为numpy格式时，如果数据不在内存中，框架会先将其复制到内存中，这会导致额外的传输开销。</li>\n</ul>\n<h2 id=\"6-3-神经网络与GPU\"><a href=\"#6-3-神经网络与GPU\" class=\"headerlink\" title=\"6.3 神经网络与GPU\"></a>6.3 神经网络与GPU</h2><ul>\n<li>神经网络模型也可以指定设备</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net = nn.Sequential(nn.Linear(<span class=\"number\">3</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">net = net.to(device=try_gpu())</span><br></pre></td></tr></table></figure>"},{"title":"5.2 图像卷积","date":"2024-04-20T11:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 2 图像卷积\n## 2.1 互相关运算\n- 卷积层表达的运算其实是相关运算（cross-correlation），而不是卷积运算。在卷积层中，输入张量与核张量通过互相关运算产生输出张量。\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/5_convolution/2_picture_cov/1.png)\n![](img/deeplearning/code/pytorch/5_convolution/2_picture_cov/1.png)\n- 计算0 × 0 + 1 × 1 + 3 × 2 + 4 × 3 = 19\n- 假设输入：$n_h \\times n_w$，卷积核：$k_h \\times k_w$，输出：$n_h - k_h + 1 \\times n_w - k_w + 1$\n\n\n\n```python\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\n# 二维卷积\ndef corr2d(X, K): #@save\n    '''计算二维互相关运算'''\n    h, w = K.shape\n    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w +1))\n    for i in range(Y.shape[0]): #卷积核横向移动多少次\n        for j in range(Y.shape[1]): #卷积核纵向移动多少次\n            Y[i,j] = (X[i:i+h, j:j+w] * K).sum()\n    return Y\nX = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\nK = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\nprint(corr2d(X, K))\n```\n\n    tensor([[19., 25.],\n            [37., 43.]])\n    \n\n## 2.2 卷积层\n- 卷积层对输入和卷积核权重进行互相关运算，并加上偏置后产生输出。\n- 卷积层需要学习的参数是卷积核权重和偏置。\n- 对于卷积核为h × w的卷积层，也成为h × w卷积层。\n\n\n```python\n# 卷积层\nclass Conv2D(nn.Module):\n    def __init__(self, kernel_size) -> None:\n        super().__init__()\n        self.weight = nn.Parameter(torch.rand(kernel_size))\n        self.bias = nn.Parameter(torch.zeros(1))\n    def forward(self, x):\n        return corr2d(x, self.weight) + self.bias\n```\n\n## 2.3 图像中目标的边缘检测\n- 卷积层的一个应用：通过找到像素变化的位置，检测图像中不同颜色的边缘。\n\n\n```python\nX = torch.ones((6,8))\nX[:, 2:6] = 0 #黑白图像，中间四列为黑色，其余为白色\nprint(X)\nK = torch.tensor([[1.0, -1.0]]) #1*2卷积核，如果水平向量的两元素相同，则输出0，否则输出非1\nY = corr2d(X,K)\nprint(Y)\n#将X转置后，无法检测出水平边缘\nY2 = corr2d(X.t(), K)\nprint(Y2)\n```\n\n    tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n            [1., 1., 0., 0., 0., 0., 1., 1.],\n            [1., 1., 0., 0., 0., 0., 1., 1.],\n            [1., 1., 0., 0., 0., 0., 1., 1.],\n            [1., 1., 0., 0., 0., 0., 1., 1.],\n            [1., 1., 0., 0., 0., 0., 1., 1.]])\n    tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n            [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n            [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n            [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n            [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n            [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])\n    tensor([[0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0.]])\n    \n\n## 2.4 学习卷积核\n- 构造一个卷积层，随机初始化卷积核，通过计算Y与卷积层输出的平方误差更新。\n\n\n```python\nconv2d = nn.Conv2d(1,1,kernel_size=(1,2), bias=False) #忽略偏置，通道数为1\nX = X.reshape((1,1,6,8)) #batch_size, channel, height, width\nY = Y.reshape((1,1,6,7)) #卷积后输出会缩小\nlr = 3e-2 # 学习率\n\nfor i in range(10):\n    Y_hat = conv2d(X)\n    l = (Y_hat - Y) ** 2\n    conv2d.zero_grad()\n    l.sum().backward()\n    # 迭代卷积核\n    conv2d.weight.data[:] -= lr * conv2d.weight.grad #w=w-lr*grad\n    if (i+1) % 2 == 0:\n        print(f'epoch{i+1}, loss{l.sum():.3f}')\n# 跌倒多次后，误差已经足够低，查看卷积核权重\nprint(conv2d.weight.data)\n```\n\n    epoch2, loss12.452\n    epoch4, loss3.237\n    epoch6, loss1.013\n    epoch8, loss0.363\n    epoch10, loss0.140\n    tensor([[[[ 0.9478, -1.0233]]]])\n    \n\n## 2.5 互相关和卷积\n- 卷积运算与互相关运算差别不大，只需要水平和垂直翻转二维卷积核张量，然后对输入张量进行互相关运算。\n## 2.6 特征映射和感受野\n- 输出的卷积层有时被称为特征映射（feature map），因为它可以被视为一个输入映射到下一层的空间维度的转换器。\n- 在卷积神经网络中，对于某一层的任意元素x，其感受野（receptivefield）是指在前向传播期间可能影响x计算的所有元素（来自所有先前层）\n- 感受野可能大于输入的实际大小。如图，如果在后面再加一个2×2的卷积层，输出就是1×1，但是感受野是3×3（原始输入）。\n","source":"_posts/deeplearning/code/pytorch/5_convolution/2_picture_cov.md","raw":"---\ntitle: 5.2 图像卷积\ndate: 2024-4-20 19:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 2 图像卷积\n## 2.1 互相关运算\n- 卷积层表达的运算其实是相关运算（cross-correlation），而不是卷积运算。在卷积层中，输入张量与核张量通过互相关运算产生输出张量。\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/5_convolution/2_picture_cov/1.png)\n![](img/deeplearning/code/pytorch/5_convolution/2_picture_cov/1.png)\n- 计算0 × 0 + 1 × 1 + 3 × 2 + 4 × 3 = 19\n- 假设输入：$n_h \\times n_w$，卷积核：$k_h \\times k_w$，输出：$n_h - k_h + 1 \\times n_w - k_w + 1$\n\n\n\n```python\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\n# 二维卷积\ndef corr2d(X, K): #@save\n    '''计算二维互相关运算'''\n    h, w = K.shape\n    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w +1))\n    for i in range(Y.shape[0]): #卷积核横向移动多少次\n        for j in range(Y.shape[1]): #卷积核纵向移动多少次\n            Y[i,j] = (X[i:i+h, j:j+w] * K).sum()\n    return Y\nX = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\nK = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\nprint(corr2d(X, K))\n```\n\n    tensor([[19., 25.],\n            [37., 43.]])\n    \n\n## 2.2 卷积层\n- 卷积层对输入和卷积核权重进行互相关运算，并加上偏置后产生输出。\n- 卷积层需要学习的参数是卷积核权重和偏置。\n- 对于卷积核为h × w的卷积层，也成为h × w卷积层。\n\n\n```python\n# 卷积层\nclass Conv2D(nn.Module):\n    def __init__(self, kernel_size) -> None:\n        super().__init__()\n        self.weight = nn.Parameter(torch.rand(kernel_size))\n        self.bias = nn.Parameter(torch.zeros(1))\n    def forward(self, x):\n        return corr2d(x, self.weight) + self.bias\n```\n\n## 2.3 图像中目标的边缘检测\n- 卷积层的一个应用：通过找到像素变化的位置，检测图像中不同颜色的边缘。\n\n\n```python\nX = torch.ones((6,8))\nX[:, 2:6] = 0 #黑白图像，中间四列为黑色，其余为白色\nprint(X)\nK = torch.tensor([[1.0, -1.0]]) #1*2卷积核，如果水平向量的两元素相同，则输出0，否则输出非1\nY = corr2d(X,K)\nprint(Y)\n#将X转置后，无法检测出水平边缘\nY2 = corr2d(X.t(), K)\nprint(Y2)\n```\n\n    tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n            [1., 1., 0., 0., 0., 0., 1., 1.],\n            [1., 1., 0., 0., 0., 0., 1., 1.],\n            [1., 1., 0., 0., 0., 0., 1., 1.],\n            [1., 1., 0., 0., 0., 0., 1., 1.],\n            [1., 1., 0., 0., 0., 0., 1., 1.]])\n    tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n            [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n            [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n            [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n            [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n            [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])\n    tensor([[0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0.]])\n    \n\n## 2.4 学习卷积核\n- 构造一个卷积层，随机初始化卷积核，通过计算Y与卷积层输出的平方误差更新。\n\n\n```python\nconv2d = nn.Conv2d(1,1,kernel_size=(1,2), bias=False) #忽略偏置，通道数为1\nX = X.reshape((1,1,6,8)) #batch_size, channel, height, width\nY = Y.reshape((1,1,6,7)) #卷积后输出会缩小\nlr = 3e-2 # 学习率\n\nfor i in range(10):\n    Y_hat = conv2d(X)\n    l = (Y_hat - Y) ** 2\n    conv2d.zero_grad()\n    l.sum().backward()\n    # 迭代卷积核\n    conv2d.weight.data[:] -= lr * conv2d.weight.grad #w=w-lr*grad\n    if (i+1) % 2 == 0:\n        print(f'epoch{i+1}, loss{l.sum():.3f}')\n# 跌倒多次后，误差已经足够低，查看卷积核权重\nprint(conv2d.weight.data)\n```\n\n    epoch2, loss12.452\n    epoch4, loss3.237\n    epoch6, loss1.013\n    epoch8, loss0.363\n    epoch10, loss0.140\n    tensor([[[[ 0.9478, -1.0233]]]])\n    \n\n## 2.5 互相关和卷积\n- 卷积运算与互相关运算差别不大，只需要水平和垂直翻转二维卷积核张量，然后对输入张量进行互相关运算。\n## 2.6 特征映射和感受野\n- 输出的卷积层有时被称为特征映射（feature map），因为它可以被视为一个输入映射到下一层的空间维度的转换器。\n- 在卷积神经网络中，对于某一层的任意元素x，其感受野（receptivefield）是指在前向传播期间可能影响x计算的所有元素（来自所有先前层）\n- 感受野可能大于输入的实际大小。如图，如果在后面再加一个2×2的卷积层，输出就是1×1，但是感受野是3×3（原始输入）。\n","slug":"deeplearning/code/pytorch/5_convolution/2_picture_cov","published":1,"updated":"2024-05-02T08:50:54.544Z","comments":1,"layout":"post","photos":[],"_id":"clvuli68r002czsvw4ro12gs7","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"2-图像卷积\"><a href=\"#2-图像卷积\" class=\"headerlink\" title=\"2 图像卷积\"></a>2 图像卷积</h1><h2 id=\"2-1-互相关运算\"><a href=\"#2-1-互相关运算\" class=\"headerlink\" title=\"2.1 互相关运算\"></a>2.1 互相关运算</h2><ul>\n<li>卷积层表达的运算其实是相关运算（cross-correlation），而不是卷积运算。在卷积层中，输入张量与核张量通过互相关运算产生输出张量。<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/5_convolution/2_picture_cov/1.png\"><br><img src=\"/img/deeplearning/code/pytorch/5_convolution/2_picture_cov/1.png\"></li>\n<li>计算0 × 0 + 1 × 1 + 3 × 2 + 4 × 3 &#x3D; 19</li>\n<li>假设输入：$n_h \\times n_w$，卷积核：$k_h \\times k_w$，输出：$n_h - k_h + 1 \\times n_w - k_w + 1$</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 二维卷积</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">corr2d</span>(<span class=\"params\">X, K</span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;计算二维互相关运算&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    h, w = K.shape</span><br><span class=\"line\">    Y = torch.zeros((X.shape[<span class=\"number\">0</span>] - h + <span class=\"number\">1</span>, X.shape[<span class=\"number\">1</span>] - w +<span class=\"number\">1</span>))</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(Y.shape[<span class=\"number\">0</span>]): <span class=\"comment\">#卷积核横向移动多少次</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(Y.shape[<span class=\"number\">1</span>]): <span class=\"comment\">#卷积核纵向移动多少次</span></span><br><span class=\"line\">            Y[i,j] = (X[i:i+h, j:j+w] * K).<span class=\"built_in\">sum</span>()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> Y</span><br><span class=\"line\">X = torch.tensor([[<span class=\"number\">0.0</span>, <span class=\"number\">1.0</span>, <span class=\"number\">2.0</span>], [<span class=\"number\">3.0</span>, <span class=\"number\">4.0</span>, <span class=\"number\">5.0</span>], [<span class=\"number\">6.0</span>, <span class=\"number\">7.0</span>, <span class=\"number\">8.0</span>]])</span><br><span class=\"line\">K = torch.tensor([[<span class=\"number\">0.0</span>, <span class=\"number\">1.0</span>], [<span class=\"number\">2.0</span>, <span class=\"number\">3.0</span>]])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(corr2d(X, K))</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[19., 25.],\n        [37., 43.]])\n</code></pre>\n<h2 id=\"2-2-卷积层\"><a href=\"#2-2-卷积层\" class=\"headerlink\" title=\"2.2 卷积层\"></a>2.2 卷积层</h2><ul>\n<li>卷积层对输入和卷积核权重进行互相关运算，并加上偏置后产生输出。</li>\n<li>卷积层需要学习的参数是卷积核权重和偏置。</li>\n<li>对于卷积核为h × w的卷积层，也成为h × w卷积层。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 卷积层</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Conv2D</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, kernel_size</span>) -&gt; <span class=\"literal\">None</span>:</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        self.weight = nn.Parameter(torch.rand(kernel_size))</span><br><span class=\"line\">        self.bias = nn.Parameter(torch.zeros(<span class=\"number\">1</span>))</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> corr2d(x, self.weight) + self.bias</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"2-3-图像中目标的边缘检测\"><a href=\"#2-3-图像中目标的边缘检测\" class=\"headerlink\" title=\"2.3 图像中目标的边缘检测\"></a>2.3 图像中目标的边缘检测</h2><ul>\n<li>卷积层的一个应用：通过找到像素变化的位置，检测图像中不同颜色的边缘。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = torch.ones((<span class=\"number\">6</span>,<span class=\"number\">8</span>))</span><br><span class=\"line\">X[:, <span class=\"number\">2</span>:<span class=\"number\">6</span>] = <span class=\"number\">0</span> <span class=\"comment\">#黑白图像，中间四列为黑色，其余为白色</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(X)</span><br><span class=\"line\">K = torch.tensor([[<span class=\"number\">1.0</span>, -<span class=\"number\">1.0</span>]]) <span class=\"comment\">#1*2卷积核，如果水平向量的两元素相同，则输出0，否则输出非1</span></span><br><span class=\"line\">Y = corr2d(X,K)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(Y)</span><br><span class=\"line\"><span class=\"comment\">#将X转置后，无法检测出水平边缘</span></span><br><span class=\"line\">Y2 = corr2d(X.t(), K)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(Y2)</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n        [1., 1., 0., 0., 0., 0., 1., 1.],\n        [1., 1., 0., 0., 0., 0., 1., 1.],\n        [1., 1., 0., 0., 0., 0., 1., 1.],\n        [1., 1., 0., 0., 0., 0., 1., 1.],\n        [1., 1., 0., 0., 0., 0., 1., 1.]])\ntensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])\ntensor([[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])\n</code></pre>\n<h2 id=\"2-4-学习卷积核\"><a href=\"#2-4-学习卷积核\" class=\"headerlink\" title=\"2.4 学习卷积核\"></a>2.4 学习卷积核</h2><ul>\n<li>构造一个卷积层，随机初始化卷积核，通过计算Y与卷积层输出的平方误差更新。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conv2d = nn.Conv2d(<span class=\"number\">1</span>,<span class=\"number\">1</span>,kernel_size=(<span class=\"number\">1</span>,<span class=\"number\">2</span>), bias=<span class=\"literal\">False</span>) <span class=\"comment\">#忽略偏置，通道数为1</span></span><br><span class=\"line\">X = X.reshape((<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">6</span>,<span class=\"number\">8</span>)) <span class=\"comment\">#batch_size, channel, height, width</span></span><br><span class=\"line\">Y = Y.reshape((<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">6</span>,<span class=\"number\">7</span>)) <span class=\"comment\">#卷积后输出会缩小</span></span><br><span class=\"line\">lr = <span class=\"number\">3e-2</span> <span class=\"comment\"># 学习率</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">10</span>):</span><br><span class=\"line\">    Y_hat = conv2d(X)</span><br><span class=\"line\">    l = (Y_hat - Y) ** <span class=\"number\">2</span></span><br><span class=\"line\">    conv2d.zero_grad()</span><br><span class=\"line\">    l.<span class=\"built_in\">sum</span>().backward()</span><br><span class=\"line\">    <span class=\"comment\"># 迭代卷积核</span></span><br><span class=\"line\">    conv2d.weight.data[:] -= lr * conv2d.weight.grad <span class=\"comment\">#w=w-lr*grad</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (i+<span class=\"number\">1</span>) % <span class=\"number\">2</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;epoch<span class=\"subst\">&#123;i+<span class=\"number\">1</span>&#125;</span>, loss<span class=\"subst\">&#123;l.<span class=\"built_in\">sum</span>():<span class=\"number\">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class=\"line\"><span class=\"comment\"># 跌倒多次后，误差已经足够低，查看卷积核权重</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(conv2d.weight.data)</span><br></pre></td></tr></table></figure>\n\n<pre><code>epoch2, loss12.452\nepoch4, loss3.237\nepoch6, loss1.013\nepoch8, loss0.363\nepoch10, loss0.140\ntensor([[[[ 0.9478, -1.0233]]]])\n</code></pre>\n<h2 id=\"2-5-互相关和卷积\"><a href=\"#2-5-互相关和卷积\" class=\"headerlink\" title=\"2.5 互相关和卷积\"></a>2.5 互相关和卷积</h2><ul>\n<li>卷积运算与互相关运算差别不大，只需要水平和垂直翻转二维卷积核张量，然后对输入张量进行互相关运算。</li>\n</ul>\n<h2 id=\"2-6-特征映射和感受野\"><a href=\"#2-6-特征映射和感受野\" class=\"headerlink\" title=\"2.6 特征映射和感受野\"></a>2.6 特征映射和感受野</h2><ul>\n<li>输出的卷积层有时被称为特征映射（feature map），因为它可以被视为一个输入映射到下一层的空间维度的转换器。</li>\n<li>在卷积神经网络中，对于某一层的任意元素x，其感受野（receptivefield）是指在前向传播期间可能影响x计算的所有元素（来自所有先前层）</li>\n<li>感受野可能大于输入的实际大小。如图，如果在后面再加一个2×2的卷积层，输出就是1×1，但是感受野是3×3（原始输入）。</li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"2-图像卷积\"><a href=\"#2-图像卷积\" class=\"headerlink\" title=\"2 图像卷积\"></a>2 图像卷积</h1><h2 id=\"2-1-互相关运算\"><a href=\"#2-1-互相关运算\" class=\"headerlink\" title=\"2.1 互相关运算\"></a>2.1 互相关运算</h2><ul>\n<li>卷积层表达的运算其实是相关运算（cross-correlation），而不是卷积运算。在卷积层中，输入张量与核张量通过互相关运算产生输出张量。<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/5_convolution/2_picture_cov/1.png\"><br><img src=\"/img/deeplearning/code/pytorch/5_convolution/2_picture_cov/1.png\"></li>\n<li>计算0 × 0 + 1 × 1 + 3 × 2 + 4 × 3 &#x3D; 19</li>\n<li>假设输入：$n_h \\times n_w$，卷积核：$k_h \\times k_w$，输出：$n_h - k_h + 1 \\times n_w - k_w + 1$</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 二维卷积</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">corr2d</span>(<span class=\"params\">X, K</span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;计算二维互相关运算&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    h, w = K.shape</span><br><span class=\"line\">    Y = torch.zeros((X.shape[<span class=\"number\">0</span>] - h + <span class=\"number\">1</span>, X.shape[<span class=\"number\">1</span>] - w +<span class=\"number\">1</span>))</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(Y.shape[<span class=\"number\">0</span>]): <span class=\"comment\">#卷积核横向移动多少次</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(Y.shape[<span class=\"number\">1</span>]): <span class=\"comment\">#卷积核纵向移动多少次</span></span><br><span class=\"line\">            Y[i,j] = (X[i:i+h, j:j+w] * K).<span class=\"built_in\">sum</span>()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> Y</span><br><span class=\"line\">X = torch.tensor([[<span class=\"number\">0.0</span>, <span class=\"number\">1.0</span>, <span class=\"number\">2.0</span>], [<span class=\"number\">3.0</span>, <span class=\"number\">4.0</span>, <span class=\"number\">5.0</span>], [<span class=\"number\">6.0</span>, <span class=\"number\">7.0</span>, <span class=\"number\">8.0</span>]])</span><br><span class=\"line\">K = torch.tensor([[<span class=\"number\">0.0</span>, <span class=\"number\">1.0</span>], [<span class=\"number\">2.0</span>, <span class=\"number\">3.0</span>]])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(corr2d(X, K))</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[19., 25.],\n        [37., 43.]])\n</code></pre>\n<h2 id=\"2-2-卷积层\"><a href=\"#2-2-卷积层\" class=\"headerlink\" title=\"2.2 卷积层\"></a>2.2 卷积层</h2><ul>\n<li>卷积层对输入和卷积核权重进行互相关运算，并加上偏置后产生输出。</li>\n<li>卷积层需要学习的参数是卷积核权重和偏置。</li>\n<li>对于卷积核为h × w的卷积层，也成为h × w卷积层。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 卷积层</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Conv2D</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, kernel_size</span>) -&gt; <span class=\"literal\">None</span>:</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        self.weight = nn.Parameter(torch.rand(kernel_size))</span><br><span class=\"line\">        self.bias = nn.Parameter(torch.zeros(<span class=\"number\">1</span>))</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> corr2d(x, self.weight) + self.bias</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"2-3-图像中目标的边缘检测\"><a href=\"#2-3-图像中目标的边缘检测\" class=\"headerlink\" title=\"2.3 图像中目标的边缘检测\"></a>2.3 图像中目标的边缘检测</h2><ul>\n<li>卷积层的一个应用：通过找到像素变化的位置，检测图像中不同颜色的边缘。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = torch.ones((<span class=\"number\">6</span>,<span class=\"number\">8</span>))</span><br><span class=\"line\">X[:, <span class=\"number\">2</span>:<span class=\"number\">6</span>] = <span class=\"number\">0</span> <span class=\"comment\">#黑白图像，中间四列为黑色，其余为白色</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(X)</span><br><span class=\"line\">K = torch.tensor([[<span class=\"number\">1.0</span>, -<span class=\"number\">1.0</span>]]) <span class=\"comment\">#1*2卷积核，如果水平向量的两元素相同，则输出0，否则输出非1</span></span><br><span class=\"line\">Y = corr2d(X,K)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(Y)</span><br><span class=\"line\"><span class=\"comment\">#将X转置后，无法检测出水平边缘</span></span><br><span class=\"line\">Y2 = corr2d(X.t(), K)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(Y2)</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n        [1., 1., 0., 0., 0., 0., 1., 1.],\n        [1., 1., 0., 0., 0., 0., 1., 1.],\n        [1., 1., 0., 0., 0., 0., 1., 1.],\n        [1., 1., 0., 0., 0., 0., 1., 1.],\n        [1., 1., 0., 0., 0., 0., 1., 1.]])\ntensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])\ntensor([[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])\n</code></pre>\n<h2 id=\"2-4-学习卷积核\"><a href=\"#2-4-学习卷积核\" class=\"headerlink\" title=\"2.4 学习卷积核\"></a>2.4 学习卷积核</h2><ul>\n<li>构造一个卷积层，随机初始化卷积核，通过计算Y与卷积层输出的平方误差更新。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conv2d = nn.Conv2d(<span class=\"number\">1</span>,<span class=\"number\">1</span>,kernel_size=(<span class=\"number\">1</span>,<span class=\"number\">2</span>), bias=<span class=\"literal\">False</span>) <span class=\"comment\">#忽略偏置，通道数为1</span></span><br><span class=\"line\">X = X.reshape((<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">6</span>,<span class=\"number\">8</span>)) <span class=\"comment\">#batch_size, channel, height, width</span></span><br><span class=\"line\">Y = Y.reshape((<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">6</span>,<span class=\"number\">7</span>)) <span class=\"comment\">#卷积后输出会缩小</span></span><br><span class=\"line\">lr = <span class=\"number\">3e-2</span> <span class=\"comment\"># 学习率</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">10</span>):</span><br><span class=\"line\">    Y_hat = conv2d(X)</span><br><span class=\"line\">    l = (Y_hat - Y) ** <span class=\"number\">2</span></span><br><span class=\"line\">    conv2d.zero_grad()</span><br><span class=\"line\">    l.<span class=\"built_in\">sum</span>().backward()</span><br><span class=\"line\">    <span class=\"comment\"># 迭代卷积核</span></span><br><span class=\"line\">    conv2d.weight.data[:] -= lr * conv2d.weight.grad <span class=\"comment\">#w=w-lr*grad</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (i+<span class=\"number\">1</span>) % <span class=\"number\">2</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;epoch<span class=\"subst\">&#123;i+<span class=\"number\">1</span>&#125;</span>, loss<span class=\"subst\">&#123;l.<span class=\"built_in\">sum</span>():<span class=\"number\">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class=\"line\"><span class=\"comment\"># 跌倒多次后，误差已经足够低，查看卷积核权重</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(conv2d.weight.data)</span><br></pre></td></tr></table></figure>\n\n<pre><code>epoch2, loss12.452\nepoch4, loss3.237\nepoch6, loss1.013\nepoch8, loss0.363\nepoch10, loss0.140\ntensor([[[[ 0.9478, -1.0233]]]])\n</code></pre>\n<h2 id=\"2-5-互相关和卷积\"><a href=\"#2-5-互相关和卷积\" class=\"headerlink\" title=\"2.5 互相关和卷积\"></a>2.5 互相关和卷积</h2><ul>\n<li>卷积运算与互相关运算差别不大，只需要水平和垂直翻转二维卷积核张量，然后对输入张量进行互相关运算。</li>\n</ul>\n<h2 id=\"2-6-特征映射和感受野\"><a href=\"#2-6-特征映射和感受野\" class=\"headerlink\" title=\"2.6 特征映射和感受野\"></a>2.6 特征映射和感受野</h2><ul>\n<li>输出的卷积层有时被称为特征映射（feature map），因为它可以被视为一个输入映射到下一层的空间维度的转换器。</li>\n<li>在卷积神经网络中，对于某一层的任意元素x，其感受野（receptivefield）是指在前向传播期间可能影响x计算的所有元素（来自所有先前层）</li>\n<li>感受野可能大于输入的实际大小。如图，如果在后面再加一个2×2的卷积层，输出就是1×1，但是感受野是3×3（原始输入）。</li>\n</ul>"},{"title":"5.1 卷积神经网络","date":"2024-04-20T10:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 1 卷积神经网络\n- 一种强大的、为图像数据而设计的神经网络\n- mlp十分适合表格数据，但是对于高维数据，mlp不实用了\n## 1.1 不变性\n- 平移不变性：不管检测对象出现在图像的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应。\n- 局部性：神经网络的前几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系\n## 1.2 多层感知机的限制\n- 假设输入图像：$\\mathbf{X}$，隐藏表示：$\\mathbf{H}$。$\\mathbf{X}$与$\\mathbf{H}$具有相同的形状。权重矩阵：$W$，偏置：$\\mathbf{U}$。全连接层可以表示成：$V$是对$W$的一个偏移。\n\n$$ \\begin{aligned} \\left[ \\mathbf{H} \\right]_ {i,j} & = [ \\mathbf{U}]_ {i,j}+ \\sum_k \\sum_l[ \\mathbf{W}]_ {i,j,k,l}[ \\mathbf{X}]_ {k,l} \\\\ \n&= [ \\mathbf{U}]_ {i,j}+ \\sum_a \\sum_b[ \\mathbf{V}]_ {i,j,a,b}[ \\mathbf{X}]_ {i+a,j+b} \\end{aligned}$$\n\n- 索引a和b通过在正偏移和负偏移之间移动覆盖了整个图像。对于像素$\\mathbf{H}_ {i,j}$，可以通过在$\\mathbf{X}$中以(i,j)为中心对像素进行加权求和得到，加权使用的权重为$V_ {i,j,a,b}$。\n### 1.2.1 平移不变性\n- 检测对象在输入$\\mathbf{X}$中的平移，应该仅导致隐藏表示$\\mathbf{H}$中的平移。也就是说，v和$\\mathbf{U}$实际上不依赖于(i,j)的值，即$V_ {i,j,a,b}=\\mathbf{V}_ {a,b}$。且 $\\mathbf{U}$是一个常数。因此， $\\mathbf{H}$的计算可以简化为：\n$$\\mathbf{H}_ {i,j} = u+\\sum_a \\sum_b \\mathbf{V}_ {a,b} \\mathbf{X}_ {i+a,j+b}$$\n- 这就是卷积（convolution）。使用系数$\\mathbf{V}_ {a,b}$对位置(i,j)附近的像素(i+a,j+b)进行加权求和，得到$\\mathbf{H}_ {i,j}$。注意，$\\mathbf{V}_ {a,b}$的系数比$V_{i,j,a,b}$少得多，因为前者不再依赖于图像中的位置。\n### 1.2.2 局部性\n- 为了收集用来训练参数$\\mathbf{H}_ {i,j}$的相关信息，我们不应该偏离到距离(i,j)很远的地方。这意味着在$|a|>\\Delta$或$|b|>\\Delta$的范围之外，我们可以设置$\\mathbf{V}_ {a,b}=0$。这样，我们可以将$\\mathbf{H}_ {i,j}$的计算简化为：\n$$\\mathbf{H}_ {i,j}=u+ \\sum_ {a=- \\Delta}^ {\\Delta} \\sum_ {b=- \\Delta}^ {\\Delta} \\mathbf{V}_ {a,b} \\mathbf{X}_ {i+a,j+b}$$\n- 上述公式就是卷积层的计算公式。\n- 卷积核（滤波器）：$\\mathbf{V}$，卷积核大小：$2\\Delta+1$。\n- 参数大幅减少的代价是，确定每个隐藏活性值时，每一层只包含局部的信息。\n- 以上所有的权重学习都将依赖于归纳偏置。当这种偏置与现实相符时，就能得到样本有效的模型，模型泛化较好；如果这种偏置与现实不符，比如当图像不满足平移不变性时，模型难以拟合训练数据。\n\n## 1.3 卷积\n- 数学上，两个函数之间的卷积定义为：\n    - 卷积相当于把一个函数翻转并位移x时，测量f和g之间的重叠。\n$$(f*g)(\\mathbf{x})=\\int f(\\mathbf{z})g(\\mathbf{x-z})d\\mathbf{z}$$\n\n\n\n\n- 在离散情况下，卷积定义为：\n$$(f*g)(i)=\\sum_a\\sum_bf(a)g(i-a)$$\n\n- 对于二维张量，则为f的索引(a,b)和g的索引(i-a,j-b)上的对应加和：\n    - 卷积核：g，输入：f，输出：f*g\n\n$$ (f*g)(i,j)=\\sum_a \\sum_b f(a,b) g(i-a,j-b) $$\n\n## 1.4 通道\n- 由于输入图像是三维的，隐藏表示$\\mathbf{H}$也最好采用三维张量。因此，隐藏表示可以想象成一系列具有二维张量的通道。这些通道也成为特征映射（feature maps）。\n- 因为每个通道都向后续层提供一组空间化的特征学习。在靠近输入的底层，一些通道专门识别边缘，一些通道专门识别纹理。\n\n    \n","source":"_posts/deeplearning/code/pytorch/5_convolution/1_cov.md","raw":"---\ntitle: 5.1 卷积神经网络\ndate: 2024-4-20 18:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 1 卷积神经网络\n- 一种强大的、为图像数据而设计的神经网络\n- mlp十分适合表格数据，但是对于高维数据，mlp不实用了\n## 1.1 不变性\n- 平移不变性：不管检测对象出现在图像的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应。\n- 局部性：神经网络的前几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系\n## 1.2 多层感知机的限制\n- 假设输入图像：$\\mathbf{X}$，隐藏表示：$\\mathbf{H}$。$\\mathbf{X}$与$\\mathbf{H}$具有相同的形状。权重矩阵：$W$，偏置：$\\mathbf{U}$。全连接层可以表示成：$V$是对$W$的一个偏移。\n\n$$ \\begin{aligned} \\left[ \\mathbf{H} \\right]_ {i,j} & = [ \\mathbf{U}]_ {i,j}+ \\sum_k \\sum_l[ \\mathbf{W}]_ {i,j,k,l}[ \\mathbf{X}]_ {k,l} \\\\ \n&= [ \\mathbf{U}]_ {i,j}+ \\sum_a \\sum_b[ \\mathbf{V}]_ {i,j,a,b}[ \\mathbf{X}]_ {i+a,j+b} \\end{aligned}$$\n\n- 索引a和b通过在正偏移和负偏移之间移动覆盖了整个图像。对于像素$\\mathbf{H}_ {i,j}$，可以通过在$\\mathbf{X}$中以(i,j)为中心对像素进行加权求和得到，加权使用的权重为$V_ {i,j,a,b}$。\n### 1.2.1 平移不变性\n- 检测对象在输入$\\mathbf{X}$中的平移，应该仅导致隐藏表示$\\mathbf{H}$中的平移。也就是说，v和$\\mathbf{U}$实际上不依赖于(i,j)的值，即$V_ {i,j,a,b}=\\mathbf{V}_ {a,b}$。且 $\\mathbf{U}$是一个常数。因此， $\\mathbf{H}$的计算可以简化为：\n$$\\mathbf{H}_ {i,j} = u+\\sum_a \\sum_b \\mathbf{V}_ {a,b} \\mathbf{X}_ {i+a,j+b}$$\n- 这就是卷积（convolution）。使用系数$\\mathbf{V}_ {a,b}$对位置(i,j)附近的像素(i+a,j+b)进行加权求和，得到$\\mathbf{H}_ {i,j}$。注意，$\\mathbf{V}_ {a,b}$的系数比$V_{i,j,a,b}$少得多，因为前者不再依赖于图像中的位置。\n### 1.2.2 局部性\n- 为了收集用来训练参数$\\mathbf{H}_ {i,j}$的相关信息，我们不应该偏离到距离(i,j)很远的地方。这意味着在$|a|>\\Delta$或$|b|>\\Delta$的范围之外，我们可以设置$\\mathbf{V}_ {a,b}=0$。这样，我们可以将$\\mathbf{H}_ {i,j}$的计算简化为：\n$$\\mathbf{H}_ {i,j}=u+ \\sum_ {a=- \\Delta}^ {\\Delta} \\sum_ {b=- \\Delta}^ {\\Delta} \\mathbf{V}_ {a,b} \\mathbf{X}_ {i+a,j+b}$$\n- 上述公式就是卷积层的计算公式。\n- 卷积核（滤波器）：$\\mathbf{V}$，卷积核大小：$2\\Delta+1$。\n- 参数大幅减少的代价是，确定每个隐藏活性值时，每一层只包含局部的信息。\n- 以上所有的权重学习都将依赖于归纳偏置。当这种偏置与现实相符时，就能得到样本有效的模型，模型泛化较好；如果这种偏置与现实不符，比如当图像不满足平移不变性时，模型难以拟合训练数据。\n\n## 1.3 卷积\n- 数学上，两个函数之间的卷积定义为：\n    - 卷积相当于把一个函数翻转并位移x时，测量f和g之间的重叠。\n$$(f*g)(\\mathbf{x})=\\int f(\\mathbf{z})g(\\mathbf{x-z})d\\mathbf{z}$$\n\n\n\n\n- 在离散情况下，卷积定义为：\n$$(f*g)(i)=\\sum_a\\sum_bf(a)g(i-a)$$\n\n- 对于二维张量，则为f的索引(a,b)和g的索引(i-a,j-b)上的对应加和：\n    - 卷积核：g，输入：f，输出：f*g\n\n$$ (f*g)(i,j)=\\sum_a \\sum_b f(a,b) g(i-a,j-b) $$\n\n## 1.4 通道\n- 由于输入图像是三维的，隐藏表示$\\mathbf{H}$也最好采用三维张量。因此，隐藏表示可以想象成一系列具有二维张量的通道。这些通道也成为特征映射（feature maps）。\n- 因为每个通道都向后续层提供一组空间化的特征学习。在靠近输入的底层，一些通道专门识别边缘，一些通道专门识别纹理。\n\n    \n","slug":"deeplearning/code/pytorch/5_convolution/1_cov","published":1,"updated":"2024-05-02T08:50:33.054Z","comments":1,"layout":"post","photos":[],"_id":"clvuli68r002dzsvw5mwa9j07","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"1-卷积神经网络\"><a href=\"#1-卷积神经网络\" class=\"headerlink\" title=\"1 卷积神经网络\"></a>1 卷积神经网络</h1><ul>\n<li>一种强大的、为图像数据而设计的神经网络</li>\n<li>mlp十分适合表格数据，但是对于高维数据，mlp不实用了</li>\n</ul>\n<h2 id=\"1-1-不变性\"><a href=\"#1-1-不变性\" class=\"headerlink\" title=\"1.1 不变性\"></a>1.1 不变性</h2><ul>\n<li>平移不变性：不管检测对象出现在图像的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应。</li>\n<li>局部性：神经网络的前几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系</li>\n</ul>\n<h2 id=\"1-2-多层感知机的限制\"><a href=\"#1-2-多层感知机的限制\" class=\"headerlink\" title=\"1.2 多层感知机的限制\"></a>1.2 多层感知机的限制</h2><ul>\n<li>假设输入图像：$\\mathbf{X}$，隐藏表示：$\\mathbf{H}$。$\\mathbf{X}$与$\\mathbf{H}$具有相同的形状。权重矩阵：$W$，偏置：$\\mathbf{U}$。全连接层可以表示成：$V$是对$W$的一个偏移。</li>\n</ul>\n<p>$$ \\begin{aligned} \\left[ \\mathbf{H} \\right]_ {i,j} &amp; &#x3D; [ \\mathbf{U}]_ {i,j}+ \\sum_k \\sum_l[ \\mathbf{W}]_ {i,j,k,l}[ \\mathbf{X}]_ {k,l} \\<br>&amp;&#x3D; [ \\mathbf{U}]_ {i,j}+ \\sum_a \\sum_b[ \\mathbf{V}]_ {i,j,a,b}[ \\mathbf{X}]_ {i+a,j+b} \\end{aligned}$$</p>\n<ul>\n<li>索引a和b通过在正偏移和负偏移之间移动覆盖了整个图像。对于像素$\\mathbf{H}_ {i,j}$，可以通过在$\\mathbf{X}$中以(i,j)为中心对像素进行加权求和得到，加权使用的权重为$V_ {i,j,a,b}$。</li>\n</ul>\n<h3 id=\"1-2-1-平移不变性\"><a href=\"#1-2-1-平移不变性\" class=\"headerlink\" title=\"1.2.1 平移不变性\"></a>1.2.1 平移不变性</h3><ul>\n<li>检测对象在输入$\\mathbf{X}$中的平移，应该仅导致隐藏表示$\\mathbf{H}$中的平移。也就是说，v和$\\mathbf{U}$实际上不依赖于(i,j)的值，即$V_ {i,j,a,b}&#x3D;\\mathbf{V}_ {a,b}$。且 $\\mathbf{U}$是一个常数。因此， $\\mathbf{H}$的计算可以简化为：<br>$$\\mathbf{H}_ {i,j} &#x3D; u+\\sum_a \\sum_b \\mathbf{V}_ {a,b} \\mathbf{X}_ {i+a,j+b}$$</li>\n<li>这就是卷积（convolution）。使用系数$\\mathbf{V}_ {a,b}$对位置(i,j)附近的像素(i+a,j+b)进行加权求和，得到$\\mathbf{H}_ {i,j}$。注意，$\\mathbf{V}_ {a,b}$的系数比$V_{i,j,a,b}$少得多，因为前者不再依赖于图像中的位置。</li>\n</ul>\n<h3 id=\"1-2-2-局部性\"><a href=\"#1-2-2-局部性\" class=\"headerlink\" title=\"1.2.2 局部性\"></a>1.2.2 局部性</h3><ul>\n<li>为了收集用来训练参数$\\mathbf{H}_ {i,j}$的相关信息，我们不应该偏离到距离(i,j)很远的地方。这意味着在$|a|&gt;\\Delta$或$|b|&gt;\\Delta$的范围之外，我们可以设置$\\mathbf{V}_ {a,b}&#x3D;0$。这样，我们可以将$\\mathbf{H}_ {i,j}$的计算简化为：<br>$$\\mathbf{H}_ {i,j}&#x3D;u+ \\sum_ {a&#x3D;- \\Delta}^ {\\Delta} \\sum_ {b&#x3D;- \\Delta}^ {\\Delta} \\mathbf{V}_ {a,b} \\mathbf{X}_ {i+a,j+b}$$</li>\n<li>上述公式就是卷积层的计算公式。</li>\n<li>卷积核（滤波器）：$\\mathbf{V}$，卷积核大小：$2\\Delta+1$。</li>\n<li>参数大幅减少的代价是，确定每个隐藏活性值时，每一层只包含局部的信息。</li>\n<li>以上所有的权重学习都将依赖于归纳偏置。当这种偏置与现实相符时，就能得到样本有效的模型，模型泛化较好；如果这种偏置与现实不符，比如当图像不满足平移不变性时，模型难以拟合训练数据。</li>\n</ul>\n<h2 id=\"1-3-卷积\"><a href=\"#1-3-卷积\" class=\"headerlink\" title=\"1.3 卷积\"></a>1.3 卷积</h2><ul>\n<li><p>数学上，两个函数之间的卷积定义为：</p>\n<ul>\n<li>卷积相当于把一个函数翻转并位移x时，测量f和g之间的重叠。<br>$$(f*g)(\\mathbf{x})&#x3D;\\int f(\\mathbf{z})g(\\mathbf{x-z})d\\mathbf{z}$$</li>\n</ul>\n</li>\n<li><p>在离散情况下，卷积定义为：<br>$$(f*g)(i)&#x3D;\\sum_a\\sum_bf(a)g(i-a)$$</p>\n</li>\n<li><p>对于二维张量，则为f的索引(a,b)和g的索引(i-a,j-b)上的对应加和：</p>\n<ul>\n<li>卷积核：g，输入：f，输出：f*g</li>\n</ul>\n</li>\n</ul>\n<p>$$ (f*g)(i,j)&#x3D;\\sum_a \\sum_b f(a,b) g(i-a,j-b) $$</p>\n<h2 id=\"1-4-通道\"><a href=\"#1-4-通道\" class=\"headerlink\" title=\"1.4 通道\"></a>1.4 通道</h2><ul>\n<li>由于输入图像是三维的，隐藏表示$\\mathbf{H}$也最好采用三维张量。因此，隐藏表示可以想象成一系列具有二维张量的通道。这些通道也成为特征映射（feature maps）。</li>\n<li>因为每个通道都向后续层提供一组空间化的特征学习。在靠近输入的底层，一些通道专门识别边缘，一些通道专门识别纹理。</li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"1-卷积神经网络\"><a href=\"#1-卷积神经网络\" class=\"headerlink\" title=\"1 卷积神经网络\"></a>1 卷积神经网络</h1><ul>\n<li>一种强大的、为图像数据而设计的神经网络</li>\n<li>mlp十分适合表格数据，但是对于高维数据，mlp不实用了</li>\n</ul>\n<h2 id=\"1-1-不变性\"><a href=\"#1-1-不变性\" class=\"headerlink\" title=\"1.1 不变性\"></a>1.1 不变性</h2><ul>\n<li>平移不变性：不管检测对象出现在图像的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应。</li>\n<li>局部性：神经网络的前几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系</li>\n</ul>\n<h2 id=\"1-2-多层感知机的限制\"><a href=\"#1-2-多层感知机的限制\" class=\"headerlink\" title=\"1.2 多层感知机的限制\"></a>1.2 多层感知机的限制</h2><ul>\n<li>假设输入图像：$\\mathbf{X}$，隐藏表示：$\\mathbf{H}$。$\\mathbf{X}$与$\\mathbf{H}$具有相同的形状。权重矩阵：$W$，偏置：$\\mathbf{U}$。全连接层可以表示成：$V$是对$W$的一个偏移。</li>\n</ul>\n<p>$$ \\begin{aligned} \\left[ \\mathbf{H} \\right]_ {i,j} &amp; &#x3D; [ \\mathbf{U}]_ {i,j}+ \\sum_k \\sum_l[ \\mathbf{W}]_ {i,j,k,l}[ \\mathbf{X}]_ {k,l} \\<br>&amp;&#x3D; [ \\mathbf{U}]_ {i,j}+ \\sum_a \\sum_b[ \\mathbf{V}]_ {i,j,a,b}[ \\mathbf{X}]_ {i+a,j+b} \\end{aligned}$$</p>\n<ul>\n<li>索引a和b通过在正偏移和负偏移之间移动覆盖了整个图像。对于像素$\\mathbf{H}_ {i,j}$，可以通过在$\\mathbf{X}$中以(i,j)为中心对像素进行加权求和得到，加权使用的权重为$V_ {i,j,a,b}$。</li>\n</ul>\n<h3 id=\"1-2-1-平移不变性\"><a href=\"#1-2-1-平移不变性\" class=\"headerlink\" title=\"1.2.1 平移不变性\"></a>1.2.1 平移不变性</h3><ul>\n<li>检测对象在输入$\\mathbf{X}$中的平移，应该仅导致隐藏表示$\\mathbf{H}$中的平移。也就是说，v和$\\mathbf{U}$实际上不依赖于(i,j)的值，即$V_ {i,j,a,b}&#x3D;\\mathbf{V}_ {a,b}$。且 $\\mathbf{U}$是一个常数。因此， $\\mathbf{H}$的计算可以简化为：<br>$$\\mathbf{H}_ {i,j} &#x3D; u+\\sum_a \\sum_b \\mathbf{V}_ {a,b} \\mathbf{X}_ {i+a,j+b}$$</li>\n<li>这就是卷积（convolution）。使用系数$\\mathbf{V}_ {a,b}$对位置(i,j)附近的像素(i+a,j+b)进行加权求和，得到$\\mathbf{H}_ {i,j}$。注意，$\\mathbf{V}_ {a,b}$的系数比$V_{i,j,a,b}$少得多，因为前者不再依赖于图像中的位置。</li>\n</ul>\n<h3 id=\"1-2-2-局部性\"><a href=\"#1-2-2-局部性\" class=\"headerlink\" title=\"1.2.2 局部性\"></a>1.2.2 局部性</h3><ul>\n<li>为了收集用来训练参数$\\mathbf{H}_ {i,j}$的相关信息，我们不应该偏离到距离(i,j)很远的地方。这意味着在$|a|&gt;\\Delta$或$|b|&gt;\\Delta$的范围之外，我们可以设置$\\mathbf{V}_ {a,b}&#x3D;0$。这样，我们可以将$\\mathbf{H}_ {i,j}$的计算简化为：<br>$$\\mathbf{H}_ {i,j}&#x3D;u+ \\sum_ {a&#x3D;- \\Delta}^ {\\Delta} \\sum_ {b&#x3D;- \\Delta}^ {\\Delta} \\mathbf{V}_ {a,b} \\mathbf{X}_ {i+a,j+b}$$</li>\n<li>上述公式就是卷积层的计算公式。</li>\n<li>卷积核（滤波器）：$\\mathbf{V}$，卷积核大小：$2\\Delta+1$。</li>\n<li>参数大幅减少的代价是，确定每个隐藏活性值时，每一层只包含局部的信息。</li>\n<li>以上所有的权重学习都将依赖于归纳偏置。当这种偏置与现实相符时，就能得到样本有效的模型，模型泛化较好；如果这种偏置与现实不符，比如当图像不满足平移不变性时，模型难以拟合训练数据。</li>\n</ul>\n<h2 id=\"1-3-卷积\"><a href=\"#1-3-卷积\" class=\"headerlink\" title=\"1.3 卷积\"></a>1.3 卷积</h2><ul>\n<li><p>数学上，两个函数之间的卷积定义为：</p>\n<ul>\n<li>卷积相当于把一个函数翻转并位移x时，测量f和g之间的重叠。<br>$$(f*g)(\\mathbf{x})&#x3D;\\int f(\\mathbf{z})g(\\mathbf{x-z})d\\mathbf{z}$$</li>\n</ul>\n</li>\n<li><p>在离散情况下，卷积定义为：<br>$$(f*g)(i)&#x3D;\\sum_a\\sum_bf(a)g(i-a)$$</p>\n</li>\n<li><p>对于二维张量，则为f的索引(a,b)和g的索引(i-a,j-b)上的对应加和：</p>\n<ul>\n<li>卷积核：g，输入：f，输出：f*g</li>\n</ul>\n</li>\n</ul>\n<p>$$ (f*g)(i,j)&#x3D;\\sum_a \\sum_b f(a,b) g(i-a,j-b) $$</p>\n<h2 id=\"1-4-通道\"><a href=\"#1-4-通道\" class=\"headerlink\" title=\"1.4 通道\"></a>1.4 通道</h2><ul>\n<li>由于输入图像是三维的，隐藏表示$\\mathbf{H}$也最好采用三维张量。因此，隐藏表示可以想象成一系列具有二维张量的通道。这些通道也成为特征映射（feature maps）。</li>\n<li>因为每个通道都向后续层提供一组空间化的特征学习。在靠近输入的底层，一些通道专门识别边缘，一些通道专门识别纹理。</li>\n</ul>"},{"title":"5.4 多输入多输出通道","date":"2024-04-21T03:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 4 多输入多输出通道\n## 4.1 多输入通道\n- 对每个通道分别进行互现观运算，然后按通道相加\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/5_convolution/4_mult_channel/1.png)\n![](img/deeplearning/code/pytorch/5_convolution/4_mult_channel/1.png)\n\n\n\n\n```python\nimport torch\nfrom d2l import torch as d2l\n\ndef corr2d_multi_in(X,K):\n    # 先遍历X和K的第0个维度（channel），再把它们加在一起\n    return sum(d2l.corr2d(x,k) for x,k in zip(X,K))\n\n# 验证上图\nX = torch.tensor([[[0.0,1.0,2.0],[3.0,4.0,5.0],[6.0,7.0,8.0]],\n                  [[1.0,2.0,3.0],[4.0,5.0,6.0],[7.0,8.0,9.0]]])\nK = torch.tensor([[[0.0,1.0],[2.0,3.0]],[[1.0,2.0],[3.0,4.0]]])\ny = corr2d_multi_in(X,K)\nprint(y)\n```\n\n    tensor([[ 56.,  72.],\n            [104., 120.]])\n    \n\n## 4.2 多输出通道\n- 在流行的网络架构中，随着层数的加深，通常会增加输出通道的维数，通过减少空间分辨率以获得更大的通道深度。\n- 可以将每个通道看作对不同特征的响应。而现实可能更为复杂，因为每个通道不是独立学习的，而是为了共同使用而优化的。\n- 输入通道数$c_i$，输出通道数$c_o$\n    - 对于一个输入通道：卷积核：$c_i \\times k_h \\times k_w$，卷积后相加获得一个二维张量。\n    - 卷积核：$c_o \\times c_i \\times k_h \\times k_w$输出通道$c_o$\n\n\n```python\ndef corr2d_multi_in_out(X,K):\n    # 迭代K的第0个维度，每次都对输入X执行互相关运算\n    # 最后将所有结果叠加在一起\n    return torch.stack([corr2d_multi_in(X,k) for k in K],0)\nK = torch.stack((K,K+1,K+2),0) #构造一个具有3输出通道的卷积核\nprint(K.shape)\ns = corr2d_multi_in_out(X,K)\nprint(s)\n```\n\n    torch.Size([3, 2, 2, 2])\n    tensor([[[ 56.,  72.],\n             [104., 120.]],\n    \n            [[ 76., 100.],\n             [148., 172.]],\n    \n            [[ 96., 128.],\n             [192., 224.]]])\n    \n\n## 4.3 1x1卷积层\n- 1x1卷积失去了卷积层特有的能力（在高度和宽度维度上，识别相邻元素间相互作用的能力。）其实1x1卷积的唯一计算发生在通道上。\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/5_convolution/4_mult_channel/2.png)\n![](img/deeplearning/code/pytorch/5_convolution/4_mult_channel/2.png)\n- 可以将1x1卷积层看作在每个像素位置应用的全连接层，以$c_i$个输入值转换成$c_o$个输出。\n- 使用全连接层实现1x1卷积\n\n\n```python\ndef corr2d_multi_in_out_1x1(X,K):\n    c_i, h, w = X.shape\n    c_o = K.shape[0]\n    X = X.reshape((c_i, h*w))\n    K = K.reshape((c_o,c_i))\n    # 全连接层中的矩阵乘法\n    Y = torch.matmul(K,X)\n    return Y.reshape((c_o,h,w))\n```\n\n- 上述函数相当于corr2d_multi_in_out函数\n\n\n```python\nX = torch.normal(0,1,(3,3,3))\nK = torch.normal(0,1,(2,3,1,1))\nY1 = corr2d_multi_in_out_1x1(X,K)\nY2 = corr2d_multi_in_out(X,K)\nassert float(torch.abs(Y1 - Y2).sum()) < 1e-6\n```\n","source":"_posts/deeplearning/code/pytorch/5_convolution/4_mult_channel.md","raw":"---\ntitle: 5.4 多输入多输出通道\ndate: 2024-4-21 11:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 4 多输入多输出通道\n## 4.1 多输入通道\n- 对每个通道分别进行互现观运算，然后按通道相加\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/5_convolution/4_mult_channel/1.png)\n![](img/deeplearning/code/pytorch/5_convolution/4_mult_channel/1.png)\n\n\n\n\n```python\nimport torch\nfrom d2l import torch as d2l\n\ndef corr2d_multi_in(X,K):\n    # 先遍历X和K的第0个维度（channel），再把它们加在一起\n    return sum(d2l.corr2d(x,k) for x,k in zip(X,K))\n\n# 验证上图\nX = torch.tensor([[[0.0,1.0,2.0],[3.0,4.0,5.0],[6.0,7.0,8.0]],\n                  [[1.0,2.0,3.0],[4.0,5.0,6.0],[7.0,8.0,9.0]]])\nK = torch.tensor([[[0.0,1.0],[2.0,3.0]],[[1.0,2.0],[3.0,4.0]]])\ny = corr2d_multi_in(X,K)\nprint(y)\n```\n\n    tensor([[ 56.,  72.],\n            [104., 120.]])\n    \n\n## 4.2 多输出通道\n- 在流行的网络架构中，随着层数的加深，通常会增加输出通道的维数，通过减少空间分辨率以获得更大的通道深度。\n- 可以将每个通道看作对不同特征的响应。而现实可能更为复杂，因为每个通道不是独立学习的，而是为了共同使用而优化的。\n- 输入通道数$c_i$，输出通道数$c_o$\n    - 对于一个输入通道：卷积核：$c_i \\times k_h \\times k_w$，卷积后相加获得一个二维张量。\n    - 卷积核：$c_o \\times c_i \\times k_h \\times k_w$输出通道$c_o$\n\n\n```python\ndef corr2d_multi_in_out(X,K):\n    # 迭代K的第0个维度，每次都对输入X执行互相关运算\n    # 最后将所有结果叠加在一起\n    return torch.stack([corr2d_multi_in(X,k) for k in K],0)\nK = torch.stack((K,K+1,K+2),0) #构造一个具有3输出通道的卷积核\nprint(K.shape)\ns = corr2d_multi_in_out(X,K)\nprint(s)\n```\n\n    torch.Size([3, 2, 2, 2])\n    tensor([[[ 56.,  72.],\n             [104., 120.]],\n    \n            [[ 76., 100.],\n             [148., 172.]],\n    \n            [[ 96., 128.],\n             [192., 224.]]])\n    \n\n## 4.3 1x1卷积层\n- 1x1卷积失去了卷积层特有的能力（在高度和宽度维度上，识别相邻元素间相互作用的能力。）其实1x1卷积的唯一计算发生在通道上。\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/5_convolution/4_mult_channel/2.png)\n![](img/deeplearning/code/pytorch/5_convolution/4_mult_channel/2.png)\n- 可以将1x1卷积层看作在每个像素位置应用的全连接层，以$c_i$个输入值转换成$c_o$个输出。\n- 使用全连接层实现1x1卷积\n\n\n```python\ndef corr2d_multi_in_out_1x1(X,K):\n    c_i, h, w = X.shape\n    c_o = K.shape[0]\n    X = X.reshape((c_i, h*w))\n    K = K.reshape((c_o,c_i))\n    # 全连接层中的矩阵乘法\n    Y = torch.matmul(K,X)\n    return Y.reshape((c_o,h,w))\n```\n\n- 上述函数相当于corr2d_multi_in_out函数\n\n\n```python\nX = torch.normal(0,1,(3,3,3))\nK = torch.normal(0,1,(2,3,1,1))\nY1 = corr2d_multi_in_out_1x1(X,K)\nY2 = corr2d_multi_in_out(X,K)\nassert float(torch.abs(Y1 - Y2).sum()) < 1e-6\n```\n","slug":"deeplearning/code/pytorch/5_convolution/4_mult_channel","published":1,"updated":"2024-05-02T08:51:15.479Z","comments":1,"layout":"post","photos":[],"_id":"clvuli68s002fzsvwc6fyauki","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"4-多输入多输出通道\"><a href=\"#4-多输入多输出通道\" class=\"headerlink\" title=\"4 多输入多输出通道\"></a>4 多输入多输出通道</h1><h2 id=\"4-1-多输入通道\"><a href=\"#4-1-多输入通道\" class=\"headerlink\" title=\"4.1 多输入通道\"></a>4.1 多输入通道</h2><ul>\n<li>对每个通道分别进行互现观运算，然后按通道相加<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/5_convolution/4_mult_channel/1.png\"><br><img src=\"/img/deeplearning/code/pytorch/5_convolution/4_mult_channel/1.png\"></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">corr2d_multi_in</span>(<span class=\"params\">X,K</span>):</span><br><span class=\"line\">    <span class=\"comment\"># 先遍历X和K的第0个维度（channel），再把它们加在一起</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">sum</span>(d2l.corr2d(x,k) <span class=\"keyword\">for</span> x,k <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(X,K))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 验证上图</span></span><br><span class=\"line\">X = torch.tensor([[[<span class=\"number\">0.0</span>,<span class=\"number\">1.0</span>,<span class=\"number\">2.0</span>],[<span class=\"number\">3.0</span>,<span class=\"number\">4.0</span>,<span class=\"number\">5.0</span>],[<span class=\"number\">6.0</span>,<span class=\"number\">7.0</span>,<span class=\"number\">8.0</span>]],</span><br><span class=\"line\">                  [[<span class=\"number\">1.0</span>,<span class=\"number\">2.0</span>,<span class=\"number\">3.0</span>],[<span class=\"number\">4.0</span>,<span class=\"number\">5.0</span>,<span class=\"number\">6.0</span>],[<span class=\"number\">7.0</span>,<span class=\"number\">8.0</span>,<span class=\"number\">9.0</span>]]])</span><br><span class=\"line\">K = torch.tensor([[[<span class=\"number\">0.0</span>,<span class=\"number\">1.0</span>],[<span class=\"number\">2.0</span>,<span class=\"number\">3.0</span>]],[[<span class=\"number\">1.0</span>,<span class=\"number\">2.0</span>],[<span class=\"number\">3.0</span>,<span class=\"number\">4.0</span>]]])</span><br><span class=\"line\">y = corr2d_multi_in(X,K)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(y)</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[ 56.,  72.],\n        [104., 120.]])\n</code></pre>\n<h2 id=\"4-2-多输出通道\"><a href=\"#4-2-多输出通道\" class=\"headerlink\" title=\"4.2 多输出通道\"></a>4.2 多输出通道</h2><ul>\n<li>在流行的网络架构中，随着层数的加深，通常会增加输出通道的维数，通过减少空间分辨率以获得更大的通道深度。</li>\n<li>可以将每个通道看作对不同特征的响应。而现实可能更为复杂，因为每个通道不是独立学习的，而是为了共同使用而优化的。</li>\n<li>输入通道数$c_i$，输出通道数$c_o$<ul>\n<li>对于一个输入通道：卷积核：$c_i \\times k_h \\times k_w$，卷积后相加获得一个二维张量。</li>\n<li>卷积核：$c_o \\times c_i \\times k_h \\times k_w$输出通道$c_o$</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">corr2d_multi_in_out</span>(<span class=\"params\">X,K</span>):</span><br><span class=\"line\">    <span class=\"comment\"># 迭代K的第0个维度，每次都对输入X执行互相关运算</span></span><br><span class=\"line\">    <span class=\"comment\"># 最后将所有结果叠加在一起</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.stack([corr2d_multi_in(X,k) <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> K],<span class=\"number\">0</span>)</span><br><span class=\"line\">K = torch.stack((K,K+<span class=\"number\">1</span>,K+<span class=\"number\">2</span>),<span class=\"number\">0</span>) <span class=\"comment\">#构造一个具有3输出通道的卷积核</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(K.shape)</span><br><span class=\"line\">s = corr2d_multi_in_out(X,K)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(s)</span><br></pre></td></tr></table></figure>\n\n<pre><code>torch.Size([3, 2, 2, 2])\ntensor([[[ 56.,  72.],\n         [104., 120.]],\n\n        [[ 76., 100.],\n         [148., 172.]],\n\n        [[ 96., 128.],\n         [192., 224.]]])\n</code></pre>\n<h2 id=\"4-3-1x1卷积层\"><a href=\"#4-3-1x1卷积层\" class=\"headerlink\" title=\"4.3 1x1卷积层\"></a>4.3 1x1卷积层</h2><ul>\n<li>1x1卷积失去了卷积层特有的能力（在高度和宽度维度上，识别相邻元素间相互作用的能力。）其实1x1卷积的唯一计算发生在通道上。<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/5_convolution/4_mult_channel/2.png\"><br><img src=\"/img/deeplearning/code/pytorch/5_convolution/4_mult_channel/2.png\"></li>\n<li>可以将1x1卷积层看作在每个像素位置应用的全连接层，以$c_i$个输入值转换成$c_o$个输出。</li>\n<li>使用全连接层实现1x1卷积</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">corr2d_multi_in_out_1x1</span>(<span class=\"params\">X,K</span>):</span><br><span class=\"line\">    c_i, h, w = X.shape</span><br><span class=\"line\">    c_o = K.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">    X = X.reshape((c_i, h*w))</span><br><span class=\"line\">    K = K.reshape((c_o,c_i))</span><br><span class=\"line\">    <span class=\"comment\"># 全连接层中的矩阵乘法</span></span><br><span class=\"line\">    Y = torch.matmul(K,X)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> Y.reshape((c_o,h,w))</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>上述函数相当于corr2d_multi_in_out函数</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = torch.normal(<span class=\"number\">0</span>,<span class=\"number\">1</span>,(<span class=\"number\">3</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">K = torch.normal(<span class=\"number\">0</span>,<span class=\"number\">1</span>,(<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">Y1 = corr2d_multi_in_out_1x1(X,K)</span><br><span class=\"line\">Y2 = corr2d_multi_in_out(X,K)</span><br><span class=\"line\"><span class=\"keyword\">assert</span> <span class=\"built_in\">float</span>(torch.<span class=\"built_in\">abs</span>(Y1 - Y2).<span class=\"built_in\">sum</span>()) &lt; <span class=\"number\">1e-6</span></span><br></pre></td></tr></table></figure>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"4-多输入多输出通道\"><a href=\"#4-多输入多输出通道\" class=\"headerlink\" title=\"4 多输入多输出通道\"></a>4 多输入多输出通道</h1><h2 id=\"4-1-多输入通道\"><a href=\"#4-1-多输入通道\" class=\"headerlink\" title=\"4.1 多输入通道\"></a>4.1 多输入通道</h2><ul>\n<li>对每个通道分别进行互现观运算，然后按通道相加<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/5_convolution/4_mult_channel/1.png\"><br><img src=\"/img/deeplearning/code/pytorch/5_convolution/4_mult_channel/1.png\"></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">corr2d_multi_in</span>(<span class=\"params\">X,K</span>):</span><br><span class=\"line\">    <span class=\"comment\"># 先遍历X和K的第0个维度（channel），再把它们加在一起</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">sum</span>(d2l.corr2d(x,k) <span class=\"keyword\">for</span> x,k <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(X,K))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 验证上图</span></span><br><span class=\"line\">X = torch.tensor([[[<span class=\"number\">0.0</span>,<span class=\"number\">1.0</span>,<span class=\"number\">2.0</span>],[<span class=\"number\">3.0</span>,<span class=\"number\">4.0</span>,<span class=\"number\">5.0</span>],[<span class=\"number\">6.0</span>,<span class=\"number\">7.0</span>,<span class=\"number\">8.0</span>]],</span><br><span class=\"line\">                  [[<span class=\"number\">1.0</span>,<span class=\"number\">2.0</span>,<span class=\"number\">3.0</span>],[<span class=\"number\">4.0</span>,<span class=\"number\">5.0</span>,<span class=\"number\">6.0</span>],[<span class=\"number\">7.0</span>,<span class=\"number\">8.0</span>,<span class=\"number\">9.0</span>]]])</span><br><span class=\"line\">K = torch.tensor([[[<span class=\"number\">0.0</span>,<span class=\"number\">1.0</span>],[<span class=\"number\">2.0</span>,<span class=\"number\">3.0</span>]],[[<span class=\"number\">1.0</span>,<span class=\"number\">2.0</span>],[<span class=\"number\">3.0</span>,<span class=\"number\">4.0</span>]]])</span><br><span class=\"line\">y = corr2d_multi_in(X,K)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(y)</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[ 56.,  72.],\n        [104., 120.]])\n</code></pre>\n<h2 id=\"4-2-多输出通道\"><a href=\"#4-2-多输出通道\" class=\"headerlink\" title=\"4.2 多输出通道\"></a>4.2 多输出通道</h2><ul>\n<li>在流行的网络架构中，随着层数的加深，通常会增加输出通道的维数，通过减少空间分辨率以获得更大的通道深度。</li>\n<li>可以将每个通道看作对不同特征的响应。而现实可能更为复杂，因为每个通道不是独立学习的，而是为了共同使用而优化的。</li>\n<li>输入通道数$c_i$，输出通道数$c_o$<ul>\n<li>对于一个输入通道：卷积核：$c_i \\times k_h \\times k_w$，卷积后相加获得一个二维张量。</li>\n<li>卷积核：$c_o \\times c_i \\times k_h \\times k_w$输出通道$c_o$</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">corr2d_multi_in_out</span>(<span class=\"params\">X,K</span>):</span><br><span class=\"line\">    <span class=\"comment\"># 迭代K的第0个维度，每次都对输入X执行互相关运算</span></span><br><span class=\"line\">    <span class=\"comment\"># 最后将所有结果叠加在一起</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.stack([corr2d_multi_in(X,k) <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> K],<span class=\"number\">0</span>)</span><br><span class=\"line\">K = torch.stack((K,K+<span class=\"number\">1</span>,K+<span class=\"number\">2</span>),<span class=\"number\">0</span>) <span class=\"comment\">#构造一个具有3输出通道的卷积核</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(K.shape)</span><br><span class=\"line\">s = corr2d_multi_in_out(X,K)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(s)</span><br></pre></td></tr></table></figure>\n\n<pre><code>torch.Size([3, 2, 2, 2])\ntensor([[[ 56.,  72.],\n         [104., 120.]],\n\n        [[ 76., 100.],\n         [148., 172.]],\n\n        [[ 96., 128.],\n         [192., 224.]]])\n</code></pre>\n<h2 id=\"4-3-1x1卷积层\"><a href=\"#4-3-1x1卷积层\" class=\"headerlink\" title=\"4.3 1x1卷积层\"></a>4.3 1x1卷积层</h2><ul>\n<li>1x1卷积失去了卷积层特有的能力（在高度和宽度维度上，识别相邻元素间相互作用的能力。）其实1x1卷积的唯一计算发生在通道上。<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/5_convolution/4_mult_channel/2.png\"><br><img src=\"/img/deeplearning/code/pytorch/5_convolution/4_mult_channel/2.png\"></li>\n<li>可以将1x1卷积层看作在每个像素位置应用的全连接层，以$c_i$个输入值转换成$c_o$个输出。</li>\n<li>使用全连接层实现1x1卷积</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">corr2d_multi_in_out_1x1</span>(<span class=\"params\">X,K</span>):</span><br><span class=\"line\">    c_i, h, w = X.shape</span><br><span class=\"line\">    c_o = K.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">    X = X.reshape((c_i, h*w))</span><br><span class=\"line\">    K = K.reshape((c_o,c_i))</span><br><span class=\"line\">    <span class=\"comment\"># 全连接层中的矩阵乘法</span></span><br><span class=\"line\">    Y = torch.matmul(K,X)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> Y.reshape((c_o,h,w))</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>上述函数相当于corr2d_multi_in_out函数</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = torch.normal(<span class=\"number\">0</span>,<span class=\"number\">1</span>,(<span class=\"number\">3</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">K = torch.normal(<span class=\"number\">0</span>,<span class=\"number\">1</span>,(<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">Y1 = corr2d_multi_in_out_1x1(X,K)</span><br><span class=\"line\">Y2 = corr2d_multi_in_out(X,K)</span><br><span class=\"line\"><span class=\"keyword\">assert</span> <span class=\"built_in\">float</span>(torch.<span class=\"built_in\">abs</span>(Y1 - Y2).<span class=\"built_in\">sum</span>()) &lt; <span class=\"number\">1e-6</span></span><br></pre></td></tr></table></figure>"},{"title":"5.3 填充和步幅","date":"2024-04-21T02:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 3 填充和步幅\n- 卷积的输出形状取决于输入形状和卷积核。\n- 填充：卷积后图像越来越小，会丢失原始图像的边界信息。\n- 步幅：我们希望大幅降低图像的高度和宽度。\n## 3.1 填充（padding）\n- 在输入图像的边界填充元素（通常是0）\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/5_convolution/3_padding/1.png)\n![](img/deeplearning/code/pytorch/5_convolution/3_padding/1.png)\n- 通常卷积核的高度和宽度为奇数（如果为偶数，则上下填充的层数会差1）\n- 对于任何二维张量X，当：\n    - 卷积核大小为奇数\n    - 所有边的padding层数相等\n    - 输出与输入具有相同高度和宽度\n    \n    有：输出Y[i,j]是通过以输入X[i,j]为中心，与卷积核进行互相关计算得到的。\n\n\n```python\nimport torch\nfrom torch import nn\n\ndef comp_conv2d(conv2d, X):\n    X = X.reshape((1,1) + X.shape) #(1,1): 扩展两个维度：batch和channel。得到(1,1,8,8)\n    Y = conv2d(X)\n    return Y.reshape(Y.shape[2:]) #省略前两个维度：batch和channel\nconv2d = nn.Conv2d(1,1,kernel_size=3,padding=1)\nX = torch.rand(size=(8,8)) #输入8*8\ns = comp_conv2d(conv2d, X).shape #输出8*8\nprint(s)\n```\n\n    torch.Size([8, 8])\n    \n\n- 当高度和宽度不同时，可以填充不同的高度和宽度，使用5*3的卷积核，高度宽度填充为2和1\n\n\n```python\nconv2d = nn.Conv2d(1,1,kernel_size=(5,3),padding=(2,1))\ns = comp_conv2d(conv2d, X).shape\nprint(s)\n```\n\n    torch.Size([8, 8])\n    \n\n## 3.2 步幅（stride）\n- 有时为了高效计算或是缩减采样次数\n- 卷积核每次滑动的行数和列数\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/5_convolution/3_padding/2.png)\n![](img/deeplearning/code/pytorch/5_convolution/3_padding/2.png)\n- 输入：$n_h \\times n_w$，卷积核：$k_h \\times k_w$，填充：$p_h \\times p_w$，步幅：$s_h \\times s_w$\n- 输出：$o_h \\times o_w$，$o_h = \\lfloor \\frac{n_h + p_h - k_h}{s_h} + 1 \\rfloor$，$o_w = \\lfloor \\frac{n_w + p_w - k_w}{s_w} + 1 \\rfloor$\n- 如果我们设置了$p_h = k_h - 1$和$p_w = k_w - 1$\n    - 那么输出形状将简化为$\\lfloor \\frac{n_h + s_h - 1}{s_h} \\rfloor \\times \\lfloor \\frac{n_w + s_w - 1}{s_w} \\rfloor$\n    - 如果输入的高度和宽度（$n_h$和$n_w$）可以被步幅整除，那么输出：$\\frac{n_h}{s_h} \\times \\frac{n_w}{s_w}$\n- 实践中，我们很少使用不一致的padding和stride\n\n\n```python\n# 缩小一倍\nconv2d = nn.Conv2d(1,1,kernel_size=3,padding=1,stride=2)\ns = comp_conv2d(conv2d, X).shape\nprint(s)\n\nconv2d = nn.Conv2d(1,1,kernel_size=(3,5),padding=(0,1),stride=(3,4))\ns = comp_conv2d(conv2d, X).shape\nprint(s)\n```\n\n    torch.Size([4, 4])\n    torch.Size([2, 2])\n    \n","source":"_posts/deeplearning/code/pytorch/5_convolution/3_padding_stride.md","raw":"---\ntitle: 5.3 填充和步幅\ndate: 2024-4-21 10:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 3 填充和步幅\n- 卷积的输出形状取决于输入形状和卷积核。\n- 填充：卷积后图像越来越小，会丢失原始图像的边界信息。\n- 步幅：我们希望大幅降低图像的高度和宽度。\n## 3.1 填充（padding）\n- 在输入图像的边界填充元素（通常是0）\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/5_convolution/3_padding/1.png)\n![](img/deeplearning/code/pytorch/5_convolution/3_padding/1.png)\n- 通常卷积核的高度和宽度为奇数（如果为偶数，则上下填充的层数会差1）\n- 对于任何二维张量X，当：\n    - 卷积核大小为奇数\n    - 所有边的padding层数相等\n    - 输出与输入具有相同高度和宽度\n    \n    有：输出Y[i,j]是通过以输入X[i,j]为中心，与卷积核进行互相关计算得到的。\n\n\n```python\nimport torch\nfrom torch import nn\n\ndef comp_conv2d(conv2d, X):\n    X = X.reshape((1,1) + X.shape) #(1,1): 扩展两个维度：batch和channel。得到(1,1,8,8)\n    Y = conv2d(X)\n    return Y.reshape(Y.shape[2:]) #省略前两个维度：batch和channel\nconv2d = nn.Conv2d(1,1,kernel_size=3,padding=1)\nX = torch.rand(size=(8,8)) #输入8*8\ns = comp_conv2d(conv2d, X).shape #输出8*8\nprint(s)\n```\n\n    torch.Size([8, 8])\n    \n\n- 当高度和宽度不同时，可以填充不同的高度和宽度，使用5*3的卷积核，高度宽度填充为2和1\n\n\n```python\nconv2d = nn.Conv2d(1,1,kernel_size=(5,3),padding=(2,1))\ns = comp_conv2d(conv2d, X).shape\nprint(s)\n```\n\n    torch.Size([8, 8])\n    \n\n## 3.2 步幅（stride）\n- 有时为了高效计算或是缩减采样次数\n- 卷积核每次滑动的行数和列数\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/5_convolution/3_padding/2.png)\n![](img/deeplearning/code/pytorch/5_convolution/3_padding/2.png)\n- 输入：$n_h \\times n_w$，卷积核：$k_h \\times k_w$，填充：$p_h \\times p_w$，步幅：$s_h \\times s_w$\n- 输出：$o_h \\times o_w$，$o_h = \\lfloor \\frac{n_h + p_h - k_h}{s_h} + 1 \\rfloor$，$o_w = \\lfloor \\frac{n_w + p_w - k_w}{s_w} + 1 \\rfloor$\n- 如果我们设置了$p_h = k_h - 1$和$p_w = k_w - 1$\n    - 那么输出形状将简化为$\\lfloor \\frac{n_h + s_h - 1}{s_h} \\rfloor \\times \\lfloor \\frac{n_w + s_w - 1}{s_w} \\rfloor$\n    - 如果输入的高度和宽度（$n_h$和$n_w$）可以被步幅整除，那么输出：$\\frac{n_h}{s_h} \\times \\frac{n_w}{s_w}$\n- 实践中，我们很少使用不一致的padding和stride\n\n\n```python\n# 缩小一倍\nconv2d = nn.Conv2d(1,1,kernel_size=3,padding=1,stride=2)\ns = comp_conv2d(conv2d, X).shape\nprint(s)\n\nconv2d = nn.Conv2d(1,1,kernel_size=(3,5),padding=(0,1),stride=(3,4))\ns = comp_conv2d(conv2d, X).shape\nprint(s)\n```\n\n    torch.Size([4, 4])\n    torch.Size([2, 2])\n    \n","slug":"deeplearning/code/pytorch/5_convolution/3_padding_stride","published":1,"updated":"2024-05-02T08:51:04.513Z","comments":1,"layout":"post","photos":[],"_id":"clvuli68s002hzsvw1fwxg3ha","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"3-填充和步幅\"><a href=\"#3-填充和步幅\" class=\"headerlink\" title=\"3 填充和步幅\"></a>3 填充和步幅</h1><ul>\n<li>卷积的输出形状取决于输入形状和卷积核。</li>\n<li>填充：卷积后图像越来越小，会丢失原始图像的边界信息。</li>\n<li>步幅：我们希望大幅降低图像的高度和宽度。</li>\n</ul>\n<h2 id=\"3-1-填充（padding）\"><a href=\"#3-1-填充（padding）\" class=\"headerlink\" title=\"3.1 填充（padding）\"></a>3.1 填充（padding）</h2><ul>\n<li><p>在输入图像的边界填充元素（通常是0）<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/5_convolution/3_padding/1.png\"><br><img src=\"/img/deeplearning/code/pytorch/5_convolution/3_padding/1.png\"></p>\n</li>\n<li><p>通常卷积核的高度和宽度为奇数（如果为偶数，则上下填充的层数会差1）</p>\n</li>\n<li><p>对于任何二维张量X，当：</p>\n<ul>\n<li>卷积核大小为奇数</li>\n<li>所有边的padding层数相等</li>\n<li>输出与输入具有相同高度和宽度</li>\n</ul>\n<p>  有：输出Y[i,j]是通过以输入X[i,j]为中心，与卷积核进行互相关计算得到的。</p>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">comp_conv2d</span>(<span class=\"params\">conv2d, X</span>):</span><br><span class=\"line\">    X = X.reshape((<span class=\"number\">1</span>,<span class=\"number\">1</span>) + X.shape) <span class=\"comment\">#(1,1): 扩展两个维度：batch和channel。得到(1,1,8,8)</span></span><br><span class=\"line\">    Y = conv2d(X)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> Y.reshape(Y.shape[<span class=\"number\">2</span>:]) <span class=\"comment\">#省略前两个维度：batch和channel</span></span><br><span class=\"line\">conv2d = nn.Conv2d(<span class=\"number\">1</span>,<span class=\"number\">1</span>,kernel_size=<span class=\"number\">3</span>,padding=<span class=\"number\">1</span>)</span><br><span class=\"line\">X = torch.rand(size=(<span class=\"number\">8</span>,<span class=\"number\">8</span>)) <span class=\"comment\">#输入8*8</span></span><br><span class=\"line\">s = comp_conv2d(conv2d, X).shape <span class=\"comment\">#输出8*8</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(s)</span><br></pre></td></tr></table></figure>\n\n<pre><code>torch.Size([8, 8])\n</code></pre>\n<ul>\n<li>当高度和宽度不同时，可以填充不同的高度和宽度，使用5*3的卷积核，高度宽度填充为2和1</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conv2d = nn.Conv2d(<span class=\"number\">1</span>,<span class=\"number\">1</span>,kernel_size=(<span class=\"number\">5</span>,<span class=\"number\">3</span>),padding=(<span class=\"number\">2</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">s = comp_conv2d(conv2d, X).shape</span><br><span class=\"line\"><span class=\"built_in\">print</span>(s)</span><br></pre></td></tr></table></figure>\n\n<pre><code>torch.Size([8, 8])\n</code></pre>\n<h2 id=\"3-2-步幅（stride）\"><a href=\"#3-2-步幅（stride）\" class=\"headerlink\" title=\"3.2 步幅（stride）\"></a>3.2 步幅（stride）</h2><ul>\n<li>有时为了高效计算或是缩减采样次数</li>\n<li>卷积核每次滑动的行数和列数<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/5_convolution/3_padding/2.png\"><br><img src=\"/img/deeplearning/code/pytorch/5_convolution/3_padding/2.png\"></li>\n<li>输入：$n_h \\times n_w$，卷积核：$k_h \\times k_w$，填充：$p_h \\times p_w$，步幅：$s_h \\times s_w$</li>\n<li>输出：$o_h \\times o_w$，$o_h &#x3D; \\lfloor \\frac{n_h + p_h - k_h}{s_h} + 1 \\rfloor$，$o_w &#x3D; \\lfloor \\frac{n_w + p_w - k_w}{s_w} + 1 \\rfloor$</li>\n<li>如果我们设置了$p_h &#x3D; k_h - 1$和$p_w &#x3D; k_w - 1$<ul>\n<li>那么输出形状将简化为$\\lfloor \\frac{n_h + s_h - 1}{s_h} \\rfloor \\times \\lfloor \\frac{n_w + s_w - 1}{s_w} \\rfloor$</li>\n<li>如果输入的高度和宽度（$n_h$和$n_w$）可以被步幅整除，那么输出：$\\frac{n_h}{s_h} \\times \\frac{n_w}{s_w}$</li>\n</ul>\n</li>\n<li>实践中，我们很少使用不一致的padding和stride</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 缩小一倍</span></span><br><span class=\"line\">conv2d = nn.Conv2d(<span class=\"number\">1</span>,<span class=\"number\">1</span>,kernel_size=<span class=\"number\">3</span>,padding=<span class=\"number\">1</span>,stride=<span class=\"number\">2</span>)</span><br><span class=\"line\">s = comp_conv2d(conv2d, X).shape</span><br><span class=\"line\"><span class=\"built_in\">print</span>(s)</span><br><span class=\"line\"></span><br><span class=\"line\">conv2d = nn.Conv2d(<span class=\"number\">1</span>,<span class=\"number\">1</span>,kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">5</span>),padding=(<span class=\"number\">0</span>,<span class=\"number\">1</span>),stride=(<span class=\"number\">3</span>,<span class=\"number\">4</span>))</span><br><span class=\"line\">s = comp_conv2d(conv2d, X).shape</span><br><span class=\"line\"><span class=\"built_in\">print</span>(s)</span><br></pre></td></tr></table></figure>\n\n<pre><code>torch.Size([4, 4])\ntorch.Size([2, 2])\n</code></pre>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"3-填充和步幅\"><a href=\"#3-填充和步幅\" class=\"headerlink\" title=\"3 填充和步幅\"></a>3 填充和步幅</h1><ul>\n<li>卷积的输出形状取决于输入形状和卷积核。</li>\n<li>填充：卷积后图像越来越小，会丢失原始图像的边界信息。</li>\n<li>步幅：我们希望大幅降低图像的高度和宽度。</li>\n</ul>\n<h2 id=\"3-1-填充（padding）\"><a href=\"#3-1-填充（padding）\" class=\"headerlink\" title=\"3.1 填充（padding）\"></a>3.1 填充（padding）</h2><ul>\n<li><p>在输入图像的边界填充元素（通常是0）<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/5_convolution/3_padding/1.png\"><br><img src=\"/img/deeplearning/code/pytorch/5_convolution/3_padding/1.png\"></p>\n</li>\n<li><p>通常卷积核的高度和宽度为奇数（如果为偶数，则上下填充的层数会差1）</p>\n</li>\n<li><p>对于任何二维张量X，当：</p>\n<ul>\n<li>卷积核大小为奇数</li>\n<li>所有边的padding层数相等</li>\n<li>输出与输入具有相同高度和宽度</li>\n</ul>\n<p>  有：输出Y[i,j]是通过以输入X[i,j]为中心，与卷积核进行互相关计算得到的。</p>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">comp_conv2d</span>(<span class=\"params\">conv2d, X</span>):</span><br><span class=\"line\">    X = X.reshape((<span class=\"number\">1</span>,<span class=\"number\">1</span>) + X.shape) <span class=\"comment\">#(1,1): 扩展两个维度：batch和channel。得到(1,1,8,8)</span></span><br><span class=\"line\">    Y = conv2d(X)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> Y.reshape(Y.shape[<span class=\"number\">2</span>:]) <span class=\"comment\">#省略前两个维度：batch和channel</span></span><br><span class=\"line\">conv2d = nn.Conv2d(<span class=\"number\">1</span>,<span class=\"number\">1</span>,kernel_size=<span class=\"number\">3</span>,padding=<span class=\"number\">1</span>)</span><br><span class=\"line\">X = torch.rand(size=(<span class=\"number\">8</span>,<span class=\"number\">8</span>)) <span class=\"comment\">#输入8*8</span></span><br><span class=\"line\">s = comp_conv2d(conv2d, X).shape <span class=\"comment\">#输出8*8</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(s)</span><br></pre></td></tr></table></figure>\n\n<pre><code>torch.Size([8, 8])\n</code></pre>\n<ul>\n<li>当高度和宽度不同时，可以填充不同的高度和宽度，使用5*3的卷积核，高度宽度填充为2和1</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conv2d = nn.Conv2d(<span class=\"number\">1</span>,<span class=\"number\">1</span>,kernel_size=(<span class=\"number\">5</span>,<span class=\"number\">3</span>),padding=(<span class=\"number\">2</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">s = comp_conv2d(conv2d, X).shape</span><br><span class=\"line\"><span class=\"built_in\">print</span>(s)</span><br></pre></td></tr></table></figure>\n\n<pre><code>torch.Size([8, 8])\n</code></pre>\n<h2 id=\"3-2-步幅（stride）\"><a href=\"#3-2-步幅（stride）\" class=\"headerlink\" title=\"3.2 步幅（stride）\"></a>3.2 步幅（stride）</h2><ul>\n<li>有时为了高效计算或是缩减采样次数</li>\n<li>卷积核每次滑动的行数和列数<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/5_convolution/3_padding/2.png\"><br><img src=\"/img/deeplearning/code/pytorch/5_convolution/3_padding/2.png\"></li>\n<li>输入：$n_h \\times n_w$，卷积核：$k_h \\times k_w$，填充：$p_h \\times p_w$，步幅：$s_h \\times s_w$</li>\n<li>输出：$o_h \\times o_w$，$o_h &#x3D; \\lfloor \\frac{n_h + p_h - k_h}{s_h} + 1 \\rfloor$，$o_w &#x3D; \\lfloor \\frac{n_w + p_w - k_w}{s_w} + 1 \\rfloor$</li>\n<li>如果我们设置了$p_h &#x3D; k_h - 1$和$p_w &#x3D; k_w - 1$<ul>\n<li>那么输出形状将简化为$\\lfloor \\frac{n_h + s_h - 1}{s_h} \\rfloor \\times \\lfloor \\frac{n_w + s_w - 1}{s_w} \\rfloor$</li>\n<li>如果输入的高度和宽度（$n_h$和$n_w$）可以被步幅整除，那么输出：$\\frac{n_h}{s_h} \\times \\frac{n_w}{s_w}$</li>\n</ul>\n</li>\n<li>实践中，我们很少使用不一致的padding和stride</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 缩小一倍</span></span><br><span class=\"line\">conv2d = nn.Conv2d(<span class=\"number\">1</span>,<span class=\"number\">1</span>,kernel_size=<span class=\"number\">3</span>,padding=<span class=\"number\">1</span>,stride=<span class=\"number\">2</span>)</span><br><span class=\"line\">s = comp_conv2d(conv2d, X).shape</span><br><span class=\"line\"><span class=\"built_in\">print</span>(s)</span><br><span class=\"line\"></span><br><span class=\"line\">conv2d = nn.Conv2d(<span class=\"number\">1</span>,<span class=\"number\">1</span>,kernel_size=(<span class=\"number\">3</span>,<span class=\"number\">5</span>),padding=(<span class=\"number\">0</span>,<span class=\"number\">1</span>),stride=(<span class=\"number\">3</span>,<span class=\"number\">4</span>))</span><br><span class=\"line\">s = comp_conv2d(conv2d, X).shape</span><br><span class=\"line\"><span class=\"built_in\">print</span>(s)</span><br></pre></td></tr></table></figure>\n\n<pre><code>torch.Size([4, 4])\ntorch.Size([2, 2])\n</code></pre>"},{"title":"5.5 汇聚层","date":"2024-04-21T04:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 5 汇聚层\n- 处理图像时，我们希望逐渐降低隐藏表示的空间分辨率、聚集信息，这样随着层数的增加，每个神经元对其敏感的感受野（输入）就越大。\n- 机器学习任务通常会跟全局图像的问题有关，所以最后一层的神经元应该对整个输入的全局敏感。通过逐渐聚合信息，生成越来越粗糙的映射，最终实现学习全局表示的目标，同时将卷积图层的所有优势保留在中间层。\n- 当检测较底层的特征时，我们通常希望这些特征保持某种程度上的平移不变性。\n- 汇聚层（pooling）层可以：降低卷积层对位置的敏感性，同时降低对空间采样表示的敏感性。\n## 5.1 最大汇聚层和平均汇聚层\n- 汇聚层使用一个类似卷积核的窗口在输入上滑动，计算方式：计算汇聚窗口中所有元素的最大值或平均值。\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/5_convolution/5_polling/1.png)\n![](img/deeplearning/code/pytorch/5_convolution/5_polling/1.png)\n\n\n\n```python\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\ndef pool2d(X, pool_size, mode='max'):\n    p_h, p_w = pool_size\n    Y = torch.zeros((X.shape[0] - p_h+1, X.shape[1] - p_w+1)) #输出的形状\n    for i in range(Y.shape[0]):\n        for j in range(Y.shape[1]):\n            if mode == 'max':\n                Y[i,j] = X[i:i+p_h,j:j+p_w].max()\n            elif mode == 'avg':\n                Y[i,j] = X[i:i+p_h,j:j+p_w].mean()\n    return Y\n\nX = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\nprint(pool2d(X, (2, 2)))\nprint(pool2d(X,(2,2),'avg'))\n```\n\n    tensor([[4., 5.],\n            [7., 8.]])\n    tensor([[2., 3.],\n            [5., 6.]])\n    \n\n## 5.2 填充和步幅\n\n\n\n```python\n#4维输入张量\nX = torch.arange(16, dtype=torch.float32).reshape((1,1,4,4))\nprint(X)\n```\n\n    tensor([[[[ 0.,  1.,  2.,  3.],\n              [ 4.,  5.,  6.,  7.],\n              [ 8.,  9., 10., 11.],\n              [12., 13., 14., 15.]]]])\n    \n\n- 默认情况下，步幅与窗口大小相同。\n\n\n```python\npool2d = nn.MaxPool2d(3)\nprint(pool2d(X))\n```\n\n    tensor([[[[10.]]]])\n    \n\n- 手动设定padding和stride\n\n\n```python\npool2d = nn.MaxPool2d(3,padding=1, stride=2)\nprint(pool2d(X))\n\npool2d = nn.MaxPool2d((2,3), stride=(2,3), padding=(0,1))\nprint(pool2d(X))\n```\n\n    tensor([[[[ 5.,  7.],\n              [13., 15.]]]])\n    tensor([[[[ 5.,  7.],\n              [13., 15.]]]])\n    \n\n## 5.3 多通道\n- 在处理多通道数据时，pooling层在每个输入通道上单独运算（与卷积不同）\n- pooling层的输出通道数与输入通道数相同\n\n\n```python\n# 构建一个2通道的输入\nX = torch.cat((X,X+1),1) #在channel维度上连接，相当于增加一个通道\nprint(X)\npool2d = nn.MaxPool2d(3, padding=1, stride=2)\nprint(pool2d(X)) #输出通道依然是2\n\n```\n\n    tensor([[[[ 0.,  1.,  2.,  3.],\n              [ 4.,  5.,  6.,  7.],\n              [ 8.,  9., 10., 11.],\n              [12., 13., 14., 15.]],\n    \n             [[ 1.,  2.,  3.,  4.],\n              [ 5.,  6.,  7.,  8.],\n              [ 9., 10., 11., 12.],\n              [13., 14., 15., 16.]]]])\n    tensor([[[[ 5.,  7.],\n              [13., 15.]],\n    \n             [[ 6.,  8.],\n              [14., 16.]]]])\n    \n","source":"_posts/deeplearning/code/pytorch/5_convolution/5_pooling.md","raw":"---\ntitle: 5.5 汇聚层\ndate: 2024-4-21 12:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 5 汇聚层\n- 处理图像时，我们希望逐渐降低隐藏表示的空间分辨率、聚集信息，这样随着层数的增加，每个神经元对其敏感的感受野（输入）就越大。\n- 机器学习任务通常会跟全局图像的问题有关，所以最后一层的神经元应该对整个输入的全局敏感。通过逐渐聚合信息，生成越来越粗糙的映射，最终实现学习全局表示的目标，同时将卷积图层的所有优势保留在中间层。\n- 当检测较底层的特征时，我们通常希望这些特征保持某种程度上的平移不变性。\n- 汇聚层（pooling）层可以：降低卷积层对位置的敏感性，同时降低对空间采样表示的敏感性。\n## 5.1 最大汇聚层和平均汇聚层\n- 汇聚层使用一个类似卷积核的窗口在输入上滑动，计算方式：计算汇聚窗口中所有元素的最大值或平均值。\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/5_convolution/5_polling/1.png)\n![](img/deeplearning/code/pytorch/5_convolution/5_polling/1.png)\n\n\n\n```python\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\ndef pool2d(X, pool_size, mode='max'):\n    p_h, p_w = pool_size\n    Y = torch.zeros((X.shape[0] - p_h+1, X.shape[1] - p_w+1)) #输出的形状\n    for i in range(Y.shape[0]):\n        for j in range(Y.shape[1]):\n            if mode == 'max':\n                Y[i,j] = X[i:i+p_h,j:j+p_w].max()\n            elif mode == 'avg':\n                Y[i,j] = X[i:i+p_h,j:j+p_w].mean()\n    return Y\n\nX = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\nprint(pool2d(X, (2, 2)))\nprint(pool2d(X,(2,2),'avg'))\n```\n\n    tensor([[4., 5.],\n            [7., 8.]])\n    tensor([[2., 3.],\n            [5., 6.]])\n    \n\n## 5.2 填充和步幅\n\n\n\n```python\n#4维输入张量\nX = torch.arange(16, dtype=torch.float32).reshape((1,1,4,4))\nprint(X)\n```\n\n    tensor([[[[ 0.,  1.,  2.,  3.],\n              [ 4.,  5.,  6.,  7.],\n              [ 8.,  9., 10., 11.],\n              [12., 13., 14., 15.]]]])\n    \n\n- 默认情况下，步幅与窗口大小相同。\n\n\n```python\npool2d = nn.MaxPool2d(3)\nprint(pool2d(X))\n```\n\n    tensor([[[[10.]]]])\n    \n\n- 手动设定padding和stride\n\n\n```python\npool2d = nn.MaxPool2d(3,padding=1, stride=2)\nprint(pool2d(X))\n\npool2d = nn.MaxPool2d((2,3), stride=(2,3), padding=(0,1))\nprint(pool2d(X))\n```\n\n    tensor([[[[ 5.,  7.],\n              [13., 15.]]]])\n    tensor([[[[ 5.,  7.],\n              [13., 15.]]]])\n    \n\n## 5.3 多通道\n- 在处理多通道数据时，pooling层在每个输入通道上单独运算（与卷积不同）\n- pooling层的输出通道数与输入通道数相同\n\n\n```python\n# 构建一个2通道的输入\nX = torch.cat((X,X+1),1) #在channel维度上连接，相当于增加一个通道\nprint(X)\npool2d = nn.MaxPool2d(3, padding=1, stride=2)\nprint(pool2d(X)) #输出通道依然是2\n\n```\n\n    tensor([[[[ 0.,  1.,  2.,  3.],\n              [ 4.,  5.,  6.,  7.],\n              [ 8.,  9., 10., 11.],\n              [12., 13., 14., 15.]],\n    \n             [[ 1.,  2.,  3.,  4.],\n              [ 5.,  6.,  7.,  8.],\n              [ 9., 10., 11., 12.],\n              [13., 14., 15., 16.]]]])\n    tensor([[[[ 5.,  7.],\n              [13., 15.]],\n    \n             [[ 6.,  8.],\n              [14., 16.]]]])\n    \n","slug":"deeplearning/code/pytorch/5_convolution/5_pooling","published":1,"updated":"2024-05-02T08:51:26.657Z","comments":1,"layout":"post","photos":[],"_id":"clvuli68t002kzsvw1iqd4yri","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"5-汇聚层\"><a href=\"#5-汇聚层\" class=\"headerlink\" title=\"5 汇聚层\"></a>5 汇聚层</h1><ul>\n<li>处理图像时，我们希望逐渐降低隐藏表示的空间分辨率、聚集信息，这样随着层数的增加，每个神经元对其敏感的感受野（输入）就越大。</li>\n<li>机器学习任务通常会跟全局图像的问题有关，所以最后一层的神经元应该对整个输入的全局敏感。通过逐渐聚合信息，生成越来越粗糙的映射，最终实现学习全局表示的目标，同时将卷积图层的所有优势保留在中间层。</li>\n<li>当检测较底层的特征时，我们通常希望这些特征保持某种程度上的平移不变性。</li>\n<li>汇聚层（pooling）层可以：降低卷积层对位置的敏感性，同时降低对空间采样表示的敏感性。</li>\n</ul>\n<h2 id=\"5-1-最大汇聚层和平均汇聚层\"><a href=\"#5-1-最大汇聚层和平均汇聚层\" class=\"headerlink\" title=\"5.1 最大汇聚层和平均汇聚层\"></a>5.1 最大汇聚层和平均汇聚层</h2><ul>\n<li>汇聚层使用一个类似卷积核的窗口在输入上滑动，计算方式：计算汇聚窗口中所有元素的最大值或平均值。<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/5_convolution/5_polling/1.png\"><br><img src=\"/img/deeplearning/code/pytorch/5_convolution/5_polling/1.png\"></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">pool2d</span>(<span class=\"params\">X, pool_size, mode=<span class=\"string\">&#x27;max&#x27;</span></span>):</span><br><span class=\"line\">    p_h, p_w = pool_size</span><br><span class=\"line\">    Y = torch.zeros((X.shape[<span class=\"number\">0</span>] - p_h+<span class=\"number\">1</span>, X.shape[<span class=\"number\">1</span>] - p_w+<span class=\"number\">1</span>)) <span class=\"comment\">#输出的形状</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(Y.shape[<span class=\"number\">0</span>]):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(Y.shape[<span class=\"number\">1</span>]):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> mode == <span class=\"string\">&#x27;max&#x27;</span>:</span><br><span class=\"line\">                Y[i,j] = X[i:i+p_h,j:j+p_w].<span class=\"built_in\">max</span>()</span><br><span class=\"line\">            <span class=\"keyword\">elif</span> mode == <span class=\"string\">&#x27;avg&#x27;</span>:</span><br><span class=\"line\">                Y[i,j] = X[i:i+p_h,j:j+p_w].mean()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> Y</span><br><span class=\"line\"></span><br><span class=\"line\">X = torch.tensor([[<span class=\"number\">0.0</span>, <span class=\"number\">1.0</span>, <span class=\"number\">2.0</span>], [<span class=\"number\">3.0</span>, <span class=\"number\">4.0</span>, <span class=\"number\">5.0</span>], [<span class=\"number\">6.0</span>, <span class=\"number\">7.0</span>, <span class=\"number\">8.0</span>]])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(pool2d(X, (<span class=\"number\">2</span>, <span class=\"number\">2</span>)))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(pool2d(X,(<span class=\"number\">2</span>,<span class=\"number\">2</span>),<span class=\"string\">&#x27;avg&#x27;</span>))</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[4., 5.],\n        [7., 8.]])\ntensor([[2., 3.],\n        [5., 6.]])\n</code></pre>\n<h2 id=\"5-2-填充和步幅\"><a href=\"#5-2-填充和步幅\" class=\"headerlink\" title=\"5.2 填充和步幅\"></a>5.2 填充和步幅</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#4维输入张量</span></span><br><span class=\"line\">X = torch.arange(<span class=\"number\">16</span>, dtype=torch.float32).reshape((<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">4</span>,<span class=\"number\">4</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(X)</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[[[ 0.,  1.,  2.,  3.],\n          [ 4.,  5.,  6.,  7.],\n          [ 8.,  9., 10., 11.],\n          [12., 13., 14., 15.]]]])\n</code></pre>\n<ul>\n<li>默认情况下，步幅与窗口大小相同。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pool2d = nn.MaxPool2d(<span class=\"number\">3</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(pool2d(X))</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[[[10.]]]])\n</code></pre>\n<ul>\n<li>手动设定padding和stride</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pool2d = nn.MaxPool2d(<span class=\"number\">3</span>,padding=<span class=\"number\">1</span>, stride=<span class=\"number\">2</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(pool2d(X))</span><br><span class=\"line\"></span><br><span class=\"line\">pool2d = nn.MaxPool2d((<span class=\"number\">2</span>,<span class=\"number\">3</span>), stride=(<span class=\"number\">2</span>,<span class=\"number\">3</span>), padding=(<span class=\"number\">0</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(pool2d(X))</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[[[ 5.,  7.],\n          [13., 15.]]]])\ntensor([[[[ 5.,  7.],\n          [13., 15.]]]])\n</code></pre>\n<h2 id=\"5-3-多通道\"><a href=\"#5-3-多通道\" class=\"headerlink\" title=\"5.3 多通道\"></a>5.3 多通道</h2><ul>\n<li>在处理多通道数据时，pooling层在每个输入通道上单独运算（与卷积不同）</li>\n<li>pooling层的输出通道数与输入通道数相同</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 构建一个2通道的输入</span></span><br><span class=\"line\">X = torch.cat((X,X+<span class=\"number\">1</span>),<span class=\"number\">1</span>) <span class=\"comment\">#在channel维度上连接，相当于增加一个通道</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(X)</span><br><span class=\"line\">pool2d = nn.MaxPool2d(<span class=\"number\">3</span>, padding=<span class=\"number\">1</span>, stride=<span class=\"number\">2</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(pool2d(X)) <span class=\"comment\">#输出通道依然是2</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[[[ 0.,  1.,  2.,  3.],\n          [ 4.,  5.,  6.,  7.],\n          [ 8.,  9., 10., 11.],\n          [12., 13., 14., 15.]],\n\n         [[ 1.,  2.,  3.,  4.],\n          [ 5.,  6.,  7.,  8.],\n          [ 9., 10., 11., 12.],\n          [13., 14., 15., 16.]]]])\ntensor([[[[ 5.,  7.],\n          [13., 15.]],\n\n         [[ 6.,  8.],\n          [14., 16.]]]])\n</code></pre>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"5-汇聚层\"><a href=\"#5-汇聚层\" class=\"headerlink\" title=\"5 汇聚层\"></a>5 汇聚层</h1><ul>\n<li>处理图像时，我们希望逐渐降低隐藏表示的空间分辨率、聚集信息，这样随着层数的增加，每个神经元对其敏感的感受野（输入）就越大。</li>\n<li>机器学习任务通常会跟全局图像的问题有关，所以最后一层的神经元应该对整个输入的全局敏感。通过逐渐聚合信息，生成越来越粗糙的映射，最终实现学习全局表示的目标，同时将卷积图层的所有优势保留在中间层。</li>\n<li>当检测较底层的特征时，我们通常希望这些特征保持某种程度上的平移不变性。</li>\n<li>汇聚层（pooling）层可以：降低卷积层对位置的敏感性，同时降低对空间采样表示的敏感性。</li>\n</ul>\n<h2 id=\"5-1-最大汇聚层和平均汇聚层\"><a href=\"#5-1-最大汇聚层和平均汇聚层\" class=\"headerlink\" title=\"5.1 最大汇聚层和平均汇聚层\"></a>5.1 最大汇聚层和平均汇聚层</h2><ul>\n<li>汇聚层使用一个类似卷积核的窗口在输入上滑动，计算方式：计算汇聚窗口中所有元素的最大值或平均值。<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/5_convolution/5_polling/1.png\"><br><img src=\"/img/deeplearning/code/pytorch/5_convolution/5_polling/1.png\"></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">pool2d</span>(<span class=\"params\">X, pool_size, mode=<span class=\"string\">&#x27;max&#x27;</span></span>):</span><br><span class=\"line\">    p_h, p_w = pool_size</span><br><span class=\"line\">    Y = torch.zeros((X.shape[<span class=\"number\">0</span>] - p_h+<span class=\"number\">1</span>, X.shape[<span class=\"number\">1</span>] - p_w+<span class=\"number\">1</span>)) <span class=\"comment\">#输出的形状</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(Y.shape[<span class=\"number\">0</span>]):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(Y.shape[<span class=\"number\">1</span>]):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> mode == <span class=\"string\">&#x27;max&#x27;</span>:</span><br><span class=\"line\">                Y[i,j] = X[i:i+p_h,j:j+p_w].<span class=\"built_in\">max</span>()</span><br><span class=\"line\">            <span class=\"keyword\">elif</span> mode == <span class=\"string\">&#x27;avg&#x27;</span>:</span><br><span class=\"line\">                Y[i,j] = X[i:i+p_h,j:j+p_w].mean()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> Y</span><br><span class=\"line\"></span><br><span class=\"line\">X = torch.tensor([[<span class=\"number\">0.0</span>, <span class=\"number\">1.0</span>, <span class=\"number\">2.0</span>], [<span class=\"number\">3.0</span>, <span class=\"number\">4.0</span>, <span class=\"number\">5.0</span>], [<span class=\"number\">6.0</span>, <span class=\"number\">7.0</span>, <span class=\"number\">8.0</span>]])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(pool2d(X, (<span class=\"number\">2</span>, <span class=\"number\">2</span>)))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(pool2d(X,(<span class=\"number\">2</span>,<span class=\"number\">2</span>),<span class=\"string\">&#x27;avg&#x27;</span>))</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[4., 5.],\n        [7., 8.]])\ntensor([[2., 3.],\n        [5., 6.]])\n</code></pre>\n<h2 id=\"5-2-填充和步幅\"><a href=\"#5-2-填充和步幅\" class=\"headerlink\" title=\"5.2 填充和步幅\"></a>5.2 填充和步幅</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#4维输入张量</span></span><br><span class=\"line\">X = torch.arange(<span class=\"number\">16</span>, dtype=torch.float32).reshape((<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">4</span>,<span class=\"number\">4</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(X)</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[[[ 0.,  1.,  2.,  3.],\n          [ 4.,  5.,  6.,  7.],\n          [ 8.,  9., 10., 11.],\n          [12., 13., 14., 15.]]]])\n</code></pre>\n<ul>\n<li>默认情况下，步幅与窗口大小相同。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pool2d = nn.MaxPool2d(<span class=\"number\">3</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(pool2d(X))</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[[[10.]]]])\n</code></pre>\n<ul>\n<li>手动设定padding和stride</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pool2d = nn.MaxPool2d(<span class=\"number\">3</span>,padding=<span class=\"number\">1</span>, stride=<span class=\"number\">2</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(pool2d(X))</span><br><span class=\"line\"></span><br><span class=\"line\">pool2d = nn.MaxPool2d((<span class=\"number\">2</span>,<span class=\"number\">3</span>), stride=(<span class=\"number\">2</span>,<span class=\"number\">3</span>), padding=(<span class=\"number\">0</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(pool2d(X))</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[[[ 5.,  7.],\n          [13., 15.]]]])\ntensor([[[[ 5.,  7.],\n          [13., 15.]]]])\n</code></pre>\n<h2 id=\"5-3-多通道\"><a href=\"#5-3-多通道\" class=\"headerlink\" title=\"5.3 多通道\"></a>5.3 多通道</h2><ul>\n<li>在处理多通道数据时，pooling层在每个输入通道上单独运算（与卷积不同）</li>\n<li>pooling层的输出通道数与输入通道数相同</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 构建一个2通道的输入</span></span><br><span class=\"line\">X = torch.cat((X,X+<span class=\"number\">1</span>),<span class=\"number\">1</span>) <span class=\"comment\">#在channel维度上连接，相当于增加一个通道</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(X)</span><br><span class=\"line\">pool2d = nn.MaxPool2d(<span class=\"number\">3</span>, padding=<span class=\"number\">1</span>, stride=<span class=\"number\">2</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(pool2d(X)) <span class=\"comment\">#输出通道依然是2</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[[[ 0.,  1.,  2.,  3.],\n          [ 4.,  5.,  6.,  7.],\n          [ 8.,  9., 10., 11.],\n          [12., 13., 14., 15.]],\n\n         [[ 1.,  2.,  3.,  4.],\n          [ 5.,  6.,  7.,  8.],\n          [ 9., 10., 11., 12.],\n          [13., 14., 15., 16.]]]])\ntensor([[[[ 5.,  7.],\n          [13., 15.]],\n\n         [[ 6.,  8.],\n          [14., 16.]]]])\n</code></pre>"},{"title":"5.6 卷积神经网络（LeNet）","date":"2024-04-21T05:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 6 卷积神经网络（LeNet）\n## 6.1 LeNet\n- LeNet-5由两个部分组成：\n    - 卷积编码器：由两个卷积层组成\n    - 全连接层密集块：由三个全连接层组成\n- 每个卷积块中基本单位是：\n    - 卷积层（5x5卷积核）\n    - sigmoid激活函数\n    - 平均池化层\n- 这些层将输入映射到多个二维特征输出，通常同时增加通道的数量\n    - 第一个卷积层有6个输出通道\n    - 第二个卷积层有16个输出通道\n    - 每个2x2pooling操作（stride=2）通过空间下采样将维数减少4倍\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/5_convolution/6_LeNet/1.png)\n![](img/deeplearning/code/pytorch/5_convolution/6_LeNet/1.png)\n- 为了将卷积块的输出传递给稠密块，我们必须在小批量中展平每个样本，因此需要将四维输入转换成二维输入全连接层。\n- LeNet的稠密块有三个全连接层，分别有120、84、10个输出。\n\n\n\n```python\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\nimport os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"]  =  \"TRUE\"\n\nnet = nn.Sequential(\n    nn.Conv2d(1,6,kernel_size=5,padding=2), nn.Sigmoid(), #通道1->6, 5x5卷积核\n    nn.AvgPool2d(kernel_size=2, stride=2), #2x2池化, 步长2\n    nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(), #通道6->16, 5x5卷积核\n    nn.AvgPool2d(kernel_size=2, stride=2), #2x2池化, 步长2\n    nn.Flatten(), #展平\n    nn.Linear(16*5*5, 120), nn.Sigmoid(), #全连接层\n    nn.Linear(120, 84), nn.Sigmoid(), #全连接层\n    nn.Linear(84,10) #全连接层\n)\n\n# 在每一层打印输出的形状\nX = torch.rand(size=(1,1,28,28), dtype= torch.float32)\nfor layer in net:\n    X = layer(X)\n    print(layer.__class__.__name__ , 'output shape:\\t', X.shape)\n```\n\n    Conv2d output shape:\t torch.Size([1, 6, 28, 28])\n    Sigmoid output shape:\t torch.Size([1, 6, 28, 28])\n    AvgPool2d output shape:\t torch.Size([1, 6, 14, 14])\n    Conv2d output shape:\t torch.Size([1, 16, 10, 10])\n    Sigmoid output shape:\t torch.Size([1, 16, 10, 10])\n    AvgPool2d output shape:\t torch.Size([1, 16, 5, 5])\n    Flatten output shape:\t torch.Size([1, 400])\n    Linear output shape:\t torch.Size([1, 120])\n    Sigmoid output shape:\t torch.Size([1, 120])\n    Linear output shape:\t torch.Size([1, 84])\n    Sigmoid output shape:\t torch.Size([1, 84])\n    Linear output shape:\t torch.Size([1, 10])\n    \n\n## 6.2 模型训练\n- 在Fashion-MNIST上训练\n\n\n```python\nbatch_size = 256\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)\n\n# 计算精度\ndef evaluate_accuracy_gpu(net, data_iter, device=None): #@save\n    '''使用GPU计算模型在数据集上的精度'''\n    if isinstance(net, nn.Module):\n        if not device:\n            device = next(iter(net.parameters())).device #网络的参数所在的设备\n    # 正确预测的数量,总预测的数量\n    metric = d2l.Accumulator(2) #返回两个值：正确预测的数量，总预测的数量\n    with torch.no_grad():\n        for X,y in data_iter: #将数据和标签复制到设备上\n            if isinstance(X, list):\n                X = [x.to(device) for x in X]\n            else:\n                X = X.to(device)\n            y = y.to(device)\n            metric.add(d2l.accuracy(net(X), y), y.numel())\n    return metric[0] / metric[1]\n\n# 训练\n#@save\ndef train_ch6(net, train_iter, test_iter, num_epochs, lr, device):\n    '''使用GPU训练模型'''\n    # 1初始化权重\n    def init_weights(m):\n        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n            nn.init.xavier_uniform_(m.weight)\n    net.apply(init_weights) #初始化权重\n\n    # 2将模型复制到设备上\n    print('training on', device)\n    net.to(device)\n\n    # 3优化器、损失、动画\n    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n    loss = nn.CrossEntropyLoss()\n    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n                            legend=['train loss', 'train acc', 'test acc'])\n    timer, num_batches = d2l.Timer(), len(train_iter) #计时器\n\n    # 4训练\n    for epoch in range(num_epochs):\n        # 训练损失之和，训练准确率之和， 样本数\n        metric = d2l.Accumulator(3) #计算一个epoch的训练损失之和，训练准确率之和， 样本数\n        net.train()\n        for i,(X,y) in enumerate(train_iter):\n            timer.start()\n            optimizer.zero_grad() #梯度清零\n            X, y = X.to(device), y.to(device) #将数据和标签复制到设备上\n            y_hat = net(X) #前向传播\n            l = loss(y_hat,y) #计算损失\n            l.backward() #反向传播\n            optimizer.step() #更新参数\n            with torch.no_grad():\n                # 添加样本，计算损失，计算准确率\n                metric.add(1*X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n            timer.stop()\n            train_l = metric[0] / metric[2] #平均损失\n            train_acc = metric[1] / metric[2] #平均准确率\n            if (i+1) % (num_batches // 5) == 0 or i == num_batches -1:\n                animator.add(epoch + (i+1) / num_batches, (train_l, train_acc, None))\n        test_acc = evaluate_accuracy_gpu(net, test_iter)\n        animator.add(epoch+1, (None,None,test_acc))\n    print(f'loss {train_l:.3f}, train acc{train_acc:.3f},test acc{test_acc:.3f}')\n    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec on {str(device)}')\n\nlr, num_epochs = 0.9, 10\ntrain_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())\n```\n\n    loss 1.000, train acc0.824,test acc0.804\n    6523.3 examples/sec on cpu\n    \n\n\n    \n![svg](6_LeNet_files/6_LeNet_3_1.svg)\n![](img/deeplearning/code/pytorch/5_convolution/6_LeNet_files/6_LeNet_3_1.svg)\n\n","source":"_posts/deeplearning/code/pytorch/5_convolution/6_LeNet.md","raw":"---\ntitle: 5.6 卷积神经网络（LeNet）\ndate: 2024-4-21 13:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 6 卷积神经网络（LeNet）\n## 6.1 LeNet\n- LeNet-5由两个部分组成：\n    - 卷积编码器：由两个卷积层组成\n    - 全连接层密集块：由三个全连接层组成\n- 每个卷积块中基本单位是：\n    - 卷积层（5x5卷积核）\n    - sigmoid激活函数\n    - 平均池化层\n- 这些层将输入映射到多个二维特征输出，通常同时增加通道的数量\n    - 第一个卷积层有6个输出通道\n    - 第二个卷积层有16个输出通道\n    - 每个2x2pooling操作（stride=2）通过空间下采样将维数减少4倍\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/5_convolution/6_LeNet/1.png)\n![](img/deeplearning/code/pytorch/5_convolution/6_LeNet/1.png)\n- 为了将卷积块的输出传递给稠密块，我们必须在小批量中展平每个样本，因此需要将四维输入转换成二维输入全连接层。\n- LeNet的稠密块有三个全连接层，分别有120、84、10个输出。\n\n\n\n```python\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\nimport os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"]  =  \"TRUE\"\n\nnet = nn.Sequential(\n    nn.Conv2d(1,6,kernel_size=5,padding=2), nn.Sigmoid(), #通道1->6, 5x5卷积核\n    nn.AvgPool2d(kernel_size=2, stride=2), #2x2池化, 步长2\n    nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(), #通道6->16, 5x5卷积核\n    nn.AvgPool2d(kernel_size=2, stride=2), #2x2池化, 步长2\n    nn.Flatten(), #展平\n    nn.Linear(16*5*5, 120), nn.Sigmoid(), #全连接层\n    nn.Linear(120, 84), nn.Sigmoid(), #全连接层\n    nn.Linear(84,10) #全连接层\n)\n\n# 在每一层打印输出的形状\nX = torch.rand(size=(1,1,28,28), dtype= torch.float32)\nfor layer in net:\n    X = layer(X)\n    print(layer.__class__.__name__ , 'output shape:\\t', X.shape)\n```\n\n    Conv2d output shape:\t torch.Size([1, 6, 28, 28])\n    Sigmoid output shape:\t torch.Size([1, 6, 28, 28])\n    AvgPool2d output shape:\t torch.Size([1, 6, 14, 14])\n    Conv2d output shape:\t torch.Size([1, 16, 10, 10])\n    Sigmoid output shape:\t torch.Size([1, 16, 10, 10])\n    AvgPool2d output shape:\t torch.Size([1, 16, 5, 5])\n    Flatten output shape:\t torch.Size([1, 400])\n    Linear output shape:\t torch.Size([1, 120])\n    Sigmoid output shape:\t torch.Size([1, 120])\n    Linear output shape:\t torch.Size([1, 84])\n    Sigmoid output shape:\t torch.Size([1, 84])\n    Linear output shape:\t torch.Size([1, 10])\n    \n\n## 6.2 模型训练\n- 在Fashion-MNIST上训练\n\n\n```python\nbatch_size = 256\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)\n\n# 计算精度\ndef evaluate_accuracy_gpu(net, data_iter, device=None): #@save\n    '''使用GPU计算模型在数据集上的精度'''\n    if isinstance(net, nn.Module):\n        if not device:\n            device = next(iter(net.parameters())).device #网络的参数所在的设备\n    # 正确预测的数量,总预测的数量\n    metric = d2l.Accumulator(2) #返回两个值：正确预测的数量，总预测的数量\n    with torch.no_grad():\n        for X,y in data_iter: #将数据和标签复制到设备上\n            if isinstance(X, list):\n                X = [x.to(device) for x in X]\n            else:\n                X = X.to(device)\n            y = y.to(device)\n            metric.add(d2l.accuracy(net(X), y), y.numel())\n    return metric[0] / metric[1]\n\n# 训练\n#@save\ndef train_ch6(net, train_iter, test_iter, num_epochs, lr, device):\n    '''使用GPU训练模型'''\n    # 1初始化权重\n    def init_weights(m):\n        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n            nn.init.xavier_uniform_(m.weight)\n    net.apply(init_weights) #初始化权重\n\n    # 2将模型复制到设备上\n    print('training on', device)\n    net.to(device)\n\n    # 3优化器、损失、动画\n    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n    loss = nn.CrossEntropyLoss()\n    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n                            legend=['train loss', 'train acc', 'test acc'])\n    timer, num_batches = d2l.Timer(), len(train_iter) #计时器\n\n    # 4训练\n    for epoch in range(num_epochs):\n        # 训练损失之和，训练准确率之和， 样本数\n        metric = d2l.Accumulator(3) #计算一个epoch的训练损失之和，训练准确率之和， 样本数\n        net.train()\n        for i,(X,y) in enumerate(train_iter):\n            timer.start()\n            optimizer.zero_grad() #梯度清零\n            X, y = X.to(device), y.to(device) #将数据和标签复制到设备上\n            y_hat = net(X) #前向传播\n            l = loss(y_hat,y) #计算损失\n            l.backward() #反向传播\n            optimizer.step() #更新参数\n            with torch.no_grad():\n                # 添加样本，计算损失，计算准确率\n                metric.add(1*X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n            timer.stop()\n            train_l = metric[0] / metric[2] #平均损失\n            train_acc = metric[1] / metric[2] #平均准确率\n            if (i+1) % (num_batches // 5) == 0 or i == num_batches -1:\n                animator.add(epoch + (i+1) / num_batches, (train_l, train_acc, None))\n        test_acc = evaluate_accuracy_gpu(net, test_iter)\n        animator.add(epoch+1, (None,None,test_acc))\n    print(f'loss {train_l:.3f}, train acc{train_acc:.3f},test acc{test_acc:.3f}')\n    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec on {str(device)}')\n\nlr, num_epochs = 0.9, 10\ntrain_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())\n```\n\n    loss 1.000, train acc0.824,test acc0.804\n    6523.3 examples/sec on cpu\n    \n\n\n    \n![svg](6_LeNet_files/6_LeNet_3_1.svg)\n![](img/deeplearning/code/pytorch/5_convolution/6_LeNet_files/6_LeNet_3_1.svg)\n\n","slug":"deeplearning/code/pytorch/5_convolution/6_LeNet","published":1,"updated":"2024-05-05T13:57:26.706Z","comments":1,"layout":"post","photos":[],"_id":"clvuli68u002nzsvwcyccalod","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"6-卷积神经网络（LeNet）\"><a href=\"#6-卷积神经网络（LeNet）\" class=\"headerlink\" title=\"6 卷积神经网络（LeNet）\"></a>6 卷积神经网络（LeNet）</h1><h2 id=\"6-1-LeNet\"><a href=\"#6-1-LeNet\" class=\"headerlink\" title=\"6.1 LeNet\"></a>6.1 LeNet</h2><ul>\n<li>LeNet-5由两个部分组成：<ul>\n<li>卷积编码器：由两个卷积层组成</li>\n<li>全连接层密集块：由三个全连接层组成</li>\n</ul>\n</li>\n<li>每个卷积块中基本单位是：<ul>\n<li>卷积层（5x5卷积核）</li>\n<li>sigmoid激活函数</li>\n<li>平均池化层</li>\n</ul>\n</li>\n<li>这些层将输入映射到多个二维特征输出，通常同时增加通道的数量<ul>\n<li>第一个卷积层有6个输出通道</li>\n<li>第二个卷积层有16个输出通道</li>\n<li>每个2x2pooling操作（stride&#x3D;2）通过空间下采样将维数减少4倍<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/5_convolution/6_LeNet/1.png\"><br><img src=\"/img/deeplearning/code/pytorch/5_convolution/6_LeNet/1.png\"></li>\n</ul>\n</li>\n<li>为了将卷积块的输出传递给稠密块，我们必须在小批量中展平每个样本，因此需要将四维输入转换成二维输入全连接层。</li>\n<li>LeNet的稠密块有三个全连接层，分别有120、84、10个输出。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>]  =  <span class=\"string\">&quot;TRUE&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">net = nn.Sequential(</span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">1</span>,<span class=\"number\">6</span>,kernel_size=<span class=\"number\">5</span>,padding=<span class=\"number\">2</span>), nn.Sigmoid(), <span class=\"comment\">#通道1-&gt;6, 5x5卷积核</span></span><br><span class=\"line\">    nn.AvgPool2d(kernel_size=<span class=\"number\">2</span>, stride=<span class=\"number\">2</span>), <span class=\"comment\">#2x2池化, 步长2</span></span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">6</span>, <span class=\"number\">16</span>, kernel_size=<span class=\"number\">5</span>), nn.Sigmoid(), <span class=\"comment\">#通道6-&gt;16, 5x5卷积核</span></span><br><span class=\"line\">    nn.AvgPool2d(kernel_size=<span class=\"number\">2</span>, stride=<span class=\"number\">2</span>), <span class=\"comment\">#2x2池化, 步长2</span></span><br><span class=\"line\">    nn.Flatten(), <span class=\"comment\">#展平</span></span><br><span class=\"line\">    nn.Linear(<span class=\"number\">16</span>*<span class=\"number\">5</span>*<span class=\"number\">5</span>, <span class=\"number\">120</span>), nn.Sigmoid(), <span class=\"comment\">#全连接层</span></span><br><span class=\"line\">    nn.Linear(<span class=\"number\">120</span>, <span class=\"number\">84</span>), nn.Sigmoid(), <span class=\"comment\">#全连接层</span></span><br><span class=\"line\">    nn.Linear(<span class=\"number\">84</span>,<span class=\"number\">10</span>) <span class=\"comment\">#全连接层</span></span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 在每一层打印输出的形状</span></span><br><span class=\"line\">X = torch.rand(size=(<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">28</span>,<span class=\"number\">28</span>), dtype= torch.float32)</span><br><span class=\"line\"><span class=\"keyword\">for</span> layer <span class=\"keyword\">in</span> net:</span><br><span class=\"line\">    X = layer(X)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(layer.__class__.__name__ , <span class=\"string\">&#x27;output shape:\\t&#x27;</span>, X.shape)</span><br></pre></td></tr></table></figure>\n\n<pre><code>Conv2d output shape:\t torch.Size([1, 6, 28, 28])\nSigmoid output shape:\t torch.Size([1, 6, 28, 28])\nAvgPool2d output shape:\t torch.Size([1, 6, 14, 14])\nConv2d output shape:\t torch.Size([1, 16, 10, 10])\nSigmoid output shape:\t torch.Size([1, 16, 10, 10])\nAvgPool2d output shape:\t torch.Size([1, 16, 5, 5])\nFlatten output shape:\t torch.Size([1, 400])\nLinear output shape:\t torch.Size([1, 120])\nSigmoid output shape:\t torch.Size([1, 120])\nLinear output shape:\t torch.Size([1, 84])\nSigmoid output shape:\t torch.Size([1, 84])\nLinear output shape:\t torch.Size([1, 10])\n</code></pre>\n<h2 id=\"6-2-模型训练\"><a href=\"#6-2-模型训练\" class=\"headerlink\" title=\"6.2 模型训练\"></a>6.2 模型训练</h2><ul>\n<li>在Fashion-MNIST上训练</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">batch_size = <span class=\"number\">256</span></span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 计算精度</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">evaluate_accuracy_gpu</span>(<span class=\"params\">net, data_iter, device=<span class=\"literal\">None</span></span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;使用GPU计算模型在数据集上的精度&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(net, nn.Module):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> device:</span><br><span class=\"line\">            device = <span class=\"built_in\">next</span>(<span class=\"built_in\">iter</span>(net.parameters())).device <span class=\"comment\">#网络的参数所在的设备</span></span><br><span class=\"line\">    <span class=\"comment\"># 正确预测的数量,总预测的数量</span></span><br><span class=\"line\">    metric = d2l.Accumulator(<span class=\"number\">2</span>) <span class=\"comment\">#返回两个值：正确预测的数量，总预测的数量</span></span><br><span class=\"line\">    <span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">        <span class=\"keyword\">for</span> X,y <span class=\"keyword\">in</span> data_iter: <span class=\"comment\">#将数据和标签复制到设备上</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(X, <span class=\"built_in\">list</span>):</span><br><span class=\"line\">                X = [x.to(device) <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> X]</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                X = X.to(device)</span><br><span class=\"line\">            y = y.to(device)</span><br><span class=\"line\">            metric.add(d2l.accuracy(net(X), y), y.numel())</span><br><span class=\"line\">    <span class=\"keyword\">return</span> metric[<span class=\"number\">0</span>] / metric[<span class=\"number\">1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 训练</span></span><br><span class=\"line\"><span class=\"comment\">#@save</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train_ch6</span>(<span class=\"params\">net, train_iter, test_iter, num_epochs, lr, device</span>):</span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;使用GPU训练模型&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"comment\"># 1初始化权重</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">init_weights</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear <span class=\"keyword\">or</span> <span class=\"built_in\">type</span>(m) == nn.Conv2d:</span><br><span class=\"line\">            nn.init.xavier_uniform_(m.weight)</span><br><span class=\"line\">    net.apply(init_weights) <span class=\"comment\">#初始化权重</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 2将模型复制到设备上</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;training on&#x27;</span>, device)</span><br><span class=\"line\">    net.to(device)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 3优化器、损失、动画</span></span><br><span class=\"line\">    optimizer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class=\"line\">    loss = nn.CrossEntropyLoss()</span><br><span class=\"line\">    animator = d2l.Animator(xlabel=<span class=\"string\">&#x27;epoch&#x27;</span>, xlim=[<span class=\"number\">1</span>, num_epochs],</span><br><span class=\"line\">                            legend=[<span class=\"string\">&#x27;train loss&#x27;</span>, <span class=\"string\">&#x27;train acc&#x27;</span>, <span class=\"string\">&#x27;test acc&#x27;</span>])</span><br><span class=\"line\">    timer, num_batches = d2l.Timer(), <span class=\"built_in\">len</span>(train_iter) <span class=\"comment\">#计时器</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 4训练</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        <span class=\"comment\"># 训练损失之和，训练准确率之和， 样本数</span></span><br><span class=\"line\">        metric = d2l.Accumulator(<span class=\"number\">3</span>) <span class=\"comment\">#计算一个epoch的训练损失之和，训练准确率之和， 样本数</span></span><br><span class=\"line\">        net.train()</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i,(X,y) <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(train_iter):</span><br><span class=\"line\">            timer.start()</span><br><span class=\"line\">            optimizer.zero_grad() <span class=\"comment\">#梯度清零</span></span><br><span class=\"line\">            X, y = X.to(device), y.to(device) <span class=\"comment\">#将数据和标签复制到设备上</span></span><br><span class=\"line\">            y_hat = net(X) <span class=\"comment\">#前向传播</span></span><br><span class=\"line\">            l = loss(y_hat,y) <span class=\"comment\">#计算损失</span></span><br><span class=\"line\">            l.backward() <span class=\"comment\">#反向传播</span></span><br><span class=\"line\">            optimizer.step() <span class=\"comment\">#更新参数</span></span><br><span class=\"line\">            <span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">                <span class=\"comment\"># 添加样本，计算损失，计算准确率</span></span><br><span class=\"line\">                metric.add(<span class=\"number\">1</span>*X.shape[<span class=\"number\">0</span>], d2l.accuracy(y_hat, y), X.shape[<span class=\"number\">0</span>])</span><br><span class=\"line\">            timer.stop()</span><br><span class=\"line\">            train_l = metric[<span class=\"number\">0</span>] / metric[<span class=\"number\">2</span>] <span class=\"comment\">#平均损失</span></span><br><span class=\"line\">            train_acc = metric[<span class=\"number\">1</span>] / metric[<span class=\"number\">2</span>] <span class=\"comment\">#平均准确率</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (i+<span class=\"number\">1</span>) % (num_batches // <span class=\"number\">5</span>) == <span class=\"number\">0</span> <span class=\"keyword\">or</span> i == num_batches -<span class=\"number\">1</span>:</span><br><span class=\"line\">                animator.add(epoch + (i+<span class=\"number\">1</span>) / num_batches, (train_l, train_acc, <span class=\"literal\">None</span>))</span><br><span class=\"line\">        test_acc = evaluate_accuracy_gpu(net, test_iter)</span><br><span class=\"line\">        animator.add(epoch+<span class=\"number\">1</span>, (<span class=\"literal\">None</span>,<span class=\"literal\">None</span>,test_acc))</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;loss <span class=\"subst\">&#123;train_l:<span class=\"number\">.3</span>f&#125;</span>, train acc<span class=\"subst\">&#123;train_acc:<span class=\"number\">.3</span>f&#125;</span>,test acc<span class=\"subst\">&#123;test_acc:<span class=\"number\">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;<span class=\"subst\">&#123;metric[<span class=\"number\">2</span>] * num_epochs / timer.<span class=\"built_in\">sum</span>():<span class=\"number\">.1</span>f&#125;</span> examples/sec on <span class=\"subst\">&#123;<span class=\"built_in\">str</span>(device)&#125;</span>&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">lr, num_epochs = <span class=\"number\">0.9</span>, <span class=\"number\">10</span></span><br><span class=\"line\">train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure>\n\n<pre><code>loss 1.000, train acc0.824,test acc0.804\n6523.3 examples/sec on cpu\n</code></pre>\n<p><img src=\"/6_LeNet_files/6_LeNet_3_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/5_convolution/6_LeNet_files/6_LeNet_3_1.svg\"></p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"6-卷积神经网络（LeNet）\"><a href=\"#6-卷积神经网络（LeNet）\" class=\"headerlink\" title=\"6 卷积神经网络（LeNet）\"></a>6 卷积神经网络（LeNet）</h1><h2 id=\"6-1-LeNet\"><a href=\"#6-1-LeNet\" class=\"headerlink\" title=\"6.1 LeNet\"></a>6.1 LeNet</h2><ul>\n<li>LeNet-5由两个部分组成：<ul>\n<li>卷积编码器：由两个卷积层组成</li>\n<li>全连接层密集块：由三个全连接层组成</li>\n</ul>\n</li>\n<li>每个卷积块中基本单位是：<ul>\n<li>卷积层（5x5卷积核）</li>\n<li>sigmoid激活函数</li>\n<li>平均池化层</li>\n</ul>\n</li>\n<li>这些层将输入映射到多个二维特征输出，通常同时增加通道的数量<ul>\n<li>第一个卷积层有6个输出通道</li>\n<li>第二个卷积层有16个输出通道</li>\n<li>每个2x2pooling操作（stride&#x3D;2）通过空间下采样将维数减少4倍<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/5_convolution/6_LeNet/1.png\"><br><img src=\"/img/deeplearning/code/pytorch/5_convolution/6_LeNet/1.png\"></li>\n</ul>\n</li>\n<li>为了将卷积块的输出传递给稠密块，我们必须在小批量中展平每个样本，因此需要将四维输入转换成二维输入全连接层。</li>\n<li>LeNet的稠密块有三个全连接层，分别有120、84、10个输出。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>]  =  <span class=\"string\">&quot;TRUE&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">net = nn.Sequential(</span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">1</span>,<span class=\"number\">6</span>,kernel_size=<span class=\"number\">5</span>,padding=<span class=\"number\">2</span>), nn.Sigmoid(), <span class=\"comment\">#通道1-&gt;6, 5x5卷积核</span></span><br><span class=\"line\">    nn.AvgPool2d(kernel_size=<span class=\"number\">2</span>, stride=<span class=\"number\">2</span>), <span class=\"comment\">#2x2池化, 步长2</span></span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">6</span>, <span class=\"number\">16</span>, kernel_size=<span class=\"number\">5</span>), nn.Sigmoid(), <span class=\"comment\">#通道6-&gt;16, 5x5卷积核</span></span><br><span class=\"line\">    nn.AvgPool2d(kernel_size=<span class=\"number\">2</span>, stride=<span class=\"number\">2</span>), <span class=\"comment\">#2x2池化, 步长2</span></span><br><span class=\"line\">    nn.Flatten(), <span class=\"comment\">#展平</span></span><br><span class=\"line\">    nn.Linear(<span class=\"number\">16</span>*<span class=\"number\">5</span>*<span class=\"number\">5</span>, <span class=\"number\">120</span>), nn.Sigmoid(), <span class=\"comment\">#全连接层</span></span><br><span class=\"line\">    nn.Linear(<span class=\"number\">120</span>, <span class=\"number\">84</span>), nn.Sigmoid(), <span class=\"comment\">#全连接层</span></span><br><span class=\"line\">    nn.Linear(<span class=\"number\">84</span>,<span class=\"number\">10</span>) <span class=\"comment\">#全连接层</span></span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 在每一层打印输出的形状</span></span><br><span class=\"line\">X = torch.rand(size=(<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">28</span>,<span class=\"number\">28</span>), dtype= torch.float32)</span><br><span class=\"line\"><span class=\"keyword\">for</span> layer <span class=\"keyword\">in</span> net:</span><br><span class=\"line\">    X = layer(X)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(layer.__class__.__name__ , <span class=\"string\">&#x27;output shape:\\t&#x27;</span>, X.shape)</span><br></pre></td></tr></table></figure>\n\n<pre><code>Conv2d output shape:\t torch.Size([1, 6, 28, 28])\nSigmoid output shape:\t torch.Size([1, 6, 28, 28])\nAvgPool2d output shape:\t torch.Size([1, 6, 14, 14])\nConv2d output shape:\t torch.Size([1, 16, 10, 10])\nSigmoid output shape:\t torch.Size([1, 16, 10, 10])\nAvgPool2d output shape:\t torch.Size([1, 16, 5, 5])\nFlatten output shape:\t torch.Size([1, 400])\nLinear output shape:\t torch.Size([1, 120])\nSigmoid output shape:\t torch.Size([1, 120])\nLinear output shape:\t torch.Size([1, 84])\nSigmoid output shape:\t torch.Size([1, 84])\nLinear output shape:\t torch.Size([1, 10])\n</code></pre>\n<h2 id=\"6-2-模型训练\"><a href=\"#6-2-模型训练\" class=\"headerlink\" title=\"6.2 模型训练\"></a>6.2 模型训练</h2><ul>\n<li>在Fashion-MNIST上训练</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">batch_size = <span class=\"number\">256</span></span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 计算精度</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">evaluate_accuracy_gpu</span>(<span class=\"params\">net, data_iter, device=<span class=\"literal\">None</span></span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;使用GPU计算模型在数据集上的精度&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(net, nn.Module):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> device:</span><br><span class=\"line\">            device = <span class=\"built_in\">next</span>(<span class=\"built_in\">iter</span>(net.parameters())).device <span class=\"comment\">#网络的参数所在的设备</span></span><br><span class=\"line\">    <span class=\"comment\"># 正确预测的数量,总预测的数量</span></span><br><span class=\"line\">    metric = d2l.Accumulator(<span class=\"number\">2</span>) <span class=\"comment\">#返回两个值：正确预测的数量，总预测的数量</span></span><br><span class=\"line\">    <span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">        <span class=\"keyword\">for</span> X,y <span class=\"keyword\">in</span> data_iter: <span class=\"comment\">#将数据和标签复制到设备上</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(X, <span class=\"built_in\">list</span>):</span><br><span class=\"line\">                X = [x.to(device) <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> X]</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                X = X.to(device)</span><br><span class=\"line\">            y = y.to(device)</span><br><span class=\"line\">            metric.add(d2l.accuracy(net(X), y), y.numel())</span><br><span class=\"line\">    <span class=\"keyword\">return</span> metric[<span class=\"number\">0</span>] / metric[<span class=\"number\">1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 训练</span></span><br><span class=\"line\"><span class=\"comment\">#@save</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train_ch6</span>(<span class=\"params\">net, train_iter, test_iter, num_epochs, lr, device</span>):</span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;使用GPU训练模型&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"comment\"># 1初始化权重</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">init_weights</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear <span class=\"keyword\">or</span> <span class=\"built_in\">type</span>(m) == nn.Conv2d:</span><br><span class=\"line\">            nn.init.xavier_uniform_(m.weight)</span><br><span class=\"line\">    net.apply(init_weights) <span class=\"comment\">#初始化权重</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 2将模型复制到设备上</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;training on&#x27;</span>, device)</span><br><span class=\"line\">    net.to(device)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 3优化器、损失、动画</span></span><br><span class=\"line\">    optimizer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class=\"line\">    loss = nn.CrossEntropyLoss()</span><br><span class=\"line\">    animator = d2l.Animator(xlabel=<span class=\"string\">&#x27;epoch&#x27;</span>, xlim=[<span class=\"number\">1</span>, num_epochs],</span><br><span class=\"line\">                            legend=[<span class=\"string\">&#x27;train loss&#x27;</span>, <span class=\"string\">&#x27;train acc&#x27;</span>, <span class=\"string\">&#x27;test acc&#x27;</span>])</span><br><span class=\"line\">    timer, num_batches = d2l.Timer(), <span class=\"built_in\">len</span>(train_iter) <span class=\"comment\">#计时器</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 4训练</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        <span class=\"comment\"># 训练损失之和，训练准确率之和， 样本数</span></span><br><span class=\"line\">        metric = d2l.Accumulator(<span class=\"number\">3</span>) <span class=\"comment\">#计算一个epoch的训练损失之和，训练准确率之和， 样本数</span></span><br><span class=\"line\">        net.train()</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i,(X,y) <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(train_iter):</span><br><span class=\"line\">            timer.start()</span><br><span class=\"line\">            optimizer.zero_grad() <span class=\"comment\">#梯度清零</span></span><br><span class=\"line\">            X, y = X.to(device), y.to(device) <span class=\"comment\">#将数据和标签复制到设备上</span></span><br><span class=\"line\">            y_hat = net(X) <span class=\"comment\">#前向传播</span></span><br><span class=\"line\">            l = loss(y_hat,y) <span class=\"comment\">#计算损失</span></span><br><span class=\"line\">            l.backward() <span class=\"comment\">#反向传播</span></span><br><span class=\"line\">            optimizer.step() <span class=\"comment\">#更新参数</span></span><br><span class=\"line\">            <span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">                <span class=\"comment\"># 添加样本，计算损失，计算准确率</span></span><br><span class=\"line\">                metric.add(<span class=\"number\">1</span>*X.shape[<span class=\"number\">0</span>], d2l.accuracy(y_hat, y), X.shape[<span class=\"number\">0</span>])</span><br><span class=\"line\">            timer.stop()</span><br><span class=\"line\">            train_l = metric[<span class=\"number\">0</span>] / metric[<span class=\"number\">2</span>] <span class=\"comment\">#平均损失</span></span><br><span class=\"line\">            train_acc = metric[<span class=\"number\">1</span>] / metric[<span class=\"number\">2</span>] <span class=\"comment\">#平均准确率</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (i+<span class=\"number\">1</span>) % (num_batches // <span class=\"number\">5</span>) == <span class=\"number\">0</span> <span class=\"keyword\">or</span> i == num_batches -<span class=\"number\">1</span>:</span><br><span class=\"line\">                animator.add(epoch + (i+<span class=\"number\">1</span>) / num_batches, (train_l, train_acc, <span class=\"literal\">None</span>))</span><br><span class=\"line\">        test_acc = evaluate_accuracy_gpu(net, test_iter)</span><br><span class=\"line\">        animator.add(epoch+<span class=\"number\">1</span>, (<span class=\"literal\">None</span>,<span class=\"literal\">None</span>,test_acc))</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;loss <span class=\"subst\">&#123;train_l:<span class=\"number\">.3</span>f&#125;</span>, train acc<span class=\"subst\">&#123;train_acc:<span class=\"number\">.3</span>f&#125;</span>,test acc<span class=\"subst\">&#123;test_acc:<span class=\"number\">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;<span class=\"subst\">&#123;metric[<span class=\"number\">2</span>] * num_epochs / timer.<span class=\"built_in\">sum</span>():<span class=\"number\">.1</span>f&#125;</span> examples/sec on <span class=\"subst\">&#123;<span class=\"built_in\">str</span>(device)&#125;</span>&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">lr, num_epochs = <span class=\"number\">0.9</span>, <span class=\"number\">10</span></span><br><span class=\"line\">train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure>\n\n<pre><code>loss 1.000, train acc0.824,test acc0.804\n6523.3 examples/sec on cpu\n</code></pre>\n<p><img src=\"/6_LeNet_files/6_LeNet_3_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/5_convolution/6_LeNet_files/6_LeNet_3_1.svg\"></p>"},{"title":"3.10 kaggle:预测房价","date":"2024-03-19T06:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 10 kaggle:预测房价\n## 10.1 下载和缓存数据集\n\n\n```python\nimport hashlib\nimport os\nimport tarfile\nimport zipfile\nimport requests\n\n#@save\nDATA_HUB = dict() #字典，将数据集名称的字符串映射到数据集相关的二元组上，每个二元组包含数据集的url和校验和密钥\nDATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/' #所有类似的数据集都托管在这个地址上\n\ndef download(name, cache_dir=os.path.join('data')): #@save\n    \"\"\"下载一个DATA_HUB中的文件，返回本地文件名。\"\"\"\n    assert name in DATA_HUB, f\"{name}不存在于{DATA_HUB}\"\n    url, sha1_hash = DATA_HUB[name]\n    os.makedirs(cache_dir, exist_ok=True)\n    fname = os.path.join(cache_dir, url.split('/')[-1]) #取出url中最后一个/后面的字符串，作为文件名\n    if os.path.exists(fname):\n        sha1 = hashlib.sha1()\n        with open(fname, 'rb') as f:\n            while True:\n                data = f.read(1048576) # 读取1MB数据\n                if not data:\n                    break\n                sha1.update(data)\n        if sha1.hexdigest() == sha1_hash:\n            return fname #命中缓存\n\n    print(f'正在从{url}下载{fname}...')\n    r = requests.get(url, stream=True, verify= True)\n    with open(fname, 'wb') as f:\n        f.write(r.content)\n    return fname\n\ndef download_extract(name, folder=None): #@save\n    \"\"\"下载并解压zip/tar文件\"\"\"\n    fname = download(name)\n    base_dir = os.path.dirname(fname)\n    data_dir, ext = os.path.splitext(fname)\n    if ext =='.zip':\n        fp = zipfile.ZipFile(fname, 'r')\n    elif ext in ('.tar', '.gz'):\n        fp = tarfile.open(fname, 'r')\n    else:\n        assert False, '只有zip/tar文件可以被解压缩'\n    fp.extractall(base_dir)\n    return os.path.join(base_dir, folder) if folder else data_dir\n\ndef download_all(): #@save\n    \"\"\"下载DATA_HUB中的所有文件\"\"\"\n    for name in DATA_HUB:\n        download(name)\n```\n\n## 10.2 访问和读取数据集\n\n\n```python\n\n#读入并处理数据\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\nDATA_HUB['kaggle_house_train']=( #@save\n    DATA_URL + 'kaggle_house_pred_train.csv',\n    '585e9cc93e70b39160e7921475f9bcd7d31219ce')\nDATA_HUB['kaggle_house_test'] = (#@save\n    DATA_URL + 'kaggle_house_pred_test.csv',\n    'fa19780a7b011d9b009e8bff8e99922a8ee2eb90')\n\n### 1. 加载数据集\ntrain_data = pd.read_csv(download('kaggle_house_train'))\ntest_data = pd.read_csv(download('kaggle_house_test'))\n#输出维度\nprint(train_data.shape)\nprint(test_data.shape)\n#输出前四个特征、最后两个特征、标签\nprint(train_data.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]])\n#去掉id列\nall_features = pd.concat((train_data.iloc[:,1:-1], test_data.iloc[:,1:]))\n```\n\n    正在从http://d2l-data.s3-accelerate.amazonaws.com/kaggle_house_pred_train.csv下载data\\kaggle_house_pred_train.csv...\n    正在从http://d2l-data.s3-accelerate.amazonaws.com/kaggle_house_pred_test.csv下载data\\kaggle_house_pred_test.csv...\n    (1460, 81)\n    (1459, 80)\n       Id  MSSubClass MSZoning  LotFrontage SaleType SaleCondition  SalePrice\n    0   1          60       RL         65.0       WD        Normal     208500\n    1   2          20       RL         80.0       WD        Normal     181500\n    2   3          60       RL         68.0       WD        Normal     223500\n    3   4          70       RL         60.0       WD       Abnorml     140000\n\n\n## 10.3 数据预处理\n- 缺失值用均值代替\n- 为了将所有特征放到一个共同的尺度上，将特征缩放到均值为0，单位方差：\n$$x \\leftarrow \\frac{x-\\mu} \\sigma$$\n- 标准化数据有两个原因：\n    - 方便优化\n    - 因为我们不知道哪些特征是相关的，所以我们不想让惩罚分配给一个特征的系数比分配给其他任何特征的系数更大\n- 离散值处理：独热编码：如MSZoning列包含值：‘RL’和‘RM’。创建两个新的指标列MSZoning_RL和MSZoning_RM，值为0，1.可以用pandas自动实现\n\n\n```python\n## 2标准化\nnumeric_features = all_features.dtypes[all_features.dtypes != 'object'].index #取出数值特征的索引\nall_features[numeric_features] = all_features[numeric_features].apply(\n    lambda x:(x-x.mean())/ x.std()\n)\n#缺失值设为0\nall_features[numeric_features] = all_features[numeric_features].fillna(0)\n\n## 3离散值处理\n#‘dummy_na=True’将缺失值也当作合法的特征值并为其创建指示特征\nall_features = pd.get_dummies(all_features, dummy_na=True)\nprint(all_features.shape) #特征数量从79 -> 331\n\n## 3转换为张量\nn_train = train_data.shape[0]\ntrain_features = torch.tensor(all_features[:n_train].values, dtype= torch.float32)\ntest_features = torch.tensor(all_features[n_train:].values, dtype=torch.float32)\ntrain_labels = torch.tensor(\n    train_data.SalePrice.values.reshape(-1,1), dtype=torch.float32\n)\n```\n\n    (2919, 331)\n\n\n## 10.4 训练\n- 先训练一个带有损失平方的线性模型，线性模型可以提供一种健全性检查，以查看数据中是否存在有意义的信息。如果线性模型不能做得比随机猜测更好，那么可能存在数据处理错误。如果一切顺利，线性模型可以作为基线模型（baseline）\n- Adam优化器主要吸引力在于对初始学习率不那么敏感\n\n\n```python\nloss= nn.MSELoss()\nin_features = train_features.shape[1]\ndef get_net():\n    net = nn.Sequential(nn.Linear(in_features,1))\n    return net\n```\n\n- 对于误差，更关心相对误差，例如北京房价预测差了10万与阳新放假预测差了10万是不同的。\n- 可以用价格预测的对数来衡量差异。将$\\sigma for |log y - log \\hat y|$转换为$e^{-\\sigma} \\leq \\frac{\\hat y}{y} \\leq e^{\\sigma}$这使得预测价格的对数与真实标签价格的对数之间出现以下均方根误差：\n$$\\sqrt{\\frac1n\\sum_{i=1}^n\\left(\\log y_i-\\log\\hat{y}_i\\right)^2}$$\n\n\n```python\ndef log_rmse(net, features, labels):\n    # 为了在取对数时进一步稳定该值，将小于1的值设置为1\n    clipped_preds = torch.clamp(net(features), 1, float('inf'))\n    rmse = torch.sqrt(loss(torch.log(clipped_preds), torch.log(labels)))\n    return rmse.item()\n\ndef train(net, train_features, train_labels, test_features, test_labels,\n          num_epochs, learning_rate, weight_decay, batch_size):\n    train_ls, test_ls = [], []\n    train_iter = d2l.load_array((train_features,train_labels),batch_size)\n    optimizer = torch.optim.Adam(net.parameters(),\n                                lr =learning_rate,\n                                weight_decay=weight_decay)\n    for epoch in range(num_epochs):\n        for X,y in train_iter:\n                optimizer.zero_grad()\n                l = loss(net(X), y)\n                l.backward()\n                optimizer.step()\n        train_ls.append(log_rmse(net, train_features,train_labels))\n        if test_labels is not None:\n                test_ls.append(log_rmse(net, test_features,test_labels))\n    return train_ls, test_ls\n\n```\n\n## 10.5 K折交叉验证\n- 有助于模型选择和超参数调整。我们首先需要定义一个函数，在K折交叉验证过程中返回第i折的数据。具体地说，它选择第i个切片作为验证数据，其余部分作为训练数据。每次在K − 1个子集上进行训练，并在剩余的一个子集上进行验证。\n\n\n```python\ndef get_k_fold_data(k, i, X, y):\n    assert k > 1\n    fold_size = X.shape[0] // k\n    X_train, y_train = None, None\n    for j in range(k):\n        idx = slice(j * fold_size, (j + 1) * fold_size)\n        X_part, y_part = X[idx, :], y[idx]\n        if j ==i:\n            X_valid, y_valid = X_part, y_part\n        elif X_train is None:\n            X_train, y_train = X_part, y_part\n        else:\n            X_train = torch.cat([X_train, X_part], 0)\n            y_train = torch.cat([y_train, y_part], 0)\n    return X_train, y_train, X_valid, y_valid\n```\n\n- 训练K次，并返回训练和验证误差的平均值\n\n\n```python\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\ndef k_fold(k, X_train, y_train, num_epochs, learning_rate, weight_decay, batch_size):\n    train_l_sum, valid_l_sum = 0, 0\n    for i in range(k):\n        data = get_k_fold_data(k, i, X_train, y_train)\n        net = get_net()\n        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate, weight_decay, batch_size)\n        train_l_sum += train_ls[-1]\n        valid_l_sum += valid_ls[-1]\n        if i == 0:\n            d2l.plot(list(range(1, num_epochs + 1)), [train_ls, valid_ls],\n                     xlabel='epoch', ylabel='rmse',\n                     legend=['train', 'valid'], yscale='log')\n        print(f'折{i + 1}, 训练log rmse {float(train_ls[-1]):f}, '\n              f'验证log rmse {float(valid_ls[-1]):f}')\n    return train_l_sum / k, valid_l_sum / k\n```\n\n## 10.6 模型选择\n- 使用未调优的超参数\n\n\n```python\nk, num_epochs, lr, weight_decay, batch_size=5, 100, 5, 0, 64\ntrain_l, valid_l= k_fold(k, train_features, train_labels,num_epochs,lr,\n                         weight_decay, batch_size)\nprint(f'{k}-折验证：平均训练log rmse：{float(train_l):f},'\n      f'平均验证log rmse：{float(valid_l):f}')\n```\n\n    折1, 训练log rmse 0.170100, 验证log rmse 0.156652\n    折2, 训练log rmse 0.162041, 验证log rmse 0.188112\n    折3, 训练log rmse 0.163980, 验证log rmse 0.168341\n    折4, 训练log rmse 0.167889, 验证log rmse 0.154215\n    折5, 训练log rmse 0.163577, 验证log rmse 0.183105\n    5-折验证：平均训练log rmse：0.165517,平均验证log rmse：0.170085\n\n  ![svg](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/10_kaggle_predict_price_files/10_kaggle_predict_price_15_1.svg) \n\n\n![svg](./img/deeplearning/code/pytorch/3_mlp/10_kaggle_predict_price_files/10_kaggle_predict_price_15_1.svg)\n\n​    \n\n\n## 10.7 提交Kaggle预测\n- 使用所有的数据进行训练，将预测保存在csv文件\n\n\n```python\ndef train_and_pred(train_features, test_features, train_labels, test_data,\n                   num_epochs, lr, weight_decay, batch_size):\n    net = get_net()\n    train_ls, _ = train(net, train_features, train_labels, None,None,\n                        num_epochs, lr, weight_decay, batch_size)\n    d2l.plot(np.arange(1, num_epochs+1), [train_ls], xlabel='epoch',\n             ylabel='log rmse', xlim=[1, num_epochs], yscale='log')\n    print(f'训练log rmse:{float(train_ls[-1]):f}')\n    #将网络用于测试集\n    preds = net(test_features).detach().numpy()\n    # 将其重新格式化以导出到kaggle\n    test_data['SalePrice']=pd.Series(preds.reshape(1, -1)[0])\n    submission = pd.concat([test_data['Id'], test_data['SalePrice']], axis=1)\n    submission.to_csv('submission.csv', index=False)\n\ntrain_and_pred(train_features, test_features, train_labels,test_data,\n               num_epochs, lr, weight_decay, batch_size)\n```\n\n    训练log rmse:0.162509\n\n![svg](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/10_kaggle_predict_price_files/10_kaggle_predict_price_17_1.svg)\n\n\n![svg](./img/deeplearning/code/pytorch/3_mlp/10_kaggle_predict_price_files/10_kaggle_predict_price_17_1.svg)\n    \n\n","source":"_posts/deeplearning/code/pytorch/3_mlp/10_kaggle_predict_price.md","raw":"---\ntitle: 3.10 kaggle:预测房价\ndate: 2024-3-19 14:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 10 kaggle:预测房价\n## 10.1 下载和缓存数据集\n\n\n```python\nimport hashlib\nimport os\nimport tarfile\nimport zipfile\nimport requests\n\n#@save\nDATA_HUB = dict() #字典，将数据集名称的字符串映射到数据集相关的二元组上，每个二元组包含数据集的url和校验和密钥\nDATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/' #所有类似的数据集都托管在这个地址上\n\ndef download(name, cache_dir=os.path.join('data')): #@save\n    \"\"\"下载一个DATA_HUB中的文件，返回本地文件名。\"\"\"\n    assert name in DATA_HUB, f\"{name}不存在于{DATA_HUB}\"\n    url, sha1_hash = DATA_HUB[name]\n    os.makedirs(cache_dir, exist_ok=True)\n    fname = os.path.join(cache_dir, url.split('/')[-1]) #取出url中最后一个/后面的字符串，作为文件名\n    if os.path.exists(fname):\n        sha1 = hashlib.sha1()\n        with open(fname, 'rb') as f:\n            while True:\n                data = f.read(1048576) # 读取1MB数据\n                if not data:\n                    break\n                sha1.update(data)\n        if sha1.hexdigest() == sha1_hash:\n            return fname #命中缓存\n\n    print(f'正在从{url}下载{fname}...')\n    r = requests.get(url, stream=True, verify= True)\n    with open(fname, 'wb') as f:\n        f.write(r.content)\n    return fname\n\ndef download_extract(name, folder=None): #@save\n    \"\"\"下载并解压zip/tar文件\"\"\"\n    fname = download(name)\n    base_dir = os.path.dirname(fname)\n    data_dir, ext = os.path.splitext(fname)\n    if ext =='.zip':\n        fp = zipfile.ZipFile(fname, 'r')\n    elif ext in ('.tar', '.gz'):\n        fp = tarfile.open(fname, 'r')\n    else:\n        assert False, '只有zip/tar文件可以被解压缩'\n    fp.extractall(base_dir)\n    return os.path.join(base_dir, folder) if folder else data_dir\n\ndef download_all(): #@save\n    \"\"\"下载DATA_HUB中的所有文件\"\"\"\n    for name in DATA_HUB:\n        download(name)\n```\n\n## 10.2 访问和读取数据集\n\n\n```python\n\n#读入并处理数据\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\nDATA_HUB['kaggle_house_train']=( #@save\n    DATA_URL + 'kaggle_house_pred_train.csv',\n    '585e9cc93e70b39160e7921475f9bcd7d31219ce')\nDATA_HUB['kaggle_house_test'] = (#@save\n    DATA_URL + 'kaggle_house_pred_test.csv',\n    'fa19780a7b011d9b009e8bff8e99922a8ee2eb90')\n\n### 1. 加载数据集\ntrain_data = pd.read_csv(download('kaggle_house_train'))\ntest_data = pd.read_csv(download('kaggle_house_test'))\n#输出维度\nprint(train_data.shape)\nprint(test_data.shape)\n#输出前四个特征、最后两个特征、标签\nprint(train_data.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]])\n#去掉id列\nall_features = pd.concat((train_data.iloc[:,1:-1], test_data.iloc[:,1:]))\n```\n\n    正在从http://d2l-data.s3-accelerate.amazonaws.com/kaggle_house_pred_train.csv下载data\\kaggle_house_pred_train.csv...\n    正在从http://d2l-data.s3-accelerate.amazonaws.com/kaggle_house_pred_test.csv下载data\\kaggle_house_pred_test.csv...\n    (1460, 81)\n    (1459, 80)\n       Id  MSSubClass MSZoning  LotFrontage SaleType SaleCondition  SalePrice\n    0   1          60       RL         65.0       WD        Normal     208500\n    1   2          20       RL         80.0       WD        Normal     181500\n    2   3          60       RL         68.0       WD        Normal     223500\n    3   4          70       RL         60.0       WD       Abnorml     140000\n\n\n## 10.3 数据预处理\n- 缺失值用均值代替\n- 为了将所有特征放到一个共同的尺度上，将特征缩放到均值为0，单位方差：\n$$x \\leftarrow \\frac{x-\\mu} \\sigma$$\n- 标准化数据有两个原因：\n    - 方便优化\n    - 因为我们不知道哪些特征是相关的，所以我们不想让惩罚分配给一个特征的系数比分配给其他任何特征的系数更大\n- 离散值处理：独热编码：如MSZoning列包含值：‘RL’和‘RM’。创建两个新的指标列MSZoning_RL和MSZoning_RM，值为0，1.可以用pandas自动实现\n\n\n```python\n## 2标准化\nnumeric_features = all_features.dtypes[all_features.dtypes != 'object'].index #取出数值特征的索引\nall_features[numeric_features] = all_features[numeric_features].apply(\n    lambda x:(x-x.mean())/ x.std()\n)\n#缺失值设为0\nall_features[numeric_features] = all_features[numeric_features].fillna(0)\n\n## 3离散值处理\n#‘dummy_na=True’将缺失值也当作合法的特征值并为其创建指示特征\nall_features = pd.get_dummies(all_features, dummy_na=True)\nprint(all_features.shape) #特征数量从79 -> 331\n\n## 3转换为张量\nn_train = train_data.shape[0]\ntrain_features = torch.tensor(all_features[:n_train].values, dtype= torch.float32)\ntest_features = torch.tensor(all_features[n_train:].values, dtype=torch.float32)\ntrain_labels = torch.tensor(\n    train_data.SalePrice.values.reshape(-1,1), dtype=torch.float32\n)\n```\n\n    (2919, 331)\n\n\n## 10.4 训练\n- 先训练一个带有损失平方的线性模型，线性模型可以提供一种健全性检查，以查看数据中是否存在有意义的信息。如果线性模型不能做得比随机猜测更好，那么可能存在数据处理错误。如果一切顺利，线性模型可以作为基线模型（baseline）\n- Adam优化器主要吸引力在于对初始学习率不那么敏感\n\n\n```python\nloss= nn.MSELoss()\nin_features = train_features.shape[1]\ndef get_net():\n    net = nn.Sequential(nn.Linear(in_features,1))\n    return net\n```\n\n- 对于误差，更关心相对误差，例如北京房价预测差了10万与阳新放假预测差了10万是不同的。\n- 可以用价格预测的对数来衡量差异。将$\\sigma for |log y - log \\hat y|$转换为$e^{-\\sigma} \\leq \\frac{\\hat y}{y} \\leq e^{\\sigma}$这使得预测价格的对数与真实标签价格的对数之间出现以下均方根误差：\n$$\\sqrt{\\frac1n\\sum_{i=1}^n\\left(\\log y_i-\\log\\hat{y}_i\\right)^2}$$\n\n\n```python\ndef log_rmse(net, features, labels):\n    # 为了在取对数时进一步稳定该值，将小于1的值设置为1\n    clipped_preds = torch.clamp(net(features), 1, float('inf'))\n    rmse = torch.sqrt(loss(torch.log(clipped_preds), torch.log(labels)))\n    return rmse.item()\n\ndef train(net, train_features, train_labels, test_features, test_labels,\n          num_epochs, learning_rate, weight_decay, batch_size):\n    train_ls, test_ls = [], []\n    train_iter = d2l.load_array((train_features,train_labels),batch_size)\n    optimizer = torch.optim.Adam(net.parameters(),\n                                lr =learning_rate,\n                                weight_decay=weight_decay)\n    for epoch in range(num_epochs):\n        for X,y in train_iter:\n                optimizer.zero_grad()\n                l = loss(net(X), y)\n                l.backward()\n                optimizer.step()\n        train_ls.append(log_rmse(net, train_features,train_labels))\n        if test_labels is not None:\n                test_ls.append(log_rmse(net, test_features,test_labels))\n    return train_ls, test_ls\n\n```\n\n## 10.5 K折交叉验证\n- 有助于模型选择和超参数调整。我们首先需要定义一个函数，在K折交叉验证过程中返回第i折的数据。具体地说，它选择第i个切片作为验证数据，其余部分作为训练数据。每次在K − 1个子集上进行训练，并在剩余的一个子集上进行验证。\n\n\n```python\ndef get_k_fold_data(k, i, X, y):\n    assert k > 1\n    fold_size = X.shape[0] // k\n    X_train, y_train = None, None\n    for j in range(k):\n        idx = slice(j * fold_size, (j + 1) * fold_size)\n        X_part, y_part = X[idx, :], y[idx]\n        if j ==i:\n            X_valid, y_valid = X_part, y_part\n        elif X_train is None:\n            X_train, y_train = X_part, y_part\n        else:\n            X_train = torch.cat([X_train, X_part], 0)\n            y_train = torch.cat([y_train, y_part], 0)\n    return X_train, y_train, X_valid, y_valid\n```\n\n- 训练K次，并返回训练和验证误差的平均值\n\n\n```python\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\ndef k_fold(k, X_train, y_train, num_epochs, learning_rate, weight_decay, batch_size):\n    train_l_sum, valid_l_sum = 0, 0\n    for i in range(k):\n        data = get_k_fold_data(k, i, X_train, y_train)\n        net = get_net()\n        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate, weight_decay, batch_size)\n        train_l_sum += train_ls[-1]\n        valid_l_sum += valid_ls[-1]\n        if i == 0:\n            d2l.plot(list(range(1, num_epochs + 1)), [train_ls, valid_ls],\n                     xlabel='epoch', ylabel='rmse',\n                     legend=['train', 'valid'], yscale='log')\n        print(f'折{i + 1}, 训练log rmse {float(train_ls[-1]):f}, '\n              f'验证log rmse {float(valid_ls[-1]):f}')\n    return train_l_sum / k, valid_l_sum / k\n```\n\n## 10.6 模型选择\n- 使用未调优的超参数\n\n\n```python\nk, num_epochs, lr, weight_decay, batch_size=5, 100, 5, 0, 64\ntrain_l, valid_l= k_fold(k, train_features, train_labels,num_epochs,lr,\n                         weight_decay, batch_size)\nprint(f'{k}-折验证：平均训练log rmse：{float(train_l):f},'\n      f'平均验证log rmse：{float(valid_l):f}')\n```\n\n    折1, 训练log rmse 0.170100, 验证log rmse 0.156652\n    折2, 训练log rmse 0.162041, 验证log rmse 0.188112\n    折3, 训练log rmse 0.163980, 验证log rmse 0.168341\n    折4, 训练log rmse 0.167889, 验证log rmse 0.154215\n    折5, 训练log rmse 0.163577, 验证log rmse 0.183105\n    5-折验证：平均训练log rmse：0.165517,平均验证log rmse：0.170085\n\n  ![svg](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/10_kaggle_predict_price_files/10_kaggle_predict_price_15_1.svg) \n\n\n![svg](./img/deeplearning/code/pytorch/3_mlp/10_kaggle_predict_price_files/10_kaggle_predict_price_15_1.svg)\n\n​    \n\n\n## 10.7 提交Kaggle预测\n- 使用所有的数据进行训练，将预测保存在csv文件\n\n\n```python\ndef train_and_pred(train_features, test_features, train_labels, test_data,\n                   num_epochs, lr, weight_decay, batch_size):\n    net = get_net()\n    train_ls, _ = train(net, train_features, train_labels, None,None,\n                        num_epochs, lr, weight_decay, batch_size)\n    d2l.plot(np.arange(1, num_epochs+1), [train_ls], xlabel='epoch',\n             ylabel='log rmse', xlim=[1, num_epochs], yscale='log')\n    print(f'训练log rmse:{float(train_ls[-1]):f}')\n    #将网络用于测试集\n    preds = net(test_features).detach().numpy()\n    # 将其重新格式化以导出到kaggle\n    test_data['SalePrice']=pd.Series(preds.reshape(1, -1)[0])\n    submission = pd.concat([test_data['Id'], test_data['SalePrice']], axis=1)\n    submission.to_csv('submission.csv', index=False)\n\ntrain_and_pred(train_features, test_features, train_labels,test_data,\n               num_epochs, lr, weight_decay, batch_size)\n```\n\n    训练log rmse:0.162509\n\n![svg](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/10_kaggle_predict_price_files/10_kaggle_predict_price_17_1.svg)\n\n\n![svg](./img/deeplearning/code/pytorch/3_mlp/10_kaggle_predict_price_files/10_kaggle_predict_price_17_1.svg)\n    \n\n","slug":"deeplearning/code/pytorch/3_mlp/10_kaggle_predict_price","published":1,"updated":"2024-05-05T14:49:44.401Z","comments":1,"layout":"post","photos":[],"_id":"clvuli6b10030zsvw3ikwbcd9","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"10-kaggle-预测房价\"><a href=\"#10-kaggle-预测房价\" class=\"headerlink\" title=\"10 kaggle:预测房价\"></a>10 kaggle:预测房价</h1><h2 id=\"10-1-下载和缓存数据集\"><a href=\"#10-1-下载和缓存数据集\" class=\"headerlink\" title=\"10.1 下载和缓存数据集\"></a>10.1 下载和缓存数据集</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> hashlib</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> tarfile</span><br><span class=\"line\"><span class=\"keyword\">import</span> zipfile</span><br><span class=\"line\"><span class=\"keyword\">import</span> requests</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#@save</span></span><br><span class=\"line\">DATA_HUB = <span class=\"built_in\">dict</span>() <span class=\"comment\">#字典，将数据集名称的字符串映射到数据集相关的二元组上，每个二元组包含数据集的url和校验和密钥</span></span><br><span class=\"line\">DATA_URL = <span class=\"string\">&#x27;http://d2l-data.s3-accelerate.amazonaws.com/&#x27;</span> <span class=\"comment\">#所有类似的数据集都托管在这个地址上</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">download</span>(<span class=\"params\">name, cache_dir=os.path.join(<span class=\"params\"><span class=\"string\">&#x27;data&#x27;</span></span>)</span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;下载一个DATA_HUB中的文件，返回本地文件名。&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> name <span class=\"keyword\">in</span> DATA_HUB, <span class=\"string\">f&quot;<span class=\"subst\">&#123;name&#125;</span>不存在于<span class=\"subst\">&#123;DATA_HUB&#125;</span>&quot;</span></span><br><span class=\"line\">    url, sha1_hash = DATA_HUB[name]</span><br><span class=\"line\">    os.makedirs(cache_dir, exist_ok=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    fname = os.path.join(cache_dir, url.split(<span class=\"string\">&#x27;/&#x27;</span>)[-<span class=\"number\">1</span>]) <span class=\"comment\">#取出url中最后一个/后面的字符串，作为文件名</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> os.path.exists(fname):</span><br><span class=\"line\">        sha1 = hashlib.sha1()</span><br><span class=\"line\">        <span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(fname, <span class=\"string\">&#x27;rb&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">            <span class=\"keyword\">while</span> <span class=\"literal\">True</span>:</span><br><span class=\"line\">                data = f.read(<span class=\"number\">1048576</span>) <span class=\"comment\"># 读取1MB数据</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> data:</span><br><span class=\"line\">                    <span class=\"keyword\">break</span></span><br><span class=\"line\">                sha1.update(data)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> sha1.hexdigest() == sha1_hash:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> fname <span class=\"comment\">#命中缓存</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;正在从<span class=\"subst\">&#123;url&#125;</span>下载<span class=\"subst\">&#123;fname&#125;</span>...&#x27;</span>)</span><br><span class=\"line\">    r = requests.get(url, stream=<span class=\"literal\">True</span>, verify= <span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(fname, <span class=\"string\">&#x27;wb&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">        f.write(r.content)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> fname</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">download_extract</span>(<span class=\"params\">name, folder=<span class=\"literal\">None</span></span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;下载并解压zip/tar文件&quot;&quot;&quot;</span></span><br><span class=\"line\">    fname = download(name)</span><br><span class=\"line\">    base_dir = os.path.dirname(fname)</span><br><span class=\"line\">    data_dir, ext = os.path.splitext(fname)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> ext ==<span class=\"string\">&#x27;.zip&#x27;</span>:</span><br><span class=\"line\">        fp = zipfile.ZipFile(fname, <span class=\"string\">&#x27;r&#x27;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> ext <span class=\"keyword\">in</span> (<span class=\"string\">&#x27;.tar&#x27;</span>, <span class=\"string\">&#x27;.gz&#x27;</span>):</span><br><span class=\"line\">        fp = tarfile.<span class=\"built_in\">open</span>(fname, <span class=\"string\">&#x27;r&#x27;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">assert</span> <span class=\"literal\">False</span>, <span class=\"string\">&#x27;只有zip/tar文件可以被解压缩&#x27;</span></span><br><span class=\"line\">    fp.extractall(base_dir)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> os.path.join(base_dir, folder) <span class=\"keyword\">if</span> folder <span class=\"keyword\">else</span> data_dir</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">download_all</span>(): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;下载DATA_HUB中的所有文件&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> name <span class=\"keyword\">in</span> DATA_HUB:</span><br><span class=\"line\">        download(name)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"10-2-访问和读取数据集\"><a href=\"#10-2-访问和读取数据集\" class=\"headerlink\" title=\"10.2 访问和读取数据集\"></a>10.2 访问和读取数据集</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#读入并处理数据</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">DATA_HUB[<span class=\"string\">&#x27;kaggle_house_train&#x27;</span>]=( <span class=\"comment\">#@save</span></span><br><span class=\"line\">    DATA_URL + <span class=\"string\">&#x27;kaggle_house_pred_train.csv&#x27;</span>,</span><br><span class=\"line\">    <span class=\"string\">&#x27;585e9cc93e70b39160e7921475f9bcd7d31219ce&#x27;</span>)</span><br><span class=\"line\">DATA_HUB[<span class=\"string\">&#x27;kaggle_house_test&#x27;</span>] = (<span class=\"comment\">#@save</span></span><br><span class=\"line\">    DATA_URL + <span class=\"string\">&#x27;kaggle_house_pred_test.csv&#x27;</span>,</span><br><span class=\"line\">    <span class=\"string\">&#x27;fa19780a7b011d9b009e8bff8e99922a8ee2eb90&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">### 1. 加载数据集</span></span><br><span class=\"line\">train_data = pd.read_csv(download(<span class=\"string\">&#x27;kaggle_house_train&#x27;</span>))</span><br><span class=\"line\">test_data = pd.read_csv(download(<span class=\"string\">&#x27;kaggle_house_test&#x27;</span>))</span><br><span class=\"line\"><span class=\"comment\">#输出维度</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(train_data.shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(test_data.shape)</span><br><span class=\"line\"><span class=\"comment\">#输出前四个特征、最后两个特征、标签</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(train_data.iloc[<span class=\"number\">0</span>:<span class=\"number\">4</span>, [<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, -<span class=\"number\">3</span>, -<span class=\"number\">2</span>, -<span class=\"number\">1</span>]])</span><br><span class=\"line\"><span class=\"comment\">#去掉id列</span></span><br><span class=\"line\">all_features = pd.concat((train_data.iloc[:,<span class=\"number\">1</span>:-<span class=\"number\">1</span>], test_data.iloc[:,<span class=\"number\">1</span>:]))</span><br></pre></td></tr></table></figure>\n\n<pre><code>正在从http://d2l-data.s3-accelerate.amazonaws.com/kaggle_house_pred_train.csv下载data\\kaggle_house_pred_train.csv...\n正在从http://d2l-data.s3-accelerate.amazonaws.com/kaggle_house_pred_test.csv下载data\\kaggle_house_pred_test.csv...\n(1460, 81)\n(1459, 80)\n   Id  MSSubClass MSZoning  LotFrontage SaleType SaleCondition  SalePrice\n0   1          60       RL         65.0       WD        Normal     208500\n1   2          20       RL         80.0       WD        Normal     181500\n2   3          60       RL         68.0       WD        Normal     223500\n3   4          70       RL         60.0       WD       Abnorml     140000\n</code></pre>\n<h2 id=\"10-3-数据预处理\"><a href=\"#10-3-数据预处理\" class=\"headerlink\" title=\"10.3 数据预处理\"></a>10.3 数据预处理</h2><ul>\n<li>缺失值用均值代替</li>\n<li>为了将所有特征放到一个共同的尺度上，将特征缩放到均值为0，单位方差：<br>$$x \\leftarrow \\frac{x-\\mu} \\sigma$$</li>\n<li>标准化数据有两个原因：<ul>\n<li>方便优化</li>\n<li>因为我们不知道哪些特征是相关的，所以我们不想让惩罚分配给一个特征的系数比分配给其他任何特征的系数更大</li>\n</ul>\n</li>\n<li>离散值处理：独热编码：如MSZoning列包含值：‘RL’和‘RM’。创建两个新的指标列MSZoning_RL和MSZoning_RM，值为0，1.可以用pandas自动实现</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">## 2标准化</span></span><br><span class=\"line\">numeric_features = all_features.dtypes[all_features.dtypes != <span class=\"string\">&#x27;object&#x27;</span>].index <span class=\"comment\">#取出数值特征的索引</span></span><br><span class=\"line\">all_features[numeric_features] = all_features[numeric_features].apply(</span><br><span class=\"line\">    <span class=\"keyword\">lambda</span> x:(x-x.mean())/ x.std()</span><br><span class=\"line\">)</span><br><span class=\"line\"><span class=\"comment\">#缺失值设为0</span></span><br><span class=\"line\">all_features[numeric_features] = all_features[numeric_features].fillna(<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## 3离散值处理</span></span><br><span class=\"line\"><span class=\"comment\">#‘dummy_na=True’将缺失值也当作合法的特征值并为其创建指示特征</span></span><br><span class=\"line\">all_features = pd.get_dummies(all_features, dummy_na=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(all_features.shape) <span class=\"comment\">#特征数量从79 -&gt; 331</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## 3转换为张量</span></span><br><span class=\"line\">n_train = train_data.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">train_features = torch.tensor(all_features[:n_train].values, dtype= torch.float32)</span><br><span class=\"line\">test_features = torch.tensor(all_features[n_train:].values, dtype=torch.float32)</span><br><span class=\"line\">train_labels = torch.tensor(</span><br><span class=\"line\">    train_data.SalePrice.values.reshape(-<span class=\"number\">1</span>,<span class=\"number\">1</span>), dtype=torch.float32</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n\n<pre><code>(2919, 331)\n</code></pre>\n<h2 id=\"10-4-训练\"><a href=\"#10-4-训练\" class=\"headerlink\" title=\"10.4 训练\"></a>10.4 训练</h2><ul>\n<li>先训练一个带有损失平方的线性模型，线性模型可以提供一种健全性检查，以查看数据中是否存在有意义的信息。如果线性模型不能做得比随机猜测更好，那么可能存在数据处理错误。如果一切顺利，线性模型可以作为基线模型（baseline）</li>\n<li>Adam优化器主要吸引力在于对初始学习率不那么敏感</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">loss= nn.MSELoss()</span><br><span class=\"line\">in_features = train_features.shape[<span class=\"number\">1</span>]</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">get_net</span>():</span><br><span class=\"line\">    net = nn.Sequential(nn.Linear(in_features,<span class=\"number\">1</span>))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> net</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>对于误差，更关心相对误差，例如北京房价预测差了10万与阳新放假预测差了10万是不同的。</li>\n<li>可以用价格预测的对数来衡量差异。将$\\sigma for |log y - log \\hat y|$转换为$e^{-\\sigma} \\leq \\frac{\\hat y}{y} \\leq e^{\\sigma}$这使得预测价格的对数与真实标签价格的对数之间出现以下均方根误差：<br>$$\\sqrt{\\frac1n\\sum_{i&#x3D;1}^n\\left(\\log y_i-\\log\\hat{y}_i\\right)^2}$$</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">log_rmse</span>(<span class=\"params\">net, features, labels</span>):</span><br><span class=\"line\">    <span class=\"comment\"># 为了在取对数时进一步稳定该值，将小于1的值设置为1</span></span><br><span class=\"line\">    clipped_preds = torch.clamp(net(features), <span class=\"number\">1</span>, <span class=\"built_in\">float</span>(<span class=\"string\">&#x27;inf&#x27;</span>))</span><br><span class=\"line\">    rmse = torch.sqrt(loss(torch.log(clipped_preds), torch.log(labels)))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> rmse.item()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train</span>(<span class=\"params\">net, train_features, train_labels, test_features, test_labels,</span></span><br><span class=\"line\"><span class=\"params\">          num_epochs, learning_rate, weight_decay, batch_size</span>):</span><br><span class=\"line\">    train_ls, test_ls = [], []</span><br><span class=\"line\">    train_iter = d2l.load_array((train_features,train_labels),batch_size)</span><br><span class=\"line\">    optimizer = torch.optim.Adam(net.parameters(),</span><br><span class=\"line\">                                lr =learning_rate,</span><br><span class=\"line\">                                weight_decay=weight_decay)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> X,y <span class=\"keyword\">in</span> train_iter:</span><br><span class=\"line\">                optimizer.zero_grad()</span><br><span class=\"line\">                l = loss(net(X), y)</span><br><span class=\"line\">                l.backward()</span><br><span class=\"line\">                optimizer.step()</span><br><span class=\"line\">        train_ls.append(log_rmse(net, train_features,train_labels))</span><br><span class=\"line\">        <span class=\"keyword\">if</span> test_labels <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">                test_ls.append(log_rmse(net, test_features,test_labels))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> train_ls, test_ls</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"10-5-K折交叉验证\"><a href=\"#10-5-K折交叉验证\" class=\"headerlink\" title=\"10.5 K折交叉验证\"></a>10.5 K折交叉验证</h2><ul>\n<li>有助于模型选择和超参数调整。我们首先需要定义一个函数，在K折交叉验证过程中返回第i折的数据。具体地说，它选择第i个切片作为验证数据，其余部分作为训练数据。每次在K − 1个子集上进行训练，并在剩余的一个子集上进行验证。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">get_k_fold_data</span>(<span class=\"params\">k, i, X, y</span>):</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> k &gt; <span class=\"number\">1</span></span><br><span class=\"line\">    fold_size = X.shape[<span class=\"number\">0</span>] // k</span><br><span class=\"line\">    X_train, y_train = <span class=\"literal\">None</span>, <span class=\"literal\">None</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(k):</span><br><span class=\"line\">        idx = <span class=\"built_in\">slice</span>(j * fold_size, (j + <span class=\"number\">1</span>) * fold_size)</span><br><span class=\"line\">        X_part, y_part = X[idx, :], y[idx]</span><br><span class=\"line\">        <span class=\"keyword\">if</span> j ==i:</span><br><span class=\"line\">            X_valid, y_valid = X_part, y_part</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> X_train <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            X_train, y_train = X_part, y_part</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            X_train = torch.cat([X_train, X_part], <span class=\"number\">0</span>)</span><br><span class=\"line\">            y_train = torch.cat([y_train, y_part], <span class=\"number\">0</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> X_train, y_train, X_valid, y_valid</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>训练K次，并返回训练和验证误差的平均值</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>] = <span class=\"string\">&#x27;True&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">k_fold</span>(<span class=\"params\">k, X_train, y_train, num_epochs, learning_rate, weight_decay, batch_size</span>):</span><br><span class=\"line\">    train_l_sum, valid_l_sum = <span class=\"number\">0</span>, <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(k):</span><br><span class=\"line\">        data = get_k_fold_data(k, i, X_train, y_train)</span><br><span class=\"line\">        net = get_net()</span><br><span class=\"line\">        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate, weight_decay, batch_size)</span><br><span class=\"line\">        train_l_sum += train_ls[-<span class=\"number\">1</span>]</span><br><span class=\"line\">        valid_l_sum += valid_ls[-<span class=\"number\">1</span>]</span><br><span class=\"line\">        <span class=\"keyword\">if</span> i == <span class=\"number\">0</span>:</span><br><span class=\"line\">            d2l.plot(<span class=\"built_in\">list</span>(<span class=\"built_in\">range</span>(<span class=\"number\">1</span>, num_epochs + <span class=\"number\">1</span>)), [train_ls, valid_ls],</span><br><span class=\"line\">                     xlabel=<span class=\"string\">&#x27;epoch&#x27;</span>, ylabel=<span class=\"string\">&#x27;rmse&#x27;</span>,</span><br><span class=\"line\">                     legend=[<span class=\"string\">&#x27;train&#x27;</span>, <span class=\"string\">&#x27;valid&#x27;</span>], yscale=<span class=\"string\">&#x27;log&#x27;</span>)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;折<span class=\"subst\">&#123;i + <span class=\"number\">1</span>&#125;</span>, 训练log rmse <span class=\"subst\">&#123;<span class=\"built_in\">float</span>(train_ls[-<span class=\"number\">1</span>]):f&#125;</span>, &#x27;</span></span><br><span class=\"line\">              <span class=\"string\">f&#x27;验证log rmse <span class=\"subst\">&#123;<span class=\"built_in\">float</span>(valid_ls[-<span class=\"number\">1</span>]):f&#125;</span>&#x27;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> train_l_sum / k, valid_l_sum / k</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"10-6-模型选择\"><a href=\"#10-6-模型选择\" class=\"headerlink\" title=\"10.6 模型选择\"></a>10.6 模型选择</h2><ul>\n<li>使用未调优的超参数</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">k, num_epochs, lr, weight_decay, batch_size=<span class=\"number\">5</span>, <span class=\"number\">100</span>, <span class=\"number\">5</span>, <span class=\"number\">0</span>, <span class=\"number\">64</span></span><br><span class=\"line\">train_l, valid_l= k_fold(k, train_features, train_labels,num_epochs,lr,</span><br><span class=\"line\">                         weight_decay, batch_size)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;<span class=\"subst\">&#123;k&#125;</span>-折验证：平均训练log rmse：<span class=\"subst\">&#123;<span class=\"built_in\">float</span>(train_l):f&#125;</span>,&#x27;</span></span><br><span class=\"line\">      <span class=\"string\">f&#x27;平均验证log rmse：<span class=\"subst\">&#123;<span class=\"built_in\">float</span>(valid_l):f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>折1, 训练log rmse 0.170100, 验证log rmse 0.156652\n折2, 训练log rmse 0.162041, 验证log rmse 0.188112\n折3, 训练log rmse 0.163980, 验证log rmse 0.168341\n折4, 训练log rmse 0.167889, 验证log rmse 0.154215\n折5, 训练log rmse 0.163577, 验证log rmse 0.183105\n5-折验证：平均训练log rmse：0.165517,平均验证log rmse：0.170085\n</code></pre>\n<p>  <img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/10_kaggle_predict_price_files/10_kaggle_predict_price_15_1.svg\" alt=\"svg\"> </p>\n<p><img src=\"/./img/deeplearning/code/pytorch/3_mlp/10_kaggle_predict_price_files/10_kaggle_predict_price_15_1.svg\" alt=\"svg\"></p>\n<p>​    </p>\n<h2 id=\"10-7-提交Kaggle预测\"><a href=\"#10-7-提交Kaggle预测\" class=\"headerlink\" title=\"10.7 提交Kaggle预测\"></a>10.7 提交Kaggle预测</h2><ul>\n<li>使用所有的数据进行训练，将预测保存在csv文件</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train_and_pred</span>(<span class=\"params\">train_features, test_features, train_labels, test_data,</span></span><br><span class=\"line\"><span class=\"params\">                   num_epochs, lr, weight_decay, batch_size</span>):</span><br><span class=\"line\">    net = get_net()</span><br><span class=\"line\">    train_ls, _ = train(net, train_features, train_labels, <span class=\"literal\">None</span>,<span class=\"literal\">None</span>,</span><br><span class=\"line\">                        num_epochs, lr, weight_decay, batch_size)</span><br><span class=\"line\">    d2l.plot(np.arange(<span class=\"number\">1</span>, num_epochs+<span class=\"number\">1</span>), [train_ls], xlabel=<span class=\"string\">&#x27;epoch&#x27;</span>,</span><br><span class=\"line\">             ylabel=<span class=\"string\">&#x27;log rmse&#x27;</span>, xlim=[<span class=\"number\">1</span>, num_epochs], yscale=<span class=\"string\">&#x27;log&#x27;</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;训练log rmse:<span class=\"subst\">&#123;<span class=\"built_in\">float</span>(train_ls[-<span class=\"number\">1</span>]):f&#125;</span>&#x27;</span>)</span><br><span class=\"line\">    <span class=\"comment\">#将网络用于测试集</span></span><br><span class=\"line\">    preds = net(test_features).detach().numpy()</span><br><span class=\"line\">    <span class=\"comment\"># 将其重新格式化以导出到kaggle</span></span><br><span class=\"line\">    test_data[<span class=\"string\">&#x27;SalePrice&#x27;</span>]=pd.Series(preds.reshape(<span class=\"number\">1</span>, -<span class=\"number\">1</span>)[<span class=\"number\">0</span>])</span><br><span class=\"line\">    submission = pd.concat([test_data[<span class=\"string\">&#x27;Id&#x27;</span>], test_data[<span class=\"string\">&#x27;SalePrice&#x27;</span>]], axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">    submission.to_csv(<span class=\"string\">&#x27;submission.csv&#x27;</span>, index=<span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">train_and_pred(train_features, test_features, train_labels,test_data,</span><br><span class=\"line\">               num_epochs, lr, weight_decay, batch_size)</span><br></pre></td></tr></table></figure>\n\n<pre><code>训练log rmse:0.162509\n</code></pre>\n<p><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/10_kaggle_predict_price_files/10_kaggle_predict_price_17_1.svg\" alt=\"svg\"></p>\n<p><img src=\"/./img/deeplearning/code/pytorch/3_mlp/10_kaggle_predict_price_files/10_kaggle_predict_price_17_1.svg\" alt=\"svg\"></p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"10-kaggle-预测房价\"><a href=\"#10-kaggle-预测房价\" class=\"headerlink\" title=\"10 kaggle:预测房价\"></a>10 kaggle:预测房价</h1><h2 id=\"10-1-下载和缓存数据集\"><a href=\"#10-1-下载和缓存数据集\" class=\"headerlink\" title=\"10.1 下载和缓存数据集\"></a>10.1 下载和缓存数据集</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> hashlib</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> tarfile</span><br><span class=\"line\"><span class=\"keyword\">import</span> zipfile</span><br><span class=\"line\"><span class=\"keyword\">import</span> requests</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#@save</span></span><br><span class=\"line\">DATA_HUB = <span class=\"built_in\">dict</span>() <span class=\"comment\">#字典，将数据集名称的字符串映射到数据集相关的二元组上，每个二元组包含数据集的url和校验和密钥</span></span><br><span class=\"line\">DATA_URL = <span class=\"string\">&#x27;http://d2l-data.s3-accelerate.amazonaws.com/&#x27;</span> <span class=\"comment\">#所有类似的数据集都托管在这个地址上</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">download</span>(<span class=\"params\">name, cache_dir=os.path.join(<span class=\"params\"><span class=\"string\">&#x27;data&#x27;</span></span>)</span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;下载一个DATA_HUB中的文件，返回本地文件名。&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> name <span class=\"keyword\">in</span> DATA_HUB, <span class=\"string\">f&quot;<span class=\"subst\">&#123;name&#125;</span>不存在于<span class=\"subst\">&#123;DATA_HUB&#125;</span>&quot;</span></span><br><span class=\"line\">    url, sha1_hash = DATA_HUB[name]</span><br><span class=\"line\">    os.makedirs(cache_dir, exist_ok=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    fname = os.path.join(cache_dir, url.split(<span class=\"string\">&#x27;/&#x27;</span>)[-<span class=\"number\">1</span>]) <span class=\"comment\">#取出url中最后一个/后面的字符串，作为文件名</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> os.path.exists(fname):</span><br><span class=\"line\">        sha1 = hashlib.sha1()</span><br><span class=\"line\">        <span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(fname, <span class=\"string\">&#x27;rb&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">            <span class=\"keyword\">while</span> <span class=\"literal\">True</span>:</span><br><span class=\"line\">                data = f.read(<span class=\"number\">1048576</span>) <span class=\"comment\"># 读取1MB数据</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> data:</span><br><span class=\"line\">                    <span class=\"keyword\">break</span></span><br><span class=\"line\">                sha1.update(data)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> sha1.hexdigest() == sha1_hash:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> fname <span class=\"comment\">#命中缓存</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;正在从<span class=\"subst\">&#123;url&#125;</span>下载<span class=\"subst\">&#123;fname&#125;</span>...&#x27;</span>)</span><br><span class=\"line\">    r = requests.get(url, stream=<span class=\"literal\">True</span>, verify= <span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(fname, <span class=\"string\">&#x27;wb&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">        f.write(r.content)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> fname</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">download_extract</span>(<span class=\"params\">name, folder=<span class=\"literal\">None</span></span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;下载并解压zip/tar文件&quot;&quot;&quot;</span></span><br><span class=\"line\">    fname = download(name)</span><br><span class=\"line\">    base_dir = os.path.dirname(fname)</span><br><span class=\"line\">    data_dir, ext = os.path.splitext(fname)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> ext ==<span class=\"string\">&#x27;.zip&#x27;</span>:</span><br><span class=\"line\">        fp = zipfile.ZipFile(fname, <span class=\"string\">&#x27;r&#x27;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> ext <span class=\"keyword\">in</span> (<span class=\"string\">&#x27;.tar&#x27;</span>, <span class=\"string\">&#x27;.gz&#x27;</span>):</span><br><span class=\"line\">        fp = tarfile.<span class=\"built_in\">open</span>(fname, <span class=\"string\">&#x27;r&#x27;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">assert</span> <span class=\"literal\">False</span>, <span class=\"string\">&#x27;只有zip/tar文件可以被解压缩&#x27;</span></span><br><span class=\"line\">    fp.extractall(base_dir)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> os.path.join(base_dir, folder) <span class=\"keyword\">if</span> folder <span class=\"keyword\">else</span> data_dir</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">download_all</span>(): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;下载DATA_HUB中的所有文件&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> name <span class=\"keyword\">in</span> DATA_HUB:</span><br><span class=\"line\">        download(name)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"10-2-访问和读取数据集\"><a href=\"#10-2-访问和读取数据集\" class=\"headerlink\" title=\"10.2 访问和读取数据集\"></a>10.2 访问和读取数据集</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#读入并处理数据</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">DATA_HUB[<span class=\"string\">&#x27;kaggle_house_train&#x27;</span>]=( <span class=\"comment\">#@save</span></span><br><span class=\"line\">    DATA_URL + <span class=\"string\">&#x27;kaggle_house_pred_train.csv&#x27;</span>,</span><br><span class=\"line\">    <span class=\"string\">&#x27;585e9cc93e70b39160e7921475f9bcd7d31219ce&#x27;</span>)</span><br><span class=\"line\">DATA_HUB[<span class=\"string\">&#x27;kaggle_house_test&#x27;</span>] = (<span class=\"comment\">#@save</span></span><br><span class=\"line\">    DATA_URL + <span class=\"string\">&#x27;kaggle_house_pred_test.csv&#x27;</span>,</span><br><span class=\"line\">    <span class=\"string\">&#x27;fa19780a7b011d9b009e8bff8e99922a8ee2eb90&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">### 1. 加载数据集</span></span><br><span class=\"line\">train_data = pd.read_csv(download(<span class=\"string\">&#x27;kaggle_house_train&#x27;</span>))</span><br><span class=\"line\">test_data = pd.read_csv(download(<span class=\"string\">&#x27;kaggle_house_test&#x27;</span>))</span><br><span class=\"line\"><span class=\"comment\">#输出维度</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(train_data.shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(test_data.shape)</span><br><span class=\"line\"><span class=\"comment\">#输出前四个特征、最后两个特征、标签</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(train_data.iloc[<span class=\"number\">0</span>:<span class=\"number\">4</span>, [<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, -<span class=\"number\">3</span>, -<span class=\"number\">2</span>, -<span class=\"number\">1</span>]])</span><br><span class=\"line\"><span class=\"comment\">#去掉id列</span></span><br><span class=\"line\">all_features = pd.concat((train_data.iloc[:,<span class=\"number\">1</span>:-<span class=\"number\">1</span>], test_data.iloc[:,<span class=\"number\">1</span>:]))</span><br></pre></td></tr></table></figure>\n\n<pre><code>正在从http://d2l-data.s3-accelerate.amazonaws.com/kaggle_house_pred_train.csv下载data\\kaggle_house_pred_train.csv...\n正在从http://d2l-data.s3-accelerate.amazonaws.com/kaggle_house_pred_test.csv下载data\\kaggle_house_pred_test.csv...\n(1460, 81)\n(1459, 80)\n   Id  MSSubClass MSZoning  LotFrontage SaleType SaleCondition  SalePrice\n0   1          60       RL         65.0       WD        Normal     208500\n1   2          20       RL         80.0       WD        Normal     181500\n2   3          60       RL         68.0       WD        Normal     223500\n3   4          70       RL         60.0       WD       Abnorml     140000\n</code></pre>\n<h2 id=\"10-3-数据预处理\"><a href=\"#10-3-数据预处理\" class=\"headerlink\" title=\"10.3 数据预处理\"></a>10.3 数据预处理</h2><ul>\n<li>缺失值用均值代替</li>\n<li>为了将所有特征放到一个共同的尺度上，将特征缩放到均值为0，单位方差：<br>$$x \\leftarrow \\frac{x-\\mu} \\sigma$$</li>\n<li>标准化数据有两个原因：<ul>\n<li>方便优化</li>\n<li>因为我们不知道哪些特征是相关的，所以我们不想让惩罚分配给一个特征的系数比分配给其他任何特征的系数更大</li>\n</ul>\n</li>\n<li>离散值处理：独热编码：如MSZoning列包含值：‘RL’和‘RM’。创建两个新的指标列MSZoning_RL和MSZoning_RM，值为0，1.可以用pandas自动实现</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">## 2标准化</span></span><br><span class=\"line\">numeric_features = all_features.dtypes[all_features.dtypes != <span class=\"string\">&#x27;object&#x27;</span>].index <span class=\"comment\">#取出数值特征的索引</span></span><br><span class=\"line\">all_features[numeric_features] = all_features[numeric_features].apply(</span><br><span class=\"line\">    <span class=\"keyword\">lambda</span> x:(x-x.mean())/ x.std()</span><br><span class=\"line\">)</span><br><span class=\"line\"><span class=\"comment\">#缺失值设为0</span></span><br><span class=\"line\">all_features[numeric_features] = all_features[numeric_features].fillna(<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## 3离散值处理</span></span><br><span class=\"line\"><span class=\"comment\">#‘dummy_na=True’将缺失值也当作合法的特征值并为其创建指示特征</span></span><br><span class=\"line\">all_features = pd.get_dummies(all_features, dummy_na=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(all_features.shape) <span class=\"comment\">#特征数量从79 -&gt; 331</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## 3转换为张量</span></span><br><span class=\"line\">n_train = train_data.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">train_features = torch.tensor(all_features[:n_train].values, dtype= torch.float32)</span><br><span class=\"line\">test_features = torch.tensor(all_features[n_train:].values, dtype=torch.float32)</span><br><span class=\"line\">train_labels = torch.tensor(</span><br><span class=\"line\">    train_data.SalePrice.values.reshape(-<span class=\"number\">1</span>,<span class=\"number\">1</span>), dtype=torch.float32</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n\n<pre><code>(2919, 331)\n</code></pre>\n<h2 id=\"10-4-训练\"><a href=\"#10-4-训练\" class=\"headerlink\" title=\"10.4 训练\"></a>10.4 训练</h2><ul>\n<li>先训练一个带有损失平方的线性模型，线性模型可以提供一种健全性检查，以查看数据中是否存在有意义的信息。如果线性模型不能做得比随机猜测更好，那么可能存在数据处理错误。如果一切顺利，线性模型可以作为基线模型（baseline）</li>\n<li>Adam优化器主要吸引力在于对初始学习率不那么敏感</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">loss= nn.MSELoss()</span><br><span class=\"line\">in_features = train_features.shape[<span class=\"number\">1</span>]</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">get_net</span>():</span><br><span class=\"line\">    net = nn.Sequential(nn.Linear(in_features,<span class=\"number\">1</span>))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> net</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>对于误差，更关心相对误差，例如北京房价预测差了10万与阳新放假预测差了10万是不同的。</li>\n<li>可以用价格预测的对数来衡量差异。将$\\sigma for |log y - log \\hat y|$转换为$e^{-\\sigma} \\leq \\frac{\\hat y}{y} \\leq e^{\\sigma}$这使得预测价格的对数与真实标签价格的对数之间出现以下均方根误差：<br>$$\\sqrt{\\frac1n\\sum_{i&#x3D;1}^n\\left(\\log y_i-\\log\\hat{y}_i\\right)^2}$$</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">log_rmse</span>(<span class=\"params\">net, features, labels</span>):</span><br><span class=\"line\">    <span class=\"comment\"># 为了在取对数时进一步稳定该值，将小于1的值设置为1</span></span><br><span class=\"line\">    clipped_preds = torch.clamp(net(features), <span class=\"number\">1</span>, <span class=\"built_in\">float</span>(<span class=\"string\">&#x27;inf&#x27;</span>))</span><br><span class=\"line\">    rmse = torch.sqrt(loss(torch.log(clipped_preds), torch.log(labels)))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> rmse.item()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train</span>(<span class=\"params\">net, train_features, train_labels, test_features, test_labels,</span></span><br><span class=\"line\"><span class=\"params\">          num_epochs, learning_rate, weight_decay, batch_size</span>):</span><br><span class=\"line\">    train_ls, test_ls = [], []</span><br><span class=\"line\">    train_iter = d2l.load_array((train_features,train_labels),batch_size)</span><br><span class=\"line\">    optimizer = torch.optim.Adam(net.parameters(),</span><br><span class=\"line\">                                lr =learning_rate,</span><br><span class=\"line\">                                weight_decay=weight_decay)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> X,y <span class=\"keyword\">in</span> train_iter:</span><br><span class=\"line\">                optimizer.zero_grad()</span><br><span class=\"line\">                l = loss(net(X), y)</span><br><span class=\"line\">                l.backward()</span><br><span class=\"line\">                optimizer.step()</span><br><span class=\"line\">        train_ls.append(log_rmse(net, train_features,train_labels))</span><br><span class=\"line\">        <span class=\"keyword\">if</span> test_labels <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">                test_ls.append(log_rmse(net, test_features,test_labels))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> train_ls, test_ls</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"10-5-K折交叉验证\"><a href=\"#10-5-K折交叉验证\" class=\"headerlink\" title=\"10.5 K折交叉验证\"></a>10.5 K折交叉验证</h2><ul>\n<li>有助于模型选择和超参数调整。我们首先需要定义一个函数，在K折交叉验证过程中返回第i折的数据。具体地说，它选择第i个切片作为验证数据，其余部分作为训练数据。每次在K − 1个子集上进行训练，并在剩余的一个子集上进行验证。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">get_k_fold_data</span>(<span class=\"params\">k, i, X, y</span>):</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> k &gt; <span class=\"number\">1</span></span><br><span class=\"line\">    fold_size = X.shape[<span class=\"number\">0</span>] // k</span><br><span class=\"line\">    X_train, y_train = <span class=\"literal\">None</span>, <span class=\"literal\">None</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(k):</span><br><span class=\"line\">        idx = <span class=\"built_in\">slice</span>(j * fold_size, (j + <span class=\"number\">1</span>) * fold_size)</span><br><span class=\"line\">        X_part, y_part = X[idx, :], y[idx]</span><br><span class=\"line\">        <span class=\"keyword\">if</span> j ==i:</span><br><span class=\"line\">            X_valid, y_valid = X_part, y_part</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> X_train <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            X_train, y_train = X_part, y_part</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            X_train = torch.cat([X_train, X_part], <span class=\"number\">0</span>)</span><br><span class=\"line\">            y_train = torch.cat([y_train, y_part], <span class=\"number\">0</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> X_train, y_train, X_valid, y_valid</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>训练K次，并返回训练和验证误差的平均值</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>] = <span class=\"string\">&#x27;True&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">k_fold</span>(<span class=\"params\">k, X_train, y_train, num_epochs, learning_rate, weight_decay, batch_size</span>):</span><br><span class=\"line\">    train_l_sum, valid_l_sum = <span class=\"number\">0</span>, <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(k):</span><br><span class=\"line\">        data = get_k_fold_data(k, i, X_train, y_train)</span><br><span class=\"line\">        net = get_net()</span><br><span class=\"line\">        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate, weight_decay, batch_size)</span><br><span class=\"line\">        train_l_sum += train_ls[-<span class=\"number\">1</span>]</span><br><span class=\"line\">        valid_l_sum += valid_ls[-<span class=\"number\">1</span>]</span><br><span class=\"line\">        <span class=\"keyword\">if</span> i == <span class=\"number\">0</span>:</span><br><span class=\"line\">            d2l.plot(<span class=\"built_in\">list</span>(<span class=\"built_in\">range</span>(<span class=\"number\">1</span>, num_epochs + <span class=\"number\">1</span>)), [train_ls, valid_ls],</span><br><span class=\"line\">                     xlabel=<span class=\"string\">&#x27;epoch&#x27;</span>, ylabel=<span class=\"string\">&#x27;rmse&#x27;</span>,</span><br><span class=\"line\">                     legend=[<span class=\"string\">&#x27;train&#x27;</span>, <span class=\"string\">&#x27;valid&#x27;</span>], yscale=<span class=\"string\">&#x27;log&#x27;</span>)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;折<span class=\"subst\">&#123;i + <span class=\"number\">1</span>&#125;</span>, 训练log rmse <span class=\"subst\">&#123;<span class=\"built_in\">float</span>(train_ls[-<span class=\"number\">1</span>]):f&#125;</span>, &#x27;</span></span><br><span class=\"line\">              <span class=\"string\">f&#x27;验证log rmse <span class=\"subst\">&#123;<span class=\"built_in\">float</span>(valid_ls[-<span class=\"number\">1</span>]):f&#125;</span>&#x27;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> train_l_sum / k, valid_l_sum / k</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"10-6-模型选择\"><a href=\"#10-6-模型选择\" class=\"headerlink\" title=\"10.6 模型选择\"></a>10.6 模型选择</h2><ul>\n<li>使用未调优的超参数</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">k, num_epochs, lr, weight_decay, batch_size=<span class=\"number\">5</span>, <span class=\"number\">100</span>, <span class=\"number\">5</span>, <span class=\"number\">0</span>, <span class=\"number\">64</span></span><br><span class=\"line\">train_l, valid_l= k_fold(k, train_features, train_labels,num_epochs,lr,</span><br><span class=\"line\">                         weight_decay, batch_size)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;<span class=\"subst\">&#123;k&#125;</span>-折验证：平均训练log rmse：<span class=\"subst\">&#123;<span class=\"built_in\">float</span>(train_l):f&#125;</span>,&#x27;</span></span><br><span class=\"line\">      <span class=\"string\">f&#x27;平均验证log rmse：<span class=\"subst\">&#123;<span class=\"built_in\">float</span>(valid_l):f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>折1, 训练log rmse 0.170100, 验证log rmse 0.156652\n折2, 训练log rmse 0.162041, 验证log rmse 0.188112\n折3, 训练log rmse 0.163980, 验证log rmse 0.168341\n折4, 训练log rmse 0.167889, 验证log rmse 0.154215\n折5, 训练log rmse 0.163577, 验证log rmse 0.183105\n5-折验证：平均训练log rmse：0.165517,平均验证log rmse：0.170085\n</code></pre>\n<p>  <img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/10_kaggle_predict_price_files/10_kaggle_predict_price_15_1.svg\" alt=\"svg\"> </p>\n<p><img src=\"/./img/deeplearning/code/pytorch/3_mlp/10_kaggle_predict_price_files/10_kaggle_predict_price_15_1.svg\" alt=\"svg\"></p>\n<p>​    </p>\n<h2 id=\"10-7-提交Kaggle预测\"><a href=\"#10-7-提交Kaggle预测\" class=\"headerlink\" title=\"10.7 提交Kaggle预测\"></a>10.7 提交Kaggle预测</h2><ul>\n<li>使用所有的数据进行训练，将预测保存在csv文件</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train_and_pred</span>(<span class=\"params\">train_features, test_features, train_labels, test_data,</span></span><br><span class=\"line\"><span class=\"params\">                   num_epochs, lr, weight_decay, batch_size</span>):</span><br><span class=\"line\">    net = get_net()</span><br><span class=\"line\">    train_ls, _ = train(net, train_features, train_labels, <span class=\"literal\">None</span>,<span class=\"literal\">None</span>,</span><br><span class=\"line\">                        num_epochs, lr, weight_decay, batch_size)</span><br><span class=\"line\">    d2l.plot(np.arange(<span class=\"number\">1</span>, num_epochs+<span class=\"number\">1</span>), [train_ls], xlabel=<span class=\"string\">&#x27;epoch&#x27;</span>,</span><br><span class=\"line\">             ylabel=<span class=\"string\">&#x27;log rmse&#x27;</span>, xlim=[<span class=\"number\">1</span>, num_epochs], yscale=<span class=\"string\">&#x27;log&#x27;</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;训练log rmse:<span class=\"subst\">&#123;<span class=\"built_in\">float</span>(train_ls[-<span class=\"number\">1</span>]):f&#125;</span>&#x27;</span>)</span><br><span class=\"line\">    <span class=\"comment\">#将网络用于测试集</span></span><br><span class=\"line\">    preds = net(test_features).detach().numpy()</span><br><span class=\"line\">    <span class=\"comment\"># 将其重新格式化以导出到kaggle</span></span><br><span class=\"line\">    test_data[<span class=\"string\">&#x27;SalePrice&#x27;</span>]=pd.Series(preds.reshape(<span class=\"number\">1</span>, -<span class=\"number\">1</span>)[<span class=\"number\">0</span>])</span><br><span class=\"line\">    submission = pd.concat([test_data[<span class=\"string\">&#x27;Id&#x27;</span>], test_data[<span class=\"string\">&#x27;SalePrice&#x27;</span>]], axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">    submission.to_csv(<span class=\"string\">&#x27;submission.csv&#x27;</span>, index=<span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">train_and_pred(train_features, test_features, train_labels,test_data,</span><br><span class=\"line\">               num_epochs, lr, weight_decay, batch_size)</span><br></pre></td></tr></table></figure>\n\n<pre><code>训练log rmse:0.162509\n</code></pre>\n<p><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/10_kaggle_predict_price_files/10_kaggle_predict_price_17_1.svg\" alt=\"svg\"></p>\n<p><img src=\"/./img/deeplearning/code/pytorch/3_mlp/10_kaggle_predict_price_files/10_kaggle_predict_price_17_1.svg\" alt=\"svg\"></p>"},{"title":"3.1 多层感知机","date":"2024-02-04T06:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 1 多层感知机\n## 1.1 隐藏层\n- 仿射变换中的线性是一个很强的假设。可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制，使其能处理更普遍的函数关系类型。我们可以把前L−1层看作表示，把最后一层看作线性预测器。这种架构通常称为多层感知机（multilayer perceptron），通常缩写为MLP\n![mlp](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\1img\\1.png)\n![mlp](img/deeplearning/code/pytorch/3_mlp/1img/1.png)\n### 1.1.1 从线性到非线性\n\n- $X \\in R^{n \\times d}$，$W^{(1)} \\in R^{d \\times h}$，$H \\in R^{n \\times h}$，$W^{(2)} \\in R^{h \\times q}$，$O \\in R^{n \\times q}$\n- $X$是输入，$W^{(1)}$是第一层权重，$H$是第一层输出，$W^{(2)}$是第二层权重，$O$是输出\n$$H = XW^{(1)}+b^{(1)}$$\n$$O = HW^{(2)}+b^{(2)}$$\n- 仅添加隐藏层并不能扩展表达能力：\n$$O = (XW^{(1)}+b^{(1)})W^{(2)}+b^{(2)} = XW^{(1)}W^{(2)}+b^{(1)}W^{(2)}+b^{(2)}=XW+b$$\n- 还需要激活函数\n- 通过使用更深（而不是更广）的网络，我们可以更容易地逼近许多函数。\n\n## 1.2 激活函数\n### 1.2.1 ReLU函数\n- 修正线性单元（linear unit）提供了一个简单的非线性变换：$$ReLU(x) = max(x,0)$$\n- 使用ReLU的原因是，它求导表现得特别好：要么让参数消失，要么让参数通过。这使得优化表现得更好，并且ReLU减轻了困扰以往神经网络的梯度消失问题\n- ReLU函数有许多变体，包括参数化ReLU（Parameterized ReLU，pReLU）函数。该变体为ReLU添加了一个线性项，因此即使参数是负的，某些信息仍然可以通过：$$pReLU(x) = max(0,x)+\\alpha min(0,x)$$\n\n\n```python\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK']='True'\nimport torch\nfrom d2l import torch as d2l\n\n#relu函数\nx=torch.arange(-8.0,8.0,0.1,requires_grad=True)\ny=torch.relu(x)\n\n#relu函数的导数\ny.backward(torch.ones_like(x),retain_graph=True)\n\n#画图\nd2l.plot(x.detach(),[y.detach(),x.grad],'x','y',figsize=(5,2.5),legend=['relu','grad'])\n```\n\n\n![svg](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\1_mlp_files\\1_mlp_1_0.svg)\n![](img/deeplearning/code/pytorch/3_mlp/1_mlp_files/1_mlp_1_0.svg)\n    \n\n\n### 1.2.2 Sigmoid函数\n- sigmoid通常称为挤压函数（squashing function）：它将范围（‐inf, inf）中的任意输入压缩到区间（0, 1）中的某个值：$$sigmoid(x) = \\frac{1}{1+e^{-x}}$$\n- 导数：$$sigmoid'(x) = sigmoid(x)(1-sigmoid(x))$$\n\n\n```python\n#sigmoid函数\ny = torch.sigmoid(x)\n\n#sigmoid函数的导数\nx.grad.data.zero_()\ny.backward(torch.ones_like(x),retain_graph=True)\n\n#画图\nd2l.plot(x.detach(),[y.detach(),x.grad],'x','y',figsize=(5,2.5),legend=['sigmoid','grad'])\n```\n\n\n![svg](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\1_mlp_files\\1_mlp_3_0.svg)\n![](img/deeplearning/code/pytorch/3_mlp/1_mlp_files/1_mlp_3_0.svg)\n    \n\n\n### 1.2.3 tanh函数\n- 函数的形状类似于sigmoid函数，不同的是tanh函数关于坐标系原点中心对称。\n$$tanh(x) = \\frac{1-e^{-2x}}{1+e^{-2x}}$$\n- 导数：$$tanh'(x) = 1-tanh^2(x)$$\n\n\n```python\n#tanh函数\ny=torch.tanh(x)\n\n#tanh函数的导数\nx.grad.data.zero_()\ny.backward(torch.ones_like(x),retain_graph=True)\n\n#画图\nd2l.plot(x.detach(),[y.detach(),x.grad],'x','y',figsize=(5,2.5),legend=['tanh','grad'])\n```\n\n\n![svg](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\1_mlp_files\\1_mlp_5_0.svg)\n![](img/deeplearning/code/pytorch/3_mlp/1_mlp_files/1_mlp_5_0.svg)\n    \n\n","source":"_posts/deeplearning/code/pytorch/3_mlp/1_mlp.md","raw":"---\ntitle: 3.1 多层感知机\ndate: 2024-2-4 14:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 1 多层感知机\n## 1.1 隐藏层\n- 仿射变换中的线性是一个很强的假设。可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制，使其能处理更普遍的函数关系类型。我们可以把前L−1层看作表示，把最后一层看作线性预测器。这种架构通常称为多层感知机（multilayer perceptron），通常缩写为MLP\n![mlp](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\1img\\1.png)\n![mlp](img/deeplearning/code/pytorch/3_mlp/1img/1.png)\n### 1.1.1 从线性到非线性\n\n- $X \\in R^{n \\times d}$，$W^{(1)} \\in R^{d \\times h}$，$H \\in R^{n \\times h}$，$W^{(2)} \\in R^{h \\times q}$，$O \\in R^{n \\times q}$\n- $X$是输入，$W^{(1)}$是第一层权重，$H$是第一层输出，$W^{(2)}$是第二层权重，$O$是输出\n$$H = XW^{(1)}+b^{(1)}$$\n$$O = HW^{(2)}+b^{(2)}$$\n- 仅添加隐藏层并不能扩展表达能力：\n$$O = (XW^{(1)}+b^{(1)})W^{(2)}+b^{(2)} = XW^{(1)}W^{(2)}+b^{(1)}W^{(2)}+b^{(2)}=XW+b$$\n- 还需要激活函数\n- 通过使用更深（而不是更广）的网络，我们可以更容易地逼近许多函数。\n\n## 1.2 激活函数\n### 1.2.1 ReLU函数\n- 修正线性单元（linear unit）提供了一个简单的非线性变换：$$ReLU(x) = max(x,0)$$\n- 使用ReLU的原因是，它求导表现得特别好：要么让参数消失，要么让参数通过。这使得优化表现得更好，并且ReLU减轻了困扰以往神经网络的梯度消失问题\n- ReLU函数有许多变体，包括参数化ReLU（Parameterized ReLU，pReLU）函数。该变体为ReLU添加了一个线性项，因此即使参数是负的，某些信息仍然可以通过：$$pReLU(x) = max(0,x)+\\alpha min(0,x)$$\n\n\n```python\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK']='True'\nimport torch\nfrom d2l import torch as d2l\n\n#relu函数\nx=torch.arange(-8.0,8.0,0.1,requires_grad=True)\ny=torch.relu(x)\n\n#relu函数的导数\ny.backward(torch.ones_like(x),retain_graph=True)\n\n#画图\nd2l.plot(x.detach(),[y.detach(),x.grad],'x','y',figsize=(5,2.5),legend=['relu','grad'])\n```\n\n\n![svg](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\1_mlp_files\\1_mlp_1_0.svg)\n![](img/deeplearning/code/pytorch/3_mlp/1_mlp_files/1_mlp_1_0.svg)\n    \n\n\n### 1.2.2 Sigmoid函数\n- sigmoid通常称为挤压函数（squashing function）：它将范围（‐inf, inf）中的任意输入压缩到区间（0, 1）中的某个值：$$sigmoid(x) = \\frac{1}{1+e^{-x}}$$\n- 导数：$$sigmoid'(x) = sigmoid(x)(1-sigmoid(x))$$\n\n\n```python\n#sigmoid函数\ny = torch.sigmoid(x)\n\n#sigmoid函数的导数\nx.grad.data.zero_()\ny.backward(torch.ones_like(x),retain_graph=True)\n\n#画图\nd2l.plot(x.detach(),[y.detach(),x.grad],'x','y',figsize=(5,2.5),legend=['sigmoid','grad'])\n```\n\n\n![svg](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\1_mlp_files\\1_mlp_3_0.svg)\n![](img/deeplearning/code/pytorch/3_mlp/1_mlp_files/1_mlp_3_0.svg)\n    \n\n\n### 1.2.3 tanh函数\n- 函数的形状类似于sigmoid函数，不同的是tanh函数关于坐标系原点中心对称。\n$$tanh(x) = \\frac{1-e^{-2x}}{1+e^{-2x}}$$\n- 导数：$$tanh'(x) = 1-tanh^2(x)$$\n\n\n```python\n#tanh函数\ny=torch.tanh(x)\n\n#tanh函数的导数\nx.grad.data.zero_()\ny.backward(torch.ones_like(x),retain_graph=True)\n\n#画图\nd2l.plot(x.detach(),[y.detach(),x.grad],'x','y',figsize=(5,2.5),legend=['tanh','grad'])\n```\n\n\n![svg](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\1_mlp_files\\1_mlp_5_0.svg)\n![](img/deeplearning/code/pytorch/3_mlp/1_mlp_files/1_mlp_5_0.svg)\n    \n\n","slug":"deeplearning/code/pytorch/3_mlp/1_mlp","published":1,"updated":"2024-05-02T08:52:45.621Z","comments":1,"layout":"post","photos":[],"_id":"clvuli6b20031zsvw0eny14nb","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"1-多层感知机\"><a href=\"#1-多层感知机\" class=\"headerlink\" title=\"1 多层感知机\"></a>1 多层感知机</h1><h2 id=\"1-1-隐藏层\"><a href=\"#1-1-隐藏层\" class=\"headerlink\" title=\"1.1 隐藏层\"></a>1.1 隐藏层</h2><ul>\n<li>仿射变换中的线性是一个很强的假设。可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制，使其能处理更普遍的函数关系类型。我们可以把前L−1层看作表示，把最后一层看作线性预测器。这种架构通常称为多层感知机（multilayer perceptron），通常缩写为MLP<br><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\1img\\1.png\" alt=\"mlp\"><br><img src=\"/img/deeplearning/code/pytorch/3_mlp/1img/1.png\" alt=\"mlp\"></li>\n</ul>\n<h3 id=\"1-1-1-从线性到非线性\"><a href=\"#1-1-1-从线性到非线性\" class=\"headerlink\" title=\"1.1.1 从线性到非线性\"></a>1.1.1 从线性到非线性</h3><ul>\n<li>$X \\in R^{n \\times d}$，$W^{(1)} \\in R^{d \\times h}$，$H \\in R^{n \\times h}$，$W^{(2)} \\in R^{h \\times q}$，$O \\in R^{n \\times q}$</li>\n<li>$X$是输入，$W^{(1)}$是第一层权重，$H$是第一层输出，$W^{(2)}$是第二层权重，$O$是输出<br>$$H &#x3D; XW^{(1)}+b^{(1)}$$<br>$$O &#x3D; HW^{(2)}+b^{(2)}$$</li>\n<li>仅添加隐藏层并不能扩展表达能力：<br>$$O &#x3D; (XW^{(1)}+b^{(1)})W^{(2)}+b^{(2)} &#x3D; XW^{(1)}W^{(2)}+b^{(1)}W^{(2)}+b^{(2)}&#x3D;XW+b$$</li>\n<li>还需要激活函数</li>\n<li>通过使用更深（而不是更广）的网络，我们可以更容易地逼近许多函数。</li>\n</ul>\n<h2 id=\"1-2-激活函数\"><a href=\"#1-2-激活函数\" class=\"headerlink\" title=\"1.2 激活函数\"></a>1.2 激活函数</h2><h3 id=\"1-2-1-ReLU函数\"><a href=\"#1-2-1-ReLU函数\" class=\"headerlink\" title=\"1.2.1 ReLU函数\"></a>1.2.1 ReLU函数</h3><ul>\n<li>修正线性单元（linear unit）提供了一个简单的非线性变换：$$ReLU(x) &#x3D; max(x,0)$$</li>\n<li>使用ReLU的原因是，它求导表现得特别好：要么让参数消失，要么让参数通过。这使得优化表现得更好，并且ReLU减轻了困扰以往神经网络的梯度消失问题</li>\n<li>ReLU函数有许多变体，包括参数化ReLU（Parameterized ReLU，pReLU）函数。该变体为ReLU添加了一个线性项，因此即使参数是负的，某些信息仍然可以通过：$$pReLU(x) &#x3D; max(0,x)+\\alpha min(0,x)$$</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>]=<span class=\"string\">&#x27;True&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#relu函数</span></span><br><span class=\"line\">x=torch.arange(-<span class=\"number\">8.0</span>,<span class=\"number\">8.0</span>,<span class=\"number\">0.1</span>,requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">y=torch.relu(x)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#relu函数的导数</span></span><br><span class=\"line\">y.backward(torch.ones_like(x),retain_graph=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#画图</span></span><br><span class=\"line\">d2l.plot(x.detach(),[y.detach(),x.grad],<span class=\"string\">&#x27;x&#x27;</span>,<span class=\"string\">&#x27;y&#x27;</span>,figsize=(<span class=\"number\">5</span>,<span class=\"number\">2.5</span>),legend=[<span class=\"string\">&#x27;relu&#x27;</span>,<span class=\"string\">&#x27;grad&#x27;</span>])</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\1_mlp_files\\1_mlp_1_0.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/3_mlp/1_mlp_files/1_mlp_1_0.svg\"></p>\n<h3 id=\"1-2-2-Sigmoid函数\"><a href=\"#1-2-2-Sigmoid函数\" class=\"headerlink\" title=\"1.2.2 Sigmoid函数\"></a>1.2.2 Sigmoid函数</h3><ul>\n<li>sigmoid通常称为挤压函数（squashing function）：它将范围（‐inf, inf）中的任意输入压缩到区间（0, 1）中的某个值：$$sigmoid(x) &#x3D; \\frac{1}{1+e^{-x}}$$</li>\n<li>导数：$$sigmoid’(x) &#x3D; sigmoid(x)(1-sigmoid(x))$$</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#sigmoid函数</span></span><br><span class=\"line\">y = torch.sigmoid(x)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#sigmoid函数的导数</span></span><br><span class=\"line\">x.grad.data.zero_()</span><br><span class=\"line\">y.backward(torch.ones_like(x),retain_graph=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#画图</span></span><br><span class=\"line\">d2l.plot(x.detach(),[y.detach(),x.grad],<span class=\"string\">&#x27;x&#x27;</span>,<span class=\"string\">&#x27;y&#x27;</span>,figsize=(<span class=\"number\">5</span>,<span class=\"number\">2.5</span>),legend=[<span class=\"string\">&#x27;sigmoid&#x27;</span>,<span class=\"string\">&#x27;grad&#x27;</span>])</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\1_mlp_files\\1_mlp_3_0.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/3_mlp/1_mlp_files/1_mlp_3_0.svg\"></p>\n<h3 id=\"1-2-3-tanh函数\"><a href=\"#1-2-3-tanh函数\" class=\"headerlink\" title=\"1.2.3 tanh函数\"></a>1.2.3 tanh函数</h3><ul>\n<li>函数的形状类似于sigmoid函数，不同的是tanh函数关于坐标系原点中心对称。<br>$$tanh(x) &#x3D; \\frac{1-e^{-2x}}{1+e^{-2x}}$$</li>\n<li>导数：$$tanh’(x) &#x3D; 1-tanh^2(x)$$</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#tanh函数</span></span><br><span class=\"line\">y=torch.tanh(x)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#tanh函数的导数</span></span><br><span class=\"line\">x.grad.data.zero_()</span><br><span class=\"line\">y.backward(torch.ones_like(x),retain_graph=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#画图</span></span><br><span class=\"line\">d2l.plot(x.detach(),[y.detach(),x.grad],<span class=\"string\">&#x27;x&#x27;</span>,<span class=\"string\">&#x27;y&#x27;</span>,figsize=(<span class=\"number\">5</span>,<span class=\"number\">2.5</span>),legend=[<span class=\"string\">&#x27;tanh&#x27;</span>,<span class=\"string\">&#x27;grad&#x27;</span>])</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\1_mlp_files\\1_mlp_5_0.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/3_mlp/1_mlp_files/1_mlp_5_0.svg\"></p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"1-多层感知机\"><a href=\"#1-多层感知机\" class=\"headerlink\" title=\"1 多层感知机\"></a>1 多层感知机</h1><h2 id=\"1-1-隐藏层\"><a href=\"#1-1-隐藏层\" class=\"headerlink\" title=\"1.1 隐藏层\"></a>1.1 隐藏层</h2><ul>\n<li>仿射变换中的线性是一个很强的假设。可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制，使其能处理更普遍的函数关系类型。我们可以把前L−1层看作表示，把最后一层看作线性预测器。这种架构通常称为多层感知机（multilayer perceptron），通常缩写为MLP<br><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\1img\\1.png\" alt=\"mlp\"><br><img src=\"/img/deeplearning/code/pytorch/3_mlp/1img/1.png\" alt=\"mlp\"></li>\n</ul>\n<h3 id=\"1-1-1-从线性到非线性\"><a href=\"#1-1-1-从线性到非线性\" class=\"headerlink\" title=\"1.1.1 从线性到非线性\"></a>1.1.1 从线性到非线性</h3><ul>\n<li>$X \\in R^{n \\times d}$，$W^{(1)} \\in R^{d \\times h}$，$H \\in R^{n \\times h}$，$W^{(2)} \\in R^{h \\times q}$，$O \\in R^{n \\times q}$</li>\n<li>$X$是输入，$W^{(1)}$是第一层权重，$H$是第一层输出，$W^{(2)}$是第二层权重，$O$是输出<br>$$H &#x3D; XW^{(1)}+b^{(1)}$$<br>$$O &#x3D; HW^{(2)}+b^{(2)}$$</li>\n<li>仅添加隐藏层并不能扩展表达能力：<br>$$O &#x3D; (XW^{(1)}+b^{(1)})W^{(2)}+b^{(2)} &#x3D; XW^{(1)}W^{(2)}+b^{(1)}W^{(2)}+b^{(2)}&#x3D;XW+b$$</li>\n<li>还需要激活函数</li>\n<li>通过使用更深（而不是更广）的网络，我们可以更容易地逼近许多函数。</li>\n</ul>\n<h2 id=\"1-2-激活函数\"><a href=\"#1-2-激活函数\" class=\"headerlink\" title=\"1.2 激活函数\"></a>1.2 激活函数</h2><h3 id=\"1-2-1-ReLU函数\"><a href=\"#1-2-1-ReLU函数\" class=\"headerlink\" title=\"1.2.1 ReLU函数\"></a>1.2.1 ReLU函数</h3><ul>\n<li>修正线性单元（linear unit）提供了一个简单的非线性变换：$$ReLU(x) &#x3D; max(x,0)$$</li>\n<li>使用ReLU的原因是，它求导表现得特别好：要么让参数消失，要么让参数通过。这使得优化表现得更好，并且ReLU减轻了困扰以往神经网络的梯度消失问题</li>\n<li>ReLU函数有许多变体，包括参数化ReLU（Parameterized ReLU，pReLU）函数。该变体为ReLU添加了一个线性项，因此即使参数是负的，某些信息仍然可以通过：$$pReLU(x) &#x3D; max(0,x)+\\alpha min(0,x)$$</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>]=<span class=\"string\">&#x27;True&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#relu函数</span></span><br><span class=\"line\">x=torch.arange(-<span class=\"number\">8.0</span>,<span class=\"number\">8.0</span>,<span class=\"number\">0.1</span>,requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">y=torch.relu(x)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#relu函数的导数</span></span><br><span class=\"line\">y.backward(torch.ones_like(x),retain_graph=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#画图</span></span><br><span class=\"line\">d2l.plot(x.detach(),[y.detach(),x.grad],<span class=\"string\">&#x27;x&#x27;</span>,<span class=\"string\">&#x27;y&#x27;</span>,figsize=(<span class=\"number\">5</span>,<span class=\"number\">2.5</span>),legend=[<span class=\"string\">&#x27;relu&#x27;</span>,<span class=\"string\">&#x27;grad&#x27;</span>])</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\1_mlp_files\\1_mlp_1_0.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/3_mlp/1_mlp_files/1_mlp_1_0.svg\"></p>\n<h3 id=\"1-2-2-Sigmoid函数\"><a href=\"#1-2-2-Sigmoid函数\" class=\"headerlink\" title=\"1.2.2 Sigmoid函数\"></a>1.2.2 Sigmoid函数</h3><ul>\n<li>sigmoid通常称为挤压函数（squashing function）：它将范围（‐inf, inf）中的任意输入压缩到区间（0, 1）中的某个值：$$sigmoid(x) &#x3D; \\frac{1}{1+e^{-x}}$$</li>\n<li>导数：$$sigmoid’(x) &#x3D; sigmoid(x)(1-sigmoid(x))$$</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#sigmoid函数</span></span><br><span class=\"line\">y = torch.sigmoid(x)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#sigmoid函数的导数</span></span><br><span class=\"line\">x.grad.data.zero_()</span><br><span class=\"line\">y.backward(torch.ones_like(x),retain_graph=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#画图</span></span><br><span class=\"line\">d2l.plot(x.detach(),[y.detach(),x.grad],<span class=\"string\">&#x27;x&#x27;</span>,<span class=\"string\">&#x27;y&#x27;</span>,figsize=(<span class=\"number\">5</span>,<span class=\"number\">2.5</span>),legend=[<span class=\"string\">&#x27;sigmoid&#x27;</span>,<span class=\"string\">&#x27;grad&#x27;</span>])</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\1_mlp_files\\1_mlp_3_0.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/3_mlp/1_mlp_files/1_mlp_3_0.svg\"></p>\n<h3 id=\"1-2-3-tanh函数\"><a href=\"#1-2-3-tanh函数\" class=\"headerlink\" title=\"1.2.3 tanh函数\"></a>1.2.3 tanh函数</h3><ul>\n<li>函数的形状类似于sigmoid函数，不同的是tanh函数关于坐标系原点中心对称。<br>$$tanh(x) &#x3D; \\frac{1-e^{-2x}}{1+e^{-2x}}$$</li>\n<li>导数：$$tanh’(x) &#x3D; 1-tanh^2(x)$$</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#tanh函数</span></span><br><span class=\"line\">y=torch.tanh(x)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#tanh函数的导数</span></span><br><span class=\"line\">x.grad.data.zero_()</span><br><span class=\"line\">y.backward(torch.ones_like(x),retain_graph=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#画图</span></span><br><span class=\"line\">d2l.plot(x.detach(),[y.detach(),x.grad],<span class=\"string\">&#x27;x&#x27;</span>,<span class=\"string\">&#x27;y&#x27;</span>,figsize=(<span class=\"number\">5</span>,<span class=\"number\">2.5</span>),legend=[<span class=\"string\">&#x27;tanh&#x27;</span>,<span class=\"string\">&#x27;grad&#x27;</span>])</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\1_mlp_files\\1_mlp_5_0.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/3_mlp/1_mlp_files/1_mlp_5_0.svg\"></p>"},{"title":"3.2 多层感知机实现","date":"2024-02-05T06:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n\n# 2 多层感知机实现\n\n\n```python\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\nbatch_size = 256\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n\n#1 初始化模型参数\nnum_inputs, num_outputs, num_hiddens = 28*28, 10, 256\nW1 = nn.Parameter(torch.randn(num_inputs, num_hiddens, requires_grad=True) * 0.01)\nb1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))\nW2 = nn.Parameter(torch.randn(num_hiddens, num_outputs, requires_grad=True) * 0.01)\nb2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))\nparams = [W1, b1, W2, b2]\n\n#2 激活函数\ndef relu(X):\n    a = torch.zeros_like(X)\n    return torch.max(X, a)\n\n#3 模型\ndef net(X):\n    X = X.reshape(-1, num_inputs)\n    H = relu(X @ W1 + b1) # @代表矩阵乘法\n    return (H @ W2 + b2)\n\n#4 损失函数\nloss = nn.CrossEntropyLoss(reduction='none')\n\n#5 训练\nnum_epochs, lr = 10, 0.1\nupdater = torch.optim.SGD(params, lr=lr)\nd2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)\n\n#6 预测\nd2l.predict_ch3(net, test_iter)\n```\n\n\n![svg](D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/2_mlp_realize_files/2_mlp_realize_1_0.svg)\n![](img/deeplearning/code/pytorch/3_mlp/2_mlp_realize_files/2_mlp_realize_1_0.svg)\n    \n\n\n\n![svg](D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/2_mlp_realize_files/2_mlp_realize_1_1.svg)\n![](img/deeplearning/code/pytorch/3_mlp/2_mlp_realize_files/2_mlp_realize_1_1.svg)\n    \n\n","source":"_posts/deeplearning/code/pytorch/3_mlp/2_mlp_realize.md","raw":"---\ntitle: 3.2 多层感知机实现\ndate: 2024-2-5 14:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n\n# 2 多层感知机实现\n\n\n```python\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\nbatch_size = 256\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n\n#1 初始化模型参数\nnum_inputs, num_outputs, num_hiddens = 28*28, 10, 256\nW1 = nn.Parameter(torch.randn(num_inputs, num_hiddens, requires_grad=True) * 0.01)\nb1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))\nW2 = nn.Parameter(torch.randn(num_hiddens, num_outputs, requires_grad=True) * 0.01)\nb2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))\nparams = [W1, b1, W2, b2]\n\n#2 激活函数\ndef relu(X):\n    a = torch.zeros_like(X)\n    return torch.max(X, a)\n\n#3 模型\ndef net(X):\n    X = X.reshape(-1, num_inputs)\n    H = relu(X @ W1 + b1) # @代表矩阵乘法\n    return (H @ W2 + b2)\n\n#4 损失函数\nloss = nn.CrossEntropyLoss(reduction='none')\n\n#5 训练\nnum_epochs, lr = 10, 0.1\nupdater = torch.optim.SGD(params, lr=lr)\nd2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)\n\n#6 预测\nd2l.predict_ch3(net, test_iter)\n```\n\n\n![svg](D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/2_mlp_realize_files/2_mlp_realize_1_0.svg)\n![](img/deeplearning/code/pytorch/3_mlp/2_mlp_realize_files/2_mlp_realize_1_0.svg)\n    \n\n\n\n![svg](D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/2_mlp_realize_files/2_mlp_realize_1_1.svg)\n![](img/deeplearning/code/pytorch/3_mlp/2_mlp_realize_files/2_mlp_realize_1_1.svg)\n    \n\n","slug":"deeplearning/code/pytorch/3_mlp/2_mlp_realize","published":1,"updated":"2024-05-02T08:52:53.843Z","comments":1,"layout":"post","photos":[],"_id":"clvuli6b30033zsvw4le22e2f","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h1 id=\"2-多层感知机实现\"><a href=\"#2-多层感知机实现\" class=\"headerlink\" title=\"2 多层感知机实现\"></a>2 多层感知机实现</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>] = <span class=\"string\">&#x27;True&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">batch_size = <span class=\"number\">256</span></span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#1 初始化模型参数</span></span><br><span class=\"line\">num_inputs, num_outputs, num_hiddens = <span class=\"number\">28</span>*<span class=\"number\">28</span>, <span class=\"number\">10</span>, <span class=\"number\">256</span></span><br><span class=\"line\">W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens, requires_grad=<span class=\"literal\">True</span>) * <span class=\"number\">0.01</span>)</span><br><span class=\"line\">b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=<span class=\"literal\">True</span>))</span><br><span class=\"line\">W2 = nn.Parameter(torch.randn(num_hiddens, num_outputs, requires_grad=<span class=\"literal\">True</span>) * <span class=\"number\">0.01</span>)</span><br><span class=\"line\">b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=<span class=\"literal\">True</span>))</span><br><span class=\"line\">params = [W1, b1, W2, b2]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#2 激活函数</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">relu</span>(<span class=\"params\">X</span>):</span><br><span class=\"line\">    a = torch.zeros_like(X)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.<span class=\"built_in\">max</span>(X, a)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#3 模型</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">net</span>(<span class=\"params\">X</span>):</span><br><span class=\"line\">    X = X.reshape(-<span class=\"number\">1</span>, num_inputs)</span><br><span class=\"line\">    H = relu(X @ W1 + b1) <span class=\"comment\"># @代表矩阵乘法</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> (H @ W2 + b2)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#4 损失函数</span></span><br><span class=\"line\">loss = nn.CrossEntropyLoss(reduction=<span class=\"string\">&#x27;none&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#5 训练</span></span><br><span class=\"line\">num_epochs, lr = <span class=\"number\">10</span>, <span class=\"number\">0.1</span></span><br><span class=\"line\">updater = torch.optim.SGD(params, lr=lr)</span><br><span class=\"line\">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#6 预测</span></span><br><span class=\"line\">d2l.predict_ch3(net, test_iter)</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/2_mlp_realize_files/2_mlp_realize_1_0.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/3_mlp/2_mlp_realize_files/2_mlp_realize_1_0.svg\"></p>\n<p><img src=\"D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/2_mlp_realize_files/2_mlp_realize_1_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/3_mlp/2_mlp_realize_files/2_mlp_realize_1_1.svg\"></p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"2-多层感知机实现\"><a href=\"#2-多层感知机实现\" class=\"headerlink\" title=\"2 多层感知机实现\"></a>2 多层感知机实现</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>] = <span class=\"string\">&#x27;True&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">batch_size = <span class=\"number\">256</span></span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#1 初始化模型参数</span></span><br><span class=\"line\">num_inputs, num_outputs, num_hiddens = <span class=\"number\">28</span>*<span class=\"number\">28</span>, <span class=\"number\">10</span>, <span class=\"number\">256</span></span><br><span class=\"line\">W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens, requires_grad=<span class=\"literal\">True</span>) * <span class=\"number\">0.01</span>)</span><br><span class=\"line\">b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=<span class=\"literal\">True</span>))</span><br><span class=\"line\">W2 = nn.Parameter(torch.randn(num_hiddens, num_outputs, requires_grad=<span class=\"literal\">True</span>) * <span class=\"number\">0.01</span>)</span><br><span class=\"line\">b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=<span class=\"literal\">True</span>))</span><br><span class=\"line\">params = [W1, b1, W2, b2]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#2 激活函数</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">relu</span>(<span class=\"params\">X</span>):</span><br><span class=\"line\">    a = torch.zeros_like(X)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.<span class=\"built_in\">max</span>(X, a)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#3 模型</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">net</span>(<span class=\"params\">X</span>):</span><br><span class=\"line\">    X = X.reshape(-<span class=\"number\">1</span>, num_inputs)</span><br><span class=\"line\">    H = relu(X @ W1 + b1) <span class=\"comment\"># @代表矩阵乘法</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> (H @ W2 + b2)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#4 损失函数</span></span><br><span class=\"line\">loss = nn.CrossEntropyLoss(reduction=<span class=\"string\">&#x27;none&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#5 训练</span></span><br><span class=\"line\">num_epochs, lr = <span class=\"number\">10</span>, <span class=\"number\">0.1</span></span><br><span class=\"line\">updater = torch.optim.SGD(params, lr=lr)</span><br><span class=\"line\">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#6 预测</span></span><br><span class=\"line\">d2l.predict_ch3(net, test_iter)</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/2_mlp_realize_files/2_mlp_realize_1_0.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/3_mlp/2_mlp_realize_files/2_mlp_realize_1_0.svg\"></p>\n<p><img src=\"D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/2_mlp_realize_files/2_mlp_realize_1_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/3_mlp/2_mlp_realize_files/2_mlp_realize_1_1.svg\"></p>"},{"title":"3.3 多层感知机简洁实现","date":"2024-02-05T06:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 3 多层感知机简洁实现\n\n\n```python\nimport os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"]= 'True'\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\n#1 模型\nnet = nn.Sequential(\n    nn.Flatten(),\n    nn.Linear(28*28,256),\n    nn.ReLU(),\n    nn.Linear(256,10)\n)\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        nn.init.normal_(m.weight,std=0.01)\nnet.apply(init_weights)\n\n#2 参数\nbatch_size, lr, num_epochs= 256, 0.1, 10\n\n#3 损失\nloss= nn.CrossEntropyLoss(reduction='none')\n\n#4 优化器\ntrainer= torch.optim.SGD(net.parameters(),lr=lr)\n\n#5 训练\ntrain_iter, test_iter= d2l.load_data_fashion_mnist(batch_size)\nd2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\n```\n\n\n  ![svg](D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/3_mlp_realize_simple_files/3_mlp_realize_simple_1_0.svg)\n![](img/deeplearning/code/pytorch/3_mlp/3_mlp_realize_simple_files/3_mlp_realize_simple_1_0.svg)\n    \n\n","source":"_posts/deeplearning/code/pytorch/3_mlp/3_mlp_realize_simple.md","raw":"---\ntitle: 3.3 多层感知机简洁实现\ndate: 2024-2-5 14:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 3 多层感知机简洁实现\n\n\n```python\nimport os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"]= 'True'\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\n#1 模型\nnet = nn.Sequential(\n    nn.Flatten(),\n    nn.Linear(28*28,256),\n    nn.ReLU(),\n    nn.Linear(256,10)\n)\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        nn.init.normal_(m.weight,std=0.01)\nnet.apply(init_weights)\n\n#2 参数\nbatch_size, lr, num_epochs= 256, 0.1, 10\n\n#3 损失\nloss= nn.CrossEntropyLoss(reduction='none')\n\n#4 优化器\ntrainer= torch.optim.SGD(net.parameters(),lr=lr)\n\n#5 训练\ntrain_iter, test_iter= d2l.load_data_fashion_mnist(batch_size)\nd2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\n```\n\n\n  ![svg](D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/3_mlp_realize_simple_files/3_mlp_realize_simple_1_0.svg)\n![](img/deeplearning/code/pytorch/3_mlp/3_mlp_realize_simple_files/3_mlp_realize_simple_1_0.svg)\n    \n\n","slug":"deeplearning/code/pytorch/3_mlp/3_mlp_realize_simple","published":1,"updated":"2024-05-02T08:53:04.058Z","comments":1,"layout":"post","photos":[],"_id":"clvuli6ct0035zsvwf3oah1x5","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"3-多层感知机简洁实现\"><a href=\"#3-多层感知机简洁实现\" class=\"headerlink\" title=\"3 多层感知机简洁实现\"></a>3 多层感知机简洁实现</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>]= <span class=\"string\">&#x27;True&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#1 模型</span></span><br><span class=\"line\">net = nn.Sequential(</span><br><span class=\"line\">    nn.Flatten(),</span><br><span class=\"line\">    nn.Linear(<span class=\"number\">28</span>*<span class=\"number\">28</span>,<span class=\"number\">256</span>),</span><br><span class=\"line\">    nn.ReLU(),</span><br><span class=\"line\">    nn.Linear(<span class=\"number\">256</span>,<span class=\"number\">10</span>)</span><br><span class=\"line\">)</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_weights</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear:</span><br><span class=\"line\">        nn.init.normal_(m.weight,std=<span class=\"number\">0.01</span>)</span><br><span class=\"line\">net.apply(init_weights)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#2 参数</span></span><br><span class=\"line\">batch_size, lr, num_epochs= <span class=\"number\">256</span>, <span class=\"number\">0.1</span>, <span class=\"number\">10</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#3 损失</span></span><br><span class=\"line\">loss= nn.CrossEntropyLoss(reduction=<span class=\"string\">&#x27;none&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#4 优化器</span></span><br><span class=\"line\">trainer= torch.optim.SGD(net.parameters(),lr=lr)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#5 训练</span></span><br><span class=\"line\">train_iter, test_iter= d2l.load_data_fashion_mnist(batch_size)</span><br><span class=\"line\">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure>\n\n\n<p>  <img src=\"D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/3_mlp_realize_simple_files/3_mlp_realize_simple_1_0.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/3_mlp/3_mlp_realize_simple_files/3_mlp_realize_simple_1_0.svg\"></p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"3-多层感知机简洁实现\"><a href=\"#3-多层感知机简洁实现\" class=\"headerlink\" title=\"3 多层感知机简洁实现\"></a>3 多层感知机简洁实现</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>]= <span class=\"string\">&#x27;True&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#1 模型</span></span><br><span class=\"line\">net = nn.Sequential(</span><br><span class=\"line\">    nn.Flatten(),</span><br><span class=\"line\">    nn.Linear(<span class=\"number\">28</span>*<span class=\"number\">28</span>,<span class=\"number\">256</span>),</span><br><span class=\"line\">    nn.ReLU(),</span><br><span class=\"line\">    nn.Linear(<span class=\"number\">256</span>,<span class=\"number\">10</span>)</span><br><span class=\"line\">)</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_weights</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear:</span><br><span class=\"line\">        nn.init.normal_(m.weight,std=<span class=\"number\">0.01</span>)</span><br><span class=\"line\">net.apply(init_weights)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#2 参数</span></span><br><span class=\"line\">batch_size, lr, num_epochs= <span class=\"number\">256</span>, <span class=\"number\">0.1</span>, <span class=\"number\">10</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#3 损失</span></span><br><span class=\"line\">loss= nn.CrossEntropyLoss(reduction=<span class=\"string\">&#x27;none&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#4 优化器</span></span><br><span class=\"line\">trainer= torch.optim.SGD(net.parameters(),lr=lr)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#5 训练</span></span><br><span class=\"line\">train_iter, test_iter= d2l.load_data_fashion_mnist(batch_size)</span><br><span class=\"line\">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure>\n\n\n<p>  <img src=\"D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/3_mlp_realize_simple_files/3_mlp_realize_simple_1_0.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/3_mlp/3_mlp_realize_simple_files/3_mlp_realize_simple_1_0.svg\"></p>"},{"title":"3.4 模型选择、欠拟合和过拟合","date":"2024-02-05T06:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 4 模型选择、欠拟合和过拟合\n- 将模型在训练数据上拟合的比在潜在分布中更接近的现象称为过拟合（overfitting），用于对抗过拟合的技术称为正则化（regularization）。\n- 如果有足够多的神经元、层数和训练迭代周期，模型最终可以在训练集上达到完美的精度，此时测试集的准确性却下降了。\n## 4.1 训练误差和泛化误差\n- 训练误差（training error）：模型在训练数据集上计算得到的误差。\n- 泛化误差（generalization error）：模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。\n### 4.1.1 统计学习理论\n- 假设训练数据和测试数据都是从相同的分布中独立提取的。这通常被称为独立同分布假设。（这意味着对数据进行采样的过程没有进行“记忆”。换句话说，抽取的第2个样本和第3个样本的相关性，并不比抽取的第2个样本和第200万个样本的相关性更强。）\n\n### 4.1.2 模型复杂性\n- 一个模型是否能很好地泛化取决于很多因素。例如，具有更多参数的模型可能被认为更复杂，参数有更大取值范围的模型可能更为复杂。通常对于神经网络，我们认为需要更多训练迭代的模型比较复杂，而需要早停（earlystopping）的模型（即较少训练迭代周期）就不那么复杂。\n- 几个影响模型泛化的因素：\n    - 可调整参数的数量。当可调整参数的数量（有时称为自由度）很大时，模型往往更容易过拟合。\n    - 参数采用的值。当权重的取值范围较大时，模型可能更容易过拟合。\n    - 训练样本的数量。即使模型很简单，也很容易过拟合只包含一两个样本的数据集。而过拟合一个有数百万个样本的数据集则需要一个极其灵活的模型。\n## 4.2 验证集\n- 为了确定候选模型中的最佳模型，我们通常会使用验证集。\n- 在我们确定所有的超参数之前，我们不希望用到测试集。如果我们在模型选择过程中使用测试数据，可能会有过拟合测试数据的风险。我们也不能仅仅依靠训练数据来选择模型，因为我们无法估计训练数据的泛化误差。\n- 验证数据和测试数据的边界模糊，我写的代码中使用的测试集严格来说是验证集。\n\n### 4.2.1 K折交叉验证\n- 当训练数据稀缺时，我们甚至可能无法提供足够的数据来构成一个合适的验证集\n- 原始训练数据被分成K个不重叠的子集。然后执行K次模型训练和验证，每次在K − 1个子集上进行训练，并在剩余的一个子集（在该轮中没有用于训练的子集）上进行验证。最后，通过对K次实验的结果取平均来估计训练和验证误差。\n\n## 4.3 欠拟合与过拟合\n- 欠拟合：训练误差和验证误差都很大，但有一点差距。说明模型表达能力不足，需要更复杂的模型。\n- 过拟合：训练误差和验证误差之间的差距很大。最终关心的是验证误差。\n- 是否过拟合或欠拟合可能取决于模型复杂性和可用训练数据集的大小。\n\n### 4.3.1 模型复杂性\n\n ![svg](D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/4img/1.png) \n![](img/deeplearning/code/pytorch/3_mlp/4img/1.png)\n\n### 4.3.2 数据集大小\n\n- 模型复杂性和数据集大小之间通常存在关系。如果没有足够的数据，简单的模型可能更有用。\\\n\n## 4.4 多项式回归\n- 为了更好地理解模型选择、欠拟合和过拟合，我们可以通过多项式回归来考虑这些问题。\n\n\n\n\n```python\nimport os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\nimport math\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n```\n\n### 4.4.1 生成数据集\n- $$y= 5+1.2x-\\frac{x^2}{2!}+5.6\\frac{x^3}{3!}+\\epsilon， \\quad \\epsilon \\sim N(0,0.1^2)$$\n\n\n```python\n#1 生成100个样本\nmax_degree= 20 #多项式的最大阶数, 用更多的阶数表示更复杂的模型为了演示过拟合，正常用四项就可，仅用两项欠拟合，用20项过拟合\nn_train, n_test= 100,100 #训练集、测试集大小\ntrue_w= np.zeros(max_degree) #分配大量的空间(20,)\ntrue_w[0:4]= np.array([5, 1.2, -3.4, 5.6])#w前4项赋值\nfeatures= np.random.normal(size=(n_train+n_test,1)) #（200,1）\nnp.random.shuffle(features)\npoly_features= np.power(features,np.arange(max_degree).reshape(1,-1)) #一个feature扩展成20项：0次方到19次方（200,20）\nfor i in range(max_degree):\n    poly_features[:,i]/=math.gamma(i+1) #gamma(n)=(n-1)! #一个feature的每一项都除以i的阶乘\nlabels= np.dot(poly_features,true_w) #（200,20）*（20,）=（200,）\nlabels += np.random.normal(scale=0.1,size=labels.shape) #加上噪声\n\n#NumPy ndarray -> tensor\ntrue_w, features, poly_features, labels= [torch.tensor(x, dtype=torch.float32) for x in [true_w, features, poly_features, labels]]\nprint(features[:2], poly_features[:2,:], labels[:2],sep='\\n') #两个x, 两个20项后的x，两个y\n```\n\n    tensor([[ 0.3571],\n            [-1.5272]])\n    tensor([[ 1.0000e+00,  3.5710e-01,  6.3759e-02,  7.5893e-03,  6.7753e-04,\n              4.8388e-05,  2.8799e-06,  1.4691e-07,  6.5578e-09,  2.6019e-10,\n              9.2914e-12,  3.0163e-13,  8.9759e-15,  2.4656e-16,  6.2889e-18,\n              1.4972e-19,  3.3414e-21,  7.0189e-23,  1.3925e-24,  2.6170e-26],\n            [ 1.0000e+00, -1.5272e+00,  1.1661e+00, -5.9360e-01,  2.2663e-01,\n             -6.9220e-02,  1.7618e-02, -3.8437e-03,  7.3374e-04, -1.2450e-04,\n              1.9014e-05, -2.6397e-06,  3.3594e-07, -3.9464e-08,  4.3048e-09,\n             -4.3827e-10,  4.1832e-11, -3.7579e-12,  3.1882e-13, -2.5626e-14]])\n    tensor([ 5.0956, -4.0147])\n\n\n### 4.4.2 训练与测试\n\n\n\n```python\n#2 评估损失\ndef evaluate_loss(net, data_iter, loss): #@save\n    \"\"\"评估给定数据集上模型的损失\"\"\"\n    metric= d2l.Accumulator(2) #损失总和，样本数量\n    for X,y in data_iter:\n        out= net(X)\n        y= y.reshape(out.shape)\n        l= loss(out,y)\n        metric.add(l.sum(),l.numel())\n    return metric[0]/metric[1] #平均损失\n\n#3 训练\ndef train(train_features, test_features, train_labels, test_labels, num_epochs=400):\n    loss= nn.MSELoss(reduction='none')\n    input_shape= train_features.shape[-1]\n    net= nn.Sequential(nn.Linear(input_shape,1,bias=False))\n    batch_size= min(10, train_labels.shape[0])\n    train_iter= d2l.load_array((train_features, train_labels.reshape(-1,1)), batch_size)\n    test_iter= d2l.load_array((test_features, test_labels.reshape(-1,1)), batch_size, is_train=False)\n    trainer= torch.optim.SGD(net.parameters(), lr=0.01)\n    animator= d2l.Animator(xlabel='epoch', ylabel='loss', yscale='log', xlim=[1, num_epochs], ylim=[1e-3, 1e2], legend=['train','test'])\n    for epoch in range(num_epochs):\n        d2l.train_epoch_ch3(net, train_iter, loss, trainer)\n        if epoch==0 or (epoch+1)%20==0:\n            animator.add(epoch+1, (evaluate_loss(net, train_iter, loss),evaluate_loss(net, test_iter, loss)))\n    print('weight:', net[0].weight.data.numpy())\n```\n\n### 4.4.3 三阶多项式函数拟合（正常）\n\n\n\n```python\n# 从多项式特征中选择前4个维度：1，x，x^2，x^3\ntrain(poly_features[:n_train,:4], poly_features[n_train:, :4], labels[:n_train], labels[n_train:])\n```\n\n    weight: [[ 4.98059    1.1953405 -3.389903   5.607252 ]]\n\n\n\n ![svg](D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/4_overfitting_files/4_overfitting_7_1.svg)\n    \n![](img/deeplearning/code/pytorch/3_mlp/4_overfitting_files/4_overfitting_7_1.svg)\n    \n\n\n### 4.4.4 线性函数拟合（欠拟合）\n\n\n\n```python\n# 从多项式特征中选择前2个维度：1，x\ntrain(poly_features[:n_train,:2], poly_features[n_train:, :2], labels[:n_train], labels[n_train:])\n```\n\n    weight: [[2.9687963 4.1943007]]\n\n\n\n![svg](D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/4_overfitting_files/4_overfitting_9_1.svg)\n![](img/deeplearning/code/pytorch/3_mlp/4_overfitting_files/4_overfitting_9_1.svg)\n    \n\n\n### 4.4.5 高阶多项式函数拟合（过拟合）\n\n\n```python\ntrain(poly_features[:n_train, :], poly_features[n_train:, :], labels[:n_train], labels[n_train:], num_epochs=1500)\n```\n\n    weight: [[ 4.96578789e+00  1.30018699e+00 -3.35006833e+00  5.17666531e+00\n      -1.19773000e-01  9.30291474e-01 -9.37245637e-02 -2.67241318e-02\n       1.06728293e-01 -1.89072818e-01 -1.45296961e-01 -1.12053894e-01\n       1.03521936e-01 -3.26722339e-02  1.13504946e-01  3.62291411e-02\n      -4.04332444e-04  3.25103179e-02 -7.38418847e-02 -1.19887017e-01]]\n\n\n\n\n ![svg](D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/4_overfitting_files/4_overfitting_11_1.svg)\n![](img/deeplearning/code/pytorch/3_mlp/4_overfitting_files/4_overfitting_11_1.svg)\n    \n\n","source":"_posts/deeplearning/code/pytorch/3_mlp/4_overfitting.md","raw":"---\ntitle: 3.4 模型选择、欠拟合和过拟合\ndate: 2024-2-5 14:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 4 模型选择、欠拟合和过拟合\n- 将模型在训练数据上拟合的比在潜在分布中更接近的现象称为过拟合（overfitting），用于对抗过拟合的技术称为正则化（regularization）。\n- 如果有足够多的神经元、层数和训练迭代周期，模型最终可以在训练集上达到完美的精度，此时测试集的准确性却下降了。\n## 4.1 训练误差和泛化误差\n- 训练误差（training error）：模型在训练数据集上计算得到的误差。\n- 泛化误差（generalization error）：模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。\n### 4.1.1 统计学习理论\n- 假设训练数据和测试数据都是从相同的分布中独立提取的。这通常被称为独立同分布假设。（这意味着对数据进行采样的过程没有进行“记忆”。换句话说，抽取的第2个样本和第3个样本的相关性，并不比抽取的第2个样本和第200万个样本的相关性更强。）\n\n### 4.1.2 模型复杂性\n- 一个模型是否能很好地泛化取决于很多因素。例如，具有更多参数的模型可能被认为更复杂，参数有更大取值范围的模型可能更为复杂。通常对于神经网络，我们认为需要更多训练迭代的模型比较复杂，而需要早停（earlystopping）的模型（即较少训练迭代周期）就不那么复杂。\n- 几个影响模型泛化的因素：\n    - 可调整参数的数量。当可调整参数的数量（有时称为自由度）很大时，模型往往更容易过拟合。\n    - 参数采用的值。当权重的取值范围较大时，模型可能更容易过拟合。\n    - 训练样本的数量。即使模型很简单，也很容易过拟合只包含一两个样本的数据集。而过拟合一个有数百万个样本的数据集则需要一个极其灵活的模型。\n## 4.2 验证集\n- 为了确定候选模型中的最佳模型，我们通常会使用验证集。\n- 在我们确定所有的超参数之前，我们不希望用到测试集。如果我们在模型选择过程中使用测试数据，可能会有过拟合测试数据的风险。我们也不能仅仅依靠训练数据来选择模型，因为我们无法估计训练数据的泛化误差。\n- 验证数据和测试数据的边界模糊，我写的代码中使用的测试集严格来说是验证集。\n\n### 4.2.1 K折交叉验证\n- 当训练数据稀缺时，我们甚至可能无法提供足够的数据来构成一个合适的验证集\n- 原始训练数据被分成K个不重叠的子集。然后执行K次模型训练和验证，每次在K − 1个子集上进行训练，并在剩余的一个子集（在该轮中没有用于训练的子集）上进行验证。最后，通过对K次实验的结果取平均来估计训练和验证误差。\n\n## 4.3 欠拟合与过拟合\n- 欠拟合：训练误差和验证误差都很大，但有一点差距。说明模型表达能力不足，需要更复杂的模型。\n- 过拟合：训练误差和验证误差之间的差距很大。最终关心的是验证误差。\n- 是否过拟合或欠拟合可能取决于模型复杂性和可用训练数据集的大小。\n\n### 4.3.1 模型复杂性\n\n ![svg](D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/4img/1.png) \n![](img/deeplearning/code/pytorch/3_mlp/4img/1.png)\n\n### 4.3.2 数据集大小\n\n- 模型复杂性和数据集大小之间通常存在关系。如果没有足够的数据，简单的模型可能更有用。\\\n\n## 4.4 多项式回归\n- 为了更好地理解模型选择、欠拟合和过拟合，我们可以通过多项式回归来考虑这些问题。\n\n\n\n\n```python\nimport os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\nimport math\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n```\n\n### 4.4.1 生成数据集\n- $$y= 5+1.2x-\\frac{x^2}{2!}+5.6\\frac{x^3}{3!}+\\epsilon， \\quad \\epsilon \\sim N(0,0.1^2)$$\n\n\n```python\n#1 生成100个样本\nmax_degree= 20 #多项式的最大阶数, 用更多的阶数表示更复杂的模型为了演示过拟合，正常用四项就可，仅用两项欠拟合，用20项过拟合\nn_train, n_test= 100,100 #训练集、测试集大小\ntrue_w= np.zeros(max_degree) #分配大量的空间(20,)\ntrue_w[0:4]= np.array([5, 1.2, -3.4, 5.6])#w前4项赋值\nfeatures= np.random.normal(size=(n_train+n_test,1)) #（200,1）\nnp.random.shuffle(features)\npoly_features= np.power(features,np.arange(max_degree).reshape(1,-1)) #一个feature扩展成20项：0次方到19次方（200,20）\nfor i in range(max_degree):\n    poly_features[:,i]/=math.gamma(i+1) #gamma(n)=(n-1)! #一个feature的每一项都除以i的阶乘\nlabels= np.dot(poly_features,true_w) #（200,20）*（20,）=（200,）\nlabels += np.random.normal(scale=0.1,size=labels.shape) #加上噪声\n\n#NumPy ndarray -> tensor\ntrue_w, features, poly_features, labels= [torch.tensor(x, dtype=torch.float32) for x in [true_w, features, poly_features, labels]]\nprint(features[:2], poly_features[:2,:], labels[:2],sep='\\n') #两个x, 两个20项后的x，两个y\n```\n\n    tensor([[ 0.3571],\n            [-1.5272]])\n    tensor([[ 1.0000e+00,  3.5710e-01,  6.3759e-02,  7.5893e-03,  6.7753e-04,\n              4.8388e-05,  2.8799e-06,  1.4691e-07,  6.5578e-09,  2.6019e-10,\n              9.2914e-12,  3.0163e-13,  8.9759e-15,  2.4656e-16,  6.2889e-18,\n              1.4972e-19,  3.3414e-21,  7.0189e-23,  1.3925e-24,  2.6170e-26],\n            [ 1.0000e+00, -1.5272e+00,  1.1661e+00, -5.9360e-01,  2.2663e-01,\n             -6.9220e-02,  1.7618e-02, -3.8437e-03,  7.3374e-04, -1.2450e-04,\n              1.9014e-05, -2.6397e-06,  3.3594e-07, -3.9464e-08,  4.3048e-09,\n             -4.3827e-10,  4.1832e-11, -3.7579e-12,  3.1882e-13, -2.5626e-14]])\n    tensor([ 5.0956, -4.0147])\n\n\n### 4.4.2 训练与测试\n\n\n\n```python\n#2 评估损失\ndef evaluate_loss(net, data_iter, loss): #@save\n    \"\"\"评估给定数据集上模型的损失\"\"\"\n    metric= d2l.Accumulator(2) #损失总和，样本数量\n    for X,y in data_iter:\n        out= net(X)\n        y= y.reshape(out.shape)\n        l= loss(out,y)\n        metric.add(l.sum(),l.numel())\n    return metric[0]/metric[1] #平均损失\n\n#3 训练\ndef train(train_features, test_features, train_labels, test_labels, num_epochs=400):\n    loss= nn.MSELoss(reduction='none')\n    input_shape= train_features.shape[-1]\n    net= nn.Sequential(nn.Linear(input_shape,1,bias=False))\n    batch_size= min(10, train_labels.shape[0])\n    train_iter= d2l.load_array((train_features, train_labels.reshape(-1,1)), batch_size)\n    test_iter= d2l.load_array((test_features, test_labels.reshape(-1,1)), batch_size, is_train=False)\n    trainer= torch.optim.SGD(net.parameters(), lr=0.01)\n    animator= d2l.Animator(xlabel='epoch', ylabel='loss', yscale='log', xlim=[1, num_epochs], ylim=[1e-3, 1e2], legend=['train','test'])\n    for epoch in range(num_epochs):\n        d2l.train_epoch_ch3(net, train_iter, loss, trainer)\n        if epoch==0 or (epoch+1)%20==0:\n            animator.add(epoch+1, (evaluate_loss(net, train_iter, loss),evaluate_loss(net, test_iter, loss)))\n    print('weight:', net[0].weight.data.numpy())\n```\n\n### 4.4.3 三阶多项式函数拟合（正常）\n\n\n\n```python\n# 从多项式特征中选择前4个维度：1，x，x^2，x^3\ntrain(poly_features[:n_train,:4], poly_features[n_train:, :4], labels[:n_train], labels[n_train:])\n```\n\n    weight: [[ 4.98059    1.1953405 -3.389903   5.607252 ]]\n\n\n\n ![svg](D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/4_overfitting_files/4_overfitting_7_1.svg)\n    \n![](img/deeplearning/code/pytorch/3_mlp/4_overfitting_files/4_overfitting_7_1.svg)\n    \n\n\n### 4.4.4 线性函数拟合（欠拟合）\n\n\n\n```python\n# 从多项式特征中选择前2个维度：1，x\ntrain(poly_features[:n_train,:2], poly_features[n_train:, :2], labels[:n_train], labels[n_train:])\n```\n\n    weight: [[2.9687963 4.1943007]]\n\n\n\n![svg](D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/4_overfitting_files/4_overfitting_9_1.svg)\n![](img/deeplearning/code/pytorch/3_mlp/4_overfitting_files/4_overfitting_9_1.svg)\n    \n\n\n### 4.4.5 高阶多项式函数拟合（过拟合）\n\n\n```python\ntrain(poly_features[:n_train, :], poly_features[n_train:, :], labels[:n_train], labels[n_train:], num_epochs=1500)\n```\n\n    weight: [[ 4.96578789e+00  1.30018699e+00 -3.35006833e+00  5.17666531e+00\n      -1.19773000e-01  9.30291474e-01 -9.37245637e-02 -2.67241318e-02\n       1.06728293e-01 -1.89072818e-01 -1.45296961e-01 -1.12053894e-01\n       1.03521936e-01 -3.26722339e-02  1.13504946e-01  3.62291411e-02\n      -4.04332444e-04  3.25103179e-02 -7.38418847e-02 -1.19887017e-01]]\n\n\n\n\n ![svg](D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/4_overfitting_files/4_overfitting_11_1.svg)\n![](img/deeplearning/code/pytorch/3_mlp/4_overfitting_files/4_overfitting_11_1.svg)\n    \n\n","slug":"deeplearning/code/pytorch/3_mlp/4_overfitting","published":1,"updated":"2024-05-02T08:53:13.928Z","comments":1,"layout":"post","photos":[],"_id":"clvuli6cu0038zsvw3bm36hfq","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"4-模型选择、欠拟合和过拟合\"><a href=\"#4-模型选择、欠拟合和过拟合\" class=\"headerlink\" title=\"4 模型选择、欠拟合和过拟合\"></a>4 模型选择、欠拟合和过拟合</h1><ul>\n<li>将模型在训练数据上拟合的比在潜在分布中更接近的现象称为过拟合（overfitting），用于对抗过拟合的技术称为正则化（regularization）。</li>\n<li>如果有足够多的神经元、层数和训练迭代周期，模型最终可以在训练集上达到完美的精度，此时测试集的准确性却下降了。</li>\n</ul>\n<h2 id=\"4-1-训练误差和泛化误差\"><a href=\"#4-1-训练误差和泛化误差\" class=\"headerlink\" title=\"4.1 训练误差和泛化误差\"></a>4.1 训练误差和泛化误差</h2><ul>\n<li>训练误差（training error）：模型在训练数据集上计算得到的误差。</li>\n<li>泛化误差（generalization error）：模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。</li>\n</ul>\n<h3 id=\"4-1-1-统计学习理论\"><a href=\"#4-1-1-统计学习理论\" class=\"headerlink\" title=\"4.1.1 统计学习理论\"></a>4.1.1 统计学习理论</h3><ul>\n<li>假设训练数据和测试数据都是从相同的分布中独立提取的。这通常被称为独立同分布假设。（这意味着对数据进行采样的过程没有进行“记忆”。换句话说，抽取的第2个样本和第3个样本的相关性，并不比抽取的第2个样本和第200万个样本的相关性更强。）</li>\n</ul>\n<h3 id=\"4-1-2-模型复杂性\"><a href=\"#4-1-2-模型复杂性\" class=\"headerlink\" title=\"4.1.2 模型复杂性\"></a>4.1.2 模型复杂性</h3><ul>\n<li>一个模型是否能很好地泛化取决于很多因素。例如，具有更多参数的模型可能被认为更复杂，参数有更大取值范围的模型可能更为复杂。通常对于神经网络，我们认为需要更多训练迭代的模型比较复杂，而需要早停（earlystopping）的模型（即较少训练迭代周期）就不那么复杂。</li>\n<li>几个影响模型泛化的因素：<ul>\n<li>可调整参数的数量。当可调整参数的数量（有时称为自由度）很大时，模型往往更容易过拟合。</li>\n<li>参数采用的值。当权重的取值范围较大时，模型可能更容易过拟合。</li>\n<li>训练样本的数量。即使模型很简单，也很容易过拟合只包含一两个样本的数据集。而过拟合一个有数百万个样本的数据集则需要一个极其灵活的模型。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"4-2-验证集\"><a href=\"#4-2-验证集\" class=\"headerlink\" title=\"4.2 验证集\"></a>4.2 验证集</h2><ul>\n<li>为了确定候选模型中的最佳模型，我们通常会使用验证集。</li>\n<li>在我们确定所有的超参数之前，我们不希望用到测试集。如果我们在模型选择过程中使用测试数据，可能会有过拟合测试数据的风险。我们也不能仅仅依靠训练数据来选择模型，因为我们无法估计训练数据的泛化误差。</li>\n<li>验证数据和测试数据的边界模糊，我写的代码中使用的测试集严格来说是验证集。</li>\n</ul>\n<h3 id=\"4-2-1-K折交叉验证\"><a href=\"#4-2-1-K折交叉验证\" class=\"headerlink\" title=\"4.2.1 K折交叉验证\"></a>4.2.1 K折交叉验证</h3><ul>\n<li>当训练数据稀缺时，我们甚至可能无法提供足够的数据来构成一个合适的验证集</li>\n<li>原始训练数据被分成K个不重叠的子集。然后执行K次模型训练和验证，每次在K − 1个子集上进行训练，并在剩余的一个子集（在该轮中没有用于训练的子集）上进行验证。最后，通过对K次实验的结果取平均来估计训练和验证误差。</li>\n</ul>\n<h2 id=\"4-3-欠拟合与过拟合\"><a href=\"#4-3-欠拟合与过拟合\" class=\"headerlink\" title=\"4.3 欠拟合与过拟合\"></a>4.3 欠拟合与过拟合</h2><ul>\n<li>欠拟合：训练误差和验证误差都很大，但有一点差距。说明模型表达能力不足，需要更复杂的模型。</li>\n<li>过拟合：训练误差和验证误差之间的差距很大。最终关心的是验证误差。</li>\n<li>是否过拟合或欠拟合可能取决于模型复杂性和可用训练数据集的大小。</li>\n</ul>\n<h3 id=\"4-3-1-模型复杂性\"><a href=\"#4-3-1-模型复杂性\" class=\"headerlink\" title=\"4.3.1 模型复杂性\"></a>4.3.1 模型复杂性</h3><p> <img src=\"D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/4img/1.png\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/3_mlp/4img/1.png\"></p>\n<h3 id=\"4-3-2-数据集大小\"><a href=\"#4-3-2-数据集大小\" class=\"headerlink\" title=\"4.3.2 数据集大小\"></a>4.3.2 数据集大小</h3><ul>\n<li>模型复杂性和数据集大小之间通常存在关系。如果没有足够的数据，简单的模型可能更有用。\\</li>\n</ul>\n<h2 id=\"4-4-多项式回归\"><a href=\"#4-4-多项式回归\" class=\"headerlink\" title=\"4.4 多项式回归\"></a>4.4 多项式回归</h2><ul>\n<li>为了更好地理解模型选择、欠拟合和过拟合，我们可以通过多项式回归来考虑这些问题。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>] = <span class=\"string\">&quot;TRUE&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> math</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"4-4-1-生成数据集\"><a href=\"#4-4-1-生成数据集\" class=\"headerlink\" title=\"4.4.1 生成数据集\"></a>4.4.1 生成数据集</h3><ul>\n<li>$$y&#x3D; 5+1.2x-\\frac{x^2}{2!}+5.6\\frac{x^3}{3!}+\\epsilon， \\quad \\epsilon \\sim N(0,0.1^2)$$</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#1 生成100个样本</span></span><br><span class=\"line\">max_degree= <span class=\"number\">20</span> <span class=\"comment\">#多项式的最大阶数, 用更多的阶数表示更复杂的模型为了演示过拟合，正常用四项就可，仅用两项欠拟合，用20项过拟合</span></span><br><span class=\"line\">n_train, n_test= <span class=\"number\">100</span>,<span class=\"number\">100</span> <span class=\"comment\">#训练集、测试集大小</span></span><br><span class=\"line\">true_w= np.zeros(max_degree) <span class=\"comment\">#分配大量的空间(20,)</span></span><br><span class=\"line\">true_w[<span class=\"number\">0</span>:<span class=\"number\">4</span>]= np.array([<span class=\"number\">5</span>, <span class=\"number\">1.2</span>, -<span class=\"number\">3.4</span>, <span class=\"number\">5.6</span>])<span class=\"comment\">#w前4项赋值</span></span><br><span class=\"line\">features= np.random.normal(size=(n_train+n_test,<span class=\"number\">1</span>)) <span class=\"comment\">#（200,1）</span></span><br><span class=\"line\">np.random.shuffle(features)</span><br><span class=\"line\">poly_features= np.power(features,np.arange(max_degree).reshape(<span class=\"number\">1</span>,-<span class=\"number\">1</span>)) <span class=\"comment\">#一个feature扩展成20项：0次方到19次方（200,20）</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(max_degree):</span><br><span class=\"line\">    poly_features[:,i]/=math.gamma(i+<span class=\"number\">1</span>) <span class=\"comment\">#gamma(n)=(n-1)! #一个feature的每一项都除以i的阶乘</span></span><br><span class=\"line\">labels= np.dot(poly_features,true_w) <span class=\"comment\">#（200,20）*（20,）=（200,）</span></span><br><span class=\"line\">labels += np.random.normal(scale=<span class=\"number\">0.1</span>,size=labels.shape) <span class=\"comment\">#加上噪声</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#NumPy ndarray -&gt; tensor</span></span><br><span class=\"line\">true_w, features, poly_features, labels= [torch.tensor(x, dtype=torch.float32) <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> [true_w, features, poly_features, labels]]</span><br><span class=\"line\"><span class=\"built_in\">print</span>(features[:<span class=\"number\">2</span>], poly_features[:<span class=\"number\">2</span>,:], labels[:<span class=\"number\">2</span>],sep=<span class=\"string\">&#x27;\\n&#x27;</span>) <span class=\"comment\">#两个x, 两个20项后的x，两个y</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[ 0.3571],\n        [-1.5272]])\ntensor([[ 1.0000e+00,  3.5710e-01,  6.3759e-02,  7.5893e-03,  6.7753e-04,\n          4.8388e-05,  2.8799e-06,  1.4691e-07,  6.5578e-09,  2.6019e-10,\n          9.2914e-12,  3.0163e-13,  8.9759e-15,  2.4656e-16,  6.2889e-18,\n          1.4972e-19,  3.3414e-21,  7.0189e-23,  1.3925e-24,  2.6170e-26],\n        [ 1.0000e+00, -1.5272e+00,  1.1661e+00, -5.9360e-01,  2.2663e-01,\n         -6.9220e-02,  1.7618e-02, -3.8437e-03,  7.3374e-04, -1.2450e-04,\n          1.9014e-05, -2.6397e-06,  3.3594e-07, -3.9464e-08,  4.3048e-09,\n         -4.3827e-10,  4.1832e-11, -3.7579e-12,  3.1882e-13, -2.5626e-14]])\ntensor([ 5.0956, -4.0147])\n</code></pre>\n<h3 id=\"4-4-2-训练与测试\"><a href=\"#4-4-2-训练与测试\" class=\"headerlink\" title=\"4.4.2 训练与测试\"></a>4.4.2 训练与测试</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#2 评估损失</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">evaluate_loss</span>(<span class=\"params\">net, data_iter, loss</span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;评估给定数据集上模型的损失&quot;&quot;&quot;</span></span><br><span class=\"line\">    metric= d2l.Accumulator(<span class=\"number\">2</span>) <span class=\"comment\">#损失总和，样本数量</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> X,y <span class=\"keyword\">in</span> data_iter:</span><br><span class=\"line\">        out= net(X)</span><br><span class=\"line\">        y= y.reshape(out.shape)</span><br><span class=\"line\">        l= loss(out,y)</span><br><span class=\"line\">        metric.add(l.<span class=\"built_in\">sum</span>(),l.numel())</span><br><span class=\"line\">    <span class=\"keyword\">return</span> metric[<span class=\"number\">0</span>]/metric[<span class=\"number\">1</span>] <span class=\"comment\">#平均损失</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#3 训练</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train</span>(<span class=\"params\">train_features, test_features, train_labels, test_labels, num_epochs=<span class=\"number\">400</span></span>):</span><br><span class=\"line\">    loss= nn.MSELoss(reduction=<span class=\"string\">&#x27;none&#x27;</span>)</span><br><span class=\"line\">    input_shape= train_features.shape[-<span class=\"number\">1</span>]</span><br><span class=\"line\">    net= nn.Sequential(nn.Linear(input_shape,<span class=\"number\">1</span>,bias=<span class=\"literal\">False</span>))</span><br><span class=\"line\">    batch_size= <span class=\"built_in\">min</span>(<span class=\"number\">10</span>, train_labels.shape[<span class=\"number\">0</span>])</span><br><span class=\"line\">    train_iter= d2l.load_array((train_features, train_labels.reshape(-<span class=\"number\">1</span>,<span class=\"number\">1</span>)), batch_size)</span><br><span class=\"line\">    test_iter= d2l.load_array((test_features, test_labels.reshape(-<span class=\"number\">1</span>,<span class=\"number\">1</span>)), batch_size, is_train=<span class=\"literal\">False</span>)</span><br><span class=\"line\">    trainer= torch.optim.SGD(net.parameters(), lr=<span class=\"number\">0.01</span>)</span><br><span class=\"line\">    animator= d2l.Animator(xlabel=<span class=\"string\">&#x27;epoch&#x27;</span>, ylabel=<span class=\"string\">&#x27;loss&#x27;</span>, yscale=<span class=\"string\">&#x27;log&#x27;</span>, xlim=[<span class=\"number\">1</span>, num_epochs], ylim=[<span class=\"number\">1e-3</span>, <span class=\"number\">1e2</span>], legend=[<span class=\"string\">&#x27;train&#x27;</span>,<span class=\"string\">&#x27;test&#x27;</span>])</span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        d2l.train_epoch_ch3(net, train_iter, loss, trainer)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> epoch==<span class=\"number\">0</span> <span class=\"keyword\">or</span> (epoch+<span class=\"number\">1</span>)%<span class=\"number\">20</span>==<span class=\"number\">0</span>:</span><br><span class=\"line\">            animator.add(epoch+<span class=\"number\">1</span>, (evaluate_loss(net, train_iter, loss),evaluate_loss(net, test_iter, loss)))</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:&#x27;</span>, net[<span class=\"number\">0</span>].weight.data.numpy())</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"4-4-3-三阶多项式函数拟合（正常）\"><a href=\"#4-4-3-三阶多项式函数拟合（正常）\" class=\"headerlink\" title=\"4.4.3 三阶多项式函数拟合（正常）\"></a>4.4.3 三阶多项式函数拟合（正常）</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 从多项式特征中选择前4个维度：1，x，x^2，x^3</span></span><br><span class=\"line\">train(poly_features[:n_train,:<span class=\"number\">4</span>], poly_features[n_train:, :<span class=\"number\">4</span>], labels[:n_train], labels[n_train:])</span><br></pre></td></tr></table></figure>\n\n<pre><code>weight: [[ 4.98059    1.1953405 -3.389903   5.607252 ]]\n</code></pre>\n<p> <img src=\"D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/4_overfitting_files/4_overfitting_7_1.svg\" alt=\"svg\"></p>\n<p><img src=\"/img/deeplearning/code/pytorch/3_mlp/4_overfitting_files/4_overfitting_7_1.svg\"></p>\n<h3 id=\"4-4-4-线性函数拟合（欠拟合）\"><a href=\"#4-4-4-线性函数拟合（欠拟合）\" class=\"headerlink\" title=\"4.4.4 线性函数拟合（欠拟合）\"></a>4.4.4 线性函数拟合（欠拟合）</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 从多项式特征中选择前2个维度：1，x</span></span><br><span class=\"line\">train(poly_features[:n_train,:<span class=\"number\">2</span>], poly_features[n_train:, :<span class=\"number\">2</span>], labels[:n_train], labels[n_train:])</span><br></pre></td></tr></table></figure>\n\n<pre><code>weight: [[2.9687963 4.1943007]]\n</code></pre>\n<p><img src=\"D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/4_overfitting_files/4_overfitting_9_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/3_mlp/4_overfitting_files/4_overfitting_9_1.svg\"></p>\n<h3 id=\"4-4-5-高阶多项式函数拟合（过拟合）\"><a href=\"#4-4-5-高阶多项式函数拟合（过拟合）\" class=\"headerlink\" title=\"4.4.5 高阶多项式函数拟合（过拟合）\"></a>4.4.5 高阶多项式函数拟合（过拟合）</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train(poly_features[:n_train, :], poly_features[n_train:, :], labels[:n_train], labels[n_train:], num_epochs=<span class=\"number\">1500</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>weight: [[ 4.96578789e+00  1.30018699e+00 -3.35006833e+00  5.17666531e+00\n  -1.19773000e-01  9.30291474e-01 -9.37245637e-02 -2.67241318e-02\n   1.06728293e-01 -1.89072818e-01 -1.45296961e-01 -1.12053894e-01\n   1.03521936e-01 -3.26722339e-02  1.13504946e-01  3.62291411e-02\n  -4.04332444e-04  3.25103179e-02 -7.38418847e-02 -1.19887017e-01]]\n</code></pre>\n<p> <img src=\"D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/4_overfitting_files/4_overfitting_11_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/3_mlp/4_overfitting_files/4_overfitting_11_1.svg\"></p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"4-模型选择、欠拟合和过拟合\"><a href=\"#4-模型选择、欠拟合和过拟合\" class=\"headerlink\" title=\"4 模型选择、欠拟合和过拟合\"></a>4 模型选择、欠拟合和过拟合</h1><ul>\n<li>将模型在训练数据上拟合的比在潜在分布中更接近的现象称为过拟合（overfitting），用于对抗过拟合的技术称为正则化（regularization）。</li>\n<li>如果有足够多的神经元、层数和训练迭代周期，模型最终可以在训练集上达到完美的精度，此时测试集的准确性却下降了。</li>\n</ul>\n<h2 id=\"4-1-训练误差和泛化误差\"><a href=\"#4-1-训练误差和泛化误差\" class=\"headerlink\" title=\"4.1 训练误差和泛化误差\"></a>4.1 训练误差和泛化误差</h2><ul>\n<li>训练误差（training error）：模型在训练数据集上计算得到的误差。</li>\n<li>泛化误差（generalization error）：模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。</li>\n</ul>\n<h3 id=\"4-1-1-统计学习理论\"><a href=\"#4-1-1-统计学习理论\" class=\"headerlink\" title=\"4.1.1 统计学习理论\"></a>4.1.1 统计学习理论</h3><ul>\n<li>假设训练数据和测试数据都是从相同的分布中独立提取的。这通常被称为独立同分布假设。（这意味着对数据进行采样的过程没有进行“记忆”。换句话说，抽取的第2个样本和第3个样本的相关性，并不比抽取的第2个样本和第200万个样本的相关性更强。）</li>\n</ul>\n<h3 id=\"4-1-2-模型复杂性\"><a href=\"#4-1-2-模型复杂性\" class=\"headerlink\" title=\"4.1.2 模型复杂性\"></a>4.1.2 模型复杂性</h3><ul>\n<li>一个模型是否能很好地泛化取决于很多因素。例如，具有更多参数的模型可能被认为更复杂，参数有更大取值范围的模型可能更为复杂。通常对于神经网络，我们认为需要更多训练迭代的模型比较复杂，而需要早停（earlystopping）的模型（即较少训练迭代周期）就不那么复杂。</li>\n<li>几个影响模型泛化的因素：<ul>\n<li>可调整参数的数量。当可调整参数的数量（有时称为自由度）很大时，模型往往更容易过拟合。</li>\n<li>参数采用的值。当权重的取值范围较大时，模型可能更容易过拟合。</li>\n<li>训练样本的数量。即使模型很简单，也很容易过拟合只包含一两个样本的数据集。而过拟合一个有数百万个样本的数据集则需要一个极其灵活的模型。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"4-2-验证集\"><a href=\"#4-2-验证集\" class=\"headerlink\" title=\"4.2 验证集\"></a>4.2 验证集</h2><ul>\n<li>为了确定候选模型中的最佳模型，我们通常会使用验证集。</li>\n<li>在我们确定所有的超参数之前，我们不希望用到测试集。如果我们在模型选择过程中使用测试数据，可能会有过拟合测试数据的风险。我们也不能仅仅依靠训练数据来选择模型，因为我们无法估计训练数据的泛化误差。</li>\n<li>验证数据和测试数据的边界模糊，我写的代码中使用的测试集严格来说是验证集。</li>\n</ul>\n<h3 id=\"4-2-1-K折交叉验证\"><a href=\"#4-2-1-K折交叉验证\" class=\"headerlink\" title=\"4.2.1 K折交叉验证\"></a>4.2.1 K折交叉验证</h3><ul>\n<li>当训练数据稀缺时，我们甚至可能无法提供足够的数据来构成一个合适的验证集</li>\n<li>原始训练数据被分成K个不重叠的子集。然后执行K次模型训练和验证，每次在K − 1个子集上进行训练，并在剩余的一个子集（在该轮中没有用于训练的子集）上进行验证。最后，通过对K次实验的结果取平均来估计训练和验证误差。</li>\n</ul>\n<h2 id=\"4-3-欠拟合与过拟合\"><a href=\"#4-3-欠拟合与过拟合\" class=\"headerlink\" title=\"4.3 欠拟合与过拟合\"></a>4.3 欠拟合与过拟合</h2><ul>\n<li>欠拟合：训练误差和验证误差都很大，但有一点差距。说明模型表达能力不足，需要更复杂的模型。</li>\n<li>过拟合：训练误差和验证误差之间的差距很大。最终关心的是验证误差。</li>\n<li>是否过拟合或欠拟合可能取决于模型复杂性和可用训练数据集的大小。</li>\n</ul>\n<h3 id=\"4-3-1-模型复杂性\"><a href=\"#4-3-1-模型复杂性\" class=\"headerlink\" title=\"4.3.1 模型复杂性\"></a>4.3.1 模型复杂性</h3><p> <img src=\"D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/4img/1.png\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/3_mlp/4img/1.png\"></p>\n<h3 id=\"4-3-2-数据集大小\"><a href=\"#4-3-2-数据集大小\" class=\"headerlink\" title=\"4.3.2 数据集大小\"></a>4.3.2 数据集大小</h3><ul>\n<li>模型复杂性和数据集大小之间通常存在关系。如果没有足够的数据，简单的模型可能更有用。\\</li>\n</ul>\n<h2 id=\"4-4-多项式回归\"><a href=\"#4-4-多项式回归\" class=\"headerlink\" title=\"4.4 多项式回归\"></a>4.4 多项式回归</h2><ul>\n<li>为了更好地理解模型选择、欠拟合和过拟合，我们可以通过多项式回归来考虑这些问题。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>] = <span class=\"string\">&quot;TRUE&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> math</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"4-4-1-生成数据集\"><a href=\"#4-4-1-生成数据集\" class=\"headerlink\" title=\"4.4.1 生成数据集\"></a>4.4.1 生成数据集</h3><ul>\n<li>$$y&#x3D; 5+1.2x-\\frac{x^2}{2!}+5.6\\frac{x^3}{3!}+\\epsilon， \\quad \\epsilon \\sim N(0,0.1^2)$$</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#1 生成100个样本</span></span><br><span class=\"line\">max_degree= <span class=\"number\">20</span> <span class=\"comment\">#多项式的最大阶数, 用更多的阶数表示更复杂的模型为了演示过拟合，正常用四项就可，仅用两项欠拟合，用20项过拟合</span></span><br><span class=\"line\">n_train, n_test= <span class=\"number\">100</span>,<span class=\"number\">100</span> <span class=\"comment\">#训练集、测试集大小</span></span><br><span class=\"line\">true_w= np.zeros(max_degree) <span class=\"comment\">#分配大量的空间(20,)</span></span><br><span class=\"line\">true_w[<span class=\"number\">0</span>:<span class=\"number\">4</span>]= np.array([<span class=\"number\">5</span>, <span class=\"number\">1.2</span>, -<span class=\"number\">3.4</span>, <span class=\"number\">5.6</span>])<span class=\"comment\">#w前4项赋值</span></span><br><span class=\"line\">features= np.random.normal(size=(n_train+n_test,<span class=\"number\">1</span>)) <span class=\"comment\">#（200,1）</span></span><br><span class=\"line\">np.random.shuffle(features)</span><br><span class=\"line\">poly_features= np.power(features,np.arange(max_degree).reshape(<span class=\"number\">1</span>,-<span class=\"number\">1</span>)) <span class=\"comment\">#一个feature扩展成20项：0次方到19次方（200,20）</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(max_degree):</span><br><span class=\"line\">    poly_features[:,i]/=math.gamma(i+<span class=\"number\">1</span>) <span class=\"comment\">#gamma(n)=(n-1)! #一个feature的每一项都除以i的阶乘</span></span><br><span class=\"line\">labels= np.dot(poly_features,true_w) <span class=\"comment\">#（200,20）*（20,）=（200,）</span></span><br><span class=\"line\">labels += np.random.normal(scale=<span class=\"number\">0.1</span>,size=labels.shape) <span class=\"comment\">#加上噪声</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#NumPy ndarray -&gt; tensor</span></span><br><span class=\"line\">true_w, features, poly_features, labels= [torch.tensor(x, dtype=torch.float32) <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> [true_w, features, poly_features, labels]]</span><br><span class=\"line\"><span class=\"built_in\">print</span>(features[:<span class=\"number\">2</span>], poly_features[:<span class=\"number\">2</span>,:], labels[:<span class=\"number\">2</span>],sep=<span class=\"string\">&#x27;\\n&#x27;</span>) <span class=\"comment\">#两个x, 两个20项后的x，两个y</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[ 0.3571],\n        [-1.5272]])\ntensor([[ 1.0000e+00,  3.5710e-01,  6.3759e-02,  7.5893e-03,  6.7753e-04,\n          4.8388e-05,  2.8799e-06,  1.4691e-07,  6.5578e-09,  2.6019e-10,\n          9.2914e-12,  3.0163e-13,  8.9759e-15,  2.4656e-16,  6.2889e-18,\n          1.4972e-19,  3.3414e-21,  7.0189e-23,  1.3925e-24,  2.6170e-26],\n        [ 1.0000e+00, -1.5272e+00,  1.1661e+00, -5.9360e-01,  2.2663e-01,\n         -6.9220e-02,  1.7618e-02, -3.8437e-03,  7.3374e-04, -1.2450e-04,\n          1.9014e-05, -2.6397e-06,  3.3594e-07, -3.9464e-08,  4.3048e-09,\n         -4.3827e-10,  4.1832e-11, -3.7579e-12,  3.1882e-13, -2.5626e-14]])\ntensor([ 5.0956, -4.0147])\n</code></pre>\n<h3 id=\"4-4-2-训练与测试\"><a href=\"#4-4-2-训练与测试\" class=\"headerlink\" title=\"4.4.2 训练与测试\"></a>4.4.2 训练与测试</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#2 评估损失</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">evaluate_loss</span>(<span class=\"params\">net, data_iter, loss</span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;评估给定数据集上模型的损失&quot;&quot;&quot;</span></span><br><span class=\"line\">    metric= d2l.Accumulator(<span class=\"number\">2</span>) <span class=\"comment\">#损失总和，样本数量</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> X,y <span class=\"keyword\">in</span> data_iter:</span><br><span class=\"line\">        out= net(X)</span><br><span class=\"line\">        y= y.reshape(out.shape)</span><br><span class=\"line\">        l= loss(out,y)</span><br><span class=\"line\">        metric.add(l.<span class=\"built_in\">sum</span>(),l.numel())</span><br><span class=\"line\">    <span class=\"keyword\">return</span> metric[<span class=\"number\">0</span>]/metric[<span class=\"number\">1</span>] <span class=\"comment\">#平均损失</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#3 训练</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train</span>(<span class=\"params\">train_features, test_features, train_labels, test_labels, num_epochs=<span class=\"number\">400</span></span>):</span><br><span class=\"line\">    loss= nn.MSELoss(reduction=<span class=\"string\">&#x27;none&#x27;</span>)</span><br><span class=\"line\">    input_shape= train_features.shape[-<span class=\"number\">1</span>]</span><br><span class=\"line\">    net= nn.Sequential(nn.Linear(input_shape,<span class=\"number\">1</span>,bias=<span class=\"literal\">False</span>))</span><br><span class=\"line\">    batch_size= <span class=\"built_in\">min</span>(<span class=\"number\">10</span>, train_labels.shape[<span class=\"number\">0</span>])</span><br><span class=\"line\">    train_iter= d2l.load_array((train_features, train_labels.reshape(-<span class=\"number\">1</span>,<span class=\"number\">1</span>)), batch_size)</span><br><span class=\"line\">    test_iter= d2l.load_array((test_features, test_labels.reshape(-<span class=\"number\">1</span>,<span class=\"number\">1</span>)), batch_size, is_train=<span class=\"literal\">False</span>)</span><br><span class=\"line\">    trainer= torch.optim.SGD(net.parameters(), lr=<span class=\"number\">0.01</span>)</span><br><span class=\"line\">    animator= d2l.Animator(xlabel=<span class=\"string\">&#x27;epoch&#x27;</span>, ylabel=<span class=\"string\">&#x27;loss&#x27;</span>, yscale=<span class=\"string\">&#x27;log&#x27;</span>, xlim=[<span class=\"number\">1</span>, num_epochs], ylim=[<span class=\"number\">1e-3</span>, <span class=\"number\">1e2</span>], legend=[<span class=\"string\">&#x27;train&#x27;</span>,<span class=\"string\">&#x27;test&#x27;</span>])</span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        d2l.train_epoch_ch3(net, train_iter, loss, trainer)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> epoch==<span class=\"number\">0</span> <span class=\"keyword\">or</span> (epoch+<span class=\"number\">1</span>)%<span class=\"number\">20</span>==<span class=\"number\">0</span>:</span><br><span class=\"line\">            animator.add(epoch+<span class=\"number\">1</span>, (evaluate_loss(net, train_iter, loss),evaluate_loss(net, test_iter, loss)))</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:&#x27;</span>, net[<span class=\"number\">0</span>].weight.data.numpy())</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"4-4-3-三阶多项式函数拟合（正常）\"><a href=\"#4-4-3-三阶多项式函数拟合（正常）\" class=\"headerlink\" title=\"4.4.3 三阶多项式函数拟合（正常）\"></a>4.4.3 三阶多项式函数拟合（正常）</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 从多项式特征中选择前4个维度：1，x，x^2，x^3</span></span><br><span class=\"line\">train(poly_features[:n_train,:<span class=\"number\">4</span>], poly_features[n_train:, :<span class=\"number\">4</span>], labels[:n_train], labels[n_train:])</span><br></pre></td></tr></table></figure>\n\n<pre><code>weight: [[ 4.98059    1.1953405 -3.389903   5.607252 ]]\n</code></pre>\n<p> <img src=\"D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/4_overfitting_files/4_overfitting_7_1.svg\" alt=\"svg\"></p>\n<p><img src=\"/img/deeplearning/code/pytorch/3_mlp/4_overfitting_files/4_overfitting_7_1.svg\"></p>\n<h3 id=\"4-4-4-线性函数拟合（欠拟合）\"><a href=\"#4-4-4-线性函数拟合（欠拟合）\" class=\"headerlink\" title=\"4.4.4 线性函数拟合（欠拟合）\"></a>4.4.4 线性函数拟合（欠拟合）</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 从多项式特征中选择前2个维度：1，x</span></span><br><span class=\"line\">train(poly_features[:n_train,:<span class=\"number\">2</span>], poly_features[n_train:, :<span class=\"number\">2</span>], labels[:n_train], labels[n_train:])</span><br></pre></td></tr></table></figure>\n\n<pre><code>weight: [[2.9687963 4.1943007]]\n</code></pre>\n<p><img src=\"D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/4_overfitting_files/4_overfitting_9_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/3_mlp/4_overfitting_files/4_overfitting_9_1.svg\"></p>\n<h3 id=\"4-4-5-高阶多项式函数拟合（过拟合）\"><a href=\"#4-4-5-高阶多项式函数拟合（过拟合）\" class=\"headerlink\" title=\"4.4.5 高阶多项式函数拟合（过拟合）\"></a>4.4.5 高阶多项式函数拟合（过拟合）</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train(poly_features[:n_train, :], poly_features[n_train:, :], labels[:n_train], labels[n_train:], num_epochs=<span class=\"number\">1500</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>weight: [[ 4.96578789e+00  1.30018699e+00 -3.35006833e+00  5.17666531e+00\n  -1.19773000e-01  9.30291474e-01 -9.37245637e-02 -2.67241318e-02\n   1.06728293e-01 -1.89072818e-01 -1.45296961e-01 -1.12053894e-01\n   1.03521936e-01 -3.26722339e-02  1.13504946e-01  3.62291411e-02\n  -4.04332444e-04  3.25103179e-02 -7.38418847e-02 -1.19887017e-01]]\n</code></pre>\n<p> <img src=\"D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/4_overfitting_files/4_overfitting_11_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/3_mlp/4_overfitting_files/4_overfitting_11_1.svg\"></p>"},{"title":"3.6 暂退法","date":"2024-02-07T06:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 6 暂退法\n- 模型简单性：\n    - 维度小\n    - 参数泛化\n    - 平滑性\n    \n- 平滑性：函数不应该对其输入的微小变化敏感。例如，当我们对图像进行分类时，我们预计向像素添加一些随机噪声应该是基本无影响的。具有输入噪声的训练等价于Tikhonov正则化。基于此，提出暂退法：在计算后续层之前向网络的每一层注入噪声。因为当训练一个有多层的深层网络时，注入噪声只会在输入‐输出映射上增强平滑性。从表面上看是在训练过程中丢弃（dropout）一些神经元。\n\n- 需要说明的是，暂退法的原始论文提到了一个关于有性繁殖的类比：神经网络过拟合与每一层都依赖于前一层激活值相关，称这种情况为“共适应性”。作者认为，暂退法会破坏共适应性，就像有性生殖会破坏共适应的基因一样。???  \n\n- 做法：\n\n  <span style=\"display:block\">\n  $h ^\\prime =\\begin{cases}0 &p \\\\ \\frac{h}{1-p} &1-p \\\\ \\end{cases}.$ \n\n  </span>\n\n- 这样，输出层的计算不能过度依赖于h1, . . . , h5的任何一个元素。通常，我们在测试时不用暂退法。(一些研究人员在测试时使用暂退法，用于估计神经网络预测的“不确定性”：如果通过许多不同的暂退法遮盖后得到的预测结果都是一致的，那么我们可以说网络发挥更稳定。)\n ![svg](D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/6img/1.png)\n![](img/deeplearning/code/pytorch/3_mlp/6img/1.png)\n\n## 6.1 代码实现\n- 1）从U[0,1]抽样\n- 2）保留大于p的点/(1-p)\n\n\n```python\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\n#1 定义dropout\ndef dropout_layer(X, dropout):\n    assert 0 <= dropout <= 1\n    if dropout == 1:\n        return torch.zeros_like(X)\n    if dropout == 0:\n        return X\n    mask = (torch.rand(X.shape) > dropout).float() #大于p的元素->1\n    return mask * X / (1.0 - dropout)\n\n#2 测试dropout\nX = torch.arange(16, dtype=torch.float32).reshape((2, 8))\nprint(X)\nprint(dropout_layer(X, 0.))\nprint(dropout_layer(X, 0.5))\nprint(dropout_layer(X, 1.))\n```\n\n    tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n            [ 8.,  9., 10., 11., 12., 13., 14., 15.]])\n    tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n            [ 8.,  9., 10., 11., 12., 13., 14., 15.]])\n    tensor([[ 0.,  2.,  0.,  6.,  0., 10., 12.,  0.],\n            [16.,  0.,  0., 22., 24.,  0.,  0.,  0.]])\n    tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0., 0., 0., 0.]])\n\n\n### 6.1.1 dropout应用到模型\n- 将暂退法应用于每个隐藏层的输出（在激活函数之后），并且可以为每一层分别设置暂退概率：常见的技巧是在靠近输入层的地方设置较低的暂退概率。\n\n\n```python\n#1 参数\nnum_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256\ndropout1, dropout2 = 0.2, 0.5\n\n#2 模型\nclass Net(nn.Module):\n    def __init__(self, num_inputs, num_outputs, num_hiddens1, num_hiddens2,\n                 is_training=True):\n        super(Net, self).__init__()\n        self.num_inputs = num_inputs\n        self.training = is_training\n        self.lin1 = nn.Linear(num_inputs, num_hiddens1)\n        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)\n        self.lin3 = nn.Linear(num_hiddens2, num_outputs)\n        self.relu = nn.ReLU()\n    def forward(self, X):\n        H1 = self.relu(self.lin1(X.reshape((-1, self.num_inputs))))\n        if self.training: # 只在训练模型时使用dropout\n            H1 = dropout_layer(H1, dropout1)\n        H2 = self.relu(self.lin2(H1))\n        if self.training: #第二个全连接层后也使用dropout\n            H2 = dropout_layer(H2, dropout2)\n        out = self.lin3(H2)\n        return out\n    \nnet=Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)\n\n#3 训练和测试\nnum_epochs, lr, batch_size = 10, 0.5, 256\nloss = nn.CrossEntropyLoss(reduction=\"none\")\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\ntrainer = torch.optim.SGD(net.parameters(), lr=lr)\nd2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\n```\n\n  ![svg](D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/6_deopout_files/6_deopout_3_0.svg)    \n![](img/deeplearning/code/pytorch/3_mlp/6_deopout_files/6_deopout_3_0.svg)    \n\n\n### 6.1.2 简洁实现\n\n\n\n```python\nnet= nn.Sequential(\n    nn.Flatten(),\n    nn.Linear(784,256),\n    nn.ReLU(), \n    nn.Dropout(dropout1), #dropout\n    nn.Linear(256,256),\n    nn.ReLU(),\n    nn.Dropout(dropout2), #dropout\n    nn.Linear(256,10)\n                   )\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        nn.init.normal_(m.weight, std=0.01)\n\nnet.apply(init_weights)\n\n# 训练和测试\ntrainer = torch.optim.SGD(net.parameters(), lr=lr)\nd2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\n```\n\n  ![svg](D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/6_deopout_files/6_deopout_5_0.svg)\n    \n![](img/deeplearning/code/pytorch/3_mlp/6_deopout_files/6_deopout_5_0.svg)\n    \n\n","source":"_posts/deeplearning/code/pytorch/3_mlp/6_deopout.md","raw":"---\ntitle: 3.6 暂退法\ndate: 2024-2-7 14:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 6 暂退法\n- 模型简单性：\n    - 维度小\n    - 参数泛化\n    - 平滑性\n    \n- 平滑性：函数不应该对其输入的微小变化敏感。例如，当我们对图像进行分类时，我们预计向像素添加一些随机噪声应该是基本无影响的。具有输入噪声的训练等价于Tikhonov正则化。基于此，提出暂退法：在计算后续层之前向网络的每一层注入噪声。因为当训练一个有多层的深层网络时，注入噪声只会在输入‐输出映射上增强平滑性。从表面上看是在训练过程中丢弃（dropout）一些神经元。\n\n- 需要说明的是，暂退法的原始论文提到了一个关于有性繁殖的类比：神经网络过拟合与每一层都依赖于前一层激活值相关，称这种情况为“共适应性”。作者认为，暂退法会破坏共适应性，就像有性生殖会破坏共适应的基因一样。???  \n\n- 做法：\n\n  <span style=\"display:block\">\n  $h ^\\prime =\\begin{cases}0 &p \\\\ \\frac{h}{1-p} &1-p \\\\ \\end{cases}.$ \n\n  </span>\n\n- 这样，输出层的计算不能过度依赖于h1, . . . , h5的任何一个元素。通常，我们在测试时不用暂退法。(一些研究人员在测试时使用暂退法，用于估计神经网络预测的“不确定性”：如果通过许多不同的暂退法遮盖后得到的预测结果都是一致的，那么我们可以说网络发挥更稳定。)\n ![svg](D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/6img/1.png)\n![](img/deeplearning/code/pytorch/3_mlp/6img/1.png)\n\n## 6.1 代码实现\n- 1）从U[0,1]抽样\n- 2）保留大于p的点/(1-p)\n\n\n```python\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\n#1 定义dropout\ndef dropout_layer(X, dropout):\n    assert 0 <= dropout <= 1\n    if dropout == 1:\n        return torch.zeros_like(X)\n    if dropout == 0:\n        return X\n    mask = (torch.rand(X.shape) > dropout).float() #大于p的元素->1\n    return mask * X / (1.0 - dropout)\n\n#2 测试dropout\nX = torch.arange(16, dtype=torch.float32).reshape((2, 8))\nprint(X)\nprint(dropout_layer(X, 0.))\nprint(dropout_layer(X, 0.5))\nprint(dropout_layer(X, 1.))\n```\n\n    tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n            [ 8.,  9., 10., 11., 12., 13., 14., 15.]])\n    tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n            [ 8.,  9., 10., 11., 12., 13., 14., 15.]])\n    tensor([[ 0.,  2.,  0.,  6.,  0., 10., 12.,  0.],\n            [16.,  0.,  0., 22., 24.,  0.,  0.,  0.]])\n    tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0., 0., 0., 0.]])\n\n\n### 6.1.1 dropout应用到模型\n- 将暂退法应用于每个隐藏层的输出（在激活函数之后），并且可以为每一层分别设置暂退概率：常见的技巧是在靠近输入层的地方设置较低的暂退概率。\n\n\n```python\n#1 参数\nnum_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256\ndropout1, dropout2 = 0.2, 0.5\n\n#2 模型\nclass Net(nn.Module):\n    def __init__(self, num_inputs, num_outputs, num_hiddens1, num_hiddens2,\n                 is_training=True):\n        super(Net, self).__init__()\n        self.num_inputs = num_inputs\n        self.training = is_training\n        self.lin1 = nn.Linear(num_inputs, num_hiddens1)\n        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)\n        self.lin3 = nn.Linear(num_hiddens2, num_outputs)\n        self.relu = nn.ReLU()\n    def forward(self, X):\n        H1 = self.relu(self.lin1(X.reshape((-1, self.num_inputs))))\n        if self.training: # 只在训练模型时使用dropout\n            H1 = dropout_layer(H1, dropout1)\n        H2 = self.relu(self.lin2(H1))\n        if self.training: #第二个全连接层后也使用dropout\n            H2 = dropout_layer(H2, dropout2)\n        out = self.lin3(H2)\n        return out\n    \nnet=Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)\n\n#3 训练和测试\nnum_epochs, lr, batch_size = 10, 0.5, 256\nloss = nn.CrossEntropyLoss(reduction=\"none\")\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\ntrainer = torch.optim.SGD(net.parameters(), lr=lr)\nd2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\n```\n\n  ![svg](D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/6_deopout_files/6_deopout_3_0.svg)    \n![](img/deeplearning/code/pytorch/3_mlp/6_deopout_files/6_deopout_3_0.svg)    \n\n\n### 6.1.2 简洁实现\n\n\n\n```python\nnet= nn.Sequential(\n    nn.Flatten(),\n    nn.Linear(784,256),\n    nn.ReLU(), \n    nn.Dropout(dropout1), #dropout\n    nn.Linear(256,256),\n    nn.ReLU(),\n    nn.Dropout(dropout2), #dropout\n    nn.Linear(256,10)\n                   )\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        nn.init.normal_(m.weight, std=0.01)\n\nnet.apply(init_weights)\n\n# 训练和测试\ntrainer = torch.optim.SGD(net.parameters(), lr=lr)\nd2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\n```\n\n  ![svg](D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/6_deopout_files/6_deopout_5_0.svg)\n    \n![](img/deeplearning/code/pytorch/3_mlp/6_deopout_files/6_deopout_5_0.svg)\n    \n\n","slug":"deeplearning/code/pytorch/3_mlp/6_deopout","published":1,"updated":"2024-05-02T08:53:34.214Z","comments":1,"layout":"post","photos":[],"_id":"clvuli6cu003bzsvw3dzw5x4x","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"6-暂退法\"><a href=\"#6-暂退法\" class=\"headerlink\" title=\"6 暂退法\"></a>6 暂退法</h1><ul>\n<li><p>模型简单性：</p>\n<ul>\n<li>维度小</li>\n<li>参数泛化</li>\n<li>平滑性</li>\n</ul>\n</li>\n<li><p>平滑性：函数不应该对其输入的微小变化敏感。例如，当我们对图像进行分类时，我们预计向像素添加一些随机噪声应该是基本无影响的。具有输入噪声的训练等价于Tikhonov正则化。基于此，提出暂退法：在计算后续层之前向网络的每一层注入噪声。因为当训练一个有多层的深层网络时，注入噪声只会在输入‐输出映射上增强平滑性。从表面上看是在训练过程中丢弃（dropout）一些神经元。</p>\n</li>\n<li><p>需要说明的是，暂退法的原始论文提到了一个关于有性繁殖的类比：神经网络过拟合与每一层都依赖于前一层激活值相关，称这种情况为“共适应性”。作者认为，暂退法会破坏共适应性，就像有性生殖会破坏共适应的基因一样。???  </p>\n</li>\n<li><p>做法：</p>\n<span style=\"display:block\">\n$h ^\\prime =\\begin{cases}0 &p \\\\ \\frac{h}{1-p} &1-p \\\\ \\end{cases}.$ \n\n</span>\n</li>\n<li><p>这样，输出层的计算不能过度依赖于h1, . . . , h5的任何一个元素。通常，我们在测试时不用暂退法。(一些研究人员在测试时使用暂退法，用于估计神经网络预测的“不确定性”：如果通过许多不同的暂退法遮盖后得到的预测结果都是一致的，那么我们可以说网络发挥更稳定。)<br> <img src=\"D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/6img/1.png\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/3_mlp/6img/1.png\"></p>\n</li>\n</ul>\n<h2 id=\"6-1-代码实现\"><a href=\"#6-1-代码实现\" class=\"headerlink\" title=\"6.1 代码实现\"></a>6.1 代码实现</h2><ul>\n<li>1）从U[0,1]抽样</li>\n<li>2）保留大于p的点&#x2F;(1-p)</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>] = <span class=\"string\">&#x27;True&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#1 定义dropout</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">dropout_layer</span>(<span class=\"params\">X, dropout</span>):</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"number\">0</span> &lt;= dropout &lt;= <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> dropout == <span class=\"number\">1</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> torch.zeros_like(X)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> dropout == <span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> X</span><br><span class=\"line\">    mask = (torch.rand(X.shape) &gt; dropout).<span class=\"built_in\">float</span>() <span class=\"comment\">#大于p的元素-&gt;1</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> mask * X / (<span class=\"number\">1.0</span> - dropout)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#2 测试dropout</span></span><br><span class=\"line\">X = torch.arange(<span class=\"number\">16</span>, dtype=torch.float32).reshape((<span class=\"number\">2</span>, <span class=\"number\">8</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(X)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(dropout_layer(X, <span class=\"number\">0.</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(dropout_layer(X, <span class=\"number\">0.5</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(dropout_layer(X, <span class=\"number\">1.</span>))</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])\ntensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])\ntensor([[ 0.,  2.,  0.,  6.,  0., 10., 12.,  0.],\n        [16.,  0.,  0., 22., 24.,  0.,  0.,  0.]])\ntensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0.]])\n</code></pre>\n<h3 id=\"6-1-1-dropout应用到模型\"><a href=\"#6-1-1-dropout应用到模型\" class=\"headerlink\" title=\"6.1.1 dropout应用到模型\"></a>6.1.1 dropout应用到模型</h3><ul>\n<li>将暂退法应用于每个隐藏层的输出（在激活函数之后），并且可以为每一层分别设置暂退概率：常见的技巧是在靠近输入层的地方设置较低的暂退概率。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#1 参数</span></span><br><span class=\"line\">num_inputs, num_outputs, num_hiddens1, num_hiddens2 = <span class=\"number\">784</span>, <span class=\"number\">10</span>, <span class=\"number\">256</span>, <span class=\"number\">256</span></span><br><span class=\"line\">dropout1, dropout2 = <span class=\"number\">0.2</span>, <span class=\"number\">0.5</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#2 模型</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Net</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, num_inputs, num_outputs, num_hiddens1, num_hiddens2,</span></span><br><span class=\"line\"><span class=\"params\">                 is_training=<span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Net, self).__init__()</span><br><span class=\"line\">        self.num_inputs = num_inputs</span><br><span class=\"line\">        self.training = is_training</span><br><span class=\"line\">        self.lin1 = nn.Linear(num_inputs, num_hiddens1)</span><br><span class=\"line\">        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)</span><br><span class=\"line\">        self.lin3 = nn.Linear(num_hiddens2, num_outputs)</span><br><span class=\"line\">        self.relu = nn.ReLU()</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        H1 = self.relu(self.lin1(X.reshape((-<span class=\"number\">1</span>, self.num_inputs))))</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.training: <span class=\"comment\"># 只在训练模型时使用dropout</span></span><br><span class=\"line\">            H1 = dropout_layer(H1, dropout1)</span><br><span class=\"line\">        H2 = self.relu(self.lin2(H1))</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.training: <span class=\"comment\">#第二个全连接层后也使用dropout</span></span><br><span class=\"line\">            H2 = dropout_layer(H2, dropout2)</span><br><span class=\"line\">        out = self.lin3(H2)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> out</span><br><span class=\"line\">    </span><br><span class=\"line\">net=Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#3 训练和测试</span></span><br><span class=\"line\">num_epochs, lr, batch_size = <span class=\"number\">10</span>, <span class=\"number\">0.5</span>, <span class=\"number\">256</span></span><br><span class=\"line\">loss = nn.CrossEntropyLoss(reduction=<span class=\"string\">&quot;none&quot;</span>)</span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class=\"line\">trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class=\"line\">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure>\n\n<p>  <img src=\"D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/6_deopout_files/6_deopout_3_0.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/3_mlp/6_deopout_files/6_deopout_3_0.svg\">    </p>\n<h3 id=\"6-1-2-简洁实现\"><a href=\"#6-1-2-简洁实现\" class=\"headerlink\" title=\"6.1.2 简洁实现\"></a>6.1.2 简洁实现</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net= nn.Sequential(</span><br><span class=\"line\">    nn.Flatten(),</span><br><span class=\"line\">    nn.Linear(<span class=\"number\">784</span>,<span class=\"number\">256</span>),</span><br><span class=\"line\">    nn.ReLU(), </span><br><span class=\"line\">    nn.Dropout(dropout1), <span class=\"comment\">#dropout</span></span><br><span class=\"line\">    nn.Linear(<span class=\"number\">256</span>,<span class=\"number\">256</span>),</span><br><span class=\"line\">    nn.ReLU(),</span><br><span class=\"line\">    nn.Dropout(dropout2), <span class=\"comment\">#dropout</span></span><br><span class=\"line\">    nn.Linear(<span class=\"number\">256</span>,<span class=\"number\">10</span>)</span><br><span class=\"line\">                   )</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_weights</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear:</span><br><span class=\"line\">        nn.init.normal_(m.weight, std=<span class=\"number\">0.01</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">net.apply(init_weights)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 训练和测试</span></span><br><span class=\"line\">trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class=\"line\">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure>\n\n<p>  <img src=\"D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/6_deopout_files/6_deopout_5_0.svg\" alt=\"svg\"></p>\n<p><img src=\"/img/deeplearning/code/pytorch/3_mlp/6_deopout_files/6_deopout_5_0.svg\"></p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"6-暂退法\"><a href=\"#6-暂退法\" class=\"headerlink\" title=\"6 暂退法\"></a>6 暂退法</h1><ul>\n<li><p>模型简单性：</p>\n<ul>\n<li>维度小</li>\n<li>参数泛化</li>\n<li>平滑性</li>\n</ul>\n</li>\n<li><p>平滑性：函数不应该对其输入的微小变化敏感。例如，当我们对图像进行分类时，我们预计向像素添加一些随机噪声应该是基本无影响的。具有输入噪声的训练等价于Tikhonov正则化。基于此，提出暂退法：在计算后续层之前向网络的每一层注入噪声。因为当训练一个有多层的深层网络时，注入噪声只会在输入‐输出映射上增强平滑性。从表面上看是在训练过程中丢弃（dropout）一些神经元。</p>\n</li>\n<li><p>需要说明的是，暂退法的原始论文提到了一个关于有性繁殖的类比：神经网络过拟合与每一层都依赖于前一层激活值相关，称这种情况为“共适应性”。作者认为，暂退法会破坏共适应性，就像有性生殖会破坏共适应的基因一样。???  </p>\n</li>\n<li><p>做法：</p>\n<span style=\"display:block\">\n$h ^\\prime =\\begin{cases}0 &p \\\\ \\frac{h}{1-p} &1-p \\\\ \\end{cases}.$ \n\n</span>\n</li>\n<li><p>这样，输出层的计算不能过度依赖于h1, . . . , h5的任何一个元素。通常，我们在测试时不用暂退法。(一些研究人员在测试时使用暂退法，用于估计神经网络预测的“不确定性”：如果通过许多不同的暂退法遮盖后得到的预测结果都是一致的，那么我们可以说网络发挥更稳定。)<br> <img src=\"D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/6img/1.png\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/3_mlp/6img/1.png\"></p>\n</li>\n</ul>\n<h2 id=\"6-1-代码实现\"><a href=\"#6-1-代码实现\" class=\"headerlink\" title=\"6.1 代码实现\"></a>6.1 代码实现</h2><ul>\n<li>1）从U[0,1]抽样</li>\n<li>2）保留大于p的点&#x2F;(1-p)</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>] = <span class=\"string\">&#x27;True&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#1 定义dropout</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">dropout_layer</span>(<span class=\"params\">X, dropout</span>):</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"number\">0</span> &lt;= dropout &lt;= <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> dropout == <span class=\"number\">1</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> torch.zeros_like(X)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> dropout == <span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> X</span><br><span class=\"line\">    mask = (torch.rand(X.shape) &gt; dropout).<span class=\"built_in\">float</span>() <span class=\"comment\">#大于p的元素-&gt;1</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> mask * X / (<span class=\"number\">1.0</span> - dropout)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#2 测试dropout</span></span><br><span class=\"line\">X = torch.arange(<span class=\"number\">16</span>, dtype=torch.float32).reshape((<span class=\"number\">2</span>, <span class=\"number\">8</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(X)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(dropout_layer(X, <span class=\"number\">0.</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(dropout_layer(X, <span class=\"number\">0.5</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(dropout_layer(X, <span class=\"number\">1.</span>))</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])\ntensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])\ntensor([[ 0.,  2.,  0.,  6.,  0., 10., 12.,  0.],\n        [16.,  0.,  0., 22., 24.,  0.,  0.,  0.]])\ntensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0.]])\n</code></pre>\n<h3 id=\"6-1-1-dropout应用到模型\"><a href=\"#6-1-1-dropout应用到模型\" class=\"headerlink\" title=\"6.1.1 dropout应用到模型\"></a>6.1.1 dropout应用到模型</h3><ul>\n<li>将暂退法应用于每个隐藏层的输出（在激活函数之后），并且可以为每一层分别设置暂退概率：常见的技巧是在靠近输入层的地方设置较低的暂退概率。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#1 参数</span></span><br><span class=\"line\">num_inputs, num_outputs, num_hiddens1, num_hiddens2 = <span class=\"number\">784</span>, <span class=\"number\">10</span>, <span class=\"number\">256</span>, <span class=\"number\">256</span></span><br><span class=\"line\">dropout1, dropout2 = <span class=\"number\">0.2</span>, <span class=\"number\">0.5</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#2 模型</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Net</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, num_inputs, num_outputs, num_hiddens1, num_hiddens2,</span></span><br><span class=\"line\"><span class=\"params\">                 is_training=<span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Net, self).__init__()</span><br><span class=\"line\">        self.num_inputs = num_inputs</span><br><span class=\"line\">        self.training = is_training</span><br><span class=\"line\">        self.lin1 = nn.Linear(num_inputs, num_hiddens1)</span><br><span class=\"line\">        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)</span><br><span class=\"line\">        self.lin3 = nn.Linear(num_hiddens2, num_outputs)</span><br><span class=\"line\">        self.relu = nn.ReLU()</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        H1 = self.relu(self.lin1(X.reshape((-<span class=\"number\">1</span>, self.num_inputs))))</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.training: <span class=\"comment\"># 只在训练模型时使用dropout</span></span><br><span class=\"line\">            H1 = dropout_layer(H1, dropout1)</span><br><span class=\"line\">        H2 = self.relu(self.lin2(H1))</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.training: <span class=\"comment\">#第二个全连接层后也使用dropout</span></span><br><span class=\"line\">            H2 = dropout_layer(H2, dropout2)</span><br><span class=\"line\">        out = self.lin3(H2)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> out</span><br><span class=\"line\">    </span><br><span class=\"line\">net=Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#3 训练和测试</span></span><br><span class=\"line\">num_epochs, lr, batch_size = <span class=\"number\">10</span>, <span class=\"number\">0.5</span>, <span class=\"number\">256</span></span><br><span class=\"line\">loss = nn.CrossEntropyLoss(reduction=<span class=\"string\">&quot;none&quot;</span>)</span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class=\"line\">trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class=\"line\">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure>\n\n<p>  <img src=\"D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/6_deopout_files/6_deopout_3_0.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/3_mlp/6_deopout_files/6_deopout_3_0.svg\">    </p>\n<h3 id=\"6-1-2-简洁实现\"><a href=\"#6-1-2-简洁实现\" class=\"headerlink\" title=\"6.1.2 简洁实现\"></a>6.1.2 简洁实现</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net= nn.Sequential(</span><br><span class=\"line\">    nn.Flatten(),</span><br><span class=\"line\">    nn.Linear(<span class=\"number\">784</span>,<span class=\"number\">256</span>),</span><br><span class=\"line\">    nn.ReLU(), </span><br><span class=\"line\">    nn.Dropout(dropout1), <span class=\"comment\">#dropout</span></span><br><span class=\"line\">    nn.Linear(<span class=\"number\">256</span>,<span class=\"number\">256</span>),</span><br><span class=\"line\">    nn.ReLU(),</span><br><span class=\"line\">    nn.Dropout(dropout2), <span class=\"comment\">#dropout</span></span><br><span class=\"line\">    nn.Linear(<span class=\"number\">256</span>,<span class=\"number\">10</span>)</span><br><span class=\"line\">                   )</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_weights</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear:</span><br><span class=\"line\">        nn.init.normal_(m.weight, std=<span class=\"number\">0.01</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">net.apply(init_weights)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 训练和测试</span></span><br><span class=\"line\">trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class=\"line\">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure>\n\n<p>  <img src=\"D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/6_deopout_files/6_deopout_5_0.svg\" alt=\"svg\"></p>\n<p><img src=\"/img/deeplearning/code/pytorch/3_mlp/6_deopout_files/6_deopout_5_0.svg\"></p>"},{"title":"3.5 权重衰减","date":"2024-02-06T06:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n\n# 5 权重衰减\n\n- 缓解过拟合：\n    - 更多数据\n    - 正则化技术\n- 权重衰减也称$L_2$正则化,通过函数与零的距离来衡量函数的复杂度。如何精确地测量一个函数和零之间的距离呢？一种简单的方法是通过线性函数 f(x) = w⊤x 中的权重向量的某个范数来度量其复杂性，例如$||W||^2$。将权重向量作为惩罚项加入损失函数。原始的损失：\n$$L(\\mathbf{w},b)=\\frac{1}{n}\\sum\\limits^{n}_{i=1}\\frac{1}{2}(\\mathbf{w}^T\\mathbf{x}^{(1)}+b-y^{(i)})^2$$\n- 加入惩罚项\n$$L=L(\\mathbf{w},b)+\\frac{\\lambda}{2}||\\mathbf{w}||^2$$\n- 除以2是为了求导时消掉，让表达式更简单。使用平方范数而不是标准范数（欧几里得距离）是为了便于计算，且它对权重向量的大分量施加了巨大的惩罚，这使得我们的学习算法偏向于在大量特征上均匀分布权重的模型。相比之下，L1惩罚会导致模型将权重集中在一小部分特征上，而将其他\n权重清除为零。这称为特征选择（feature selection）\n- 加入正则化项后的梯度更新：\n$$\\mathbf{w} \\leftarrow (1-\\eta \\lambda)\\mathbf{w}-\\frac{\\eta}{| \\Beta |}\\sum\\limits_{ i\\in \\Beta }\\mathbf{x}^{(i)}(\\mathbf{w}^T\\mathbf{x}^{(i)}+b-y^{(i)})$$\n## 5.1 高维线性回归(实现权重衰减)\n$$ y= 0.05+ \\sum\\limits^{d}_{i=1}0.01x_i+\\epsilon, \\epsilon \\in N(0,0.01^2)$$\n\n\n```python\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK']='True'\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\nn_train, n_test, num_inputs, batch_size = 20, 100, 200, 5\ntrue_w, true_b = torch.ones((num_inputs, 1)) * 0.01, 0.05\ntraind_data = d2l.synthetic_data(true_w, true_b, n_train)\ntraind_iter = d2l.load_array(traind_data, batch_size)\ntest_data = d2l.synthetic_data(true_w, true_b, n_test)\ntest_iter = d2l.load_array(test_data, batch_size, is_train=False)\n\n#1 初始化模型参数\ndef init_params():\n    w = torch.normal(0, 1, size=(num_inputs, 1), requires_grad=True)\n    b = torch.zeros(1, requires_grad=True)\n    return [w, b]\n\n#2 定义L2范数惩罚\ndef l2_penalty(w):\n    return torch.sum(w.pow(2)) / 2\n\n#3 定义训练代码\ndef train(lambd):\n    w, b = init_params()\n    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss\n    num_epochs, lr = 100, 0.003\n    animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log', xlim=[5, num_epochs], legend=['train', 'test'])\n    for epoch in range(num_epochs):\n        for X, y in traind_iter:\n            l = loss(net(X), y) + lambd * l2_penalty(w) #计算损失后加上L2范数惩罚项\n            l.sum().backward()\n            d2l.sgd([w, b], lr, batch_size)\n        if (epoch + 1) % 5 == 0:\n            animator.add(epoch + 1, (d2l.evaluate_loss(net, traind_iter, loss), d2l.evaluate_loss(net, test_iter, loss)))\n    print('w的L2范数是：', torch.norm(w).item())\n\n#4 测试\ntrain(lambd=0) #没有使用权重衰减\n```\n\n    w的L2范数是： 12.524763107299805\n\n\n\n ![svg](D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_1_1.svg)\n![](img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_1_1.svg)\n    \n\n\n\n```python\ntrain(lambd=3) #使用权重衰减\n```\n\n    w的L2范数是： 0.3719361424446106\n\n\n\n ![svg](D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_2_1.svg)\n![](img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_2_1.svg)\n    \n\n\n## 5.2 简洁实现\n- 通常将权重衰减集成到优化算法中，以便与任何损失函数结合使用，并允许在不增加任何额外的计算开销的情况下向算法中添加权重衰减。\n\n\n```python\ndef train_concise(wd):\n    net = nn.Sequential(nn.Linear(num_inputs, 1))\n    for param in net.parameters():\n        param.data.normal_()\n    loss = nn.MSELoss(reduction='none')\n    num_epochs, lr = 100, 0.003\n    # 偏置参数没有衰减\n    trainer = torch.optim.SGD([\n        { \"params\": net[0].weight, 'weight_decay': wd}, \n        { \"params\": net[0].bias}], lr=lr)\n    animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log', xlim=[5, num_epochs], legend=['train', 'test'])\n    for epoch in range(num_epochs):\n        for X, y in traind_iter:\n            trainer.zero_grad()\n            l = loss(net(X), y).mean()\n            trainer.zero_grad()\n            l.backward()\n            trainer.step()\n        if (epoch + 1) % 5 == 0:\n            animator.add(epoch + 1, (d2l.evaluate_loss(net, traind_iter, loss), d2l.evaluate_loss(net, test_iter, loss)))\n    print('w的L2范数：', net[0].weight.norm().item())\n\ntrain_concise(0)\n```\n\n    w的L2范数： 12.297795295715332\n\n\n\n ![svg](D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_4_1.svg)\n![](img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_4_1.svg)\n    \n\n\n\n```python\ntrain_concise(3)\n```\n\n    w的L2范数： 0.40956297516822815\n\n\n\n ![svg](D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_5_1.svg)\n![](img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_5_1.svg)\n    \n\n","source":"_posts/deeplearning/code/pytorch/3_mlp/5_weight_decay.md","raw":"---\ntitle: 3.5 权重衰减\ndate: 2024-2-6 14:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n\n# 5 权重衰减\n\n- 缓解过拟合：\n    - 更多数据\n    - 正则化技术\n- 权重衰减也称$L_2$正则化,通过函数与零的距离来衡量函数的复杂度。如何精确地测量一个函数和零之间的距离呢？一种简单的方法是通过线性函数 f(x) = w⊤x 中的权重向量的某个范数来度量其复杂性，例如$||W||^2$。将权重向量作为惩罚项加入损失函数。原始的损失：\n$$L(\\mathbf{w},b)=\\frac{1}{n}\\sum\\limits^{n}_{i=1}\\frac{1}{2}(\\mathbf{w}^T\\mathbf{x}^{(1)}+b-y^{(i)})^2$$\n- 加入惩罚项\n$$L=L(\\mathbf{w},b)+\\frac{\\lambda}{2}||\\mathbf{w}||^2$$\n- 除以2是为了求导时消掉，让表达式更简单。使用平方范数而不是标准范数（欧几里得距离）是为了便于计算，且它对权重向量的大分量施加了巨大的惩罚，这使得我们的学习算法偏向于在大量特征上均匀分布权重的模型。相比之下，L1惩罚会导致模型将权重集中在一小部分特征上，而将其他\n权重清除为零。这称为特征选择（feature selection）\n- 加入正则化项后的梯度更新：\n$$\\mathbf{w} \\leftarrow (1-\\eta \\lambda)\\mathbf{w}-\\frac{\\eta}{| \\Beta |}\\sum\\limits_{ i\\in \\Beta }\\mathbf{x}^{(i)}(\\mathbf{w}^T\\mathbf{x}^{(i)}+b-y^{(i)})$$\n## 5.1 高维线性回归(实现权重衰减)\n$$ y= 0.05+ \\sum\\limits^{d}_{i=1}0.01x_i+\\epsilon, \\epsilon \\in N(0,0.01^2)$$\n\n\n```python\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK']='True'\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\nn_train, n_test, num_inputs, batch_size = 20, 100, 200, 5\ntrue_w, true_b = torch.ones((num_inputs, 1)) * 0.01, 0.05\ntraind_data = d2l.synthetic_data(true_w, true_b, n_train)\ntraind_iter = d2l.load_array(traind_data, batch_size)\ntest_data = d2l.synthetic_data(true_w, true_b, n_test)\ntest_iter = d2l.load_array(test_data, batch_size, is_train=False)\n\n#1 初始化模型参数\ndef init_params():\n    w = torch.normal(0, 1, size=(num_inputs, 1), requires_grad=True)\n    b = torch.zeros(1, requires_grad=True)\n    return [w, b]\n\n#2 定义L2范数惩罚\ndef l2_penalty(w):\n    return torch.sum(w.pow(2)) / 2\n\n#3 定义训练代码\ndef train(lambd):\n    w, b = init_params()\n    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss\n    num_epochs, lr = 100, 0.003\n    animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log', xlim=[5, num_epochs], legend=['train', 'test'])\n    for epoch in range(num_epochs):\n        for X, y in traind_iter:\n            l = loss(net(X), y) + lambd * l2_penalty(w) #计算损失后加上L2范数惩罚项\n            l.sum().backward()\n            d2l.sgd([w, b], lr, batch_size)\n        if (epoch + 1) % 5 == 0:\n            animator.add(epoch + 1, (d2l.evaluate_loss(net, traind_iter, loss), d2l.evaluate_loss(net, test_iter, loss)))\n    print('w的L2范数是：', torch.norm(w).item())\n\n#4 测试\ntrain(lambd=0) #没有使用权重衰减\n```\n\n    w的L2范数是： 12.524763107299805\n\n\n\n ![svg](D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_1_1.svg)\n![](img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_1_1.svg)\n    \n\n\n\n```python\ntrain(lambd=3) #使用权重衰减\n```\n\n    w的L2范数是： 0.3719361424446106\n\n\n\n ![svg](D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_2_1.svg)\n![](img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_2_1.svg)\n    \n\n\n## 5.2 简洁实现\n- 通常将权重衰减集成到优化算法中，以便与任何损失函数结合使用，并允许在不增加任何额外的计算开销的情况下向算法中添加权重衰减。\n\n\n```python\ndef train_concise(wd):\n    net = nn.Sequential(nn.Linear(num_inputs, 1))\n    for param in net.parameters():\n        param.data.normal_()\n    loss = nn.MSELoss(reduction='none')\n    num_epochs, lr = 100, 0.003\n    # 偏置参数没有衰减\n    trainer = torch.optim.SGD([\n        { \"params\": net[0].weight, 'weight_decay': wd}, \n        { \"params\": net[0].bias}], lr=lr)\n    animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log', xlim=[5, num_epochs], legend=['train', 'test'])\n    for epoch in range(num_epochs):\n        for X, y in traind_iter:\n            trainer.zero_grad()\n            l = loss(net(X), y).mean()\n            trainer.zero_grad()\n            l.backward()\n            trainer.step()\n        if (epoch + 1) % 5 == 0:\n            animator.add(epoch + 1, (d2l.evaluate_loss(net, traind_iter, loss), d2l.evaluate_loss(net, test_iter, loss)))\n    print('w的L2范数：', net[0].weight.norm().item())\n\ntrain_concise(0)\n```\n\n    w的L2范数： 12.297795295715332\n\n\n\n ![svg](D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_4_1.svg)\n![](img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_4_1.svg)\n    \n\n\n\n```python\ntrain_concise(3)\n```\n\n    w的L2范数： 0.40956297516822815\n\n\n\n ![svg](D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_5_1.svg)\n![](img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_5_1.svg)\n    \n\n","slug":"deeplearning/code/pytorch/3_mlp/5_weight_decay","published":1,"updated":"2024-05-02T08:53:25.241Z","comments":1,"layout":"post","photos":[],"_id":"clvuli6cv003ezsvwdbt65e74","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h1 id=\"5-权重衰减\"><a href=\"#5-权重衰减\" class=\"headerlink\" title=\"5 权重衰减\"></a>5 权重衰减</h1><ul>\n<li>缓解过拟合：<ul>\n<li>更多数据</li>\n<li>正则化技术</li>\n</ul>\n</li>\n<li>权重衰减也称$L_2$正则化,通过函数与零的距离来衡量函数的复杂度。如何精确地测量一个函数和零之间的距离呢？一种简单的方法是通过线性函数 f(x) &#x3D; w⊤x 中的权重向量的某个范数来度量其复杂性，例如$||W||^2$。将权重向量作为惩罚项加入损失函数。原始的损失：<br>$$L(\\mathbf{w},b)&#x3D;\\frac{1}{n}\\sum\\limits^{n}_{i&#x3D;1}\\frac{1}{2}(\\mathbf{w}^T\\mathbf{x}^{(1)}+b-y^{(i)})^2$$</li>\n<li>加入惩罚项<br>$$L&#x3D;L(\\mathbf{w},b)+\\frac{\\lambda}{2}||\\mathbf{w}||^2$$</li>\n<li>除以2是为了求导时消掉，让表达式更简单。使用平方范数而不是标准范数（欧几里得距离）是为了便于计算，且它对权重向量的大分量施加了巨大的惩罚，这使得我们的学习算法偏向于在大量特征上均匀分布权重的模型。相比之下，L1惩罚会导致模型将权重集中在一小部分特征上，而将其他<br>权重清除为零。这称为特征选择（feature selection）</li>\n<li>加入正则化项后的梯度更新：<br>$$\\mathbf{w} \\leftarrow (1-\\eta \\lambda)\\mathbf{w}-\\frac{\\eta}{| \\Beta |}\\sum\\limits_{ i\\in \\Beta }\\mathbf{x}^{(i)}(\\mathbf{w}^T\\mathbf{x}^{(i)}+b-y^{(i)})$$</li>\n</ul>\n<h2 id=\"5-1-高维线性回归-实现权重衰减\"><a href=\"#5-1-高维线性回归-实现权重衰减\" class=\"headerlink\" title=\"5.1 高维线性回归(实现权重衰减)\"></a>5.1 高维线性回归(实现权重衰减)</h2><p>$$ y&#x3D; 0.05+ \\sum\\limits^{d}_{i&#x3D;1}0.01x_i+\\epsilon, \\epsilon \\in N(0,0.01^2)$$</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>]=<span class=\"string\">&#x27;True&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">n_train, n_test, num_inputs, batch_size = <span class=\"number\">20</span>, <span class=\"number\">100</span>, <span class=\"number\">200</span>, <span class=\"number\">5</span></span><br><span class=\"line\">true_w, true_b = torch.ones((num_inputs, <span class=\"number\">1</span>)) * <span class=\"number\">0.01</span>, <span class=\"number\">0.05</span></span><br><span class=\"line\">traind_data = d2l.synthetic_data(true_w, true_b, n_train)</span><br><span class=\"line\">traind_iter = d2l.load_array(traind_data, batch_size)</span><br><span class=\"line\">test_data = d2l.synthetic_data(true_w, true_b, n_test)</span><br><span class=\"line\">test_iter = d2l.load_array(test_data, batch_size, is_train=<span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#1 初始化模型参数</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_params</span>():</span><br><span class=\"line\">    w = torch.normal(<span class=\"number\">0</span>, <span class=\"number\">1</span>, size=(num_inputs, <span class=\"number\">1</span>), requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    b = torch.zeros(<span class=\"number\">1</span>, requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> [w, b]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#2 定义L2范数惩罚</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">l2_penalty</span>(<span class=\"params\">w</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.<span class=\"built_in\">sum</span>(w.<span class=\"built_in\">pow</span>(<span class=\"number\">2</span>)) / <span class=\"number\">2</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#3 定义训练代码</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train</span>(<span class=\"params\">lambd</span>):</span><br><span class=\"line\">    w, b = init_params()</span><br><span class=\"line\">    net, loss = <span class=\"keyword\">lambda</span> X: d2l.linreg(X, w, b), d2l.squared_loss</span><br><span class=\"line\">    num_epochs, lr = <span class=\"number\">100</span>, <span class=\"number\">0.003</span></span><br><span class=\"line\">    animator = d2l.Animator(xlabel=<span class=\"string\">&#x27;epochs&#x27;</span>, ylabel=<span class=\"string\">&#x27;loss&#x27;</span>, yscale=<span class=\"string\">&#x27;log&#x27;</span>, xlim=[<span class=\"number\">5</span>, num_epochs], legend=[<span class=\"string\">&#x27;train&#x27;</span>, <span class=\"string\">&#x27;test&#x27;</span>])</span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> traind_iter:</span><br><span class=\"line\">            l = loss(net(X), y) + lambd * l2_penalty(w) <span class=\"comment\">#计算损失后加上L2范数惩罚项</span></span><br><span class=\"line\">            l.<span class=\"built_in\">sum</span>().backward()</span><br><span class=\"line\">            d2l.sgd([w, b], lr, batch_size)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (epoch + <span class=\"number\">1</span>) % <span class=\"number\">5</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            animator.add(epoch + <span class=\"number\">1</span>, (d2l.evaluate_loss(net, traind_iter, loss), d2l.evaluate_loss(net, test_iter, loss)))</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;w的L2范数是：&#x27;</span>, torch.norm(w).item())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#4 测试</span></span><br><span class=\"line\">train(lambd=<span class=\"number\">0</span>) <span class=\"comment\">#没有使用权重衰减</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>w的L2范数是： 12.524763107299805\n</code></pre>\n<p> <img src=\"D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_1_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_1_1.svg\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train(lambd=<span class=\"number\">3</span>) <span class=\"comment\">#使用权重衰减</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>w的L2范数是： 0.3719361424446106\n</code></pre>\n<p> <img src=\"D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_2_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_2_1.svg\"></p>\n<h2 id=\"5-2-简洁实现\"><a href=\"#5-2-简洁实现\" class=\"headerlink\" title=\"5.2 简洁实现\"></a>5.2 简洁实现</h2><ul>\n<li>通常将权重衰减集成到优化算法中，以便与任何损失函数结合使用，并允许在不增加任何额外的计算开销的情况下向算法中添加权重衰减。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train_concise</span>(<span class=\"params\">wd</span>):</span><br><span class=\"line\">    net = nn.Sequential(nn.Linear(num_inputs, <span class=\"number\">1</span>))</span><br><span class=\"line\">    <span class=\"keyword\">for</span> param <span class=\"keyword\">in</span> net.parameters():</span><br><span class=\"line\">        param.data.normal_()</span><br><span class=\"line\">    loss = nn.MSELoss(reduction=<span class=\"string\">&#x27;none&#x27;</span>)</span><br><span class=\"line\">    num_epochs, lr = <span class=\"number\">100</span>, <span class=\"number\">0.003</span></span><br><span class=\"line\">    <span class=\"comment\"># 偏置参数没有衰减</span></span><br><span class=\"line\">    trainer = torch.optim.SGD([</span><br><span class=\"line\">        &#123; <span class=\"string\">&quot;params&quot;</span>: net[<span class=\"number\">0</span>].weight, <span class=\"string\">&#x27;weight_decay&#x27;</span>: wd&#125;, </span><br><span class=\"line\">        &#123; <span class=\"string\">&quot;params&quot;</span>: net[<span class=\"number\">0</span>].bias&#125;], lr=lr)</span><br><span class=\"line\">    animator = d2l.Animator(xlabel=<span class=\"string\">&#x27;epochs&#x27;</span>, ylabel=<span class=\"string\">&#x27;loss&#x27;</span>, yscale=<span class=\"string\">&#x27;log&#x27;</span>, xlim=[<span class=\"number\">5</span>, num_epochs], legend=[<span class=\"string\">&#x27;train&#x27;</span>, <span class=\"string\">&#x27;test&#x27;</span>])</span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> traind_iter:</span><br><span class=\"line\">            trainer.zero_grad()</span><br><span class=\"line\">            l = loss(net(X), y).mean()</span><br><span class=\"line\">            trainer.zero_grad()</span><br><span class=\"line\">            l.backward()</span><br><span class=\"line\">            trainer.step()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (epoch + <span class=\"number\">1</span>) % <span class=\"number\">5</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            animator.add(epoch + <span class=\"number\">1</span>, (d2l.evaluate_loss(net, traind_iter, loss), d2l.evaluate_loss(net, test_iter, loss)))</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;w的L2范数：&#x27;</span>, net[<span class=\"number\">0</span>].weight.norm().item())</span><br><span class=\"line\"></span><br><span class=\"line\">train_concise(<span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>w的L2范数： 12.297795295715332\n</code></pre>\n<p> <img src=\"D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_4_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_4_1.svg\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train_concise(<span class=\"number\">3</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>w的L2范数： 0.40956297516822815\n</code></pre>\n<p> <img src=\"D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_5_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_5_1.svg\"></p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"5-权重衰减\"><a href=\"#5-权重衰减\" class=\"headerlink\" title=\"5 权重衰减\"></a>5 权重衰减</h1><ul>\n<li>缓解过拟合：<ul>\n<li>更多数据</li>\n<li>正则化技术</li>\n</ul>\n</li>\n<li>权重衰减也称$L_2$正则化,通过函数与零的距离来衡量函数的复杂度。如何精确地测量一个函数和零之间的距离呢？一种简单的方法是通过线性函数 f(x) &#x3D; w⊤x 中的权重向量的某个范数来度量其复杂性，例如$||W||^2$。将权重向量作为惩罚项加入损失函数。原始的损失：<br>$$L(\\mathbf{w},b)&#x3D;\\frac{1}{n}\\sum\\limits^{n}_{i&#x3D;1}\\frac{1}{2}(\\mathbf{w}^T\\mathbf{x}^{(1)}+b-y^{(i)})^2$$</li>\n<li>加入惩罚项<br>$$L&#x3D;L(\\mathbf{w},b)+\\frac{\\lambda}{2}||\\mathbf{w}||^2$$</li>\n<li>除以2是为了求导时消掉，让表达式更简单。使用平方范数而不是标准范数（欧几里得距离）是为了便于计算，且它对权重向量的大分量施加了巨大的惩罚，这使得我们的学习算法偏向于在大量特征上均匀分布权重的模型。相比之下，L1惩罚会导致模型将权重集中在一小部分特征上，而将其他<br>权重清除为零。这称为特征选择（feature selection）</li>\n<li>加入正则化项后的梯度更新：<br>$$\\mathbf{w} \\leftarrow (1-\\eta \\lambda)\\mathbf{w}-\\frac{\\eta}{| \\Beta |}\\sum\\limits_{ i\\in \\Beta }\\mathbf{x}^{(i)}(\\mathbf{w}^T\\mathbf{x}^{(i)}+b-y^{(i)})$$</li>\n</ul>\n<h2 id=\"5-1-高维线性回归-实现权重衰减\"><a href=\"#5-1-高维线性回归-实现权重衰减\" class=\"headerlink\" title=\"5.1 高维线性回归(实现权重衰减)\"></a>5.1 高维线性回归(实现权重衰减)</h2><p>$$ y&#x3D; 0.05+ \\sum\\limits^{d}_{i&#x3D;1}0.01x_i+\\epsilon, \\epsilon \\in N(0,0.01^2)$$</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>]=<span class=\"string\">&#x27;True&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">n_train, n_test, num_inputs, batch_size = <span class=\"number\">20</span>, <span class=\"number\">100</span>, <span class=\"number\">200</span>, <span class=\"number\">5</span></span><br><span class=\"line\">true_w, true_b = torch.ones((num_inputs, <span class=\"number\">1</span>)) * <span class=\"number\">0.01</span>, <span class=\"number\">0.05</span></span><br><span class=\"line\">traind_data = d2l.synthetic_data(true_w, true_b, n_train)</span><br><span class=\"line\">traind_iter = d2l.load_array(traind_data, batch_size)</span><br><span class=\"line\">test_data = d2l.synthetic_data(true_w, true_b, n_test)</span><br><span class=\"line\">test_iter = d2l.load_array(test_data, batch_size, is_train=<span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#1 初始化模型参数</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_params</span>():</span><br><span class=\"line\">    w = torch.normal(<span class=\"number\">0</span>, <span class=\"number\">1</span>, size=(num_inputs, <span class=\"number\">1</span>), requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    b = torch.zeros(<span class=\"number\">1</span>, requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> [w, b]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#2 定义L2范数惩罚</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">l2_penalty</span>(<span class=\"params\">w</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.<span class=\"built_in\">sum</span>(w.<span class=\"built_in\">pow</span>(<span class=\"number\">2</span>)) / <span class=\"number\">2</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#3 定义训练代码</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train</span>(<span class=\"params\">lambd</span>):</span><br><span class=\"line\">    w, b = init_params()</span><br><span class=\"line\">    net, loss = <span class=\"keyword\">lambda</span> X: d2l.linreg(X, w, b), d2l.squared_loss</span><br><span class=\"line\">    num_epochs, lr = <span class=\"number\">100</span>, <span class=\"number\">0.003</span></span><br><span class=\"line\">    animator = d2l.Animator(xlabel=<span class=\"string\">&#x27;epochs&#x27;</span>, ylabel=<span class=\"string\">&#x27;loss&#x27;</span>, yscale=<span class=\"string\">&#x27;log&#x27;</span>, xlim=[<span class=\"number\">5</span>, num_epochs], legend=[<span class=\"string\">&#x27;train&#x27;</span>, <span class=\"string\">&#x27;test&#x27;</span>])</span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> traind_iter:</span><br><span class=\"line\">            l = loss(net(X), y) + lambd * l2_penalty(w) <span class=\"comment\">#计算损失后加上L2范数惩罚项</span></span><br><span class=\"line\">            l.<span class=\"built_in\">sum</span>().backward()</span><br><span class=\"line\">            d2l.sgd([w, b], lr, batch_size)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (epoch + <span class=\"number\">1</span>) % <span class=\"number\">5</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            animator.add(epoch + <span class=\"number\">1</span>, (d2l.evaluate_loss(net, traind_iter, loss), d2l.evaluate_loss(net, test_iter, loss)))</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;w的L2范数是：&#x27;</span>, torch.norm(w).item())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#4 测试</span></span><br><span class=\"line\">train(lambd=<span class=\"number\">0</span>) <span class=\"comment\">#没有使用权重衰减</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>w的L2范数是： 12.524763107299805\n</code></pre>\n<p> <img src=\"D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_1_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_1_1.svg\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train(lambd=<span class=\"number\">3</span>) <span class=\"comment\">#使用权重衰减</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>w的L2范数是： 0.3719361424446106\n</code></pre>\n<p> <img src=\"D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_2_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_2_1.svg\"></p>\n<h2 id=\"5-2-简洁实现\"><a href=\"#5-2-简洁实现\" class=\"headerlink\" title=\"5.2 简洁实现\"></a>5.2 简洁实现</h2><ul>\n<li>通常将权重衰减集成到优化算法中，以便与任何损失函数结合使用，并允许在不增加任何额外的计算开销的情况下向算法中添加权重衰减。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train_concise</span>(<span class=\"params\">wd</span>):</span><br><span class=\"line\">    net = nn.Sequential(nn.Linear(num_inputs, <span class=\"number\">1</span>))</span><br><span class=\"line\">    <span class=\"keyword\">for</span> param <span class=\"keyword\">in</span> net.parameters():</span><br><span class=\"line\">        param.data.normal_()</span><br><span class=\"line\">    loss = nn.MSELoss(reduction=<span class=\"string\">&#x27;none&#x27;</span>)</span><br><span class=\"line\">    num_epochs, lr = <span class=\"number\">100</span>, <span class=\"number\">0.003</span></span><br><span class=\"line\">    <span class=\"comment\"># 偏置参数没有衰减</span></span><br><span class=\"line\">    trainer = torch.optim.SGD([</span><br><span class=\"line\">        &#123; <span class=\"string\">&quot;params&quot;</span>: net[<span class=\"number\">0</span>].weight, <span class=\"string\">&#x27;weight_decay&#x27;</span>: wd&#125;, </span><br><span class=\"line\">        &#123; <span class=\"string\">&quot;params&quot;</span>: net[<span class=\"number\">0</span>].bias&#125;], lr=lr)</span><br><span class=\"line\">    animator = d2l.Animator(xlabel=<span class=\"string\">&#x27;epochs&#x27;</span>, ylabel=<span class=\"string\">&#x27;loss&#x27;</span>, yscale=<span class=\"string\">&#x27;log&#x27;</span>, xlim=[<span class=\"number\">5</span>, num_epochs], legend=[<span class=\"string\">&#x27;train&#x27;</span>, <span class=\"string\">&#x27;test&#x27;</span>])</span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> traind_iter:</span><br><span class=\"line\">            trainer.zero_grad()</span><br><span class=\"line\">            l = loss(net(X), y).mean()</span><br><span class=\"line\">            trainer.zero_grad()</span><br><span class=\"line\">            l.backward()</span><br><span class=\"line\">            trainer.step()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (epoch + <span class=\"number\">1</span>) % <span class=\"number\">5</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            animator.add(epoch + <span class=\"number\">1</span>, (d2l.evaluate_loss(net, traind_iter, loss), d2l.evaluate_loss(net, test_iter, loss)))</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;w的L2范数：&#x27;</span>, net[<span class=\"number\">0</span>].weight.norm().item())</span><br><span class=\"line\"></span><br><span class=\"line\">train_concise(<span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>w的L2范数： 12.297795295715332\n</code></pre>\n<p> <img src=\"D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_4_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_4_1.svg\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train_concise(<span class=\"number\">3</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>w的L2范数： 0.40956297516822815\n</code></pre>\n<p> <img src=\"D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_5_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/3_mlp/5_weight_decay_files/5_weight_decay_5_1.svg\"></p>"},{"title":"3.9 环境和分布偏移","date":"2024-03-17T06:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 9 环境和分布偏移\n- （不好懂，建议跳过这一节）\n- 在不同的应用场景模型决策方案会出现漏洞\n## 9.1 分布偏移的类型\n- 考虑一个二元分类问题：区分狗和猫。如果分布可以以任意方式偏移，那么我们的情景允许病态的情况，即输入的分布保持不变：$p_S(x) = p_T (x)$，但标签全部翻转：$p_S(y|x) = 1 − p_T (y|x)$。\n### 9.1.1 协变量偏移\n- 虽然输入的分布可能随时间而改变，但标签函数（即条件分布P(y | x)）没有改变。统计学家称之为协变量偏移（covariate shift），因为这个问题是由于协变量（特征）分布的变化而产生的。\n- 比如用真的猫狗图像做训练，卡通猫狗做测试。\n### 9.1.2 标签偏移\n- 与协变量偏移相反。假设标签边缘概率P(y)可以改变，但是类别条件分布P(x | y)在不同的领域之间保持不变。当我们认为y导致x时，标签偏移是一个合理的假设。例如，预测患者的疾病，我们可能根据症状来判断，即使疾病的相对流行率随着时间的推移而变化。\n- 在另一些情况下，标签偏移和协变量偏移假设可以同时成立。例如，当标签是确定的，即使y导致x，协变量偏移假设也会得到满足。有趣的是，在这些情况下，使用基于标签偏移假设的方法通常是有利的。这是因为这些方法倾向于包含看起来像标签（通常是低维）的对象，而不是像输入（通常是高维的）对象。\n### 9.1.3 概念偏移\n- 当标签的定义发生变化时，就会出现这种问题。\n- 如果我们要建立一个机器翻译系统，P(y | x)的分布可能会因我们的位置不同而得到不同的翻译。这个问题可能很难被发现。所以，我们最好可以利用在时间或空间上逐渐发生偏移的知识。\n## 9.2 分布偏移示例\n### 9.2.1 医学诊断\n- 假设我们想设计一个检测癌症的算法，从健康人和病人那里收集数据，然后训练算法。收集训练数据的分布和在实际中遇到的数据分布可能有很大的不同。用近乎完美的精度来区分健康和患病人群确实很容易。然而，这可能是因为受试者在年龄、激素水平、体力活动、饮食、饮酒以及其他许多与疾病无关的因素上存在差异。这对检测疾病的分类器可能并不适用。这些抽样可能会遇到极端的协变量偏移。此外，这种情况不太可能通过常规方法加以纠正。\n### 9.2.2 自动驾驶汽车\n- 对于一家想利用机器学习来开发自动驾驶汽车的公司，一个关键部件是“路沿检测器”。由于真实的注释数据获取成本很高，他们想出了一个“聪明”的想法：将游戏渲染引擎中的合成数据用作额外的训练数据。这对从渲染引擎中抽取的“测试数据”非常有效，但应用在一辆真正的汽车里真是一场灾难。正如事实证明的那样，路沿被渲染成一种非常简单的纹理。更重要的是，所有的路沿都被渲染成了相同的纹理，路沿检测器很快就学习到了这个“特征”。\n- 当美军第一次试图在森林中探测坦克时，也发生了类似的事情。他们在没有坦克的情况下拍摄了森林的航拍照片，然后把坦克开进森林，拍摄了另一组照片。使用这两组数据训练的分类器似乎工作得很好。不幸的是，分类器仅仅学会了如何区分有阴影的树和没有阴影的树：第一组照片是在清晨拍摄的，而第二组是在中午拍摄的。\n### 9.2.3 非平稳分布\n- 当分布变化缓慢并且模型没有得到充分更新时，就会出现更微妙的情况：非平稳分布（nonstationary distri‐bution）\n    - 训练一个计算广告模型，但却没有经常更新（例如，一个2009年训练的模型不知道一个叫iPad的不知名新设备刚刚上市）；\n    - 建立一个垃圾邮件过滤器，它能很好地检测到所有垃圾邮件。但是，垃圾邮件发送者们变得聪明起来，制造出新的信息，看起来不像我们以前见过的任何垃圾邮件；\n    - 建立一个产品推荐系统，它在整个冬天都有效，但圣诞节过后很久还会继续推荐圣诞帽。\n\n## 9.3 分布偏移纠正\n- 通过一些策略应对分布偏移\n### 9.3.1 经验风险与实际风险\n- 不考虑正则化，训练损失为：$\\operatorname * {minimize}_ f \\frac1n \\sum_ {i=1}^ nl(f( \\mathbf{x}_ i),y_ i)$，也称经验风险，其中$l$是损失函数。\n- 经验风险是为了近似真实风险：整个训练数据上的平均损失，即从真实分布$p(x,y)$中抽取的所有数据的总体损失的期望值：\n    $$E_{p(\\mathbf{x},y)}[l(f(\\mathbf{x}),y)]=\\int\\int l(f(\\mathbf{x}),y)p(\\mathbf{x},y)~d\\mathbf{x}dy$$\n    然而，我们通常无法获得总体数据。\n\n### 9.3.2 协变量偏移纠正\n- 假设对于带标签的数据(xi, yi)，我们要评估P(y | x)。然而观测值xi是从某些源分布q(x)中得出的，而不是从目标分布p(x)中得出的。幸运的是，依赖性假设意味着条件分布保持不变，即：p(y | x) = q(y | x)。如果源分布q(x)是“错误的”，我们可以通过在真实风险的计算中，使用以下简单的恒等式来进行纠正：\n$$\\begin{aligned}\\int\\int l(f(\\mathbf{x}),y)p(y\\mid\\mathbf{x})p(\\mathbf{x})d\\mathbf{x}dy&=\\int\\int l(f(\\mathbf{x}),y)q(y\\mid\\mathbf{x})q(\\mathbf{x})\\frac{p(\\mathbf{x})}{q(\\mathbf{x})}d\\mathbf{x}dy\\end{aligned}$$\n- 因此我们需要根据数据来自正确分布与来自错误分布的概率之比，来重新衡量每个数据样本的权重：\n$$\\beta_i\\stackrel{\\mathrm{def}}{=}\\frac{p(\\mathbf{x}_i)}{q(\\mathbf{x}_i)}$$\n- 将权重带入每个数据样本，得到目标函数：加权经验风险最小化\n$$\\min_f\\text{mize }\\frac1n\\sum_{i=1}^n\\beta_il(f(\\mathbf{x}_i),y_i)$$\n- 由于不知道这个比率，我们需要估计它。需要从两个分布中抽取样本：“真实”的分布p，通过访问测试数据获取；训练集q，通过人工合成的很容易获得。我们只需要特征x ∼ p(x)，不需要访问标签y ∼ p(y)。\n- 一种非常有效的方法：对数几率回归，这是用于二元分类的softmax回归的一个特例。我们学习了一个分类器来区分从p(x)抽取的数据和从q(x)抽取的数据。如果无法区分这两个分布，则意味着相关的样本可能来自这两个分布中的任何一个。另一方面，任何可以很好区分的样本都应该相应地显著增加或减少权重。\n- 假设我们分别从p(x)和q(x) 两个分布中抽取相同数量的样本。现在用z标签表示：从p抽取的数据为1，从q抽取的数据为−1。然后，混合数据集中的概率由下式给出\n$$P(z=1\\mid\\mathbf{x})=\\frac{p(\\mathbf{x})}{p(\\mathbf{x})+q(\\mathbf{x})}\\text{ and hence }\\frac{P(z=1\\mid\\mathbf{x})}{P(z=-1\\mid\\mathbf{x})}=\\frac{p(\\mathbf{x})}{q(\\mathbf{x})}$$\n- 因此，如果我们使用对数几率回归方法，其中$P(z=1\\mid\\mathbf{x})=\\frac{1}{1+\\exp(-h(\\mathbf{x}))}$（h是一个参数化函数），则（？？？）\n$$\\beta_i=\\frac{1/(1+\\exp(-h(\\mathbf{x}_i)))}{\\exp(-h(\\mathbf{x}_i))/(1+\\exp(-h(\\mathbf{x}_i)))}=\\exp(h(\\mathbf{x}_i))$$\n- 因此，我们需要解决两个问题：第一个问题是关于区分来自两个分布的数据；第二个问题是关于 (4.9.5) 中的\n加权经验风险的最小化问题。\n- 假设我们有一个训练集{$(x_1,y_1),···，(x_n,y_n)$}和一个未标记的测试集{$u_1,···,u_m$}.假设1 $\\leq$ i $\\leq$ n的$x_i$来自某个源分布，$u_i$来自目标分布。纠正协变量偏移的典型算法：\n    1. 生成一个二元分类训练集：{$(x_1,-1),···,(x_n,-1),(u_1,1),···,(u_m,1)$}\n    2. 用对数几率回归训练二元分类器得到函数h。\n    3. 使用$\\beta_i = exp(h(x_i))$或更好的$\\beta_i = min(exp(h(x_i)),c)$(c为常量)对训练数据进行加权。\n    4. 使用权重$\\beta_i$进行加权经验风险最小化的训练。\n- 上述算法依赖于一个重要的假设：需要目标分布(例如，测试分布)中的每个数据样本在训练时出现的概率非零。如果我们找到p(x) > 0但q(x) = 0的点，那么相应的重要性权重会是无穷大。\n### 9.3.3 标签偏移纠正\n- 假设标签的分布随时间变化：q(y) ̸= p(y)，但类别条件分布保持不变：q(x | y) = p(x | y)。如果源分布q(y)是“错误的”，我们可以根据真实风险的定义进行更正：\n$$\\int\\int l(f(\\mathbf{x}),y)p(\\mathbf{x}\\mid y)p(y)d\\mathbf{x}dy=\\int\\int l(f(\\mathbf{x}),y)q(\\mathbf{x}\\mid y)q(y)\\frac{p(y)}{q(y)}d\\mathbf{x}dy$$\n- 重要性权重应用于标签似然比率：\n$$\\beta_i\\stackrel{\\mathrm{def}}{=}\\frac{p(y_i)}{q(y_i)}$$\n- 为了估计目标标签分布，我们首先采用性能相当好的现成的分类器（通常基于训练数据进行训练），并使用验证集（也来自训练分布）计算其混淆矩阵。混淆矩阵C是一个k × k矩阵，其中每列对应于标签类别，每行对应于模型的预测类别。每个单元格的值cij是验证集中，真实标签为j，而我们的模型预测为i的样本数量所占的比例。\n- 我们不能直接计算目标数据上的混淆矩阵，因为我们无法看到真实环境下的样本的标签，除非我们再搭建一个复杂的实时标注流程。然而，我们所能做的是将所有模型在测试时的预测取平均数，得到平均模型输出$\\mu(\\hat{\\mathbf{y}})\\in\\mathbb{R}^k$，其中第i个元素$\\mu(\\hat y_i)$是我们模型预测测试集中i的总预测分数。\n- 如果我们的分类器一开始就相当准确，并且目标数据只包含我们以前见过的类别，以及如果标签偏移假设成立（这里最强的假设），我们就可以通过求解一个简单的线性系统来估计测试集的标签分布\n$$\\mathbf{C}p(\\mathbf{y})=\\mu(\\hat{\\mathbf{y}})$$\n- 因为作为一个估计，$\\begin{aligned}\\sum_{j=1}^kc_{ij}p(y_j)=\\mu(\\hat{y}_i)\\end{aligned}$对所有$1\\leq i \\leq k$成立，其中$p(y_i)$是k维标签分布向量p(y)的第$j^{th}$元素。如果我们的分类器一开始就足够精确，那么混淆矩阵$\\mathbf C$将是可逆的，进而我们可以得到一个解$p(y)=\\mathbf C^{-1}\\mu(\\hat y)$因为我们观测源数据上的标签，所以很容易估计分布q(y)。那么对于标签为yi的任何训练样本i，我们可以使用我们估计的p(yi)/q(yi)比率来计算权重$\\beta_i$，并将其代入加权经验风险最小化中。\n### 9.3.4 概念偏移纠正\n- 概念偏移很难用原则性的方式解决。例如，在一个问题突然从“区分猫和狗”偏移为“区分白色和黑色动物”的情况下，除了从零开始收集新标签和训练，别无妙方。幸运的是，在实践中这种极端的偏移是罕见的。相反，通常情况下，概念的变化总是缓慢的。比如下面是一些例子：\n    - 在计算广告中，新产品推出后，旧产品变得不那么受欢迎了。这意味着广告的分布和受欢迎程度是逐渐\n    变化的，任何点击率预测器都需要随之逐渐变化；\n    - 由于环境的磨损，交通摄像头的镜头会逐渐退化，影响摄像头的图像质量；\n    - 新闻内容逐渐变化（即新新闻的出现）。\n- 在这种情况下，我们可以使用与训练网络相同的方法，使其适应数据的变化。换言之，我们使用新数据更新现有的网络权重，而不是从头开始训练。\n## 9.4 学习问题的分类法\n\n### 9.4.1 批量学习\n- 在批量学习（batch learning）中，我们可以访问一组训练特征和标签 {(x1, y1), . . . ,(xn, yn)}，我们使用这些特性和标签训练f(x)。然后，我们部署此模型来对来自同一分布的新数据(x, y)进行评分。例如，我们可以根据猫和狗的大量图片训练猫检测器。一旦我们训练了它，我们就把它作为智能猫门计算视觉系统的一部分，来控制只允许猫进入。然后这个系统会被安装在客户家中，基本再也不会更新。\n\n### 9.4.2 在线学习\n- 除了“批量”地学习，我们还可以单个“在线”学习数据(xi, yi)。更具体地说，我们首先观测到xi，然后我们得出一个估计值f(xi)，只有当我们做到这一点后，我们才观测到yi。然后根据我们的决定，我们会得到奖励或损失。许多实际问题都属于这一类。例如，我们需要预测明天的股票价格，这样我们就可以根据这个预测进行交易。在一天结束时，我们会评估我们的预测是否盈利。换句话说，在在线学习（online learning）中，我们有以下的循环。在这个循环中，给定新的观测结果，我们会不断地改进我们的模型。\n$$ \\text{model }f_ t \\longrightarrow \\text{data } \\mathbf{x}_ t \\longrightarrow \\text{estimate }f_ t( \\mathbf{x}_ t) \\longrightarrow \\text{observation }y_t \\longrightarrow \\text{loss }l(y_ t,f_ t( \\mathbf{x}_ t)) \\longrightarrow \\text{model }f_ {t+1}$$\n\n### 9.4.3 老虎机\n- 老虎机（bandits）是上述问题的一个特例。虽然在大多数学习问题中，我们有一个连续参数化的函数f（例如，一个深度网络）。但在一个老虎机问题中，我们只有有限数量的手臂可以拉动。也就是说，我们可以采取的行动是有限的。对于这个更简单的问题，可以获得更强的最优性理论保证，这并不令人惊讶。我们之所以列出它，主要是因为这个问题经常被视为一个单独的学习问题的情景。\n### 9.4.4 控制\n- 在很多情况下，环境会记住我们所做的事。不一定是以一种对抗的方式，但它会记住，而且它的反应将取决于之前发生的事情。例如，咖啡锅炉控制器将根据之前是否加热锅炉来观测到不同的温度。在这种情况下，PID（比例—积分—微分）控制器算法是一个流行的选择。同样，一个用户在新闻网站上的行为将取决于之前向她展示的内容（例如，大多数新闻她只阅读一次）。许多这样的算法形成了一个环境模型，在这个模型中，他们的行为使得他们的决策看起来不那么随机。近年来，控制理论（如PID的变体）也被用于自动调整超参数，以\n获得更好的解构和重建质量，提高生成文本的多样性和生成图像的重建质量。\n### 9.4.5 强化学习\n- 强化学习（reinforcement learning）强调如何基于环境而行动，以取得最大化的预期利益。国际象棋、围棋、西洋双陆棋或星际争霸都是强化学习的应用实例。再比如，为自动驾驶汽车制造一个控制器，或者以其他方式对自动驾驶汽车的驾驶方式做出反应（例如，试图避开某物体，试图造成事故，或者试图与其合作）。\n### 9.4.6 考虑到环境\n- 上述不同情况之间的一个关键区别是：在静止环境中可能一直有效的相同策略，在环境能够改变的情况下可能不会始终有效。例如，一个交易者发现的套利机会很可能在他开始利用它时就消失了。环境变化的速度和方式在很大程度上决定了我们可以采用的算法类型。例如，如果我们知道事情只会缓慢地变化，就可以迫使任何估计也只能缓慢地发生改变。如果我们知道环境可能会瞬间发生变化，但这种变化非常罕见，我们就可以在使用算法时考虑到这一点。当一个数据科学家试图解决的问题会随着时间的推移而发生变化时，这些类\n型的知识至关重要。\n\n","source":"_posts/deeplearning/code/pytorch/3_mlp/9_distribution_shift.md","raw":"---\ntitle: 3.9 环境和分布偏移\ndate: 2024-3-17 14:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 9 环境和分布偏移\n- （不好懂，建议跳过这一节）\n- 在不同的应用场景模型决策方案会出现漏洞\n## 9.1 分布偏移的类型\n- 考虑一个二元分类问题：区分狗和猫。如果分布可以以任意方式偏移，那么我们的情景允许病态的情况，即输入的分布保持不变：$p_S(x) = p_T (x)$，但标签全部翻转：$p_S(y|x) = 1 − p_T (y|x)$。\n### 9.1.1 协变量偏移\n- 虽然输入的分布可能随时间而改变，但标签函数（即条件分布P(y | x)）没有改变。统计学家称之为协变量偏移（covariate shift），因为这个问题是由于协变量（特征）分布的变化而产生的。\n- 比如用真的猫狗图像做训练，卡通猫狗做测试。\n### 9.1.2 标签偏移\n- 与协变量偏移相反。假设标签边缘概率P(y)可以改变，但是类别条件分布P(x | y)在不同的领域之间保持不变。当我们认为y导致x时，标签偏移是一个合理的假设。例如，预测患者的疾病，我们可能根据症状来判断，即使疾病的相对流行率随着时间的推移而变化。\n- 在另一些情况下，标签偏移和协变量偏移假设可以同时成立。例如，当标签是确定的，即使y导致x，协变量偏移假设也会得到满足。有趣的是，在这些情况下，使用基于标签偏移假设的方法通常是有利的。这是因为这些方法倾向于包含看起来像标签（通常是低维）的对象，而不是像输入（通常是高维的）对象。\n### 9.1.3 概念偏移\n- 当标签的定义发生变化时，就会出现这种问题。\n- 如果我们要建立一个机器翻译系统，P(y | x)的分布可能会因我们的位置不同而得到不同的翻译。这个问题可能很难被发现。所以，我们最好可以利用在时间或空间上逐渐发生偏移的知识。\n## 9.2 分布偏移示例\n### 9.2.1 医学诊断\n- 假设我们想设计一个检测癌症的算法，从健康人和病人那里收集数据，然后训练算法。收集训练数据的分布和在实际中遇到的数据分布可能有很大的不同。用近乎完美的精度来区分健康和患病人群确实很容易。然而，这可能是因为受试者在年龄、激素水平、体力活动、饮食、饮酒以及其他许多与疾病无关的因素上存在差异。这对检测疾病的分类器可能并不适用。这些抽样可能会遇到极端的协变量偏移。此外，这种情况不太可能通过常规方法加以纠正。\n### 9.2.2 自动驾驶汽车\n- 对于一家想利用机器学习来开发自动驾驶汽车的公司，一个关键部件是“路沿检测器”。由于真实的注释数据获取成本很高，他们想出了一个“聪明”的想法：将游戏渲染引擎中的合成数据用作额外的训练数据。这对从渲染引擎中抽取的“测试数据”非常有效，但应用在一辆真正的汽车里真是一场灾难。正如事实证明的那样，路沿被渲染成一种非常简单的纹理。更重要的是，所有的路沿都被渲染成了相同的纹理，路沿检测器很快就学习到了这个“特征”。\n- 当美军第一次试图在森林中探测坦克时，也发生了类似的事情。他们在没有坦克的情况下拍摄了森林的航拍照片，然后把坦克开进森林，拍摄了另一组照片。使用这两组数据训练的分类器似乎工作得很好。不幸的是，分类器仅仅学会了如何区分有阴影的树和没有阴影的树：第一组照片是在清晨拍摄的，而第二组是在中午拍摄的。\n### 9.2.3 非平稳分布\n- 当分布变化缓慢并且模型没有得到充分更新时，就会出现更微妙的情况：非平稳分布（nonstationary distri‐bution）\n    - 训练一个计算广告模型，但却没有经常更新（例如，一个2009年训练的模型不知道一个叫iPad的不知名新设备刚刚上市）；\n    - 建立一个垃圾邮件过滤器，它能很好地检测到所有垃圾邮件。但是，垃圾邮件发送者们变得聪明起来，制造出新的信息，看起来不像我们以前见过的任何垃圾邮件；\n    - 建立一个产品推荐系统，它在整个冬天都有效，但圣诞节过后很久还会继续推荐圣诞帽。\n\n## 9.3 分布偏移纠正\n- 通过一些策略应对分布偏移\n### 9.3.1 经验风险与实际风险\n- 不考虑正则化，训练损失为：$\\operatorname * {minimize}_ f \\frac1n \\sum_ {i=1}^ nl(f( \\mathbf{x}_ i),y_ i)$，也称经验风险，其中$l$是损失函数。\n- 经验风险是为了近似真实风险：整个训练数据上的平均损失，即从真实分布$p(x,y)$中抽取的所有数据的总体损失的期望值：\n    $$E_{p(\\mathbf{x},y)}[l(f(\\mathbf{x}),y)]=\\int\\int l(f(\\mathbf{x}),y)p(\\mathbf{x},y)~d\\mathbf{x}dy$$\n    然而，我们通常无法获得总体数据。\n\n### 9.3.2 协变量偏移纠正\n- 假设对于带标签的数据(xi, yi)，我们要评估P(y | x)。然而观测值xi是从某些源分布q(x)中得出的，而不是从目标分布p(x)中得出的。幸运的是，依赖性假设意味着条件分布保持不变，即：p(y | x) = q(y | x)。如果源分布q(x)是“错误的”，我们可以通过在真实风险的计算中，使用以下简单的恒等式来进行纠正：\n$$\\begin{aligned}\\int\\int l(f(\\mathbf{x}),y)p(y\\mid\\mathbf{x})p(\\mathbf{x})d\\mathbf{x}dy&=\\int\\int l(f(\\mathbf{x}),y)q(y\\mid\\mathbf{x})q(\\mathbf{x})\\frac{p(\\mathbf{x})}{q(\\mathbf{x})}d\\mathbf{x}dy\\end{aligned}$$\n- 因此我们需要根据数据来自正确分布与来自错误分布的概率之比，来重新衡量每个数据样本的权重：\n$$\\beta_i\\stackrel{\\mathrm{def}}{=}\\frac{p(\\mathbf{x}_i)}{q(\\mathbf{x}_i)}$$\n- 将权重带入每个数据样本，得到目标函数：加权经验风险最小化\n$$\\min_f\\text{mize }\\frac1n\\sum_{i=1}^n\\beta_il(f(\\mathbf{x}_i),y_i)$$\n- 由于不知道这个比率，我们需要估计它。需要从两个分布中抽取样本：“真实”的分布p，通过访问测试数据获取；训练集q，通过人工合成的很容易获得。我们只需要特征x ∼ p(x)，不需要访问标签y ∼ p(y)。\n- 一种非常有效的方法：对数几率回归，这是用于二元分类的softmax回归的一个特例。我们学习了一个分类器来区分从p(x)抽取的数据和从q(x)抽取的数据。如果无法区分这两个分布，则意味着相关的样本可能来自这两个分布中的任何一个。另一方面，任何可以很好区分的样本都应该相应地显著增加或减少权重。\n- 假设我们分别从p(x)和q(x) 两个分布中抽取相同数量的样本。现在用z标签表示：从p抽取的数据为1，从q抽取的数据为−1。然后，混合数据集中的概率由下式给出\n$$P(z=1\\mid\\mathbf{x})=\\frac{p(\\mathbf{x})}{p(\\mathbf{x})+q(\\mathbf{x})}\\text{ and hence }\\frac{P(z=1\\mid\\mathbf{x})}{P(z=-1\\mid\\mathbf{x})}=\\frac{p(\\mathbf{x})}{q(\\mathbf{x})}$$\n- 因此，如果我们使用对数几率回归方法，其中$P(z=1\\mid\\mathbf{x})=\\frac{1}{1+\\exp(-h(\\mathbf{x}))}$（h是一个参数化函数），则（？？？）\n$$\\beta_i=\\frac{1/(1+\\exp(-h(\\mathbf{x}_i)))}{\\exp(-h(\\mathbf{x}_i))/(1+\\exp(-h(\\mathbf{x}_i)))}=\\exp(h(\\mathbf{x}_i))$$\n- 因此，我们需要解决两个问题：第一个问题是关于区分来自两个分布的数据；第二个问题是关于 (4.9.5) 中的\n加权经验风险的最小化问题。\n- 假设我们有一个训练集{$(x_1,y_1),···，(x_n,y_n)$}和一个未标记的测试集{$u_1,···,u_m$}.假设1 $\\leq$ i $\\leq$ n的$x_i$来自某个源分布，$u_i$来自目标分布。纠正协变量偏移的典型算法：\n    1. 生成一个二元分类训练集：{$(x_1,-1),···,(x_n,-1),(u_1,1),···,(u_m,1)$}\n    2. 用对数几率回归训练二元分类器得到函数h。\n    3. 使用$\\beta_i = exp(h(x_i))$或更好的$\\beta_i = min(exp(h(x_i)),c)$(c为常量)对训练数据进行加权。\n    4. 使用权重$\\beta_i$进行加权经验风险最小化的训练。\n- 上述算法依赖于一个重要的假设：需要目标分布(例如，测试分布)中的每个数据样本在训练时出现的概率非零。如果我们找到p(x) > 0但q(x) = 0的点，那么相应的重要性权重会是无穷大。\n### 9.3.3 标签偏移纠正\n- 假设标签的分布随时间变化：q(y) ̸= p(y)，但类别条件分布保持不变：q(x | y) = p(x | y)。如果源分布q(y)是“错误的”，我们可以根据真实风险的定义进行更正：\n$$\\int\\int l(f(\\mathbf{x}),y)p(\\mathbf{x}\\mid y)p(y)d\\mathbf{x}dy=\\int\\int l(f(\\mathbf{x}),y)q(\\mathbf{x}\\mid y)q(y)\\frac{p(y)}{q(y)}d\\mathbf{x}dy$$\n- 重要性权重应用于标签似然比率：\n$$\\beta_i\\stackrel{\\mathrm{def}}{=}\\frac{p(y_i)}{q(y_i)}$$\n- 为了估计目标标签分布，我们首先采用性能相当好的现成的分类器（通常基于训练数据进行训练），并使用验证集（也来自训练分布）计算其混淆矩阵。混淆矩阵C是一个k × k矩阵，其中每列对应于标签类别，每行对应于模型的预测类别。每个单元格的值cij是验证集中，真实标签为j，而我们的模型预测为i的样本数量所占的比例。\n- 我们不能直接计算目标数据上的混淆矩阵，因为我们无法看到真实环境下的样本的标签，除非我们再搭建一个复杂的实时标注流程。然而，我们所能做的是将所有模型在测试时的预测取平均数，得到平均模型输出$\\mu(\\hat{\\mathbf{y}})\\in\\mathbb{R}^k$，其中第i个元素$\\mu(\\hat y_i)$是我们模型预测测试集中i的总预测分数。\n- 如果我们的分类器一开始就相当准确，并且目标数据只包含我们以前见过的类别，以及如果标签偏移假设成立（这里最强的假设），我们就可以通过求解一个简单的线性系统来估计测试集的标签分布\n$$\\mathbf{C}p(\\mathbf{y})=\\mu(\\hat{\\mathbf{y}})$$\n- 因为作为一个估计，$\\begin{aligned}\\sum_{j=1}^kc_{ij}p(y_j)=\\mu(\\hat{y}_i)\\end{aligned}$对所有$1\\leq i \\leq k$成立，其中$p(y_i)$是k维标签分布向量p(y)的第$j^{th}$元素。如果我们的分类器一开始就足够精确，那么混淆矩阵$\\mathbf C$将是可逆的，进而我们可以得到一个解$p(y)=\\mathbf C^{-1}\\mu(\\hat y)$因为我们观测源数据上的标签，所以很容易估计分布q(y)。那么对于标签为yi的任何训练样本i，我们可以使用我们估计的p(yi)/q(yi)比率来计算权重$\\beta_i$，并将其代入加权经验风险最小化中。\n### 9.3.4 概念偏移纠正\n- 概念偏移很难用原则性的方式解决。例如，在一个问题突然从“区分猫和狗”偏移为“区分白色和黑色动物”的情况下，除了从零开始收集新标签和训练，别无妙方。幸运的是，在实践中这种极端的偏移是罕见的。相反，通常情况下，概念的变化总是缓慢的。比如下面是一些例子：\n    - 在计算广告中，新产品推出后，旧产品变得不那么受欢迎了。这意味着广告的分布和受欢迎程度是逐渐\n    变化的，任何点击率预测器都需要随之逐渐变化；\n    - 由于环境的磨损，交通摄像头的镜头会逐渐退化，影响摄像头的图像质量；\n    - 新闻内容逐渐变化（即新新闻的出现）。\n- 在这种情况下，我们可以使用与训练网络相同的方法，使其适应数据的变化。换言之，我们使用新数据更新现有的网络权重，而不是从头开始训练。\n## 9.4 学习问题的分类法\n\n### 9.4.1 批量学习\n- 在批量学习（batch learning）中，我们可以访问一组训练特征和标签 {(x1, y1), . . . ,(xn, yn)}，我们使用这些特性和标签训练f(x)。然后，我们部署此模型来对来自同一分布的新数据(x, y)进行评分。例如，我们可以根据猫和狗的大量图片训练猫检测器。一旦我们训练了它，我们就把它作为智能猫门计算视觉系统的一部分，来控制只允许猫进入。然后这个系统会被安装在客户家中，基本再也不会更新。\n\n### 9.4.2 在线学习\n- 除了“批量”地学习，我们还可以单个“在线”学习数据(xi, yi)。更具体地说，我们首先观测到xi，然后我们得出一个估计值f(xi)，只有当我们做到这一点后，我们才观测到yi。然后根据我们的决定，我们会得到奖励或损失。许多实际问题都属于这一类。例如，我们需要预测明天的股票价格，这样我们就可以根据这个预测进行交易。在一天结束时，我们会评估我们的预测是否盈利。换句话说，在在线学习（online learning）中，我们有以下的循环。在这个循环中，给定新的观测结果，我们会不断地改进我们的模型。\n$$ \\text{model }f_ t \\longrightarrow \\text{data } \\mathbf{x}_ t \\longrightarrow \\text{estimate }f_ t( \\mathbf{x}_ t) \\longrightarrow \\text{observation }y_t \\longrightarrow \\text{loss }l(y_ t,f_ t( \\mathbf{x}_ t)) \\longrightarrow \\text{model }f_ {t+1}$$\n\n### 9.4.3 老虎机\n- 老虎机（bandits）是上述问题的一个特例。虽然在大多数学习问题中，我们有一个连续参数化的函数f（例如，一个深度网络）。但在一个老虎机问题中，我们只有有限数量的手臂可以拉动。也就是说，我们可以采取的行动是有限的。对于这个更简单的问题，可以获得更强的最优性理论保证，这并不令人惊讶。我们之所以列出它，主要是因为这个问题经常被视为一个单独的学习问题的情景。\n### 9.4.4 控制\n- 在很多情况下，环境会记住我们所做的事。不一定是以一种对抗的方式，但它会记住，而且它的反应将取决于之前发生的事情。例如，咖啡锅炉控制器将根据之前是否加热锅炉来观测到不同的温度。在这种情况下，PID（比例—积分—微分）控制器算法是一个流行的选择。同样，一个用户在新闻网站上的行为将取决于之前向她展示的内容（例如，大多数新闻她只阅读一次）。许多这样的算法形成了一个环境模型，在这个模型中，他们的行为使得他们的决策看起来不那么随机。近年来，控制理论（如PID的变体）也被用于自动调整超参数，以\n获得更好的解构和重建质量，提高生成文本的多样性和生成图像的重建质量。\n### 9.4.5 强化学习\n- 强化学习（reinforcement learning）强调如何基于环境而行动，以取得最大化的预期利益。国际象棋、围棋、西洋双陆棋或星际争霸都是强化学习的应用实例。再比如，为自动驾驶汽车制造一个控制器，或者以其他方式对自动驾驶汽车的驾驶方式做出反应（例如，试图避开某物体，试图造成事故，或者试图与其合作）。\n### 9.4.6 考虑到环境\n- 上述不同情况之间的一个关键区别是：在静止环境中可能一直有效的相同策略，在环境能够改变的情况下可能不会始终有效。例如，一个交易者发现的套利机会很可能在他开始利用它时就消失了。环境变化的速度和方式在很大程度上决定了我们可以采用的算法类型。例如，如果我们知道事情只会缓慢地变化，就可以迫使任何估计也只能缓慢地发生改变。如果我们知道环境可能会瞬间发生变化，但这种变化非常罕见，我们就可以在使用算法时考虑到这一点。当一个数据科学家试图解决的问题会随着时间的推移而发生变化时，这些类\n型的知识至关重要。\n\n","slug":"deeplearning/code/pytorch/3_mlp/9_distribution_shift","published":1,"updated":"2024-05-02T08:54:07.845Z","comments":1,"layout":"post","photos":[],"_id":"clvuli6cv003hzsvw95m997nn","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"9-环境和分布偏移\"><a href=\"#9-环境和分布偏移\" class=\"headerlink\" title=\"9 环境和分布偏移\"></a>9 环境和分布偏移</h1><ul>\n<li>（不好懂，建议跳过这一节）</li>\n<li>在不同的应用场景模型决策方案会出现漏洞</li>\n</ul>\n<h2 id=\"9-1-分布偏移的类型\"><a href=\"#9-1-分布偏移的类型\" class=\"headerlink\" title=\"9.1 分布偏移的类型\"></a>9.1 分布偏移的类型</h2><ul>\n<li>考虑一个二元分类问题：区分狗和猫。如果分布可以以任意方式偏移，那么我们的情景允许病态的情况，即输入的分布保持不变：$p_S(x) &#x3D; p_T (x)$，但标签全部翻转：$p_S(y|x) &#x3D; 1 − p_T (y|x)$。</li>\n</ul>\n<h3 id=\"9-1-1-协变量偏移\"><a href=\"#9-1-1-协变量偏移\" class=\"headerlink\" title=\"9.1.1 协变量偏移\"></a>9.1.1 协变量偏移</h3><ul>\n<li>虽然输入的分布可能随时间而改变，但标签函数（即条件分布P(y | x)）没有改变。统计学家称之为协变量偏移（covariate shift），因为这个问题是由于协变量（特征）分布的变化而产生的。</li>\n<li>比如用真的猫狗图像做训练，卡通猫狗做测试。</li>\n</ul>\n<h3 id=\"9-1-2-标签偏移\"><a href=\"#9-1-2-标签偏移\" class=\"headerlink\" title=\"9.1.2 标签偏移\"></a>9.1.2 标签偏移</h3><ul>\n<li>与协变量偏移相反。假设标签边缘概率P(y)可以改变，但是类别条件分布P(x | y)在不同的领域之间保持不变。当我们认为y导致x时，标签偏移是一个合理的假设。例如，预测患者的疾病，我们可能根据症状来判断，即使疾病的相对流行率随着时间的推移而变化。</li>\n<li>在另一些情况下，标签偏移和协变量偏移假设可以同时成立。例如，当标签是确定的，即使y导致x，协变量偏移假设也会得到满足。有趣的是，在这些情况下，使用基于标签偏移假设的方法通常是有利的。这是因为这些方法倾向于包含看起来像标签（通常是低维）的对象，而不是像输入（通常是高维的）对象。</li>\n</ul>\n<h3 id=\"9-1-3-概念偏移\"><a href=\"#9-1-3-概念偏移\" class=\"headerlink\" title=\"9.1.3 概念偏移\"></a>9.1.3 概念偏移</h3><ul>\n<li>当标签的定义发生变化时，就会出现这种问题。</li>\n<li>如果我们要建立一个机器翻译系统，P(y | x)的分布可能会因我们的位置不同而得到不同的翻译。这个问题可能很难被发现。所以，我们最好可以利用在时间或空间上逐渐发生偏移的知识。</li>\n</ul>\n<h2 id=\"9-2-分布偏移示例\"><a href=\"#9-2-分布偏移示例\" class=\"headerlink\" title=\"9.2 分布偏移示例\"></a>9.2 分布偏移示例</h2><h3 id=\"9-2-1-医学诊断\"><a href=\"#9-2-1-医学诊断\" class=\"headerlink\" title=\"9.2.1 医学诊断\"></a>9.2.1 医学诊断</h3><ul>\n<li>假设我们想设计一个检测癌症的算法，从健康人和病人那里收集数据，然后训练算法。收集训练数据的分布和在实际中遇到的数据分布可能有很大的不同。用近乎完美的精度来区分健康和患病人群确实很容易。然而，这可能是因为受试者在年龄、激素水平、体力活动、饮食、饮酒以及其他许多与疾病无关的因素上存在差异。这对检测疾病的分类器可能并不适用。这些抽样可能会遇到极端的协变量偏移。此外，这种情况不太可能通过常规方法加以纠正。</li>\n</ul>\n<h3 id=\"9-2-2-自动驾驶汽车\"><a href=\"#9-2-2-自动驾驶汽车\" class=\"headerlink\" title=\"9.2.2 自动驾驶汽车\"></a>9.2.2 自动驾驶汽车</h3><ul>\n<li>对于一家想利用机器学习来开发自动驾驶汽车的公司，一个关键部件是“路沿检测器”。由于真实的注释数据获取成本很高，他们想出了一个“聪明”的想法：将游戏渲染引擎中的合成数据用作额外的训练数据。这对从渲染引擎中抽取的“测试数据”非常有效，但应用在一辆真正的汽车里真是一场灾难。正如事实证明的那样，路沿被渲染成一种非常简单的纹理。更重要的是，所有的路沿都被渲染成了相同的纹理，路沿检测器很快就学习到了这个“特征”。</li>\n<li>当美军第一次试图在森林中探测坦克时，也发生了类似的事情。他们在没有坦克的情况下拍摄了森林的航拍照片，然后把坦克开进森林，拍摄了另一组照片。使用这两组数据训练的分类器似乎工作得很好。不幸的是，分类器仅仅学会了如何区分有阴影的树和没有阴影的树：第一组照片是在清晨拍摄的，而第二组是在中午拍摄的。</li>\n</ul>\n<h3 id=\"9-2-3-非平稳分布\"><a href=\"#9-2-3-非平稳分布\" class=\"headerlink\" title=\"9.2.3 非平稳分布\"></a>9.2.3 非平稳分布</h3><ul>\n<li>当分布变化缓慢并且模型没有得到充分更新时，就会出现更微妙的情况：非平稳分布（nonstationary distri‐bution）<ul>\n<li>训练一个计算广告模型，但却没有经常更新（例如，一个2009年训练的模型不知道一个叫iPad的不知名新设备刚刚上市）；</li>\n<li>建立一个垃圾邮件过滤器，它能很好地检测到所有垃圾邮件。但是，垃圾邮件发送者们变得聪明起来，制造出新的信息，看起来不像我们以前见过的任何垃圾邮件；</li>\n<li>建立一个产品推荐系统，它在整个冬天都有效，但圣诞节过后很久还会继续推荐圣诞帽。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"9-3-分布偏移纠正\"><a href=\"#9-3-分布偏移纠正\" class=\"headerlink\" title=\"9.3 分布偏移纠正\"></a>9.3 分布偏移纠正</h2><ul>\n<li>通过一些策略应对分布偏移</li>\n</ul>\n<h3 id=\"9-3-1-经验风险与实际风险\"><a href=\"#9-3-1-经验风险与实际风险\" class=\"headerlink\" title=\"9.3.1 经验风险与实际风险\"></a>9.3.1 经验风险与实际风险</h3><ul>\n<li>不考虑正则化，训练损失为：$\\operatorname * {minimize}_ f \\frac1n \\sum_ {i&#x3D;1}^ nl(f( \\mathbf{x}_ i),y_ i)$，也称经验风险，其中$l$是损失函数。</li>\n<li>经验风险是为了近似真实风险：整个训练数据上的平均损失，即从真实分布$p(x,y)$中抽取的所有数据的总体损失的期望值：<br>  $$E_{p(\\mathbf{x},y)}[l(f(\\mathbf{x}),y)]&#x3D;\\int\\int l(f(\\mathbf{x}),y)p(\\mathbf{x},y)~d\\mathbf{x}dy$$<br>  然而，我们通常无法获得总体数据。</li>\n</ul>\n<h3 id=\"9-3-2-协变量偏移纠正\"><a href=\"#9-3-2-协变量偏移纠正\" class=\"headerlink\" title=\"9.3.2 协变量偏移纠正\"></a>9.3.2 协变量偏移纠正</h3><ul>\n<li>假设对于带标签的数据(xi, yi)，我们要评估P(y | x)。然而观测值xi是从某些源分布q(x)中得出的，而不是从目标分布p(x)中得出的。幸运的是，依赖性假设意味着条件分布保持不变，即：p(y | x) &#x3D; q(y | x)。如果源分布q(x)是“错误的”，我们可以通过在真实风险的计算中，使用以下简单的恒等式来进行纠正：<br>$$\\begin{aligned}\\int\\int l(f(\\mathbf{x}),y)p(y\\mid\\mathbf{x})p(\\mathbf{x})d\\mathbf{x}dy&amp;&#x3D;\\int\\int l(f(\\mathbf{x}),y)q(y\\mid\\mathbf{x})q(\\mathbf{x})\\frac{p(\\mathbf{x})}{q(\\mathbf{x})}d\\mathbf{x}dy\\end{aligned}$$</li>\n<li>因此我们需要根据数据来自正确分布与来自错误分布的概率之比，来重新衡量每个数据样本的权重：<br>$$\\beta_i\\stackrel{\\mathrm{def}}{&#x3D;}\\frac{p(\\mathbf{x}_i)}{q(\\mathbf{x}_i)}$$</li>\n<li>将权重带入每个数据样本，得到目标函数：加权经验风险最小化<br>$$\\min_f\\text{mize }\\frac1n\\sum_{i&#x3D;1}^n\\beta_il(f(\\mathbf{x}_i),y_i)$$</li>\n<li>由于不知道这个比率，我们需要估计它。需要从两个分布中抽取样本：“真实”的分布p，通过访问测试数据获取；训练集q，通过人工合成的很容易获得。我们只需要特征x ∼ p(x)，不需要访问标签y ∼ p(y)。</li>\n<li>一种非常有效的方法：对数几率回归，这是用于二元分类的softmax回归的一个特例。我们学习了一个分类器来区分从p(x)抽取的数据和从q(x)抽取的数据。如果无法区分这两个分布，则意味着相关的样本可能来自这两个分布中的任何一个。另一方面，任何可以很好区分的样本都应该相应地显著增加或减少权重。</li>\n<li>假设我们分别从p(x)和q(x) 两个分布中抽取相同数量的样本。现在用z标签表示：从p抽取的数据为1，从q抽取的数据为−1。然后，混合数据集中的概率由下式给出<br>$$P(z&#x3D;1\\mid\\mathbf{x})&#x3D;\\frac{p(\\mathbf{x})}{p(\\mathbf{x})+q(\\mathbf{x})}\\text{ and hence }\\frac{P(z&#x3D;1\\mid\\mathbf{x})}{P(z&#x3D;-1\\mid\\mathbf{x})}&#x3D;\\frac{p(\\mathbf{x})}{q(\\mathbf{x})}$$</li>\n<li>因此，如果我们使用对数几率回归方法，其中$P(z&#x3D;1\\mid\\mathbf{x})&#x3D;\\frac{1}{1+\\exp(-h(\\mathbf{x}))}$（h是一个参数化函数），则（？？？）<br>$$\\beta_i&#x3D;\\frac{1&#x2F;(1+\\exp(-h(\\mathbf{x}_i)))}{\\exp(-h(\\mathbf{x}_i))&#x2F;(1+\\exp(-h(\\mathbf{x}_i)))}&#x3D;\\exp(h(\\mathbf{x}_i))$$</li>\n<li>因此，我们需要解决两个问题：第一个问题是关于区分来自两个分布的数据；第二个问题是关于 (4.9.5) 中的<br>加权经验风险的最小化问题。</li>\n<li>假设我们有一个训练集{$(x_1,y_1),···，(x_n,y_n)$}和一个未标记的测试集{$u_1,···,u_m$}.假设1 $\\leq$ i $\\leq$ n的$x_i$来自某个源分布，$u_i$来自目标分布。纠正协变量偏移的典型算法：<ol>\n<li>生成一个二元分类训练集：{$(x_1,-1),···,(x_n,-1),(u_1,1),···,(u_m,1)$}</li>\n<li>用对数几率回归训练二元分类器得到函数h。</li>\n<li>使用$\\beta_i &#x3D; exp(h(x_i))$或更好的$\\beta_i &#x3D; min(exp(h(x_i)),c)$(c为常量)对训练数据进行加权。</li>\n<li>使用权重$\\beta_i$进行加权经验风险最小化的训练。</li>\n</ol>\n</li>\n<li>上述算法依赖于一个重要的假设：需要目标分布(例如，测试分布)中的每个数据样本在训练时出现的概率非零。如果我们找到p(x) &gt; 0但q(x) &#x3D; 0的点，那么相应的重要性权重会是无穷大。</li>\n</ul>\n<h3 id=\"9-3-3-标签偏移纠正\"><a href=\"#9-3-3-标签偏移纠正\" class=\"headerlink\" title=\"9.3.3 标签偏移纠正\"></a>9.3.3 标签偏移纠正</h3><ul>\n<li>假设标签的分布随时间变化：q(y) ̸&#x3D; p(y)，但类别条件分布保持不变：q(x | y) &#x3D; p(x | y)。如果源分布q(y)是“错误的”，我们可以根据真实风险的定义进行更正：<br>$$\\int\\int l(f(\\mathbf{x}),y)p(\\mathbf{x}\\mid y)p(y)d\\mathbf{x}dy&#x3D;\\int\\int l(f(\\mathbf{x}),y)q(\\mathbf{x}\\mid y)q(y)\\frac{p(y)}{q(y)}d\\mathbf{x}dy$$</li>\n<li>重要性权重应用于标签似然比率：<br>$$\\beta_i\\stackrel{\\mathrm{def}}{&#x3D;}\\frac{p(y_i)}{q(y_i)}$$</li>\n<li>为了估计目标标签分布，我们首先采用性能相当好的现成的分类器（通常基于训练数据进行训练），并使用验证集（也来自训练分布）计算其混淆矩阵。混淆矩阵C是一个k × k矩阵，其中每列对应于标签类别，每行对应于模型的预测类别。每个单元格的值cij是验证集中，真实标签为j，而我们的模型预测为i的样本数量所占的比例。</li>\n<li>我们不能直接计算目标数据上的混淆矩阵，因为我们无法看到真实环境下的样本的标签，除非我们再搭建一个复杂的实时标注流程。然而，我们所能做的是将所有模型在测试时的预测取平均数，得到平均模型输出$\\mu(\\hat{\\mathbf{y}})\\in\\mathbb{R}^k$，其中第i个元素$\\mu(\\hat y_i)$是我们模型预测测试集中i的总预测分数。</li>\n<li>如果我们的分类器一开始就相当准确，并且目标数据只包含我们以前见过的类别，以及如果标签偏移假设成立（这里最强的假设），我们就可以通过求解一个简单的线性系统来估计测试集的标签分布<br>$$\\mathbf{C}p(\\mathbf{y})&#x3D;\\mu(\\hat{\\mathbf{y}})$$</li>\n<li>因为作为一个估计，$\\begin{aligned}\\sum_{j&#x3D;1}^kc_{ij}p(y_j)&#x3D;\\mu(\\hat{y}_i)\\end{aligned}$对所有$1\\leq i \\leq k$成立，其中$p(y_i)$是k维标签分布向量p(y)的第$j^{th}$元素。如果我们的分类器一开始就足够精确，那么混淆矩阵$\\mathbf C$将是可逆的，进而我们可以得到一个解$p(y)&#x3D;\\mathbf C^{-1}\\mu(\\hat y)$因为我们观测源数据上的标签，所以很容易估计分布q(y)。那么对于标签为yi的任何训练样本i，我们可以使用我们估计的p(yi)&#x2F;q(yi)比率来计算权重$\\beta_i$，并将其代入加权经验风险最小化中。</li>\n</ul>\n<h3 id=\"9-3-4-概念偏移纠正\"><a href=\"#9-3-4-概念偏移纠正\" class=\"headerlink\" title=\"9.3.4 概念偏移纠正\"></a>9.3.4 概念偏移纠正</h3><ul>\n<li>概念偏移很难用原则性的方式解决。例如，在一个问题突然从“区分猫和狗”偏移为“区分白色和黑色动物”的情况下，除了从零开始收集新标签和训练，别无妙方。幸运的是，在实践中这种极端的偏移是罕见的。相反，通常情况下，概念的变化总是缓慢的。比如下面是一些例子：<ul>\n<li>在计算广告中，新产品推出后，旧产品变得不那么受欢迎了。这意味着广告的分布和受欢迎程度是逐渐<br>  变化的，任何点击率预测器都需要随之逐渐变化；</li>\n<li>由于环境的磨损，交通摄像头的镜头会逐渐退化，影响摄像头的图像质量；</li>\n<li>新闻内容逐渐变化（即新新闻的出现）。</li>\n</ul>\n</li>\n<li>在这种情况下，我们可以使用与训练网络相同的方法，使其适应数据的变化。换言之，我们使用新数据更新现有的网络权重，而不是从头开始训练。</li>\n</ul>\n<h2 id=\"9-4-学习问题的分类法\"><a href=\"#9-4-学习问题的分类法\" class=\"headerlink\" title=\"9.4 学习问题的分类法\"></a>9.4 学习问题的分类法</h2><h3 id=\"9-4-1-批量学习\"><a href=\"#9-4-1-批量学习\" class=\"headerlink\" title=\"9.4.1 批量学习\"></a>9.4.1 批量学习</h3><ul>\n<li>在批量学习（batch learning）中，我们可以访问一组训练特征和标签 {(x1, y1), . . . ,(xn, yn)}，我们使用这些特性和标签训练f(x)。然后，我们部署此模型来对来自同一分布的新数据(x, y)进行评分。例如，我们可以根据猫和狗的大量图片训练猫检测器。一旦我们训练了它，我们就把它作为智能猫门计算视觉系统的一部分，来控制只允许猫进入。然后这个系统会被安装在客户家中，基本再也不会更新。</li>\n</ul>\n<h3 id=\"9-4-2-在线学习\"><a href=\"#9-4-2-在线学习\" class=\"headerlink\" title=\"9.4.2 在线学习\"></a>9.4.2 在线学习</h3><ul>\n<li>除了“批量”地学习，我们还可以单个“在线”学习数据(xi, yi)。更具体地说，我们首先观测到xi，然后我们得出一个估计值f(xi)，只有当我们做到这一点后，我们才观测到yi。然后根据我们的决定，我们会得到奖励或损失。许多实际问题都属于这一类。例如，我们需要预测明天的股票价格，这样我们就可以根据这个预测进行交易。在一天结束时，我们会评估我们的预测是否盈利。换句话说，在在线学习（online learning）中，我们有以下的循环。在这个循环中，给定新的观测结果，我们会不断地改进我们的模型。<br>$$ \\text{model }f_ t \\longrightarrow \\text{data } \\mathbf{x}_ t \\longrightarrow \\text{estimate }f_ t( \\mathbf{x}_ t) \\longrightarrow \\text{observation }y_t \\longrightarrow \\text{loss }l(y_ t,f_ t( \\mathbf{x}_ t)) \\longrightarrow \\text{model }f_ {t+1}$$</li>\n</ul>\n<h3 id=\"9-4-3-老虎机\"><a href=\"#9-4-3-老虎机\" class=\"headerlink\" title=\"9.4.3 老虎机\"></a>9.4.3 老虎机</h3><ul>\n<li>老虎机（bandits）是上述问题的一个特例。虽然在大多数学习问题中，我们有一个连续参数化的函数f（例如，一个深度网络）。但在一个老虎机问题中，我们只有有限数量的手臂可以拉动。也就是说，我们可以采取的行动是有限的。对于这个更简单的问题，可以获得更强的最优性理论保证，这并不令人惊讶。我们之所以列出它，主要是因为这个问题经常被视为一个单独的学习问题的情景。</li>\n</ul>\n<h3 id=\"9-4-4-控制\"><a href=\"#9-4-4-控制\" class=\"headerlink\" title=\"9.4.4 控制\"></a>9.4.4 控制</h3><ul>\n<li>在很多情况下，环境会记住我们所做的事。不一定是以一种对抗的方式，但它会记住，而且它的反应将取决于之前发生的事情。例如，咖啡锅炉控制器将根据之前是否加热锅炉来观测到不同的温度。在这种情况下，PID（比例—积分—微分）控制器算法是一个流行的选择。同样，一个用户在新闻网站上的行为将取决于之前向她展示的内容（例如，大多数新闻她只阅读一次）。许多这样的算法形成了一个环境模型，在这个模型中，他们的行为使得他们的决策看起来不那么随机。近年来，控制理论（如PID的变体）也被用于自动调整超参数，以<br>获得更好的解构和重建质量，提高生成文本的多样性和生成图像的重建质量。</li>\n</ul>\n<h3 id=\"9-4-5-强化学习\"><a href=\"#9-4-5-强化学习\" class=\"headerlink\" title=\"9.4.5 强化学习\"></a>9.4.5 强化学习</h3><ul>\n<li>强化学习（reinforcement learning）强调如何基于环境而行动，以取得最大化的预期利益。国际象棋、围棋、西洋双陆棋或星际争霸都是强化学习的应用实例。再比如，为自动驾驶汽车制造一个控制器，或者以其他方式对自动驾驶汽车的驾驶方式做出反应（例如，试图避开某物体，试图造成事故，或者试图与其合作）。</li>\n</ul>\n<h3 id=\"9-4-6-考虑到环境\"><a href=\"#9-4-6-考虑到环境\" class=\"headerlink\" title=\"9.4.6 考虑到环境\"></a>9.4.6 考虑到环境</h3><ul>\n<li>上述不同情况之间的一个关键区别是：在静止环境中可能一直有效的相同策略，在环境能够改变的情况下可能不会始终有效。例如，一个交易者发现的套利机会很可能在他开始利用它时就消失了。环境变化的速度和方式在很大程度上决定了我们可以采用的算法类型。例如，如果我们知道事情只会缓慢地变化，就可以迫使任何估计也只能缓慢地发生改变。如果我们知道环境可能会瞬间发生变化，但这种变化非常罕见，我们就可以在使用算法时考虑到这一点。当一个数据科学家试图解决的问题会随着时间的推移而发生变化时，这些类<br>型的知识至关重要。</li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"9-环境和分布偏移\"><a href=\"#9-环境和分布偏移\" class=\"headerlink\" title=\"9 环境和分布偏移\"></a>9 环境和分布偏移</h1><ul>\n<li>（不好懂，建议跳过这一节）</li>\n<li>在不同的应用场景模型决策方案会出现漏洞</li>\n</ul>\n<h2 id=\"9-1-分布偏移的类型\"><a href=\"#9-1-分布偏移的类型\" class=\"headerlink\" title=\"9.1 分布偏移的类型\"></a>9.1 分布偏移的类型</h2><ul>\n<li>考虑一个二元分类问题：区分狗和猫。如果分布可以以任意方式偏移，那么我们的情景允许病态的情况，即输入的分布保持不变：$p_S(x) &#x3D; p_T (x)$，但标签全部翻转：$p_S(y|x) &#x3D; 1 − p_T (y|x)$。</li>\n</ul>\n<h3 id=\"9-1-1-协变量偏移\"><a href=\"#9-1-1-协变量偏移\" class=\"headerlink\" title=\"9.1.1 协变量偏移\"></a>9.1.1 协变量偏移</h3><ul>\n<li>虽然输入的分布可能随时间而改变，但标签函数（即条件分布P(y | x)）没有改变。统计学家称之为协变量偏移（covariate shift），因为这个问题是由于协变量（特征）分布的变化而产生的。</li>\n<li>比如用真的猫狗图像做训练，卡通猫狗做测试。</li>\n</ul>\n<h3 id=\"9-1-2-标签偏移\"><a href=\"#9-1-2-标签偏移\" class=\"headerlink\" title=\"9.1.2 标签偏移\"></a>9.1.2 标签偏移</h3><ul>\n<li>与协变量偏移相反。假设标签边缘概率P(y)可以改变，但是类别条件分布P(x | y)在不同的领域之间保持不变。当我们认为y导致x时，标签偏移是一个合理的假设。例如，预测患者的疾病，我们可能根据症状来判断，即使疾病的相对流行率随着时间的推移而变化。</li>\n<li>在另一些情况下，标签偏移和协变量偏移假设可以同时成立。例如，当标签是确定的，即使y导致x，协变量偏移假设也会得到满足。有趣的是，在这些情况下，使用基于标签偏移假设的方法通常是有利的。这是因为这些方法倾向于包含看起来像标签（通常是低维）的对象，而不是像输入（通常是高维的）对象。</li>\n</ul>\n<h3 id=\"9-1-3-概念偏移\"><a href=\"#9-1-3-概念偏移\" class=\"headerlink\" title=\"9.1.3 概念偏移\"></a>9.1.3 概念偏移</h3><ul>\n<li>当标签的定义发生变化时，就会出现这种问题。</li>\n<li>如果我们要建立一个机器翻译系统，P(y | x)的分布可能会因我们的位置不同而得到不同的翻译。这个问题可能很难被发现。所以，我们最好可以利用在时间或空间上逐渐发生偏移的知识。</li>\n</ul>\n<h2 id=\"9-2-分布偏移示例\"><a href=\"#9-2-分布偏移示例\" class=\"headerlink\" title=\"9.2 分布偏移示例\"></a>9.2 分布偏移示例</h2><h3 id=\"9-2-1-医学诊断\"><a href=\"#9-2-1-医学诊断\" class=\"headerlink\" title=\"9.2.1 医学诊断\"></a>9.2.1 医学诊断</h3><ul>\n<li>假设我们想设计一个检测癌症的算法，从健康人和病人那里收集数据，然后训练算法。收集训练数据的分布和在实际中遇到的数据分布可能有很大的不同。用近乎完美的精度来区分健康和患病人群确实很容易。然而，这可能是因为受试者在年龄、激素水平、体力活动、饮食、饮酒以及其他许多与疾病无关的因素上存在差异。这对检测疾病的分类器可能并不适用。这些抽样可能会遇到极端的协变量偏移。此外，这种情况不太可能通过常规方法加以纠正。</li>\n</ul>\n<h3 id=\"9-2-2-自动驾驶汽车\"><a href=\"#9-2-2-自动驾驶汽车\" class=\"headerlink\" title=\"9.2.2 自动驾驶汽车\"></a>9.2.2 自动驾驶汽车</h3><ul>\n<li>对于一家想利用机器学习来开发自动驾驶汽车的公司，一个关键部件是“路沿检测器”。由于真实的注释数据获取成本很高，他们想出了一个“聪明”的想法：将游戏渲染引擎中的合成数据用作额外的训练数据。这对从渲染引擎中抽取的“测试数据”非常有效，但应用在一辆真正的汽车里真是一场灾难。正如事实证明的那样，路沿被渲染成一种非常简单的纹理。更重要的是，所有的路沿都被渲染成了相同的纹理，路沿检测器很快就学习到了这个“特征”。</li>\n<li>当美军第一次试图在森林中探测坦克时，也发生了类似的事情。他们在没有坦克的情况下拍摄了森林的航拍照片，然后把坦克开进森林，拍摄了另一组照片。使用这两组数据训练的分类器似乎工作得很好。不幸的是，分类器仅仅学会了如何区分有阴影的树和没有阴影的树：第一组照片是在清晨拍摄的，而第二组是在中午拍摄的。</li>\n</ul>\n<h3 id=\"9-2-3-非平稳分布\"><a href=\"#9-2-3-非平稳分布\" class=\"headerlink\" title=\"9.2.3 非平稳分布\"></a>9.2.3 非平稳分布</h3><ul>\n<li>当分布变化缓慢并且模型没有得到充分更新时，就会出现更微妙的情况：非平稳分布（nonstationary distri‐bution）<ul>\n<li>训练一个计算广告模型，但却没有经常更新（例如，一个2009年训练的模型不知道一个叫iPad的不知名新设备刚刚上市）；</li>\n<li>建立一个垃圾邮件过滤器，它能很好地检测到所有垃圾邮件。但是，垃圾邮件发送者们变得聪明起来，制造出新的信息，看起来不像我们以前见过的任何垃圾邮件；</li>\n<li>建立一个产品推荐系统，它在整个冬天都有效，但圣诞节过后很久还会继续推荐圣诞帽。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"9-3-分布偏移纠正\"><a href=\"#9-3-分布偏移纠正\" class=\"headerlink\" title=\"9.3 分布偏移纠正\"></a>9.3 分布偏移纠正</h2><ul>\n<li>通过一些策略应对分布偏移</li>\n</ul>\n<h3 id=\"9-3-1-经验风险与实际风险\"><a href=\"#9-3-1-经验风险与实际风险\" class=\"headerlink\" title=\"9.3.1 经验风险与实际风险\"></a>9.3.1 经验风险与实际风险</h3><ul>\n<li>不考虑正则化，训练损失为：$\\operatorname * {minimize}_ f \\frac1n \\sum_ {i&#x3D;1}^ nl(f( \\mathbf{x}_ i),y_ i)$，也称经验风险，其中$l$是损失函数。</li>\n<li>经验风险是为了近似真实风险：整个训练数据上的平均损失，即从真实分布$p(x,y)$中抽取的所有数据的总体损失的期望值：<br>  $$E_{p(\\mathbf{x},y)}[l(f(\\mathbf{x}),y)]&#x3D;\\int\\int l(f(\\mathbf{x}),y)p(\\mathbf{x},y)~d\\mathbf{x}dy$$<br>  然而，我们通常无法获得总体数据。</li>\n</ul>\n<h3 id=\"9-3-2-协变量偏移纠正\"><a href=\"#9-3-2-协变量偏移纠正\" class=\"headerlink\" title=\"9.3.2 协变量偏移纠正\"></a>9.3.2 协变量偏移纠正</h3><ul>\n<li>假设对于带标签的数据(xi, yi)，我们要评估P(y | x)。然而观测值xi是从某些源分布q(x)中得出的，而不是从目标分布p(x)中得出的。幸运的是，依赖性假设意味着条件分布保持不变，即：p(y | x) &#x3D; q(y | x)。如果源分布q(x)是“错误的”，我们可以通过在真实风险的计算中，使用以下简单的恒等式来进行纠正：<br>$$\\begin{aligned}\\int\\int l(f(\\mathbf{x}),y)p(y\\mid\\mathbf{x})p(\\mathbf{x})d\\mathbf{x}dy&amp;&#x3D;\\int\\int l(f(\\mathbf{x}),y)q(y\\mid\\mathbf{x})q(\\mathbf{x})\\frac{p(\\mathbf{x})}{q(\\mathbf{x})}d\\mathbf{x}dy\\end{aligned}$$</li>\n<li>因此我们需要根据数据来自正确分布与来自错误分布的概率之比，来重新衡量每个数据样本的权重：<br>$$\\beta_i\\stackrel{\\mathrm{def}}{&#x3D;}\\frac{p(\\mathbf{x}_i)}{q(\\mathbf{x}_i)}$$</li>\n<li>将权重带入每个数据样本，得到目标函数：加权经验风险最小化<br>$$\\min_f\\text{mize }\\frac1n\\sum_{i&#x3D;1}^n\\beta_il(f(\\mathbf{x}_i),y_i)$$</li>\n<li>由于不知道这个比率，我们需要估计它。需要从两个分布中抽取样本：“真实”的分布p，通过访问测试数据获取；训练集q，通过人工合成的很容易获得。我们只需要特征x ∼ p(x)，不需要访问标签y ∼ p(y)。</li>\n<li>一种非常有效的方法：对数几率回归，这是用于二元分类的softmax回归的一个特例。我们学习了一个分类器来区分从p(x)抽取的数据和从q(x)抽取的数据。如果无法区分这两个分布，则意味着相关的样本可能来自这两个分布中的任何一个。另一方面，任何可以很好区分的样本都应该相应地显著增加或减少权重。</li>\n<li>假设我们分别从p(x)和q(x) 两个分布中抽取相同数量的样本。现在用z标签表示：从p抽取的数据为1，从q抽取的数据为−1。然后，混合数据集中的概率由下式给出<br>$$P(z&#x3D;1\\mid\\mathbf{x})&#x3D;\\frac{p(\\mathbf{x})}{p(\\mathbf{x})+q(\\mathbf{x})}\\text{ and hence }\\frac{P(z&#x3D;1\\mid\\mathbf{x})}{P(z&#x3D;-1\\mid\\mathbf{x})}&#x3D;\\frac{p(\\mathbf{x})}{q(\\mathbf{x})}$$</li>\n<li>因此，如果我们使用对数几率回归方法，其中$P(z&#x3D;1\\mid\\mathbf{x})&#x3D;\\frac{1}{1+\\exp(-h(\\mathbf{x}))}$（h是一个参数化函数），则（？？？）<br>$$\\beta_i&#x3D;\\frac{1&#x2F;(1+\\exp(-h(\\mathbf{x}_i)))}{\\exp(-h(\\mathbf{x}_i))&#x2F;(1+\\exp(-h(\\mathbf{x}_i)))}&#x3D;\\exp(h(\\mathbf{x}_i))$$</li>\n<li>因此，我们需要解决两个问题：第一个问题是关于区分来自两个分布的数据；第二个问题是关于 (4.9.5) 中的<br>加权经验风险的最小化问题。</li>\n<li>假设我们有一个训练集{$(x_1,y_1),···，(x_n,y_n)$}和一个未标记的测试集{$u_1,···,u_m$}.假设1 $\\leq$ i $\\leq$ n的$x_i$来自某个源分布，$u_i$来自目标分布。纠正协变量偏移的典型算法：<ol>\n<li>生成一个二元分类训练集：{$(x_1,-1),···,(x_n,-1),(u_1,1),···,(u_m,1)$}</li>\n<li>用对数几率回归训练二元分类器得到函数h。</li>\n<li>使用$\\beta_i &#x3D; exp(h(x_i))$或更好的$\\beta_i &#x3D; min(exp(h(x_i)),c)$(c为常量)对训练数据进行加权。</li>\n<li>使用权重$\\beta_i$进行加权经验风险最小化的训练。</li>\n</ol>\n</li>\n<li>上述算法依赖于一个重要的假设：需要目标分布(例如，测试分布)中的每个数据样本在训练时出现的概率非零。如果我们找到p(x) &gt; 0但q(x) &#x3D; 0的点，那么相应的重要性权重会是无穷大。</li>\n</ul>\n<h3 id=\"9-3-3-标签偏移纠正\"><a href=\"#9-3-3-标签偏移纠正\" class=\"headerlink\" title=\"9.3.3 标签偏移纠正\"></a>9.3.3 标签偏移纠正</h3><ul>\n<li>假设标签的分布随时间变化：q(y) ̸&#x3D; p(y)，但类别条件分布保持不变：q(x | y) &#x3D; p(x | y)。如果源分布q(y)是“错误的”，我们可以根据真实风险的定义进行更正：<br>$$\\int\\int l(f(\\mathbf{x}),y)p(\\mathbf{x}\\mid y)p(y)d\\mathbf{x}dy&#x3D;\\int\\int l(f(\\mathbf{x}),y)q(\\mathbf{x}\\mid y)q(y)\\frac{p(y)}{q(y)}d\\mathbf{x}dy$$</li>\n<li>重要性权重应用于标签似然比率：<br>$$\\beta_i\\stackrel{\\mathrm{def}}{&#x3D;}\\frac{p(y_i)}{q(y_i)}$$</li>\n<li>为了估计目标标签分布，我们首先采用性能相当好的现成的分类器（通常基于训练数据进行训练），并使用验证集（也来自训练分布）计算其混淆矩阵。混淆矩阵C是一个k × k矩阵，其中每列对应于标签类别，每行对应于模型的预测类别。每个单元格的值cij是验证集中，真实标签为j，而我们的模型预测为i的样本数量所占的比例。</li>\n<li>我们不能直接计算目标数据上的混淆矩阵，因为我们无法看到真实环境下的样本的标签，除非我们再搭建一个复杂的实时标注流程。然而，我们所能做的是将所有模型在测试时的预测取平均数，得到平均模型输出$\\mu(\\hat{\\mathbf{y}})\\in\\mathbb{R}^k$，其中第i个元素$\\mu(\\hat y_i)$是我们模型预测测试集中i的总预测分数。</li>\n<li>如果我们的分类器一开始就相当准确，并且目标数据只包含我们以前见过的类别，以及如果标签偏移假设成立（这里最强的假设），我们就可以通过求解一个简单的线性系统来估计测试集的标签分布<br>$$\\mathbf{C}p(\\mathbf{y})&#x3D;\\mu(\\hat{\\mathbf{y}})$$</li>\n<li>因为作为一个估计，$\\begin{aligned}\\sum_{j&#x3D;1}^kc_{ij}p(y_j)&#x3D;\\mu(\\hat{y}_i)\\end{aligned}$对所有$1\\leq i \\leq k$成立，其中$p(y_i)$是k维标签分布向量p(y)的第$j^{th}$元素。如果我们的分类器一开始就足够精确，那么混淆矩阵$\\mathbf C$将是可逆的，进而我们可以得到一个解$p(y)&#x3D;\\mathbf C^{-1}\\mu(\\hat y)$因为我们观测源数据上的标签，所以很容易估计分布q(y)。那么对于标签为yi的任何训练样本i，我们可以使用我们估计的p(yi)&#x2F;q(yi)比率来计算权重$\\beta_i$，并将其代入加权经验风险最小化中。</li>\n</ul>\n<h3 id=\"9-3-4-概念偏移纠正\"><a href=\"#9-3-4-概念偏移纠正\" class=\"headerlink\" title=\"9.3.4 概念偏移纠正\"></a>9.3.4 概念偏移纠正</h3><ul>\n<li>概念偏移很难用原则性的方式解决。例如，在一个问题突然从“区分猫和狗”偏移为“区分白色和黑色动物”的情况下，除了从零开始收集新标签和训练，别无妙方。幸运的是，在实践中这种极端的偏移是罕见的。相反，通常情况下，概念的变化总是缓慢的。比如下面是一些例子：<ul>\n<li>在计算广告中，新产品推出后，旧产品变得不那么受欢迎了。这意味着广告的分布和受欢迎程度是逐渐<br>  变化的，任何点击率预测器都需要随之逐渐变化；</li>\n<li>由于环境的磨损，交通摄像头的镜头会逐渐退化，影响摄像头的图像质量；</li>\n<li>新闻内容逐渐变化（即新新闻的出现）。</li>\n</ul>\n</li>\n<li>在这种情况下，我们可以使用与训练网络相同的方法，使其适应数据的变化。换言之，我们使用新数据更新现有的网络权重，而不是从头开始训练。</li>\n</ul>\n<h2 id=\"9-4-学习问题的分类法\"><a href=\"#9-4-学习问题的分类法\" class=\"headerlink\" title=\"9.4 学习问题的分类法\"></a>9.4 学习问题的分类法</h2><h3 id=\"9-4-1-批量学习\"><a href=\"#9-4-1-批量学习\" class=\"headerlink\" title=\"9.4.1 批量学习\"></a>9.4.1 批量学习</h3><ul>\n<li>在批量学习（batch learning）中，我们可以访问一组训练特征和标签 {(x1, y1), . . . ,(xn, yn)}，我们使用这些特性和标签训练f(x)。然后，我们部署此模型来对来自同一分布的新数据(x, y)进行评分。例如，我们可以根据猫和狗的大量图片训练猫检测器。一旦我们训练了它，我们就把它作为智能猫门计算视觉系统的一部分，来控制只允许猫进入。然后这个系统会被安装在客户家中，基本再也不会更新。</li>\n</ul>\n<h3 id=\"9-4-2-在线学习\"><a href=\"#9-4-2-在线学习\" class=\"headerlink\" title=\"9.4.2 在线学习\"></a>9.4.2 在线学习</h3><ul>\n<li>除了“批量”地学习，我们还可以单个“在线”学习数据(xi, yi)。更具体地说，我们首先观测到xi，然后我们得出一个估计值f(xi)，只有当我们做到这一点后，我们才观测到yi。然后根据我们的决定，我们会得到奖励或损失。许多实际问题都属于这一类。例如，我们需要预测明天的股票价格，这样我们就可以根据这个预测进行交易。在一天结束时，我们会评估我们的预测是否盈利。换句话说，在在线学习（online learning）中，我们有以下的循环。在这个循环中，给定新的观测结果，我们会不断地改进我们的模型。<br>$$ \\text{model }f_ t \\longrightarrow \\text{data } \\mathbf{x}_ t \\longrightarrow \\text{estimate }f_ t( \\mathbf{x}_ t) \\longrightarrow \\text{observation }y_t \\longrightarrow \\text{loss }l(y_ t,f_ t( \\mathbf{x}_ t)) \\longrightarrow \\text{model }f_ {t+1}$$</li>\n</ul>\n<h3 id=\"9-4-3-老虎机\"><a href=\"#9-4-3-老虎机\" class=\"headerlink\" title=\"9.4.3 老虎机\"></a>9.4.3 老虎机</h3><ul>\n<li>老虎机（bandits）是上述问题的一个特例。虽然在大多数学习问题中，我们有一个连续参数化的函数f（例如，一个深度网络）。但在一个老虎机问题中，我们只有有限数量的手臂可以拉动。也就是说，我们可以采取的行动是有限的。对于这个更简单的问题，可以获得更强的最优性理论保证，这并不令人惊讶。我们之所以列出它，主要是因为这个问题经常被视为一个单独的学习问题的情景。</li>\n</ul>\n<h3 id=\"9-4-4-控制\"><a href=\"#9-4-4-控制\" class=\"headerlink\" title=\"9.4.4 控制\"></a>9.4.4 控制</h3><ul>\n<li>在很多情况下，环境会记住我们所做的事。不一定是以一种对抗的方式，但它会记住，而且它的反应将取决于之前发生的事情。例如，咖啡锅炉控制器将根据之前是否加热锅炉来观测到不同的温度。在这种情况下，PID（比例—积分—微分）控制器算法是一个流行的选择。同样，一个用户在新闻网站上的行为将取决于之前向她展示的内容（例如，大多数新闻她只阅读一次）。许多这样的算法形成了一个环境模型，在这个模型中，他们的行为使得他们的决策看起来不那么随机。近年来，控制理论（如PID的变体）也被用于自动调整超参数，以<br>获得更好的解构和重建质量，提高生成文本的多样性和生成图像的重建质量。</li>\n</ul>\n<h3 id=\"9-4-5-强化学习\"><a href=\"#9-4-5-强化学习\" class=\"headerlink\" title=\"9.4.5 强化学习\"></a>9.4.5 强化学习</h3><ul>\n<li>强化学习（reinforcement learning）强调如何基于环境而行动，以取得最大化的预期利益。国际象棋、围棋、西洋双陆棋或星际争霸都是强化学习的应用实例。再比如，为自动驾驶汽车制造一个控制器，或者以其他方式对自动驾驶汽车的驾驶方式做出反应（例如，试图避开某物体，试图造成事故，或者试图与其合作）。</li>\n</ul>\n<h3 id=\"9-4-6-考虑到环境\"><a href=\"#9-4-6-考虑到环境\" class=\"headerlink\" title=\"9.4.6 考虑到环境\"></a>9.4.6 考虑到环境</h3><ul>\n<li>上述不同情况之间的一个关键区别是：在静止环境中可能一直有效的相同策略，在环境能够改变的情况下可能不会始终有效。例如，一个交易者发现的套利机会很可能在他开始利用它时就消失了。环境变化的速度和方式在很大程度上决定了我们可以采用的算法类型。例如，如果我们知道事情只会缓慢地变化，就可以迫使任何估计也只能缓慢地发生改变。如果我们知道环境可能会瞬间发生变化，但这种变化非常罕见，我们就可以在使用算法时考虑到这一点。当一个数据科学家试图解决的问题会随着时间的推移而发生变化时，这些类<br>型的知识至关重要。</li>\n</ul>"},{"title":"3.7 前向传播、反向传播和计算图","date":"2024-02-07T06:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 7 前向传播、反向传播和计算图\n- 以带dropout的单层隐藏层mlp为例\n\n## 7.1 前向传播\n- 按顺序（从输入层到输出层）计算和存储神经网络中每层的结果s。\n    - 1）输入是$\\mathbf{x} \\in \\mathbb{R}^d$，隐藏层不含偏置,得到中间变量：\n    $$\\mathbf{z}=\\mathbf{W}^{(1)}\\mathbf{x}$$\n    - 2）激活：\n    $$\\mathbf{h}=\\phi(\\mathbf{z})$$\n    - 3）输出：\n    $$\\mathbf{o}=\\mathbf{W}^{(2)}\\mathbf{h}$$\n    - 4）损失：\n    $$L=l(\\mathbf{o},y)$$\n    - 5）正则化项（权重衰减）：\n        $$s=\\frac{\\lambda}{2}(||\\mathbf{W}^{(1)}||^2_F+||\\mathbf{W}^{(2)}||^2_F)$$\n        - 其中矩阵的Frobenius范数是将矩阵展平成向量后应用$L_2$范数。\n    - 6）目标函数：\n    $$J=L+s$$\n    \n## 7.2 前向传播计算图\n- 7.1的计算图：\n![](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\7\\1.png)\n![](img/deeplearning/code/pytorch/3_mlp/7/1.png)\n\n\n## 7.3 反向传播\n- 根据微积分中的链式规则，按相反的顺序从输出层到输入层遍历网络。\n- 链式法则：假设有$Y=f(X)$和$Z=g(Y)$，有:\n    $$\\frac{\\partial{Z}}{\\partial{X}}=prod(\\frac{\\partial{Z}}{\\partial{Y}},\\frac{\\partial{Y}}{\\partial{X}})$$\n    - prod表示一定的操作\n- 针对前向传播计算图计算反向梯度：\n    $$ \\frac{ \\partial J}{ \\partial L} = 1 \\mathrm{ ~ and ~ } \\frac{ \\partial J}{ \\partial s} = 1 $$\n\n    $$\\frac{\\partial J}{\\partial\\mathbf{o}}=\\text{prod}\\left(\\frac{\\partial J}{\\partial L},\\frac{\\partial L}{\\partial\\mathbf{o}}\\right)=\\frac{\\partial L}{\\partial\\mathbf{o}}\\in\\mathbb{R}^q$$\n\n    $$ \\frac{ \\partial s}{ \\partial \\mathbf{W}^ {(1)}}= \\lambda \\mathbf{W}^ {(1)} \\mathrm{~ and ~} \\frac{ \\partial s}{ \\partial \\mathbf{W}^ {(2)}}= \\lambda \\mathbf{W}^ {(2)}$$\n\n    $$\\frac{\\partial J}{\\partial\\mathbf{W}^{(2)}}=\\mathrm{prod}\\left(\\frac{\\partial J}{\\partial\\mathbf{o}},\\frac{\\partial\\mathbf{o}}{\\partial\\mathbf{W}^{(2)}}\\right)+\\mathrm{prod}\\left(\\frac{\\partial J}{\\partial s},\\frac{\\partial s}{\\partial\\mathbf{W}^{(2)}}\\right)=\\frac{\\partial J}{\\partial\\mathbf{o}}\\mathbf{h}^\\top+\\lambda\\mathbf{W}^{(2)}$$\n\n    $$\\frac{\\partial J}{\\partial\\mathbf{h}}=\\text{prod}\\left(\\frac{\\partial J}{\\partial\\mathbf{o}},\\frac{\\partial\\mathbf{o}}{\\partial\\mathbf{h}}\\right)=\\mathbf{W}^{(2)^\\top}\\frac{\\partial J}{\\partial\\mathbf{o}}$$\n    由于激活函数$\\phi$是按元素计算的，因此计算$\\frac{\\partial J}{\\partial \\mathbf{z}}$需要使用按元素乘法运算符：\n    $$\\frac{\\partial J}{\\partial\\mathbf{z}}=\\mathrm{prod}\\left(\\frac{\\partial J}{\\partial\\mathbf{h}},\\frac{\\partial\\mathbf{h}}{\\partial\\mathbf{z}}\\right)=\\frac{\\partial J}{\\partial\\mathbf{h}}\\odot\\phi^{\\prime}\\left(\\mathbf{z}\\right)$$\n\n    $$\\begin{aligned}\\frac{\\partial J}{\\partial\\mathbf{W}^{(1)}}&=\\text{prod}\\left(\\frac{\\partial J}{\\partial\\mathbf{z}},\\frac{\\partial\\mathbf{z}}{\\partial\\mathbf{W}^{(1)}}\\right)+\\text{prod}\\left(\\frac{\\partial J}{\\partial s},\\frac{\\partial s}{\\partial\\mathbf{W}^{(1)}}\\right)=\\frac{\\partial J}{\\partial\\mathbf{z}}\\mathbf{x}^\\top+\\lambda\\mathbf{W}^{(1)}\\end{aligned}$$\n\n## 7.4 训练神经网络\n- 训练神经网络时，对于前向传播，沿着依赖的方向遍历计算图并计算路径上的所有变量。然后将这些用于反向传播，其中计算顺序与计算图的相反。\n- 一方面，在前向传播期间计算正则项取决于模型参数W(1)和 W(2)的当前值。它们是由优化算法根据最近迭代的反向传播给出的。\n- 另一方面，反向传播期间参数$\\frac{\\partial J}{\\partial W^{(2)}}$的梯度计算，取决于由前向传播给出的隐藏变量h的当前值。\n- 因此，在训练神经网络时，在初始化模型参数后，我们交替使用前向传播和反向传播，利用反向传播给出的梯度来更新模型参数。\n- 反向传播重复利用前向传播中存储的中间值，以避免重复计算。带来的影响之一是我们需要保留中间值，直到反向传播完成。\n","source":"_posts/deeplearning/code/pytorch/3_mlp/7_backward.md","raw":"---\ntitle: 3.7 前向传播、反向传播和计算图\ndate: 2024-2-7 14:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 7 前向传播、反向传播和计算图\n- 以带dropout的单层隐藏层mlp为例\n\n## 7.1 前向传播\n- 按顺序（从输入层到输出层）计算和存储神经网络中每层的结果s。\n    - 1）输入是$\\mathbf{x} \\in \\mathbb{R}^d$，隐藏层不含偏置,得到中间变量：\n    $$\\mathbf{z}=\\mathbf{W}^{(1)}\\mathbf{x}$$\n    - 2）激活：\n    $$\\mathbf{h}=\\phi(\\mathbf{z})$$\n    - 3）输出：\n    $$\\mathbf{o}=\\mathbf{W}^{(2)}\\mathbf{h}$$\n    - 4）损失：\n    $$L=l(\\mathbf{o},y)$$\n    - 5）正则化项（权重衰减）：\n        $$s=\\frac{\\lambda}{2}(||\\mathbf{W}^{(1)}||^2_F+||\\mathbf{W}^{(2)}||^2_F)$$\n        - 其中矩阵的Frobenius范数是将矩阵展平成向量后应用$L_2$范数。\n    - 6）目标函数：\n    $$J=L+s$$\n    \n## 7.2 前向传播计算图\n- 7.1的计算图：\n![](D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\7\\1.png)\n![](img/deeplearning/code/pytorch/3_mlp/7/1.png)\n\n\n## 7.3 反向传播\n- 根据微积分中的链式规则，按相反的顺序从输出层到输入层遍历网络。\n- 链式法则：假设有$Y=f(X)$和$Z=g(Y)$，有:\n    $$\\frac{\\partial{Z}}{\\partial{X}}=prod(\\frac{\\partial{Z}}{\\partial{Y}},\\frac{\\partial{Y}}{\\partial{X}})$$\n    - prod表示一定的操作\n- 针对前向传播计算图计算反向梯度：\n    $$ \\frac{ \\partial J}{ \\partial L} = 1 \\mathrm{ ~ and ~ } \\frac{ \\partial J}{ \\partial s} = 1 $$\n\n    $$\\frac{\\partial J}{\\partial\\mathbf{o}}=\\text{prod}\\left(\\frac{\\partial J}{\\partial L},\\frac{\\partial L}{\\partial\\mathbf{o}}\\right)=\\frac{\\partial L}{\\partial\\mathbf{o}}\\in\\mathbb{R}^q$$\n\n    $$ \\frac{ \\partial s}{ \\partial \\mathbf{W}^ {(1)}}= \\lambda \\mathbf{W}^ {(1)} \\mathrm{~ and ~} \\frac{ \\partial s}{ \\partial \\mathbf{W}^ {(2)}}= \\lambda \\mathbf{W}^ {(2)}$$\n\n    $$\\frac{\\partial J}{\\partial\\mathbf{W}^{(2)}}=\\mathrm{prod}\\left(\\frac{\\partial J}{\\partial\\mathbf{o}},\\frac{\\partial\\mathbf{o}}{\\partial\\mathbf{W}^{(2)}}\\right)+\\mathrm{prod}\\left(\\frac{\\partial J}{\\partial s},\\frac{\\partial s}{\\partial\\mathbf{W}^{(2)}}\\right)=\\frac{\\partial J}{\\partial\\mathbf{o}}\\mathbf{h}^\\top+\\lambda\\mathbf{W}^{(2)}$$\n\n    $$\\frac{\\partial J}{\\partial\\mathbf{h}}=\\text{prod}\\left(\\frac{\\partial J}{\\partial\\mathbf{o}},\\frac{\\partial\\mathbf{o}}{\\partial\\mathbf{h}}\\right)=\\mathbf{W}^{(2)^\\top}\\frac{\\partial J}{\\partial\\mathbf{o}}$$\n    由于激活函数$\\phi$是按元素计算的，因此计算$\\frac{\\partial J}{\\partial \\mathbf{z}}$需要使用按元素乘法运算符：\n    $$\\frac{\\partial J}{\\partial\\mathbf{z}}=\\mathrm{prod}\\left(\\frac{\\partial J}{\\partial\\mathbf{h}},\\frac{\\partial\\mathbf{h}}{\\partial\\mathbf{z}}\\right)=\\frac{\\partial J}{\\partial\\mathbf{h}}\\odot\\phi^{\\prime}\\left(\\mathbf{z}\\right)$$\n\n    $$\\begin{aligned}\\frac{\\partial J}{\\partial\\mathbf{W}^{(1)}}&=\\text{prod}\\left(\\frac{\\partial J}{\\partial\\mathbf{z}},\\frac{\\partial\\mathbf{z}}{\\partial\\mathbf{W}^{(1)}}\\right)+\\text{prod}\\left(\\frac{\\partial J}{\\partial s},\\frac{\\partial s}{\\partial\\mathbf{W}^{(1)}}\\right)=\\frac{\\partial J}{\\partial\\mathbf{z}}\\mathbf{x}^\\top+\\lambda\\mathbf{W}^{(1)}\\end{aligned}$$\n\n## 7.4 训练神经网络\n- 训练神经网络时，对于前向传播，沿着依赖的方向遍历计算图并计算路径上的所有变量。然后将这些用于反向传播，其中计算顺序与计算图的相反。\n- 一方面，在前向传播期间计算正则项取决于模型参数W(1)和 W(2)的当前值。它们是由优化算法根据最近迭代的反向传播给出的。\n- 另一方面，反向传播期间参数$\\frac{\\partial J}{\\partial W^{(2)}}$的梯度计算，取决于由前向传播给出的隐藏变量h的当前值。\n- 因此，在训练神经网络时，在初始化模型参数后，我们交替使用前向传播和反向传播，利用反向传播给出的梯度来更新模型参数。\n- 反向传播重复利用前向传播中存储的中间值，以避免重复计算。带来的影响之一是我们需要保留中间值，直到反向传播完成。\n","slug":"deeplearning/code/pytorch/3_mlp/7_backward","published":1,"updated":"2024-05-05T14:46:29.994Z","comments":1,"layout":"post","photos":[],"_id":"clvuli6cw003kzsvw3xxbcso9","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"7-前向传播、反向传播和计算图\"><a href=\"#7-前向传播、反向传播和计算图\" class=\"headerlink\" title=\"7 前向传播、反向传播和计算图\"></a>7 前向传播、反向传播和计算图</h1><ul>\n<li>以带dropout的单层隐藏层mlp为例</li>\n</ul>\n<h2 id=\"7-1-前向传播\"><a href=\"#7-1-前向传播\" class=\"headerlink\" title=\"7.1 前向传播\"></a>7.1 前向传播</h2><ul>\n<li>按顺序（从输入层到输出层）计算和存储神经网络中每层的结果s。<ul>\n<li>1）输入是$\\mathbf{x} \\in \\mathbb{R}^d$，隐藏层不含偏置,得到中间变量：<br>  $$\\mathbf{z}&#x3D;\\mathbf{W}^{(1)}\\mathbf{x}$$</li>\n<li>2）激活：<br>  $$\\mathbf{h}&#x3D;\\phi(\\mathbf{z})$$</li>\n<li>3）输出：<br>  $$\\mathbf{o}&#x3D;\\mathbf{W}^{(2)}\\mathbf{h}$$</li>\n<li>4）损失：<br>  $$L&#x3D;l(\\mathbf{o},y)$$</li>\n<li>5）正则化项（权重衰减）：<br>  $$s&#x3D;\\frac{\\lambda}{2}(||\\mathbf{W}^{(1)}||^2_F+||\\mathbf{W}^{(2)}||^2_F)$$<ul>\n<li>其中矩阵的Frobenius范数是将矩阵展平成向量后应用$L_2$范数。</li>\n</ul>\n</li>\n<li>6）目标函数：<br>  $$J&#x3D;L+s$$</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"7-2-前向传播计算图\"><a href=\"#7-2-前向传播计算图\" class=\"headerlink\" title=\"7.2 前向传播计算图\"></a>7.2 前向传播计算图</h2><ul>\n<li>7.1的计算图：<br><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\7\\1.png\"><br><img src=\"/img/deeplearning/code/pytorch/3_mlp/7/1.png\"></li>\n</ul>\n<h2 id=\"7-3-反向传播\"><a href=\"#7-3-反向传播\" class=\"headerlink\" title=\"7.3 反向传播\"></a>7.3 反向传播</h2><ul>\n<li><p>根据微积分中的链式规则，按相反的顺序从输出层到输入层遍历网络。</p>\n</li>\n<li><p>链式法则：假设有$Y&#x3D;f(X)$和$Z&#x3D;g(Y)$，有:<br>  $$\\frac{\\partial{Z}}{\\partial{X}}&#x3D;prod(\\frac{\\partial{Z}}{\\partial{Y}},\\frac{\\partial{Y}}{\\partial{X}})$$</p>\n<ul>\n<li>prod表示一定的操作</li>\n</ul>\n</li>\n<li><p>针对前向传播计算图计算反向梯度：<br>  $$ \\frac{ \\partial J}{ \\partial L} &#x3D; 1 \\mathrm{ ~ and ~ } \\frac{ \\partial J}{ \\partial s} &#x3D; 1 $$</p>\n<p>  $$\\frac{\\partial J}{\\partial\\mathbf{o}}&#x3D;\\text{prod}\\left(\\frac{\\partial J}{\\partial L},\\frac{\\partial L}{\\partial\\mathbf{o}}\\right)&#x3D;\\frac{\\partial L}{\\partial\\mathbf{o}}\\in\\mathbb{R}^q$$</p>\n<p>  $$ \\frac{ \\partial s}{ \\partial \\mathbf{W}^ {(1)}}&#x3D; \\lambda \\mathbf{W}^ {(1)} \\mathrm{~ and ~} \\frac{ \\partial s}{ \\partial \\mathbf{W}^ {(2)}}&#x3D; \\lambda \\mathbf{W}^ {(2)}$$</p>\n<p>  $$\\frac{\\partial J}{\\partial\\mathbf{W}^{(2)}}&#x3D;\\mathrm{prod}\\left(\\frac{\\partial J}{\\partial\\mathbf{o}},\\frac{\\partial\\mathbf{o}}{\\partial\\mathbf{W}^{(2)}}\\right)+\\mathrm{prod}\\left(\\frac{\\partial J}{\\partial s},\\frac{\\partial s}{\\partial\\mathbf{W}^{(2)}}\\right)&#x3D;\\frac{\\partial J}{\\partial\\mathbf{o}}\\mathbf{h}^\\top+\\lambda\\mathbf{W}^{(2)}$$</p>\n<p>  $$\\frac{\\partial J}{\\partial\\mathbf{h}}&#x3D;\\text{prod}\\left(\\frac{\\partial J}{\\partial\\mathbf{o}},\\frac{\\partial\\mathbf{o}}{\\partial\\mathbf{h}}\\right)&#x3D;\\mathbf{W}^{(2)^\\top}\\frac{\\partial J}{\\partial\\mathbf{o}}$$<br>  由于激活函数$\\phi$是按元素计算的，因此计算$\\frac{\\partial J}{\\partial \\mathbf{z}}$需要使用按元素乘法运算符：<br>  $$\\frac{\\partial J}{\\partial\\mathbf{z}}&#x3D;\\mathrm{prod}\\left(\\frac{\\partial J}{\\partial\\mathbf{h}},\\frac{\\partial\\mathbf{h}}{\\partial\\mathbf{z}}\\right)&#x3D;\\frac{\\partial J}{\\partial\\mathbf{h}}\\odot\\phi^{\\prime}\\left(\\mathbf{z}\\right)$$</p>\n<p>  $$\\begin{aligned}\\frac{\\partial J}{\\partial\\mathbf{W}^{(1)}}&amp;&#x3D;\\text{prod}\\left(\\frac{\\partial J}{\\partial\\mathbf{z}},\\frac{\\partial\\mathbf{z}}{\\partial\\mathbf{W}^{(1)}}\\right)+\\text{prod}\\left(\\frac{\\partial J}{\\partial s},\\frac{\\partial s}{\\partial\\mathbf{W}^{(1)}}\\right)&#x3D;\\frac{\\partial J}{\\partial\\mathbf{z}}\\mathbf{x}^\\top+\\lambda\\mathbf{W}^{(1)}\\end{aligned}$$</p>\n</li>\n</ul>\n<h2 id=\"7-4-训练神经网络\"><a href=\"#7-4-训练神经网络\" class=\"headerlink\" title=\"7.4 训练神经网络\"></a>7.4 训练神经网络</h2><ul>\n<li>训练神经网络时，对于前向传播，沿着依赖的方向遍历计算图并计算路径上的所有变量。然后将这些用于反向传播，其中计算顺序与计算图的相反。</li>\n<li>一方面，在前向传播期间计算正则项取决于模型参数W(1)和 W(2)的当前值。它们是由优化算法根据最近迭代的反向传播给出的。</li>\n<li>另一方面，反向传播期间参数$\\frac{\\partial J}{\\partial W^{(2)}}$的梯度计算，取决于由前向传播给出的隐藏变量h的当前值。</li>\n<li>因此，在训练神经网络时，在初始化模型参数后，我们交替使用前向传播和反向传播，利用反向传播给出的梯度来更新模型参数。</li>\n<li>反向传播重复利用前向传播中存储的中间值，以避免重复计算。带来的影响之一是我们需要保留中间值，直到反向传播完成。</li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"7-前向传播、反向传播和计算图\"><a href=\"#7-前向传播、反向传播和计算图\" class=\"headerlink\" title=\"7 前向传播、反向传播和计算图\"></a>7 前向传播、反向传播和计算图</h1><ul>\n<li>以带dropout的单层隐藏层mlp为例</li>\n</ul>\n<h2 id=\"7-1-前向传播\"><a href=\"#7-1-前向传播\" class=\"headerlink\" title=\"7.1 前向传播\"></a>7.1 前向传播</h2><ul>\n<li>按顺序（从输入层到输出层）计算和存储神经网络中每层的结果s。<ul>\n<li>1）输入是$\\mathbf{x} \\in \\mathbb{R}^d$，隐藏层不含偏置,得到中间变量：<br>  $$\\mathbf{z}&#x3D;\\mathbf{W}^{(1)}\\mathbf{x}$$</li>\n<li>2）激活：<br>  $$\\mathbf{h}&#x3D;\\phi(\\mathbf{z})$$</li>\n<li>3）输出：<br>  $$\\mathbf{o}&#x3D;\\mathbf{W}^{(2)}\\mathbf{h}$$</li>\n<li>4）损失：<br>  $$L&#x3D;l(\\mathbf{o},y)$$</li>\n<li>5）正则化项（权重衰减）：<br>  $$s&#x3D;\\frac{\\lambda}{2}(||\\mathbf{W}^{(1)}||^2_F+||\\mathbf{W}^{(2)}||^2_F)$$<ul>\n<li>其中矩阵的Frobenius范数是将矩阵展平成向量后应用$L_2$范数。</li>\n</ul>\n</li>\n<li>6）目标函数：<br>  $$J&#x3D;L+s$$</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"7-2-前向传播计算图\"><a href=\"#7-2-前向传播计算图\" class=\"headerlink\" title=\"7.2 前向传播计算图\"></a>7.2 前向传播计算图</h2><ul>\n<li>7.1的计算图：<br><img src=\"D:\\blog\\themes\\yilia\\source\\img\\deeplearning\\code\\pytorch\\3_mlp\\7\\1.png\"><br><img src=\"/img/deeplearning/code/pytorch/3_mlp/7/1.png\"></li>\n</ul>\n<h2 id=\"7-3-反向传播\"><a href=\"#7-3-反向传播\" class=\"headerlink\" title=\"7.3 反向传播\"></a>7.3 反向传播</h2><ul>\n<li><p>根据微积分中的链式规则，按相反的顺序从输出层到输入层遍历网络。</p>\n</li>\n<li><p>链式法则：假设有$Y&#x3D;f(X)$和$Z&#x3D;g(Y)$，有:<br>  $$\\frac{\\partial{Z}}{\\partial{X}}&#x3D;prod(\\frac{\\partial{Z}}{\\partial{Y}},\\frac{\\partial{Y}}{\\partial{X}})$$</p>\n<ul>\n<li>prod表示一定的操作</li>\n</ul>\n</li>\n<li><p>针对前向传播计算图计算反向梯度：<br>  $$ \\frac{ \\partial J}{ \\partial L} &#x3D; 1 \\mathrm{ ~ and ~ } \\frac{ \\partial J}{ \\partial s} &#x3D; 1 $$</p>\n<p>  $$\\frac{\\partial J}{\\partial\\mathbf{o}}&#x3D;\\text{prod}\\left(\\frac{\\partial J}{\\partial L},\\frac{\\partial L}{\\partial\\mathbf{o}}\\right)&#x3D;\\frac{\\partial L}{\\partial\\mathbf{o}}\\in\\mathbb{R}^q$$</p>\n<p>  $$ \\frac{ \\partial s}{ \\partial \\mathbf{W}^ {(1)}}&#x3D; \\lambda \\mathbf{W}^ {(1)} \\mathrm{~ and ~} \\frac{ \\partial s}{ \\partial \\mathbf{W}^ {(2)}}&#x3D; \\lambda \\mathbf{W}^ {(2)}$$</p>\n<p>  $$\\frac{\\partial J}{\\partial\\mathbf{W}^{(2)}}&#x3D;\\mathrm{prod}\\left(\\frac{\\partial J}{\\partial\\mathbf{o}},\\frac{\\partial\\mathbf{o}}{\\partial\\mathbf{W}^{(2)}}\\right)+\\mathrm{prod}\\left(\\frac{\\partial J}{\\partial s},\\frac{\\partial s}{\\partial\\mathbf{W}^{(2)}}\\right)&#x3D;\\frac{\\partial J}{\\partial\\mathbf{o}}\\mathbf{h}^\\top+\\lambda\\mathbf{W}^{(2)}$$</p>\n<p>  $$\\frac{\\partial J}{\\partial\\mathbf{h}}&#x3D;\\text{prod}\\left(\\frac{\\partial J}{\\partial\\mathbf{o}},\\frac{\\partial\\mathbf{o}}{\\partial\\mathbf{h}}\\right)&#x3D;\\mathbf{W}^{(2)^\\top}\\frac{\\partial J}{\\partial\\mathbf{o}}$$<br>  由于激活函数$\\phi$是按元素计算的，因此计算$\\frac{\\partial J}{\\partial \\mathbf{z}}$需要使用按元素乘法运算符：<br>  $$\\frac{\\partial J}{\\partial\\mathbf{z}}&#x3D;\\mathrm{prod}\\left(\\frac{\\partial J}{\\partial\\mathbf{h}},\\frac{\\partial\\mathbf{h}}{\\partial\\mathbf{z}}\\right)&#x3D;\\frac{\\partial J}{\\partial\\mathbf{h}}\\odot\\phi^{\\prime}\\left(\\mathbf{z}\\right)$$</p>\n<p>  $$\\begin{aligned}\\frac{\\partial J}{\\partial\\mathbf{W}^{(1)}}&amp;&#x3D;\\text{prod}\\left(\\frac{\\partial J}{\\partial\\mathbf{z}},\\frac{\\partial\\mathbf{z}}{\\partial\\mathbf{W}^{(1)}}\\right)+\\text{prod}\\left(\\frac{\\partial J}{\\partial s},\\frac{\\partial s}{\\partial\\mathbf{W}^{(1)}}\\right)&#x3D;\\frac{\\partial J}{\\partial\\mathbf{z}}\\mathbf{x}^\\top+\\lambda\\mathbf{W}^{(1)}\\end{aligned}$$</p>\n</li>\n</ul>\n<h2 id=\"7-4-训练神经网络\"><a href=\"#7-4-训练神经网络\" class=\"headerlink\" title=\"7.4 训练神经网络\"></a>7.4 训练神经网络</h2><ul>\n<li>训练神经网络时，对于前向传播，沿着依赖的方向遍历计算图并计算路径上的所有变量。然后将这些用于反向传播，其中计算顺序与计算图的相反。</li>\n<li>一方面，在前向传播期间计算正则项取决于模型参数W(1)和 W(2)的当前值。它们是由优化算法根据最近迭代的反向传播给出的。</li>\n<li>另一方面，反向传播期间参数$\\frac{\\partial J}{\\partial W^{(2)}}$的梯度计算，取决于由前向传播给出的隐藏变量h的当前值。</li>\n<li>因此，在训练神经网络时，在初始化模型参数后，我们交替使用前向传播和反向传播，利用反向传播给出的梯度来更新模型参数。</li>\n<li>反向传播重复利用前向传播中存储的中间值，以避免重复计算。带来的影响之一是我们需要保留中间值，直到反向传播完成。</li>\n</ul>"},{"title":"3.8 数值稳定性和模型初始化","date":"2024-03-16T06:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 8 数值稳定性和模型初始化\n- 初始化方案的选择在神经网络学习中起着举足轻重的作用，它对保持数值稳定性至关重要。\n\n## 8.1 梯度消失和梯度爆炸\n- 假设网络有L层，输入X，输出o，每一层$l$由变换$f_l$定义，该变换的参数为权重$\\mathbf{W}^{(l)}$，其隐藏变量是$\\mathbf{h}^{(l)}$。网络定义为：\n    $$\\mathbf{h}^{(l)}= f_l(\\mathbf{h}^{(l-1)})$$\n    - 因此：$\\mathbf{o}=f_L\\circ\\ldots\\circ f_1(\\mathbf{x})$\n    - $\\mathbf{o}$关于任何一组参数$\\mathbf{W}^{(l)}$的梯度：\n        $$ \\partial_ { \\mathbf{W}^ {(l)}} \\mathbf{o}= \\underbrace{ \\partial_ { \\mathbf{h}^ {(L-1)}} \\mathbf{h}^ {(L)}}_ {\\mathbf{M}^ {(L)} \\overset{ \\mathrm{def}}{ \\operatorname*{ = }}} \\cdot \\ldots \\cdot \\underbrace{ \\partial_ { \\mathbf{h}^ {(l)}} \\mathbf{h}^ {( l + 1 )}}_ { \\mathbf{M}^ {(l + 1)} \\overset{ \\mathrm{ def }}{ \\operatorname * { = }}} \\underbrace{ \\partial_ { \\mathbf{ W }^ {(l)}} \\mathbf{h}^ {(l)}}_ { \\mathbf{v}^ {(l)} \\overset{ \\mathrm{ def }}{ \\operatorname * {=}}}$$\n    - 该梯度是L − l个矩阵与梯度向量$\\mathbf{v}^{(l)}$的乘积。因此，我们容易受到数值下溢问题的影响. 当将太多的概率乘在一起时，这些问题经常会出现。在处理概率时，一个常见的技巧是切换到对数空间。不幸的是，上面的问题更为严重：最初，矩阵 M(l) 可能具有各种各样的特征值。他们可能很小，也可能很大；他们的乘积可能非常大，也可能非常小。\n    - 不稳定梯度带来的风险不止在于数值表示；也威胁到我们优化算法的稳定性。梯度爆炸（gradient exploding）问题：参数更新过大，破坏了模型的稳定收敛；是梯度消失（gradient vanishing）问题：参数更新过小，在每次更新时几乎不会移动，导致模型无法学习。\n### 8.1.1 梯度消失\n- sigmoid函数更符合生物神经元的原理，但可能梯度消失：\n\n    \n\n\n```python\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\nimport torch\nfrom d2l import torch as d2l\n\nx = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)\ny= torch.sigmoid(x)\ny.backward(torch.ones_like(x))\n\n#注意用的是x.grad\nd2l.plot(x.detach().numpy(), [y.detach().numpy(), x.grad.numpy()] , legend=['sigmoid','gradient'], figsize=(4.5, 2.5))\n```\n\n​      ![svg](D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/8_init_files/8_init_2_0.svg)\n\n![svg](img/deeplearning/code/pytorch/3_mlp/8_init_files/8_init_2_0.svg)\n    \n\n\n- 当sigmoid函数输入很大或很小时，他的梯度会消失。此外，当反向传播通过许多层时，除非我们在刚刚好的地方，否则整个乘积的梯度可能会消失。当我们的网络有很多层时，除非我们很小心，否则在某一层可能会切断梯度。因此，更稳定的ReLU系列函数已经成为从业者的默认选择（虽然在神经科学的角度看起来不太合理）。\n### 8.1.2 梯度爆炸\n- 生成100个高斯随机矩阵，将他们与某个初始矩阵相乘，矩阵乘积会发生梯度爆炸。当这种情况是由于深度网络的初始化所导致时，我们没有机会让梯度下降优化器收敛。\n\n\n```python\nM = torch.normal(0,1, size=(4,4))\nprint('第一个矩阵\\n',M)\nfor i in range(100):\n    M=torch.mm(M, torch.normal(0,1,size=(4,4)))\nprint('乘以100个矩阵后\\n',M)\n```\n\n    第一个矩阵\n     tensor([[ 0.5260, -0.8163, -0.2948,  1.3394],\n            [-0.6026,  0.9170,  0.2718,  1.0166],\n            [ 1.3275, -0.1824,  1.1752,  0.2823],\n            [ 0.8749,  0.4343,  0.0216,  0.8288]])\n    乘以100个矩阵后\n     tensor([[ 1.5284e+25,  3.4216e+24, -2.3112e+23, -1.6804e+25],\n            [ 5.5275e+24,  1.2374e+24, -8.3584e+22, -6.0772e+24],\n            [ 2.6396e+25,  5.9094e+24, -3.9915e+23, -2.9022e+25],\n            [ 1.8637e+25,  4.1723e+24, -2.8182e+23, -2.0490e+25]])\n\n\n### 8.1.3 打破对称性\n- 一个隐藏层中的所有隐藏单元的地位是相同的，具有排列对称性。\n- 如果将隐藏层的所有参数初始化为相同的常量，那么我们可能永远也无法实现网络的表达能力，隐藏层的行为就好像只有一个单元。虽然小批量随机梯度下降不会打破这种对称性，但暂退法正则化可以。\n## 8.2 参数初始化\n- 解决或减轻上述问题\n### 8.2.1 默认初始化\n- 如果我们不指定初始化方法，框架将使用默认的随机初始化方法，对于中等难度的问题，这种方法通常很有效。\n### 8.2.2 Xavier初始化\n- 假设某层有$n_{in}$个输入：$x_j$，权重为$w_ij$，输出为：$o_i=\\sum_{j=1}^{n_{\\mathrm{in}}}w_{ij}x_j$\n- 假设权重$w_{ij}$服从$ N(0, {\\sigma} ^ 2) $，输入$x_j$服从$N(0,\\gamma ^2)$。（这并不意味着分布必须是高斯的，只是均值和方差需要存在。）\n- 输出$o_i$的均值方差：\n    $$\\begin{aligned}E[o_{i}] &= \\sum_{j=1}^{n_\\mathrm{in}}E[w_{ij}x_j]  \\\\ &= \\sum_{j=1}^{n_\\mathrm{in}}E[w_{ij}]E[x_j] \\\\ &= 0.\\end{aligned}$$\n\n    $$\\begin{aligned}\\operatorname{Var}[o_i] &= E[o_i^2]-(E[o_i])^2  \\\\ &= \\sum_{j=1}^{n_{\\mathrm{in}}}E[w_{ij}^2x_j^2]-0 \\\\ &= \\sum_{j=1}^{n_{\\mathrm{in}}}E[w_{ij}^2]E[x_j^2] \\\\ &= n_\\text{in}\\sigma^2\\gamma^2.\\end{aligned}$$\n- 保持方差不变的一种方法是设置$n_{in} \\sigma^2=1$。考虑反向传播，使用与前向传播相同的推断，我们可以看到，除非$n_{out} \\sigma^2=1$否则梯度的方差可能会增大，我们不可能同时满足这两个条件，因此提出Xavier初始化：\n    $$\\frac12(n_{\\mathrm{in}}+n_{\\mathrm{out}})\\sigma^2=1 \\rightarrow \\sigma= \\sqrt{\\frac{2}{n_{in}+n_{out}}}$$\n    - 或者改为从$U\\left(-\\sqrt{\\frac6{n_\\mathrm{in}+n_\\mathrm{out}}},\\sqrt{\\frac6{n_\\mathrm{in}+n_\\mathrm{out}}}\\right)$\n","source":"_posts/deeplearning/code/pytorch/3_mlp/8_init.md","raw":"---\ntitle: 3.8 数值稳定性和模型初始化\ndate: 2024-3-16 14:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 8 数值稳定性和模型初始化\n- 初始化方案的选择在神经网络学习中起着举足轻重的作用，它对保持数值稳定性至关重要。\n\n## 8.1 梯度消失和梯度爆炸\n- 假设网络有L层，输入X，输出o，每一层$l$由变换$f_l$定义，该变换的参数为权重$\\mathbf{W}^{(l)}$，其隐藏变量是$\\mathbf{h}^{(l)}$。网络定义为：\n    $$\\mathbf{h}^{(l)}= f_l(\\mathbf{h}^{(l-1)})$$\n    - 因此：$\\mathbf{o}=f_L\\circ\\ldots\\circ f_1(\\mathbf{x})$\n    - $\\mathbf{o}$关于任何一组参数$\\mathbf{W}^{(l)}$的梯度：\n        $$ \\partial_ { \\mathbf{W}^ {(l)}} \\mathbf{o}= \\underbrace{ \\partial_ { \\mathbf{h}^ {(L-1)}} \\mathbf{h}^ {(L)}}_ {\\mathbf{M}^ {(L)} \\overset{ \\mathrm{def}}{ \\operatorname*{ = }}} \\cdot \\ldots \\cdot \\underbrace{ \\partial_ { \\mathbf{h}^ {(l)}} \\mathbf{h}^ {( l + 1 )}}_ { \\mathbf{M}^ {(l + 1)} \\overset{ \\mathrm{ def }}{ \\operatorname * { = }}} \\underbrace{ \\partial_ { \\mathbf{ W }^ {(l)}} \\mathbf{h}^ {(l)}}_ { \\mathbf{v}^ {(l)} \\overset{ \\mathrm{ def }}{ \\operatorname * {=}}}$$\n    - 该梯度是L − l个矩阵与梯度向量$\\mathbf{v}^{(l)}$的乘积。因此，我们容易受到数值下溢问题的影响. 当将太多的概率乘在一起时，这些问题经常会出现。在处理概率时，一个常见的技巧是切换到对数空间。不幸的是，上面的问题更为严重：最初，矩阵 M(l) 可能具有各种各样的特征值。他们可能很小，也可能很大；他们的乘积可能非常大，也可能非常小。\n    - 不稳定梯度带来的风险不止在于数值表示；也威胁到我们优化算法的稳定性。梯度爆炸（gradient exploding）问题：参数更新过大，破坏了模型的稳定收敛；是梯度消失（gradient vanishing）问题：参数更新过小，在每次更新时几乎不会移动，导致模型无法学习。\n### 8.1.1 梯度消失\n- sigmoid函数更符合生物神经元的原理，但可能梯度消失：\n\n    \n\n\n```python\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\nimport torch\nfrom d2l import torch as d2l\n\nx = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)\ny= torch.sigmoid(x)\ny.backward(torch.ones_like(x))\n\n#注意用的是x.grad\nd2l.plot(x.detach().numpy(), [y.detach().numpy(), x.grad.numpy()] , legend=['sigmoid','gradient'], figsize=(4.5, 2.5))\n```\n\n​      ![svg](D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/8_init_files/8_init_2_0.svg)\n\n![svg](img/deeplearning/code/pytorch/3_mlp/8_init_files/8_init_2_0.svg)\n    \n\n\n- 当sigmoid函数输入很大或很小时，他的梯度会消失。此外，当反向传播通过许多层时，除非我们在刚刚好的地方，否则整个乘积的梯度可能会消失。当我们的网络有很多层时，除非我们很小心，否则在某一层可能会切断梯度。因此，更稳定的ReLU系列函数已经成为从业者的默认选择（虽然在神经科学的角度看起来不太合理）。\n### 8.1.2 梯度爆炸\n- 生成100个高斯随机矩阵，将他们与某个初始矩阵相乘，矩阵乘积会发生梯度爆炸。当这种情况是由于深度网络的初始化所导致时，我们没有机会让梯度下降优化器收敛。\n\n\n```python\nM = torch.normal(0,1, size=(4,4))\nprint('第一个矩阵\\n',M)\nfor i in range(100):\n    M=torch.mm(M, torch.normal(0,1,size=(4,4)))\nprint('乘以100个矩阵后\\n',M)\n```\n\n    第一个矩阵\n     tensor([[ 0.5260, -0.8163, -0.2948,  1.3394],\n            [-0.6026,  0.9170,  0.2718,  1.0166],\n            [ 1.3275, -0.1824,  1.1752,  0.2823],\n            [ 0.8749,  0.4343,  0.0216,  0.8288]])\n    乘以100个矩阵后\n     tensor([[ 1.5284e+25,  3.4216e+24, -2.3112e+23, -1.6804e+25],\n            [ 5.5275e+24,  1.2374e+24, -8.3584e+22, -6.0772e+24],\n            [ 2.6396e+25,  5.9094e+24, -3.9915e+23, -2.9022e+25],\n            [ 1.8637e+25,  4.1723e+24, -2.8182e+23, -2.0490e+25]])\n\n\n### 8.1.3 打破对称性\n- 一个隐藏层中的所有隐藏单元的地位是相同的，具有排列对称性。\n- 如果将隐藏层的所有参数初始化为相同的常量，那么我们可能永远也无法实现网络的表达能力，隐藏层的行为就好像只有一个单元。虽然小批量随机梯度下降不会打破这种对称性，但暂退法正则化可以。\n## 8.2 参数初始化\n- 解决或减轻上述问题\n### 8.2.1 默认初始化\n- 如果我们不指定初始化方法，框架将使用默认的随机初始化方法，对于中等难度的问题，这种方法通常很有效。\n### 8.2.2 Xavier初始化\n- 假设某层有$n_{in}$个输入：$x_j$，权重为$w_ij$，输出为：$o_i=\\sum_{j=1}^{n_{\\mathrm{in}}}w_{ij}x_j$\n- 假设权重$w_{ij}$服从$ N(0, {\\sigma} ^ 2) $，输入$x_j$服从$N(0,\\gamma ^2)$。（这并不意味着分布必须是高斯的，只是均值和方差需要存在。）\n- 输出$o_i$的均值方差：\n    $$\\begin{aligned}E[o_{i}] &= \\sum_{j=1}^{n_\\mathrm{in}}E[w_{ij}x_j]  \\\\ &= \\sum_{j=1}^{n_\\mathrm{in}}E[w_{ij}]E[x_j] \\\\ &= 0.\\end{aligned}$$\n\n    $$\\begin{aligned}\\operatorname{Var}[o_i] &= E[o_i^2]-(E[o_i])^2  \\\\ &= \\sum_{j=1}^{n_{\\mathrm{in}}}E[w_{ij}^2x_j^2]-0 \\\\ &= \\sum_{j=1}^{n_{\\mathrm{in}}}E[w_{ij}^2]E[x_j^2] \\\\ &= n_\\text{in}\\sigma^2\\gamma^2.\\end{aligned}$$\n- 保持方差不变的一种方法是设置$n_{in} \\sigma^2=1$。考虑反向传播，使用与前向传播相同的推断，我们可以看到，除非$n_{out} \\sigma^2=1$否则梯度的方差可能会增大，我们不可能同时满足这两个条件，因此提出Xavier初始化：\n    $$\\frac12(n_{\\mathrm{in}}+n_{\\mathrm{out}})\\sigma^2=1 \\rightarrow \\sigma= \\sqrt{\\frac{2}{n_{in}+n_{out}}}$$\n    - 或者改为从$U\\left(-\\sqrt{\\frac6{n_\\mathrm{in}+n_\\mathrm{out}}},\\sqrt{\\frac6{n_\\mathrm{in}+n_\\mathrm{out}}}\\right)$\n","slug":"deeplearning/code/pytorch/3_mlp/8_init","published":1,"updated":"2024-05-02T08:53:59.316Z","comments":1,"layout":"post","photos":[],"_id":"clvuli6cw003nzsvwgu6p1uru","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"8-数值稳定性和模型初始化\"><a href=\"#8-数值稳定性和模型初始化\" class=\"headerlink\" title=\"8 数值稳定性和模型初始化\"></a>8 数值稳定性和模型初始化</h1><ul>\n<li>初始化方案的选择在神经网络学习中起着举足轻重的作用，它对保持数值稳定性至关重要。</li>\n</ul>\n<h2 id=\"8-1-梯度消失和梯度爆炸\"><a href=\"#8-1-梯度消失和梯度爆炸\" class=\"headerlink\" title=\"8.1 梯度消失和梯度爆炸\"></a>8.1 梯度消失和梯度爆炸</h2><ul>\n<li>假设网络有L层，输入X，输出o，每一层$l$由变换$f_l$定义，该变换的参数为权重$\\mathbf{W}^{(l)}$，其隐藏变量是$\\mathbf{h}^{(l)}$。网络定义为：<br>  $$\\mathbf{h}^{(l)}&#x3D; f_l(\\mathbf{h}^{(l-1)})$$<ul>\n<li>因此：$\\mathbf{o}&#x3D;f_L\\circ\\ldots\\circ f_1(\\mathbf{x})$</li>\n<li>$\\mathbf{o}$关于任何一组参数$\\mathbf{W}^{(l)}$的梯度：<br>  $$ \\partial_ { \\mathbf{W}^ {(l)}} \\mathbf{o}&#x3D; \\underbrace{ \\partial_ { \\mathbf{h}^ {(L-1)}} \\mathbf{h}^ {(L)}}_ {\\mathbf{M}^ {(L)} \\overset{ \\mathrm{def}}{ \\operatorname*{ &#x3D; }}} \\cdot \\ldots \\cdot \\underbrace{ \\partial_ { \\mathbf{h}^ {(l)}} \\mathbf{h}^ {( l + 1 )}}_ { \\mathbf{M}^ {(l + 1)} \\overset{ \\mathrm{ def }}{ \\operatorname * { &#x3D; }}} \\underbrace{ \\partial_ { \\mathbf{ W }^ {(l)}} \\mathbf{h}^ {(l)}}_ { \\mathbf{v}^ {(l)} \\overset{ \\mathrm{ def }}{ \\operatorname * {&#x3D;}}}$$</li>\n<li>该梯度是L − l个矩阵与梯度向量$\\mathbf{v}^{(l)}$的乘积。因此，我们容易受到数值下溢问题的影响. 当将太多的概率乘在一起时，这些问题经常会出现。在处理概率时，一个常见的技巧是切换到对数空间。不幸的是，上面的问题更为严重：最初，矩阵 M(l) 可能具有各种各样的特征值。他们可能很小，也可能很大；他们的乘积可能非常大，也可能非常小。</li>\n<li>不稳定梯度带来的风险不止在于数值表示；也威胁到我们优化算法的稳定性。梯度爆炸（gradient exploding）问题：参数更新过大，破坏了模型的稳定收敛；是梯度消失（gradient vanishing）问题：参数更新过小，在每次更新时几乎不会移动，导致模型无法学习。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"8-1-1-梯度消失\"><a href=\"#8-1-1-梯度消失\" class=\"headerlink\" title=\"8.1.1 梯度消失\"></a>8.1.1 梯度消失</h3><ul>\n<li>sigmoid函数更符合生物神经元的原理，但可能梯度消失：</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>] = <span class=\"string\">&#x27;True&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">x = torch.arange(-<span class=\"number\">8.0</span>, <span class=\"number\">8.0</span>, <span class=\"number\">0.1</span>, requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">y= torch.sigmoid(x)</span><br><span class=\"line\">y.backward(torch.ones_like(x))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#注意用的是x.grad</span></span><br><span class=\"line\">d2l.plot(x.detach().numpy(), [y.detach().numpy(), x.grad.numpy()] , legend=[<span class=\"string\">&#x27;sigmoid&#x27;</span>,<span class=\"string\">&#x27;gradient&#x27;</span>], figsize=(<span class=\"number\">4.5</span>, <span class=\"number\">2.5</span>))</span><br></pre></td></tr></table></figure>\n\n<p>​      <img src=\"D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/8_init_files/8_init_2_0.svg\" alt=\"svg\"></p>\n<p><img src=\"/img/deeplearning/code/pytorch/3_mlp/8_init_files/8_init_2_0.svg\" alt=\"svg\"></p>\n<ul>\n<li>当sigmoid函数输入很大或很小时，他的梯度会消失。此外，当反向传播通过许多层时，除非我们在刚刚好的地方，否则整个乘积的梯度可能会消失。当我们的网络有很多层时，除非我们很小心，否则在某一层可能会切断梯度。因此，更稳定的ReLU系列函数已经成为从业者的默认选择（虽然在神经科学的角度看起来不太合理）。</li>\n</ul>\n<h3 id=\"8-1-2-梯度爆炸\"><a href=\"#8-1-2-梯度爆炸\" class=\"headerlink\" title=\"8.1.2 梯度爆炸\"></a>8.1.2 梯度爆炸</h3><ul>\n<li>生成100个高斯随机矩阵，将他们与某个初始矩阵相乘，矩阵乘积会发生梯度爆炸。当这种情况是由于深度网络的初始化所导致时，我们没有机会让梯度下降优化器收敛。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">M = torch.normal(<span class=\"number\">0</span>,<span class=\"number\">1</span>, size=(<span class=\"number\">4</span>,<span class=\"number\">4</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;第一个矩阵\\n&#x27;</span>,M)</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">100</span>):</span><br><span class=\"line\">    M=torch.mm(M, torch.normal(<span class=\"number\">0</span>,<span class=\"number\">1</span>,size=(<span class=\"number\">4</span>,<span class=\"number\">4</span>)))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;乘以100个矩阵后\\n&#x27;</span>,M)</span><br></pre></td></tr></table></figure>\n\n<pre><code>第一个矩阵\n tensor([[ 0.5260, -0.8163, -0.2948,  1.3394],\n        [-0.6026,  0.9170,  0.2718,  1.0166],\n        [ 1.3275, -0.1824,  1.1752,  0.2823],\n        [ 0.8749,  0.4343,  0.0216,  0.8288]])\n乘以100个矩阵后\n tensor([[ 1.5284e+25,  3.4216e+24, -2.3112e+23, -1.6804e+25],\n        [ 5.5275e+24,  1.2374e+24, -8.3584e+22, -6.0772e+24],\n        [ 2.6396e+25,  5.9094e+24, -3.9915e+23, -2.9022e+25],\n        [ 1.8637e+25,  4.1723e+24, -2.8182e+23, -2.0490e+25]])\n</code></pre>\n<h3 id=\"8-1-3-打破对称性\"><a href=\"#8-1-3-打破对称性\" class=\"headerlink\" title=\"8.1.3 打破对称性\"></a>8.1.3 打破对称性</h3><ul>\n<li>一个隐藏层中的所有隐藏单元的地位是相同的，具有排列对称性。</li>\n<li>如果将隐藏层的所有参数初始化为相同的常量，那么我们可能永远也无法实现网络的表达能力，隐藏层的行为就好像只有一个单元。虽然小批量随机梯度下降不会打破这种对称性，但暂退法正则化可以。</li>\n</ul>\n<h2 id=\"8-2-参数初始化\"><a href=\"#8-2-参数初始化\" class=\"headerlink\" title=\"8.2 参数初始化\"></a>8.2 参数初始化</h2><ul>\n<li>解决或减轻上述问题</li>\n</ul>\n<h3 id=\"8-2-1-默认初始化\"><a href=\"#8-2-1-默认初始化\" class=\"headerlink\" title=\"8.2.1 默认初始化\"></a>8.2.1 默认初始化</h3><ul>\n<li>如果我们不指定初始化方法，框架将使用默认的随机初始化方法，对于中等难度的问题，这种方法通常很有效。</li>\n</ul>\n<h3 id=\"8-2-2-Xavier初始化\"><a href=\"#8-2-2-Xavier初始化\" class=\"headerlink\" title=\"8.2.2 Xavier初始化\"></a>8.2.2 Xavier初始化</h3><ul>\n<li><p>假设某层有$n_{in}$个输入：$x_j$，权重为$w_ij$，输出为：$o_i&#x3D;\\sum_{j&#x3D;1}^{n_{\\mathrm{in}}}w_{ij}x_j$</p>\n</li>\n<li><p>假设权重$w_{ij}$服从$ N(0, {\\sigma} ^ 2) $，输入$x_j$服从$N(0,\\gamma ^2)$。（这并不意味着分布必须是高斯的，只是均值和方差需要存在。）</p>\n</li>\n<li><p>输出$o_i$的均值方差：<br>  $$\\begin{aligned}E[o_{i}] &amp;&#x3D; \\sum_{j&#x3D;1}^{n_\\mathrm{in}}E[w_{ij}x_j]  \\ &amp;&#x3D; \\sum_{j&#x3D;1}^{n_\\mathrm{in}}E[w_{ij}]E[x_j] \\ &amp;&#x3D; 0.\\end{aligned}$$</p>\n<p>  $$\\begin{aligned}\\operatorname{Var}[o_i] &amp;&#x3D; E[o_i^2]-(E[o_i])^2  \\ &amp;&#x3D; \\sum_{j&#x3D;1}^{n_{\\mathrm{in}}}E[w_{ij}^2x_j^2]-0 \\ &amp;&#x3D; \\sum_{j&#x3D;1}^{n_{\\mathrm{in}}}E[w_{ij}^2]E[x_j^2] \\ &amp;&#x3D; n_\\text{in}\\sigma^2\\gamma^2.\\end{aligned}$$</p>\n</li>\n<li><p>保持方差不变的一种方法是设置$n_{in} \\sigma^2&#x3D;1$。考虑反向传播，使用与前向传播相同的推断，我们可以看到，除非$n_{out} \\sigma^2&#x3D;1$否则梯度的方差可能会增大，我们不可能同时满足这两个条件，因此提出Xavier初始化：<br>  $$\\frac12(n_{\\mathrm{in}}+n_{\\mathrm{out}})\\sigma^2&#x3D;1 \\rightarrow \\sigma&#x3D; \\sqrt{\\frac{2}{n_{in}+n_{out}}}$$</p>\n<ul>\n<li>或者改为从$U\\left(-\\sqrt{\\frac6{n_\\mathrm{in}+n_\\mathrm{out}}},\\sqrt{\\frac6{n_\\mathrm{in}+n_\\mathrm{out}}}\\right)$</li>\n</ul>\n</li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"8-数值稳定性和模型初始化\"><a href=\"#8-数值稳定性和模型初始化\" class=\"headerlink\" title=\"8 数值稳定性和模型初始化\"></a>8 数值稳定性和模型初始化</h1><ul>\n<li>初始化方案的选择在神经网络学习中起着举足轻重的作用，它对保持数值稳定性至关重要。</li>\n</ul>\n<h2 id=\"8-1-梯度消失和梯度爆炸\"><a href=\"#8-1-梯度消失和梯度爆炸\" class=\"headerlink\" title=\"8.1 梯度消失和梯度爆炸\"></a>8.1 梯度消失和梯度爆炸</h2><ul>\n<li>假设网络有L层，输入X，输出o，每一层$l$由变换$f_l$定义，该变换的参数为权重$\\mathbf{W}^{(l)}$，其隐藏变量是$\\mathbf{h}^{(l)}$。网络定义为：<br>  $$\\mathbf{h}^{(l)}&#x3D; f_l(\\mathbf{h}^{(l-1)})$$<ul>\n<li>因此：$\\mathbf{o}&#x3D;f_L\\circ\\ldots\\circ f_1(\\mathbf{x})$</li>\n<li>$\\mathbf{o}$关于任何一组参数$\\mathbf{W}^{(l)}$的梯度：<br>  $$ \\partial_ { \\mathbf{W}^ {(l)}} \\mathbf{o}&#x3D; \\underbrace{ \\partial_ { \\mathbf{h}^ {(L-1)}} \\mathbf{h}^ {(L)}}_ {\\mathbf{M}^ {(L)} \\overset{ \\mathrm{def}}{ \\operatorname*{ &#x3D; }}} \\cdot \\ldots \\cdot \\underbrace{ \\partial_ { \\mathbf{h}^ {(l)}} \\mathbf{h}^ {( l + 1 )}}_ { \\mathbf{M}^ {(l + 1)} \\overset{ \\mathrm{ def }}{ \\operatorname * { &#x3D; }}} \\underbrace{ \\partial_ { \\mathbf{ W }^ {(l)}} \\mathbf{h}^ {(l)}}_ { \\mathbf{v}^ {(l)} \\overset{ \\mathrm{ def }}{ \\operatorname * {&#x3D;}}}$$</li>\n<li>该梯度是L − l个矩阵与梯度向量$\\mathbf{v}^{(l)}$的乘积。因此，我们容易受到数值下溢问题的影响. 当将太多的概率乘在一起时，这些问题经常会出现。在处理概率时，一个常见的技巧是切换到对数空间。不幸的是，上面的问题更为严重：最初，矩阵 M(l) 可能具有各种各样的特征值。他们可能很小，也可能很大；他们的乘积可能非常大，也可能非常小。</li>\n<li>不稳定梯度带来的风险不止在于数值表示；也威胁到我们优化算法的稳定性。梯度爆炸（gradient exploding）问题：参数更新过大，破坏了模型的稳定收敛；是梯度消失（gradient vanishing）问题：参数更新过小，在每次更新时几乎不会移动，导致模型无法学习。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"8-1-1-梯度消失\"><a href=\"#8-1-1-梯度消失\" class=\"headerlink\" title=\"8.1.1 梯度消失\"></a>8.1.1 梯度消失</h3><ul>\n<li>sigmoid函数更符合生物神经元的原理，但可能梯度消失：</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>] = <span class=\"string\">&#x27;True&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">x = torch.arange(-<span class=\"number\">8.0</span>, <span class=\"number\">8.0</span>, <span class=\"number\">0.1</span>, requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">y= torch.sigmoid(x)</span><br><span class=\"line\">y.backward(torch.ones_like(x))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#注意用的是x.grad</span></span><br><span class=\"line\">d2l.plot(x.detach().numpy(), [y.detach().numpy(), x.grad.numpy()] , legend=[<span class=\"string\">&#x27;sigmoid&#x27;</span>,<span class=\"string\">&#x27;gradient&#x27;</span>], figsize=(<span class=\"number\">4.5</span>, <span class=\"number\">2.5</span>))</span><br></pre></td></tr></table></figure>\n\n<p>​      <img src=\"D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/8_init_files/8_init_2_0.svg\" alt=\"svg\"></p>\n<p><img src=\"/img/deeplearning/code/pytorch/3_mlp/8_init_files/8_init_2_0.svg\" alt=\"svg\"></p>\n<ul>\n<li>当sigmoid函数输入很大或很小时，他的梯度会消失。此外，当反向传播通过许多层时，除非我们在刚刚好的地方，否则整个乘积的梯度可能会消失。当我们的网络有很多层时，除非我们很小心，否则在某一层可能会切断梯度。因此，更稳定的ReLU系列函数已经成为从业者的默认选择（虽然在神经科学的角度看起来不太合理）。</li>\n</ul>\n<h3 id=\"8-1-2-梯度爆炸\"><a href=\"#8-1-2-梯度爆炸\" class=\"headerlink\" title=\"8.1.2 梯度爆炸\"></a>8.1.2 梯度爆炸</h3><ul>\n<li>生成100个高斯随机矩阵，将他们与某个初始矩阵相乘，矩阵乘积会发生梯度爆炸。当这种情况是由于深度网络的初始化所导致时，我们没有机会让梯度下降优化器收敛。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">M = torch.normal(<span class=\"number\">0</span>,<span class=\"number\">1</span>, size=(<span class=\"number\">4</span>,<span class=\"number\">4</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;第一个矩阵\\n&#x27;</span>,M)</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">100</span>):</span><br><span class=\"line\">    M=torch.mm(M, torch.normal(<span class=\"number\">0</span>,<span class=\"number\">1</span>,size=(<span class=\"number\">4</span>,<span class=\"number\">4</span>)))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;乘以100个矩阵后\\n&#x27;</span>,M)</span><br></pre></td></tr></table></figure>\n\n<pre><code>第一个矩阵\n tensor([[ 0.5260, -0.8163, -0.2948,  1.3394],\n        [-0.6026,  0.9170,  0.2718,  1.0166],\n        [ 1.3275, -0.1824,  1.1752,  0.2823],\n        [ 0.8749,  0.4343,  0.0216,  0.8288]])\n乘以100个矩阵后\n tensor([[ 1.5284e+25,  3.4216e+24, -2.3112e+23, -1.6804e+25],\n        [ 5.5275e+24,  1.2374e+24, -8.3584e+22, -6.0772e+24],\n        [ 2.6396e+25,  5.9094e+24, -3.9915e+23, -2.9022e+25],\n        [ 1.8637e+25,  4.1723e+24, -2.8182e+23, -2.0490e+25]])\n</code></pre>\n<h3 id=\"8-1-3-打破对称性\"><a href=\"#8-1-3-打破对称性\" class=\"headerlink\" title=\"8.1.3 打破对称性\"></a>8.1.3 打破对称性</h3><ul>\n<li>一个隐藏层中的所有隐藏单元的地位是相同的，具有排列对称性。</li>\n<li>如果将隐藏层的所有参数初始化为相同的常量，那么我们可能永远也无法实现网络的表达能力，隐藏层的行为就好像只有一个单元。虽然小批量随机梯度下降不会打破这种对称性，但暂退法正则化可以。</li>\n</ul>\n<h2 id=\"8-2-参数初始化\"><a href=\"#8-2-参数初始化\" class=\"headerlink\" title=\"8.2 参数初始化\"></a>8.2 参数初始化</h2><ul>\n<li>解决或减轻上述问题</li>\n</ul>\n<h3 id=\"8-2-1-默认初始化\"><a href=\"#8-2-1-默认初始化\" class=\"headerlink\" title=\"8.2.1 默认初始化\"></a>8.2.1 默认初始化</h3><ul>\n<li>如果我们不指定初始化方法，框架将使用默认的随机初始化方法，对于中等难度的问题，这种方法通常很有效。</li>\n</ul>\n<h3 id=\"8-2-2-Xavier初始化\"><a href=\"#8-2-2-Xavier初始化\" class=\"headerlink\" title=\"8.2.2 Xavier初始化\"></a>8.2.2 Xavier初始化</h3><ul>\n<li><p>假设某层有$n_{in}$个输入：$x_j$，权重为$w_ij$，输出为：$o_i&#x3D;\\sum_{j&#x3D;1}^{n_{\\mathrm{in}}}w_{ij}x_j$</p>\n</li>\n<li><p>假设权重$w_{ij}$服从$ N(0, {\\sigma} ^ 2) $，输入$x_j$服从$N(0,\\gamma ^2)$。（这并不意味着分布必须是高斯的，只是均值和方差需要存在。）</p>\n</li>\n<li><p>输出$o_i$的均值方差：<br>  $$\\begin{aligned}E[o_{i}] &amp;&#x3D; \\sum_{j&#x3D;1}^{n_\\mathrm{in}}E[w_{ij}x_j]  \\ &amp;&#x3D; \\sum_{j&#x3D;1}^{n_\\mathrm{in}}E[w_{ij}]E[x_j] \\ &amp;&#x3D; 0.\\end{aligned}$$</p>\n<p>  $$\\begin{aligned}\\operatorname{Var}[o_i] &amp;&#x3D; E[o_i^2]-(E[o_i])^2  \\ &amp;&#x3D; \\sum_{j&#x3D;1}^{n_{\\mathrm{in}}}E[w_{ij}^2x_j^2]-0 \\ &amp;&#x3D; \\sum_{j&#x3D;1}^{n_{\\mathrm{in}}}E[w_{ij}^2]E[x_j^2] \\ &amp;&#x3D; n_\\text{in}\\sigma^2\\gamma^2.\\end{aligned}$$</p>\n</li>\n<li><p>保持方差不变的一种方法是设置$n_{in} \\sigma^2&#x3D;1$。考虑反向传播，使用与前向传播相同的推断，我们可以看到，除非$n_{out} \\sigma^2&#x3D;1$否则梯度的方差可能会增大，我们不可能同时满足这两个条件，因此提出Xavier初始化：<br>  $$\\frac12(n_{\\mathrm{in}}+n_{\\mathrm{out}})\\sigma^2&#x3D;1 \\rightarrow \\sigma&#x3D; \\sqrt{\\frac{2}{n_{in}+n_{out}}}$$</p>\n<ul>\n<li>或者改为从$U\\left(-\\sqrt{\\frac6{n_\\mathrm{in}+n_\\mathrm{out}}},\\sqrt{\\frac6{n_\\mathrm{in}+n_\\mathrm{out}}}\\right)$</li>\n</ul>\n</li>\n</ul>"},{"title":"6.2 使用块的网络（VGG）","date":"2024-04-22T03:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 2 使用块的网络（VGG）\n## 2.1 VGG块\n- 经典卷积神经网络的基本组成部分是如下序列：\n    1. 带填充以保持分辨率的卷积层\n    2. 非线性激活函数，如ReLU\n    3. 汇聚层，如最大汇聚层\n\n\n\n\n```python\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\ndef vgg_block(num_convs, in_channels, out_channels):\n    layers = []\n    for _ in range(num_convs):\n        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n        layers.append(nn.ReLU())\n        in_channels = out_channels\n    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n    return nn.Sequential(*layers)\n```\n\n## 2.2 VGG网络\n- 从AlexNet到VGG，它们本质上都是块设计。\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/2_vgg/1.png)\n![](img/deeplearning/code/pytorch/6_moden_conv/2_vgg/1.png)\n- 原始VGG网络有5个卷积块，前两个块各有一个卷积层，后三个块各包含两个卷积层，因此共有8个卷积层。\n- 第一个模块有64输出通道，每个后续模块将输出通道翻倍，直到该数字达到512.\n- 由于该网络使用8个卷积层和3个全连接层，因此它通常被称为VGG-11。\n\n\n```python\n# 指定了每个VGG块里卷积层个数和输出通道数。\nconv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))\n\ndef vgg(conv_arch):\n    conv_blks = []\n    in_channels = 1\n    # 卷积层部分\n    for (num_convs, out_channels) in conv_arch:\n        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))\n        in_channels = out_channels\n    return nn.Sequential(\n        *conv_blks, nn.Flatten(),\n        # 全连接层部分\n        nn.Linear(out_channels*7*7, 4096), nn.ReLU(), nn.Dropout(0.5), #为啥要*7：因为经过五个块后图像变成了7*7\n        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n        nn.Linear(4096, 10)\n    )\nnet = vgg(conv_arch)\n```\n\n- 查看输出形状\n\n\n```python\nX = torch.randn(size=(1, 1, 224, 224))\nfor blk in net:\n    X = blk(X)\n    print(blk.__class__.__name__,'output shape:\\t', X.shape)\n```\n\n    Sequential output shape:\t torch.Size([1, 64, 112, 112])\n    Sequential output shape:\t torch.Size([1, 128, 56, 56])\n    Sequential output shape:\t torch.Size([1, 256, 28, 28])\n    Sequential output shape:\t torch.Size([1, 512, 14, 14])\n    Sequential output shape:\t torch.Size([1, 512, 7, 7])\n    Flatten output shape:\t torch.Size([1, 25088])\n    Linear output shape:\t torch.Size([1, 4096])\n    ReLU output shape:\t torch.Size([1, 4096])\n    Dropout output shape:\t torch.Size([1, 4096])\n    Linear output shape:\t torch.Size([1, 4096])\n    ReLU output shape:\t torch.Size([1, 4096])\n    Dropout output shape:\t torch.Size([1, 4096])\n    Linear output shape:\t torch.Size([1, 10])\n    \n\n## 2.3 训练模型\n- 由于VGG比AlexNet计算量更大，因此我们构建了一个通道数较少的网络，用于训练Fashion-MNIST数据集\n\n\n```python\nimport os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\nratio = 4\nsmall_conv_arch = [(pair[0], pair[1] // ratio) for pair in conv_arch] #卷积层个数和输出通道数\nnet = vgg(small_conv_arch)\n\nlr, num_epochs, batch_size = 0.05, 10, 128\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)\nd2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())\n```\n\n\n    \n![svg](2_vgg_files/2_vgg_7_0.svg)\n![](img/deeplearning/code/pytorch/6_moden_conv/2_vgg_files/2_vgg_7_0.svg)\n\n","source":"_posts/deeplearning/code/pytorch/6_moden_convolution/2_vgg.md","raw":"---\ntitle: 6.2 使用块的网络（VGG）\ndate: 2024-4-22 11:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 2 使用块的网络（VGG）\n## 2.1 VGG块\n- 经典卷积神经网络的基本组成部分是如下序列：\n    1. 带填充以保持分辨率的卷积层\n    2. 非线性激活函数，如ReLU\n    3. 汇聚层，如最大汇聚层\n\n\n\n\n```python\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\ndef vgg_block(num_convs, in_channels, out_channels):\n    layers = []\n    for _ in range(num_convs):\n        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n        layers.append(nn.ReLU())\n        in_channels = out_channels\n    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n    return nn.Sequential(*layers)\n```\n\n## 2.2 VGG网络\n- 从AlexNet到VGG，它们本质上都是块设计。\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/2_vgg/1.png)\n![](img/deeplearning/code/pytorch/6_moden_conv/2_vgg/1.png)\n- 原始VGG网络有5个卷积块，前两个块各有一个卷积层，后三个块各包含两个卷积层，因此共有8个卷积层。\n- 第一个模块有64输出通道，每个后续模块将输出通道翻倍，直到该数字达到512.\n- 由于该网络使用8个卷积层和3个全连接层，因此它通常被称为VGG-11。\n\n\n```python\n# 指定了每个VGG块里卷积层个数和输出通道数。\nconv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))\n\ndef vgg(conv_arch):\n    conv_blks = []\n    in_channels = 1\n    # 卷积层部分\n    for (num_convs, out_channels) in conv_arch:\n        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))\n        in_channels = out_channels\n    return nn.Sequential(\n        *conv_blks, nn.Flatten(),\n        # 全连接层部分\n        nn.Linear(out_channels*7*7, 4096), nn.ReLU(), nn.Dropout(0.5), #为啥要*7：因为经过五个块后图像变成了7*7\n        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n        nn.Linear(4096, 10)\n    )\nnet = vgg(conv_arch)\n```\n\n- 查看输出形状\n\n\n```python\nX = torch.randn(size=(1, 1, 224, 224))\nfor blk in net:\n    X = blk(X)\n    print(blk.__class__.__name__,'output shape:\\t', X.shape)\n```\n\n    Sequential output shape:\t torch.Size([1, 64, 112, 112])\n    Sequential output shape:\t torch.Size([1, 128, 56, 56])\n    Sequential output shape:\t torch.Size([1, 256, 28, 28])\n    Sequential output shape:\t torch.Size([1, 512, 14, 14])\n    Sequential output shape:\t torch.Size([1, 512, 7, 7])\n    Flatten output shape:\t torch.Size([1, 25088])\n    Linear output shape:\t torch.Size([1, 4096])\n    ReLU output shape:\t torch.Size([1, 4096])\n    Dropout output shape:\t torch.Size([1, 4096])\n    Linear output shape:\t torch.Size([1, 4096])\n    ReLU output shape:\t torch.Size([1, 4096])\n    Dropout output shape:\t torch.Size([1, 4096])\n    Linear output shape:\t torch.Size([1, 10])\n    \n\n## 2.3 训练模型\n- 由于VGG比AlexNet计算量更大，因此我们构建了一个通道数较少的网络，用于训练Fashion-MNIST数据集\n\n\n```python\nimport os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\nratio = 4\nsmall_conv_arch = [(pair[0], pair[1] // ratio) for pair in conv_arch] #卷积层个数和输出通道数\nnet = vgg(small_conv_arch)\n\nlr, num_epochs, batch_size = 0.05, 10, 128\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)\nd2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())\n```\n\n\n    \n![svg](2_vgg_files/2_vgg_7_0.svg)\n![](img/deeplearning/code/pytorch/6_moden_conv/2_vgg_files/2_vgg_7_0.svg)\n\n","slug":"deeplearning/code/pytorch/6_moden_convolution/2_vgg","published":1,"updated":"2024-05-05T14:03:50.745Z","comments":1,"layout":"post","photos":[],"_id":"clvuli6da0044zsvw8qet92i3","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"2-使用块的网络（VGG）\"><a href=\"#2-使用块的网络（VGG）\" class=\"headerlink\" title=\"2 使用块的网络（VGG）\"></a>2 使用块的网络（VGG）</h1><h2 id=\"2-1-VGG块\"><a href=\"#2-1-VGG块\" class=\"headerlink\" title=\"2.1 VGG块\"></a>2.1 VGG块</h2><ul>\n<li>经典卷积神经网络的基本组成部分是如下序列：<ol>\n<li>带填充以保持分辨率的卷积层</li>\n<li>非线性激活函数，如ReLU</li>\n<li>汇聚层，如最大汇聚层</li>\n</ol>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">vgg_block</span>(<span class=\"params\">num_convs, in_channels, out_channels</span>):</span><br><span class=\"line\">    layers = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_convs):</span><br><span class=\"line\">        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=<span class=\"number\">3</span>, padding=<span class=\"number\">1</span>))</span><br><span class=\"line\">        layers.append(nn.ReLU())</span><br><span class=\"line\">        in_channels = out_channels</span><br><span class=\"line\">    layers.append(nn.MaxPool2d(kernel_size=<span class=\"number\">2</span>, stride=<span class=\"number\">2</span>))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nn.Sequential(*layers)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"2-2-VGG网络\"><a href=\"#2-2-VGG网络\" class=\"headerlink\" title=\"2.2 VGG网络\"></a>2.2 VGG网络</h2><ul>\n<li>从AlexNet到VGG，它们本质上都是块设计。<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/2_vgg/1.png\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/2_vgg/1.png\"></li>\n<li>原始VGG网络有5个卷积块，前两个块各有一个卷积层，后三个块各包含两个卷积层，因此共有8个卷积层。</li>\n<li>第一个模块有64输出通道，每个后续模块将输出通道翻倍，直到该数字达到512.</li>\n<li>由于该网络使用8个卷积层和3个全连接层，因此它通常被称为VGG-11。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 指定了每个VGG块里卷积层个数和输出通道数。</span></span><br><span class=\"line\">conv_arch = ((<span class=\"number\">1</span>, <span class=\"number\">64</span>), (<span class=\"number\">1</span>, <span class=\"number\">128</span>), (<span class=\"number\">2</span>, <span class=\"number\">256</span>), (<span class=\"number\">2</span>, <span class=\"number\">512</span>), (<span class=\"number\">2</span>, <span class=\"number\">512</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">vgg</span>(<span class=\"params\">conv_arch</span>):</span><br><span class=\"line\">    conv_blks = []</span><br><span class=\"line\">    in_channels = <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"comment\"># 卷积层部分</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (num_convs, out_channels) <span class=\"keyword\">in</span> conv_arch:</span><br><span class=\"line\">        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))</span><br><span class=\"line\">        in_channels = out_channels</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nn.Sequential(</span><br><span class=\"line\">        *conv_blks, nn.Flatten(),</span><br><span class=\"line\">        <span class=\"comment\"># 全连接层部分</span></span><br><span class=\"line\">        nn.Linear(out_channels*<span class=\"number\">7</span>*<span class=\"number\">7</span>, <span class=\"number\">4096</span>), nn.ReLU(), nn.Dropout(<span class=\"number\">0.5</span>), <span class=\"comment\">#为啥要*7：因为经过五个块后图像变成了7*7</span></span><br><span class=\"line\">        nn.Linear(<span class=\"number\">4096</span>, <span class=\"number\">4096</span>), nn.ReLU(), nn.Dropout(<span class=\"number\">0.5</span>),</span><br><span class=\"line\">        nn.Linear(<span class=\"number\">4096</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\">    )</span><br><span class=\"line\">net = vgg(conv_arch)</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>查看输出形状</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = torch.randn(size=(<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">224</span>, <span class=\"number\">224</span>))</span><br><span class=\"line\"><span class=\"keyword\">for</span> blk <span class=\"keyword\">in</span> net:</span><br><span class=\"line\">    X = blk(X)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(blk.__class__.__name__,<span class=\"string\">&#x27;output shape:\\t&#x27;</span>, X.shape)</span><br></pre></td></tr></table></figure>\n\n<pre><code>Sequential output shape:\t torch.Size([1, 64, 112, 112])\nSequential output shape:\t torch.Size([1, 128, 56, 56])\nSequential output shape:\t torch.Size([1, 256, 28, 28])\nSequential output shape:\t torch.Size([1, 512, 14, 14])\nSequential output shape:\t torch.Size([1, 512, 7, 7])\nFlatten output shape:\t torch.Size([1, 25088])\nLinear output shape:\t torch.Size([1, 4096])\nReLU output shape:\t torch.Size([1, 4096])\nDropout output shape:\t torch.Size([1, 4096])\nLinear output shape:\t torch.Size([1, 4096])\nReLU output shape:\t torch.Size([1, 4096])\nDropout output shape:\t torch.Size([1, 4096])\nLinear output shape:\t torch.Size([1, 10])\n</code></pre>\n<h2 id=\"2-3-训练模型\"><a href=\"#2-3-训练模型\" class=\"headerlink\" title=\"2.3 训练模型\"></a>2.3 训练模型</h2><ul>\n<li>由于VGG比AlexNet计算量更大，因此我们构建了一个通道数较少的网络，用于训练Fashion-MNIST数据集</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>] = <span class=\"string\">&quot;TRUE&quot;</span></span><br><span class=\"line\">ratio = <span class=\"number\">4</span></span><br><span class=\"line\">small_conv_arch = [(pair[<span class=\"number\">0</span>], pair[<span class=\"number\">1</span>] // ratio) <span class=\"keyword\">for</span> pair <span class=\"keyword\">in</span> conv_arch] <span class=\"comment\">#卷积层个数和输出通道数</span></span><br><span class=\"line\">net = vgg(small_conv_arch)</span><br><span class=\"line\"></span><br><span class=\"line\">lr, num_epochs, batch_size = <span class=\"number\">0.05</span>, <span class=\"number\">10</span>, <span class=\"number\">128</span></span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class=\"number\">224</span>)</span><br><span class=\"line\">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/2_vgg_files/2_vgg_7_0.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/2_vgg_files/2_vgg_7_0.svg\"></p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"2-使用块的网络（VGG）\"><a href=\"#2-使用块的网络（VGG）\" class=\"headerlink\" title=\"2 使用块的网络（VGG）\"></a>2 使用块的网络（VGG）</h1><h2 id=\"2-1-VGG块\"><a href=\"#2-1-VGG块\" class=\"headerlink\" title=\"2.1 VGG块\"></a>2.1 VGG块</h2><ul>\n<li>经典卷积神经网络的基本组成部分是如下序列：<ol>\n<li>带填充以保持分辨率的卷积层</li>\n<li>非线性激活函数，如ReLU</li>\n<li>汇聚层，如最大汇聚层</li>\n</ol>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">vgg_block</span>(<span class=\"params\">num_convs, in_channels, out_channels</span>):</span><br><span class=\"line\">    layers = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_convs):</span><br><span class=\"line\">        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=<span class=\"number\">3</span>, padding=<span class=\"number\">1</span>))</span><br><span class=\"line\">        layers.append(nn.ReLU())</span><br><span class=\"line\">        in_channels = out_channels</span><br><span class=\"line\">    layers.append(nn.MaxPool2d(kernel_size=<span class=\"number\">2</span>, stride=<span class=\"number\">2</span>))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nn.Sequential(*layers)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"2-2-VGG网络\"><a href=\"#2-2-VGG网络\" class=\"headerlink\" title=\"2.2 VGG网络\"></a>2.2 VGG网络</h2><ul>\n<li>从AlexNet到VGG，它们本质上都是块设计。<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/2_vgg/1.png\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/2_vgg/1.png\"></li>\n<li>原始VGG网络有5个卷积块，前两个块各有一个卷积层，后三个块各包含两个卷积层，因此共有8个卷积层。</li>\n<li>第一个模块有64输出通道，每个后续模块将输出通道翻倍，直到该数字达到512.</li>\n<li>由于该网络使用8个卷积层和3个全连接层，因此它通常被称为VGG-11。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 指定了每个VGG块里卷积层个数和输出通道数。</span></span><br><span class=\"line\">conv_arch = ((<span class=\"number\">1</span>, <span class=\"number\">64</span>), (<span class=\"number\">1</span>, <span class=\"number\">128</span>), (<span class=\"number\">2</span>, <span class=\"number\">256</span>), (<span class=\"number\">2</span>, <span class=\"number\">512</span>), (<span class=\"number\">2</span>, <span class=\"number\">512</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">vgg</span>(<span class=\"params\">conv_arch</span>):</span><br><span class=\"line\">    conv_blks = []</span><br><span class=\"line\">    in_channels = <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"comment\"># 卷积层部分</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (num_convs, out_channels) <span class=\"keyword\">in</span> conv_arch:</span><br><span class=\"line\">        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))</span><br><span class=\"line\">        in_channels = out_channels</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nn.Sequential(</span><br><span class=\"line\">        *conv_blks, nn.Flatten(),</span><br><span class=\"line\">        <span class=\"comment\"># 全连接层部分</span></span><br><span class=\"line\">        nn.Linear(out_channels*<span class=\"number\">7</span>*<span class=\"number\">7</span>, <span class=\"number\">4096</span>), nn.ReLU(), nn.Dropout(<span class=\"number\">0.5</span>), <span class=\"comment\">#为啥要*7：因为经过五个块后图像变成了7*7</span></span><br><span class=\"line\">        nn.Linear(<span class=\"number\">4096</span>, <span class=\"number\">4096</span>), nn.ReLU(), nn.Dropout(<span class=\"number\">0.5</span>),</span><br><span class=\"line\">        nn.Linear(<span class=\"number\">4096</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\">    )</span><br><span class=\"line\">net = vgg(conv_arch)</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>查看输出形状</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = torch.randn(size=(<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">224</span>, <span class=\"number\">224</span>))</span><br><span class=\"line\"><span class=\"keyword\">for</span> blk <span class=\"keyword\">in</span> net:</span><br><span class=\"line\">    X = blk(X)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(blk.__class__.__name__,<span class=\"string\">&#x27;output shape:\\t&#x27;</span>, X.shape)</span><br></pre></td></tr></table></figure>\n\n<pre><code>Sequential output shape:\t torch.Size([1, 64, 112, 112])\nSequential output shape:\t torch.Size([1, 128, 56, 56])\nSequential output shape:\t torch.Size([1, 256, 28, 28])\nSequential output shape:\t torch.Size([1, 512, 14, 14])\nSequential output shape:\t torch.Size([1, 512, 7, 7])\nFlatten output shape:\t torch.Size([1, 25088])\nLinear output shape:\t torch.Size([1, 4096])\nReLU output shape:\t torch.Size([1, 4096])\nDropout output shape:\t torch.Size([1, 4096])\nLinear output shape:\t torch.Size([1, 4096])\nReLU output shape:\t torch.Size([1, 4096])\nDropout output shape:\t torch.Size([1, 4096])\nLinear output shape:\t torch.Size([1, 10])\n</code></pre>\n<h2 id=\"2-3-训练模型\"><a href=\"#2-3-训练模型\" class=\"headerlink\" title=\"2.3 训练模型\"></a>2.3 训练模型</h2><ul>\n<li>由于VGG比AlexNet计算量更大，因此我们构建了一个通道数较少的网络，用于训练Fashion-MNIST数据集</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>] = <span class=\"string\">&quot;TRUE&quot;</span></span><br><span class=\"line\">ratio = <span class=\"number\">4</span></span><br><span class=\"line\">small_conv_arch = [(pair[<span class=\"number\">0</span>], pair[<span class=\"number\">1</span>] // ratio) <span class=\"keyword\">for</span> pair <span class=\"keyword\">in</span> conv_arch] <span class=\"comment\">#卷积层个数和输出通道数</span></span><br><span class=\"line\">net = vgg(small_conv_arch)</span><br><span class=\"line\"></span><br><span class=\"line\">lr, num_epochs, batch_size = <span class=\"number\">0.05</span>, <span class=\"number\">10</span>, <span class=\"number\">128</span></span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class=\"number\">224</span>)</span><br><span class=\"line\">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/2_vgg_files/2_vgg_7_0.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/2_vgg_files/2_vgg_7_0.svg\"></p>"},{"title":"6.1 深度卷积神经网络（AlexNet）","date":"2024-04-22T02:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 1 深度卷积神经网络（AlexNet）\n- 与训练端到端（从像素到分类结果）系统的不同，经典机器学习的流水线看起来更像：\n    1.获取一个数据集。\n    2.根据光学、几何学、其他知识以及偶然的发现，手工对特征数据集进行预处理。\n    3.通过标准的特征提取算法，如SIFT（尺度不变特征变换）和SURF（加速鲁棒特征）或其他手动调整的流水线来输入数据。\n    4.将提取的特征输入分类器，以训练分类器。\n## 1.1 学习表征\n- 在合理地复杂性前提下，特征应该由多个共同学习的神经网络层组成，每个层都有可学习的参数。在机器视觉中，最底层可能检测边缘、颜色和纹理。 \n- 在网络的最底层，模型学习到了一些类似于传统滤波器的特征抽取器。\n- AlexNet的更高层建立在这些底层表示的基础上，以表示更大的特征，如鼻子、眼睛等。而更高的层可以检测整个物体，如人、飞机等。最终的隐藏神经元可以学习图像的综合表示，从而使属于不同类别的数据易于区分。\n- 深度卷积神经网络的突破归因于两个因素：\n    - 数据\n    - 硬件：图形处理器（Graphics Processing Unit，GPU）\n## 1.2 AlexNet\n- 从LeNet（左）到AlexNet（右）\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/1_AlexNet/1.png)\n![](img/deeplearning/code/pytorch/6_moden_conv/1_AlexNet/1.png)\n- AlexNet和LeNet的设计理念非常相似，但也存在显著差异。\n    1. AlexNet比相对较小的LeNet5要深得多。\n    2. AlexNet使用ReLU而不是sigmoid作为其激活函数。\n### 1.2.1 模型设计\n- AlexNet的第一层，卷积窗口是11×11。因为ImageNet中绝大多数图像的宽和高比MNIST图像的多10倍以上，所以需要一个更大的卷积窗口来捕获目标。\n- 第二层中的卷积窗口减小到5×5，然后是3×3。此外，在第一、第二和第五个卷积层之后，加入窗口大小为3×3、步幅为2的最大池化层。\n- AlexNet的卷积通道数目是LeNet的10倍。\n- 最后一个卷积层后有两个全连接层，分别有4096个输出。\n### 1.2.2 激活函数\n- ReLU激活函数的计算更简单，它不需要如sigmoid激活函数那般复杂的求幂运算。\n- 当使用不同的初始化方法时，ReLU激活函数使训练模型更加容易。\n- 当sigmoid激活函数的输出非常接近于0或1时，这些区域的梯度几乎为0，因此反向传播无法继续更新一些模型参数。相反，ReLU激活函数在正区间的梯度总是1。因此，如果模型参数没有正确初始化，sigmoid函数可能在正区间内得到几乎为0的梯度，从而使模型无法得到有效的训练。\n### 1.2.3 容量控制和预处理\n- AlexNet通过暂退法控制全连接层的模型复杂度，而LeNet只使用了权重衰减。\n- 为了进一步扩充数据，AlexNet在训练时增加了大量的图像增强数据，如翻转、裁剪、变色。这使得模型更加健壮，更大的样本量有效地减少了过拟合。\n\n\n```python\nimport torch\nfrom torch import nn\nfrom d2l import  torch as d2l\n\nnet = nn.Sequential(\n    # 用11*11的更大窗口捕捉对象\n    # 步幅为4，以减少输出的高度和宽度\n    # 输出通道的数量比LeNet大数倍\n    nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(),\n    nn.MaxPool2d(kernel_size=3, stride=2),\n    # 减小卷积窗口，使用padding=2来使输入与输出的高和宽一致，且增大输出通道数\n    nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),\n    nn.MaxPool2d(kernel_size=3, stride=2),\n    # 使用三个连续的卷积层和较小的卷积窗口\n    # 除了最后的卷积层， 输出通道的数量进一步增加\n    # 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度\n    nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),\n    nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),\n    nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),\n    nn.MaxPool2d(kernel_size=3, stride=2),\n    nn.Flatten(),\n    # 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合\n    nn.Linear(6400, 4096), nn.ReLU(),\n    nn.Dropout(p=0.5),\n    nn.Linear(4096, 4096), nn.ReLU(),\n    nn.Dropout(p=0.5),\n    # 最后是输出层。由于这里使用Fashion-MNIST，所以类别数为10，而非论文中的1000\n    nn.Linear(4096, 10)\n)\n```\n\n- 构造一个(1,244,244)的单通道数据，来观察每一层输出的形状。\n\n\n```python\nX = torch.randn(1, 1, 224, 224)\nfor layer in net:\n    X = layer(X)\n    print(layer.__class__.__name__,'output shape:\\t',X.shape)\n```\n\n    Conv2d output shape:\t torch.Size([1, 96, 54, 54])\n    ReLU output shape:\t torch.Size([1, 96, 54, 54])\n    MaxPool2d output shape:\t torch.Size([1, 96, 26, 26])\n    Conv2d output shape:\t torch.Size([1, 256, 26, 26])\n    ReLU output shape:\t torch.Size([1, 256, 26, 26])\n    MaxPool2d output shape:\t torch.Size([1, 256, 12, 12])\n    Conv2d output shape:\t torch.Size([1, 384, 12, 12])\n    ReLU output shape:\t torch.Size([1, 384, 12, 12])\n    Conv2d output shape:\t torch.Size([1, 384, 12, 12])\n    ReLU output shape:\t torch.Size([1, 384, 12, 12])\n    Conv2d output shape:\t torch.Size([1, 256, 12, 12])\n    ReLU output shape:\t torch.Size([1, 256, 12, 12])\n    MaxPool2d output shape:\t torch.Size([1, 256, 5, 5])\n    Flatten output shape:\t torch.Size([1, 6400])\n    Linear output shape:\t torch.Size([1, 4096])\n    ReLU output shape:\t torch.Size([1, 4096])\n    Dropout output shape:\t torch.Size([1, 4096])\n    Linear output shape:\t torch.Size([1, 4096])\n    ReLU output shape:\t torch.Size([1, 4096])\n    Dropout output shape:\t torch.Size([1, 4096])\n    Linear output shape:\t torch.Size([1, 10])\n    \n\n## 1.3 读取数据集\n- 选取Fashion-MNIST数据集（28x28）。我们需要将图像大小调整到224x224。\n\n\n```python\nbatch_size = 128\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)\n```\n\n## 1.4 训练AlexNet\n- 与LeNet相比，这里使用更小的学习率，这是因为网络更深更广、图像分辨率更高，训练卷积神经网络就更昂贵。\n\n\n```python\nimport os\nos.environ[\"kmp_DUPLICATE_LIB_OK\"] = \"TRUE\"\nlr, num_epochs = 0.01, 10\nd2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())\n```\n\n    loss 0.327, train acc 0.880, test acc 0.883\n    20.6 examples/sec on cpu\n    \n\n\n    \n![svg](1_AlexNet_files/1_AlexNet_7_1.svg)\n![](img/deeplearning/code/pytorch/6_moden_conv/1_AlexNet_files/1_AlexNet_0_1.svg)\n\n","source":"_posts/deeplearning/code/pytorch/6_moden_convolution/1_AlexNet.md","raw":"---\ntitle: 6.1 深度卷积神经网络（AlexNet）\ndate: 2024-4-22 10:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 1 深度卷积神经网络（AlexNet）\n- 与训练端到端（从像素到分类结果）系统的不同，经典机器学习的流水线看起来更像：\n    1.获取一个数据集。\n    2.根据光学、几何学、其他知识以及偶然的发现，手工对特征数据集进行预处理。\n    3.通过标准的特征提取算法，如SIFT（尺度不变特征变换）和SURF（加速鲁棒特征）或其他手动调整的流水线来输入数据。\n    4.将提取的特征输入分类器，以训练分类器。\n## 1.1 学习表征\n- 在合理地复杂性前提下，特征应该由多个共同学习的神经网络层组成，每个层都有可学习的参数。在机器视觉中，最底层可能检测边缘、颜色和纹理。 \n- 在网络的最底层，模型学习到了一些类似于传统滤波器的特征抽取器。\n- AlexNet的更高层建立在这些底层表示的基础上，以表示更大的特征，如鼻子、眼睛等。而更高的层可以检测整个物体，如人、飞机等。最终的隐藏神经元可以学习图像的综合表示，从而使属于不同类别的数据易于区分。\n- 深度卷积神经网络的突破归因于两个因素：\n    - 数据\n    - 硬件：图形处理器（Graphics Processing Unit，GPU）\n## 1.2 AlexNet\n- 从LeNet（左）到AlexNet（右）\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/1_AlexNet/1.png)\n![](img/deeplearning/code/pytorch/6_moden_conv/1_AlexNet/1.png)\n- AlexNet和LeNet的设计理念非常相似，但也存在显著差异。\n    1. AlexNet比相对较小的LeNet5要深得多。\n    2. AlexNet使用ReLU而不是sigmoid作为其激活函数。\n### 1.2.1 模型设计\n- AlexNet的第一层，卷积窗口是11×11。因为ImageNet中绝大多数图像的宽和高比MNIST图像的多10倍以上，所以需要一个更大的卷积窗口来捕获目标。\n- 第二层中的卷积窗口减小到5×5，然后是3×3。此外，在第一、第二和第五个卷积层之后，加入窗口大小为3×3、步幅为2的最大池化层。\n- AlexNet的卷积通道数目是LeNet的10倍。\n- 最后一个卷积层后有两个全连接层，分别有4096个输出。\n### 1.2.2 激活函数\n- ReLU激活函数的计算更简单，它不需要如sigmoid激活函数那般复杂的求幂运算。\n- 当使用不同的初始化方法时，ReLU激活函数使训练模型更加容易。\n- 当sigmoid激活函数的输出非常接近于0或1时，这些区域的梯度几乎为0，因此反向传播无法继续更新一些模型参数。相反，ReLU激活函数在正区间的梯度总是1。因此，如果模型参数没有正确初始化，sigmoid函数可能在正区间内得到几乎为0的梯度，从而使模型无法得到有效的训练。\n### 1.2.3 容量控制和预处理\n- AlexNet通过暂退法控制全连接层的模型复杂度，而LeNet只使用了权重衰减。\n- 为了进一步扩充数据，AlexNet在训练时增加了大量的图像增强数据，如翻转、裁剪、变色。这使得模型更加健壮，更大的样本量有效地减少了过拟合。\n\n\n```python\nimport torch\nfrom torch import nn\nfrom d2l import  torch as d2l\n\nnet = nn.Sequential(\n    # 用11*11的更大窗口捕捉对象\n    # 步幅为4，以减少输出的高度和宽度\n    # 输出通道的数量比LeNet大数倍\n    nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(),\n    nn.MaxPool2d(kernel_size=3, stride=2),\n    # 减小卷积窗口，使用padding=2来使输入与输出的高和宽一致，且增大输出通道数\n    nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),\n    nn.MaxPool2d(kernel_size=3, stride=2),\n    # 使用三个连续的卷积层和较小的卷积窗口\n    # 除了最后的卷积层， 输出通道的数量进一步增加\n    # 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度\n    nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),\n    nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),\n    nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),\n    nn.MaxPool2d(kernel_size=3, stride=2),\n    nn.Flatten(),\n    # 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合\n    nn.Linear(6400, 4096), nn.ReLU(),\n    nn.Dropout(p=0.5),\n    nn.Linear(4096, 4096), nn.ReLU(),\n    nn.Dropout(p=0.5),\n    # 最后是输出层。由于这里使用Fashion-MNIST，所以类别数为10，而非论文中的1000\n    nn.Linear(4096, 10)\n)\n```\n\n- 构造一个(1,244,244)的单通道数据，来观察每一层输出的形状。\n\n\n```python\nX = torch.randn(1, 1, 224, 224)\nfor layer in net:\n    X = layer(X)\n    print(layer.__class__.__name__,'output shape:\\t',X.shape)\n```\n\n    Conv2d output shape:\t torch.Size([1, 96, 54, 54])\n    ReLU output shape:\t torch.Size([1, 96, 54, 54])\n    MaxPool2d output shape:\t torch.Size([1, 96, 26, 26])\n    Conv2d output shape:\t torch.Size([1, 256, 26, 26])\n    ReLU output shape:\t torch.Size([1, 256, 26, 26])\n    MaxPool2d output shape:\t torch.Size([1, 256, 12, 12])\n    Conv2d output shape:\t torch.Size([1, 384, 12, 12])\n    ReLU output shape:\t torch.Size([1, 384, 12, 12])\n    Conv2d output shape:\t torch.Size([1, 384, 12, 12])\n    ReLU output shape:\t torch.Size([1, 384, 12, 12])\n    Conv2d output shape:\t torch.Size([1, 256, 12, 12])\n    ReLU output shape:\t torch.Size([1, 256, 12, 12])\n    MaxPool2d output shape:\t torch.Size([1, 256, 5, 5])\n    Flatten output shape:\t torch.Size([1, 6400])\n    Linear output shape:\t torch.Size([1, 4096])\n    ReLU output shape:\t torch.Size([1, 4096])\n    Dropout output shape:\t torch.Size([1, 4096])\n    Linear output shape:\t torch.Size([1, 4096])\n    ReLU output shape:\t torch.Size([1, 4096])\n    Dropout output shape:\t torch.Size([1, 4096])\n    Linear output shape:\t torch.Size([1, 10])\n    \n\n## 1.3 读取数据集\n- 选取Fashion-MNIST数据集（28x28）。我们需要将图像大小调整到224x224。\n\n\n```python\nbatch_size = 128\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)\n```\n\n## 1.4 训练AlexNet\n- 与LeNet相比，这里使用更小的学习率，这是因为网络更深更广、图像分辨率更高，训练卷积神经网络就更昂贵。\n\n\n```python\nimport os\nos.environ[\"kmp_DUPLICATE_LIB_OK\"] = \"TRUE\"\nlr, num_epochs = 0.01, 10\nd2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())\n```\n\n    loss 0.327, train acc 0.880, test acc 0.883\n    20.6 examples/sec on cpu\n    \n\n\n    \n![svg](1_AlexNet_files/1_AlexNet_7_1.svg)\n![](img/deeplearning/code/pytorch/6_moden_conv/1_AlexNet_files/1_AlexNet_0_1.svg)\n\n","slug":"deeplearning/code/pytorch/6_moden_convolution/1_AlexNet","published":1,"updated":"2024-05-05T14:01:52.516Z","comments":1,"layout":"post","photos":[],"_id":"clvuli6dk0045zsvw3km77rmq","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"1-深度卷积神经网络（AlexNet）\"><a href=\"#1-深度卷积神经网络（AlexNet）\" class=\"headerlink\" title=\"1 深度卷积神经网络（AlexNet）\"></a>1 深度卷积神经网络（AlexNet）</h1><ul>\n<li>与训练端到端（从像素到分类结果）系统的不同，经典机器学习的流水线看起来更像：<br>  1.获取一个数据集。<br>  2.根据光学、几何学、其他知识以及偶然的发现，手工对特征数据集进行预处理。<br>  3.通过标准的特征提取算法，如SIFT（尺度不变特征变换）和SURF（加速鲁棒特征）或其他手动调整的流水线来输入数据。<br>  4.将提取的特征输入分类器，以训练分类器。</li>\n</ul>\n<h2 id=\"1-1-学习表征\"><a href=\"#1-1-学习表征\" class=\"headerlink\" title=\"1.1 学习表征\"></a>1.1 学习表征</h2><ul>\n<li>在合理地复杂性前提下，特征应该由多个共同学习的神经网络层组成，每个层都有可学习的参数。在机器视觉中，最底层可能检测边缘、颜色和纹理。 </li>\n<li>在网络的最底层，模型学习到了一些类似于传统滤波器的特征抽取器。</li>\n<li>AlexNet的更高层建立在这些底层表示的基础上，以表示更大的特征，如鼻子、眼睛等。而更高的层可以检测整个物体，如人、飞机等。最终的隐藏神经元可以学习图像的综合表示，从而使属于不同类别的数据易于区分。</li>\n<li>深度卷积神经网络的突破归因于两个因素：<ul>\n<li>数据</li>\n<li>硬件：图形处理器（Graphics Processing Unit，GPU）</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"1-2-AlexNet\"><a href=\"#1-2-AlexNet\" class=\"headerlink\" title=\"1.2 AlexNet\"></a>1.2 AlexNet</h2><ul>\n<li>从LeNet（左）到AlexNet（右）<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/1_AlexNet/1.png\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/1_AlexNet/1.png\"></li>\n<li>AlexNet和LeNet的设计理念非常相似，但也存在显著差异。<ol>\n<li>AlexNet比相对较小的LeNet5要深得多。</li>\n<li>AlexNet使用ReLU而不是sigmoid作为其激活函数。</li>\n</ol>\n</li>\n</ul>\n<h3 id=\"1-2-1-模型设计\"><a href=\"#1-2-1-模型设计\" class=\"headerlink\" title=\"1.2.1 模型设计\"></a>1.2.1 模型设计</h3><ul>\n<li>AlexNet的第一层，卷积窗口是11×11。因为ImageNet中绝大多数图像的宽和高比MNIST图像的多10倍以上，所以需要一个更大的卷积窗口来捕获目标。</li>\n<li>第二层中的卷积窗口减小到5×5，然后是3×3。此外，在第一、第二和第五个卷积层之后，加入窗口大小为3×3、步幅为2的最大池化层。</li>\n<li>AlexNet的卷积通道数目是LeNet的10倍。</li>\n<li>最后一个卷积层后有两个全连接层，分别有4096个输出。</li>\n</ul>\n<h3 id=\"1-2-2-激活函数\"><a href=\"#1-2-2-激活函数\" class=\"headerlink\" title=\"1.2.2 激活函数\"></a>1.2.2 激活函数</h3><ul>\n<li>ReLU激活函数的计算更简单，它不需要如sigmoid激活函数那般复杂的求幂运算。</li>\n<li>当使用不同的初始化方法时，ReLU激活函数使训练模型更加容易。</li>\n<li>当sigmoid激活函数的输出非常接近于0或1时，这些区域的梯度几乎为0，因此反向传播无法继续更新一些模型参数。相反，ReLU激活函数在正区间的梯度总是1。因此，如果模型参数没有正确初始化，sigmoid函数可能在正区间内得到几乎为0的梯度，从而使模型无法得到有效的训练。</li>\n</ul>\n<h3 id=\"1-2-3-容量控制和预处理\"><a href=\"#1-2-3-容量控制和预处理\" class=\"headerlink\" title=\"1.2.3 容量控制和预处理\"></a>1.2.3 容量控制和预处理</h3><ul>\n<li>AlexNet通过暂退法控制全连接层的模型复杂度，而LeNet只使用了权重衰减。</li>\n<li>为了进一步扩充数据，AlexNet在训练时增加了大量的图像增强数据，如翻转、裁剪、变色。这使得模型更加健壮，更大的样本量有效地减少了过拟合。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span>  torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">net = nn.Sequential(</span><br><span class=\"line\">    <span class=\"comment\"># 用11*11的更大窗口捕捉对象</span></span><br><span class=\"line\">    <span class=\"comment\"># 步幅为4，以减少输出的高度和宽度</span></span><br><span class=\"line\">    <span class=\"comment\"># 输出通道的数量比LeNet大数倍</span></span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">96</span>, kernel_size=<span class=\"number\">11</span>, stride=<span class=\"number\">4</span>, padding=<span class=\"number\">1</span>), nn.ReLU(),</span><br><span class=\"line\">    nn.MaxPool2d(kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>),</span><br><span class=\"line\">    <span class=\"comment\"># 减小卷积窗口，使用padding=2来使输入与输出的高和宽一致，且增大输出通道数</span></span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">96</span>, <span class=\"number\">256</span>, kernel_size=<span class=\"number\">5</span>, padding=<span class=\"number\">2</span>), nn.ReLU(),</span><br><span class=\"line\">    nn.MaxPool2d(kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>),</span><br><span class=\"line\">    <span class=\"comment\"># 使用三个连续的卷积层和较小的卷积窗口</span></span><br><span class=\"line\">    <span class=\"comment\"># 除了最后的卷积层， 输出通道的数量进一步增加</span></span><br><span class=\"line\">    <span class=\"comment\"># 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度</span></span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">256</span>, <span class=\"number\">384</span>, kernel_size=<span class=\"number\">3</span>, padding=<span class=\"number\">1</span>), nn.ReLU(),</span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">384</span>, <span class=\"number\">384</span>, kernel_size=<span class=\"number\">3</span>, padding=<span class=\"number\">1</span>), nn.ReLU(),</span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">384</span>, <span class=\"number\">256</span>, kernel_size=<span class=\"number\">3</span>, padding=<span class=\"number\">1</span>), nn.ReLU(),</span><br><span class=\"line\">    nn.MaxPool2d(kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>),</span><br><span class=\"line\">    nn.Flatten(),</span><br><span class=\"line\">    <span class=\"comment\"># 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合</span></span><br><span class=\"line\">    nn.Linear(<span class=\"number\">6400</span>, <span class=\"number\">4096</span>), nn.ReLU(),</span><br><span class=\"line\">    nn.Dropout(p=<span class=\"number\">0.5</span>),</span><br><span class=\"line\">    nn.Linear(<span class=\"number\">4096</span>, <span class=\"number\">4096</span>), nn.ReLU(),</span><br><span class=\"line\">    nn.Dropout(p=<span class=\"number\">0.5</span>),</span><br><span class=\"line\">    <span class=\"comment\"># 最后是输出层。由于这里使用Fashion-MNIST，所以类别数为10，而非论文中的1000</span></span><br><span class=\"line\">    nn.Linear(<span class=\"number\">4096</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>构造一个(1,244,244)的单通道数据，来观察每一层输出的形状。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = torch.randn(<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">224</span>, <span class=\"number\">224</span>)</span><br><span class=\"line\"><span class=\"keyword\">for</span> layer <span class=\"keyword\">in</span> net:</span><br><span class=\"line\">    X = layer(X)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(layer.__class__.__name__,<span class=\"string\">&#x27;output shape:\\t&#x27;</span>,X.shape)</span><br></pre></td></tr></table></figure>\n\n<pre><code>Conv2d output shape:\t torch.Size([1, 96, 54, 54])\nReLU output shape:\t torch.Size([1, 96, 54, 54])\nMaxPool2d output shape:\t torch.Size([1, 96, 26, 26])\nConv2d output shape:\t torch.Size([1, 256, 26, 26])\nReLU output shape:\t torch.Size([1, 256, 26, 26])\nMaxPool2d output shape:\t torch.Size([1, 256, 12, 12])\nConv2d output shape:\t torch.Size([1, 384, 12, 12])\nReLU output shape:\t torch.Size([1, 384, 12, 12])\nConv2d output shape:\t torch.Size([1, 384, 12, 12])\nReLU output shape:\t torch.Size([1, 384, 12, 12])\nConv2d output shape:\t torch.Size([1, 256, 12, 12])\nReLU output shape:\t torch.Size([1, 256, 12, 12])\nMaxPool2d output shape:\t torch.Size([1, 256, 5, 5])\nFlatten output shape:\t torch.Size([1, 6400])\nLinear output shape:\t torch.Size([1, 4096])\nReLU output shape:\t torch.Size([1, 4096])\nDropout output shape:\t torch.Size([1, 4096])\nLinear output shape:\t torch.Size([1, 4096])\nReLU output shape:\t torch.Size([1, 4096])\nDropout output shape:\t torch.Size([1, 4096])\nLinear output shape:\t torch.Size([1, 10])\n</code></pre>\n<h2 id=\"1-3-读取数据集\"><a href=\"#1-3-读取数据集\" class=\"headerlink\" title=\"1.3 读取数据集\"></a>1.3 读取数据集</h2><ul>\n<li>选取Fashion-MNIST数据集（28x28）。我们需要将图像大小调整到224x224。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">batch_size = <span class=\"number\">128</span></span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class=\"number\">224</span>)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"1-4-训练AlexNet\"><a href=\"#1-4-训练AlexNet\" class=\"headerlink\" title=\"1.4 训练AlexNet\"></a>1.4 训练AlexNet</h2><ul>\n<li>与LeNet相比，这里使用更小的学习率，这是因为网络更深更广、图像分辨率更高，训练卷积神经网络就更昂贵。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&quot;kmp_DUPLICATE_LIB_OK&quot;</span>] = <span class=\"string\">&quot;TRUE&quot;</span></span><br><span class=\"line\">lr, num_epochs = <span class=\"number\">0.01</span>, <span class=\"number\">10</span></span><br><span class=\"line\">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure>\n\n<pre><code>loss 0.327, train acc 0.880, test acc 0.883\n20.6 examples/sec on cpu\n</code></pre>\n<p><img src=\"/1_AlexNet_files/1_AlexNet_7_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/1_AlexNet_files/1_AlexNet_0_1.svg\"></p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"1-深度卷积神经网络（AlexNet）\"><a href=\"#1-深度卷积神经网络（AlexNet）\" class=\"headerlink\" title=\"1 深度卷积神经网络（AlexNet）\"></a>1 深度卷积神经网络（AlexNet）</h1><ul>\n<li>与训练端到端（从像素到分类结果）系统的不同，经典机器学习的流水线看起来更像：<br>  1.获取一个数据集。<br>  2.根据光学、几何学、其他知识以及偶然的发现，手工对特征数据集进行预处理。<br>  3.通过标准的特征提取算法，如SIFT（尺度不变特征变换）和SURF（加速鲁棒特征）或其他手动调整的流水线来输入数据。<br>  4.将提取的特征输入分类器，以训练分类器。</li>\n</ul>\n<h2 id=\"1-1-学习表征\"><a href=\"#1-1-学习表征\" class=\"headerlink\" title=\"1.1 学习表征\"></a>1.1 学习表征</h2><ul>\n<li>在合理地复杂性前提下，特征应该由多个共同学习的神经网络层组成，每个层都有可学习的参数。在机器视觉中，最底层可能检测边缘、颜色和纹理。 </li>\n<li>在网络的最底层，模型学习到了一些类似于传统滤波器的特征抽取器。</li>\n<li>AlexNet的更高层建立在这些底层表示的基础上，以表示更大的特征，如鼻子、眼睛等。而更高的层可以检测整个物体，如人、飞机等。最终的隐藏神经元可以学习图像的综合表示，从而使属于不同类别的数据易于区分。</li>\n<li>深度卷积神经网络的突破归因于两个因素：<ul>\n<li>数据</li>\n<li>硬件：图形处理器（Graphics Processing Unit，GPU）</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"1-2-AlexNet\"><a href=\"#1-2-AlexNet\" class=\"headerlink\" title=\"1.2 AlexNet\"></a>1.2 AlexNet</h2><ul>\n<li>从LeNet（左）到AlexNet（右）<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/1_AlexNet/1.png\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/1_AlexNet/1.png\"></li>\n<li>AlexNet和LeNet的设计理念非常相似，但也存在显著差异。<ol>\n<li>AlexNet比相对较小的LeNet5要深得多。</li>\n<li>AlexNet使用ReLU而不是sigmoid作为其激活函数。</li>\n</ol>\n</li>\n</ul>\n<h3 id=\"1-2-1-模型设计\"><a href=\"#1-2-1-模型设计\" class=\"headerlink\" title=\"1.2.1 模型设计\"></a>1.2.1 模型设计</h3><ul>\n<li>AlexNet的第一层，卷积窗口是11×11。因为ImageNet中绝大多数图像的宽和高比MNIST图像的多10倍以上，所以需要一个更大的卷积窗口来捕获目标。</li>\n<li>第二层中的卷积窗口减小到5×5，然后是3×3。此外，在第一、第二和第五个卷积层之后，加入窗口大小为3×3、步幅为2的最大池化层。</li>\n<li>AlexNet的卷积通道数目是LeNet的10倍。</li>\n<li>最后一个卷积层后有两个全连接层，分别有4096个输出。</li>\n</ul>\n<h3 id=\"1-2-2-激活函数\"><a href=\"#1-2-2-激活函数\" class=\"headerlink\" title=\"1.2.2 激活函数\"></a>1.2.2 激活函数</h3><ul>\n<li>ReLU激活函数的计算更简单，它不需要如sigmoid激活函数那般复杂的求幂运算。</li>\n<li>当使用不同的初始化方法时，ReLU激活函数使训练模型更加容易。</li>\n<li>当sigmoid激活函数的输出非常接近于0或1时，这些区域的梯度几乎为0，因此反向传播无法继续更新一些模型参数。相反，ReLU激活函数在正区间的梯度总是1。因此，如果模型参数没有正确初始化，sigmoid函数可能在正区间内得到几乎为0的梯度，从而使模型无法得到有效的训练。</li>\n</ul>\n<h3 id=\"1-2-3-容量控制和预处理\"><a href=\"#1-2-3-容量控制和预处理\" class=\"headerlink\" title=\"1.2.3 容量控制和预处理\"></a>1.2.3 容量控制和预处理</h3><ul>\n<li>AlexNet通过暂退法控制全连接层的模型复杂度，而LeNet只使用了权重衰减。</li>\n<li>为了进一步扩充数据，AlexNet在训练时增加了大量的图像增强数据，如翻转、裁剪、变色。这使得模型更加健壮，更大的样本量有效地减少了过拟合。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span>  torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">net = nn.Sequential(</span><br><span class=\"line\">    <span class=\"comment\"># 用11*11的更大窗口捕捉对象</span></span><br><span class=\"line\">    <span class=\"comment\"># 步幅为4，以减少输出的高度和宽度</span></span><br><span class=\"line\">    <span class=\"comment\"># 输出通道的数量比LeNet大数倍</span></span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">96</span>, kernel_size=<span class=\"number\">11</span>, stride=<span class=\"number\">4</span>, padding=<span class=\"number\">1</span>), nn.ReLU(),</span><br><span class=\"line\">    nn.MaxPool2d(kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>),</span><br><span class=\"line\">    <span class=\"comment\"># 减小卷积窗口，使用padding=2来使输入与输出的高和宽一致，且增大输出通道数</span></span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">96</span>, <span class=\"number\">256</span>, kernel_size=<span class=\"number\">5</span>, padding=<span class=\"number\">2</span>), nn.ReLU(),</span><br><span class=\"line\">    nn.MaxPool2d(kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>),</span><br><span class=\"line\">    <span class=\"comment\"># 使用三个连续的卷积层和较小的卷积窗口</span></span><br><span class=\"line\">    <span class=\"comment\"># 除了最后的卷积层， 输出通道的数量进一步增加</span></span><br><span class=\"line\">    <span class=\"comment\"># 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度</span></span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">256</span>, <span class=\"number\">384</span>, kernel_size=<span class=\"number\">3</span>, padding=<span class=\"number\">1</span>), nn.ReLU(),</span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">384</span>, <span class=\"number\">384</span>, kernel_size=<span class=\"number\">3</span>, padding=<span class=\"number\">1</span>), nn.ReLU(),</span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">384</span>, <span class=\"number\">256</span>, kernel_size=<span class=\"number\">3</span>, padding=<span class=\"number\">1</span>), nn.ReLU(),</span><br><span class=\"line\">    nn.MaxPool2d(kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>),</span><br><span class=\"line\">    nn.Flatten(),</span><br><span class=\"line\">    <span class=\"comment\"># 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合</span></span><br><span class=\"line\">    nn.Linear(<span class=\"number\">6400</span>, <span class=\"number\">4096</span>), nn.ReLU(),</span><br><span class=\"line\">    nn.Dropout(p=<span class=\"number\">0.5</span>),</span><br><span class=\"line\">    nn.Linear(<span class=\"number\">4096</span>, <span class=\"number\">4096</span>), nn.ReLU(),</span><br><span class=\"line\">    nn.Dropout(p=<span class=\"number\">0.5</span>),</span><br><span class=\"line\">    <span class=\"comment\"># 最后是输出层。由于这里使用Fashion-MNIST，所以类别数为10，而非论文中的1000</span></span><br><span class=\"line\">    nn.Linear(<span class=\"number\">4096</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>构造一个(1,244,244)的单通道数据，来观察每一层输出的形状。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = torch.randn(<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">224</span>, <span class=\"number\">224</span>)</span><br><span class=\"line\"><span class=\"keyword\">for</span> layer <span class=\"keyword\">in</span> net:</span><br><span class=\"line\">    X = layer(X)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(layer.__class__.__name__,<span class=\"string\">&#x27;output shape:\\t&#x27;</span>,X.shape)</span><br></pre></td></tr></table></figure>\n\n<pre><code>Conv2d output shape:\t torch.Size([1, 96, 54, 54])\nReLU output shape:\t torch.Size([1, 96, 54, 54])\nMaxPool2d output shape:\t torch.Size([1, 96, 26, 26])\nConv2d output shape:\t torch.Size([1, 256, 26, 26])\nReLU output shape:\t torch.Size([1, 256, 26, 26])\nMaxPool2d output shape:\t torch.Size([1, 256, 12, 12])\nConv2d output shape:\t torch.Size([1, 384, 12, 12])\nReLU output shape:\t torch.Size([1, 384, 12, 12])\nConv2d output shape:\t torch.Size([1, 384, 12, 12])\nReLU output shape:\t torch.Size([1, 384, 12, 12])\nConv2d output shape:\t torch.Size([1, 256, 12, 12])\nReLU output shape:\t torch.Size([1, 256, 12, 12])\nMaxPool2d output shape:\t torch.Size([1, 256, 5, 5])\nFlatten output shape:\t torch.Size([1, 6400])\nLinear output shape:\t torch.Size([1, 4096])\nReLU output shape:\t torch.Size([1, 4096])\nDropout output shape:\t torch.Size([1, 4096])\nLinear output shape:\t torch.Size([1, 4096])\nReLU output shape:\t torch.Size([1, 4096])\nDropout output shape:\t torch.Size([1, 4096])\nLinear output shape:\t torch.Size([1, 10])\n</code></pre>\n<h2 id=\"1-3-读取数据集\"><a href=\"#1-3-读取数据集\" class=\"headerlink\" title=\"1.3 读取数据集\"></a>1.3 读取数据集</h2><ul>\n<li>选取Fashion-MNIST数据集（28x28）。我们需要将图像大小调整到224x224。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">batch_size = <span class=\"number\">128</span></span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class=\"number\">224</span>)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"1-4-训练AlexNet\"><a href=\"#1-4-训练AlexNet\" class=\"headerlink\" title=\"1.4 训练AlexNet\"></a>1.4 训练AlexNet</h2><ul>\n<li>与LeNet相比，这里使用更小的学习率，这是因为网络更深更广、图像分辨率更高，训练卷积神经网络就更昂贵。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&quot;kmp_DUPLICATE_LIB_OK&quot;</span>] = <span class=\"string\">&quot;TRUE&quot;</span></span><br><span class=\"line\">lr, num_epochs = <span class=\"number\">0.01</span>, <span class=\"number\">10</span></span><br><span class=\"line\">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure>\n\n<pre><code>loss 0.327, train acc 0.880, test acc 0.883\n20.6 examples/sec on cpu\n</code></pre>\n<p><img src=\"/1_AlexNet_files/1_AlexNet_7_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/1_AlexNet_files/1_AlexNet_0_1.svg\"></p>"},{"title":"6.3 网络中的网络（NiN）","date":"2024-04-22T04:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 3 网络中的网络（NiN）\n- LeNet、AlexNet和VGG都有一个共同设计模式：\n    - 通过一系列的卷积层与汇聚层来提取空间结构特征；然后通过全连接层对特征的表征进行处理。AlexNet和VGG对LeNet的改进主要在于如何扩大和加深这两个模块。\n- 如果使用了全连接层，可能会完全放弃表征的空间结构。网络中的网络（NiN）提供了一个非常简单的解决方案：在每个像素的通道上分别使用多层感知机\n## 3.1 NiN块\n- NiN的想法是在每个像素位置（针对每个高度和宽度）应用一个全连接层。如果我们将权重连接到每个空间位置，我们可以将其视为1 × 1卷积层，或作为在每个像素位置上独立作用的全连接层。\n- 从另一个角度看，即将空间维度中的每个像素视为单个样本，将通道维度视为不同特征（feature）。\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/3_NiN/1.png)\n![](img/deeplearning/code/pytorch/6_moden_conv/3_NiN/1.png)\n- NiN块以一个普通卷积层开始，后面是两个1 × 1的卷积层。这两个1 × 1卷积层充当带有ReLU激活函数的逐像素全连接层。第一层的卷积窗口形状通常由用户设置。随后的卷积窗口形状固定为1 × 1。\n\n\n```python\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\ndef nin_block(in_channels, out_channels, kernel_size, strides, padding):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),\n        nn.ReLU(),\n        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU(),\n        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU()\n    )\n```\n\n## 3.2 NiN模型\n- NiN使用窗口形状为11×11、5×5和3×3的卷积层，输出通道数量与AlexNet中的相同。每个NiN块后有一个最大汇聚层，汇聚窗口形状为3 × 3，步幅为2。\n- NiN和AlexNet之间的一个显著区别是NiN完全取消了全连接层。相反，NiN使用一个NiN块，其输出通道数等于标签类别的数量。最后放一个全局平均汇聚层（global average pooling layer），生成一个对数几率（logits）。\n- NiN设计的一个优点是，它显著减少了模型所需参数的数量。然而，在实践中，这种设计有时会增加训练模型的时间。\n\n\n\n```python\nnet = nn.Sequential(\n    nin_block(1, 96, kernel_size=11, strides=4, padding=0),\n    nn.MaxPool2d(3, stride=2),\n    nin_block(96, 256, kernel_size=5, strides=1, padding=2),\n    nn.MaxPool2d(3, stride=2),\n    nin_block(256, 384, kernel_size=3, strides=1, padding=1),\n    nn.MaxPool2d(3, stride=2),\n    nn.Dropout(0.5),\n    # 标签类别数是10\n    nin_block(384, 10, kernel_size=3, strides=1, padding=1),\n    nn.AdaptiveAvgPool2d((1,1)),\n    # 将四维输出转换成二维输出，其形状为（batch_size, 10）\n    nn.Flatten()\n)\n```\n\n- 查看形状\n\n\n```python\nX = torch.rand(size=(1,1,224,224))\nfor layer in net:\n    X = layer(X)\n    print(layer.__class__.__name__,'output shape:\\t',X.shape)\n```\n\n    Sequential output shape:\t torch.Size([1, 96, 54, 54])\n    MaxPool2d output shape:\t torch.Size([1, 96, 26, 26])\n    Sequential output shape:\t torch.Size([1, 256, 26, 26])\n    MaxPool2d output shape:\t torch.Size([1, 256, 12, 12])\n    Sequential output shape:\t torch.Size([1, 384, 12, 12])\n    MaxPool2d output shape:\t torch.Size([1, 384, 5, 5])\n    Dropout output shape:\t torch.Size([1, 384, 5, 5])\n    Sequential output shape:\t torch.Size([1, 10, 5, 5])\n    AdaptiveAvgPool2d output shape:\t torch.Size([1, 10, 1, 1])\n    Flatten output shape:\t torch.Size([1, 10])\n    \n\n## 3.3 训练模型\n\n\n\n```python\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\nlr, num_epochs, batch_size = 0.1, 10, 128\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)\nd2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())\n```\n\n\n    \n![svg](3_NiN_files/3_NiN_7_0.svg)\n![](img/deeplearning/code/pytorch/6_moden_conv/3_NiN_files/3_NiN_7_0.svg)\n\n","source":"_posts/deeplearning/code/pytorch/6_moden_convolution/3_NiN.md","raw":"---\ntitle: 6.3 网络中的网络（NiN）\ndate: 2024-4-22 12:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 3 网络中的网络（NiN）\n- LeNet、AlexNet和VGG都有一个共同设计模式：\n    - 通过一系列的卷积层与汇聚层来提取空间结构特征；然后通过全连接层对特征的表征进行处理。AlexNet和VGG对LeNet的改进主要在于如何扩大和加深这两个模块。\n- 如果使用了全连接层，可能会完全放弃表征的空间结构。网络中的网络（NiN）提供了一个非常简单的解决方案：在每个像素的通道上分别使用多层感知机\n## 3.1 NiN块\n- NiN的想法是在每个像素位置（针对每个高度和宽度）应用一个全连接层。如果我们将权重连接到每个空间位置，我们可以将其视为1 × 1卷积层，或作为在每个像素位置上独立作用的全连接层。\n- 从另一个角度看，即将空间维度中的每个像素视为单个样本，将通道维度视为不同特征（feature）。\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/3_NiN/1.png)\n![](img/deeplearning/code/pytorch/6_moden_conv/3_NiN/1.png)\n- NiN块以一个普通卷积层开始，后面是两个1 × 1的卷积层。这两个1 × 1卷积层充当带有ReLU激活函数的逐像素全连接层。第一层的卷积窗口形状通常由用户设置。随后的卷积窗口形状固定为1 × 1。\n\n\n```python\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\ndef nin_block(in_channels, out_channels, kernel_size, strides, padding):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),\n        nn.ReLU(),\n        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU(),\n        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU()\n    )\n```\n\n## 3.2 NiN模型\n- NiN使用窗口形状为11×11、5×5和3×3的卷积层，输出通道数量与AlexNet中的相同。每个NiN块后有一个最大汇聚层，汇聚窗口形状为3 × 3，步幅为2。\n- NiN和AlexNet之间的一个显著区别是NiN完全取消了全连接层。相反，NiN使用一个NiN块，其输出通道数等于标签类别的数量。最后放一个全局平均汇聚层（global average pooling layer），生成一个对数几率（logits）。\n- NiN设计的一个优点是，它显著减少了模型所需参数的数量。然而，在实践中，这种设计有时会增加训练模型的时间。\n\n\n\n```python\nnet = nn.Sequential(\n    nin_block(1, 96, kernel_size=11, strides=4, padding=0),\n    nn.MaxPool2d(3, stride=2),\n    nin_block(96, 256, kernel_size=5, strides=1, padding=2),\n    nn.MaxPool2d(3, stride=2),\n    nin_block(256, 384, kernel_size=3, strides=1, padding=1),\n    nn.MaxPool2d(3, stride=2),\n    nn.Dropout(0.5),\n    # 标签类别数是10\n    nin_block(384, 10, kernel_size=3, strides=1, padding=1),\n    nn.AdaptiveAvgPool2d((1,1)),\n    # 将四维输出转换成二维输出，其形状为（batch_size, 10）\n    nn.Flatten()\n)\n```\n\n- 查看形状\n\n\n```python\nX = torch.rand(size=(1,1,224,224))\nfor layer in net:\n    X = layer(X)\n    print(layer.__class__.__name__,'output shape:\\t',X.shape)\n```\n\n    Sequential output shape:\t torch.Size([1, 96, 54, 54])\n    MaxPool2d output shape:\t torch.Size([1, 96, 26, 26])\n    Sequential output shape:\t torch.Size([1, 256, 26, 26])\n    MaxPool2d output shape:\t torch.Size([1, 256, 12, 12])\n    Sequential output shape:\t torch.Size([1, 384, 12, 12])\n    MaxPool2d output shape:\t torch.Size([1, 384, 5, 5])\n    Dropout output shape:\t torch.Size([1, 384, 5, 5])\n    Sequential output shape:\t torch.Size([1, 10, 5, 5])\n    AdaptiveAvgPool2d output shape:\t torch.Size([1, 10, 1, 1])\n    Flatten output shape:\t torch.Size([1, 10])\n    \n\n## 3.3 训练模型\n\n\n\n```python\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\nlr, num_epochs, batch_size = 0.1, 10, 128\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)\nd2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())\n```\n\n\n    \n![svg](3_NiN_files/3_NiN_7_0.svg)\n![](img/deeplearning/code/pytorch/6_moden_conv/3_NiN_files/3_NiN_7_0.svg)\n\n","slug":"deeplearning/code/pytorch/6_moden_convolution/3_NiN","published":1,"updated":"2024-05-05T14:04:49.120Z","comments":1,"layout":"post","photos":[],"_id":"clvuli6dm0047zsvwgzxv3z0n","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"3-网络中的网络（NiN）\"><a href=\"#3-网络中的网络（NiN）\" class=\"headerlink\" title=\"3 网络中的网络（NiN）\"></a>3 网络中的网络（NiN）</h1><ul>\n<li>LeNet、AlexNet和VGG都有一个共同设计模式：<ul>\n<li>通过一系列的卷积层与汇聚层来提取空间结构特征；然后通过全连接层对特征的表征进行处理。AlexNet和VGG对LeNet的改进主要在于如何扩大和加深这两个模块。</li>\n</ul>\n</li>\n<li>如果使用了全连接层，可能会完全放弃表征的空间结构。网络中的网络（NiN）提供了一个非常简单的解决方案：在每个像素的通道上分别使用多层感知机</li>\n</ul>\n<h2 id=\"3-1-NiN块\"><a href=\"#3-1-NiN块\" class=\"headerlink\" title=\"3.1 NiN块\"></a>3.1 NiN块</h2><ul>\n<li>NiN的想法是在每个像素位置（针对每个高度和宽度）应用一个全连接层。如果我们将权重连接到每个空间位置，我们可以将其视为1 × 1卷积层，或作为在每个像素位置上独立作用的全连接层。</li>\n<li>从另一个角度看，即将空间维度中的每个像素视为单个样本，将通道维度视为不同特征（feature）。<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/3_NiN/1.png\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/3_NiN/1.png\"></li>\n<li>NiN块以一个普通卷积层开始，后面是两个1 × 1的卷积层。这两个1 × 1卷积层充当带有ReLU激活函数的逐像素全连接层。第一层的卷积窗口形状通常由用户设置。随后的卷积窗口形状固定为1 × 1。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">nin_block</span>(<span class=\"params\">in_channels, out_channels, kernel_size, strides, padding</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nn.Sequential(</span><br><span class=\"line\">        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),</span><br><span class=\"line\">        nn.ReLU(),</span><br><span class=\"line\">        nn.Conv2d(out_channels, out_channels, kernel_size=<span class=\"number\">1</span>), nn.ReLU(),</span><br><span class=\"line\">        nn.Conv2d(out_channels, out_channels, kernel_size=<span class=\"number\">1</span>), nn.ReLU()</span><br><span class=\"line\">    )</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"3-2-NiN模型\"><a href=\"#3-2-NiN模型\" class=\"headerlink\" title=\"3.2 NiN模型\"></a>3.2 NiN模型</h2><ul>\n<li>NiN使用窗口形状为11×11、5×5和3×3的卷积层，输出通道数量与AlexNet中的相同。每个NiN块后有一个最大汇聚层，汇聚窗口形状为3 × 3，步幅为2。</li>\n<li>NiN和AlexNet之间的一个显著区别是NiN完全取消了全连接层。相反，NiN使用一个NiN块，其输出通道数等于标签类别的数量。最后放一个全局平均汇聚层（global average pooling layer），生成一个对数几率（logits）。</li>\n<li>NiN设计的一个优点是，它显著减少了模型所需参数的数量。然而，在实践中，这种设计有时会增加训练模型的时间。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net = nn.Sequential(</span><br><span class=\"line\">    nin_block(<span class=\"number\">1</span>, <span class=\"number\">96</span>, kernel_size=<span class=\"number\">11</span>, strides=<span class=\"number\">4</span>, padding=<span class=\"number\">0</span>),</span><br><span class=\"line\">    nn.MaxPool2d(<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>),</span><br><span class=\"line\">    nin_block(<span class=\"number\">96</span>, <span class=\"number\">256</span>, kernel_size=<span class=\"number\">5</span>, strides=<span class=\"number\">1</span>, padding=<span class=\"number\">2</span>),</span><br><span class=\"line\">    nn.MaxPool2d(<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>),</span><br><span class=\"line\">    nin_block(<span class=\"number\">256</span>, <span class=\"number\">384</span>, kernel_size=<span class=\"number\">3</span>, strides=<span class=\"number\">1</span>, padding=<span class=\"number\">1</span>),</span><br><span class=\"line\">    nn.MaxPool2d(<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>),</span><br><span class=\"line\">    nn.Dropout(<span class=\"number\">0.5</span>),</span><br><span class=\"line\">    <span class=\"comment\"># 标签类别数是10</span></span><br><span class=\"line\">    nin_block(<span class=\"number\">384</span>, <span class=\"number\">10</span>, kernel_size=<span class=\"number\">3</span>, strides=<span class=\"number\">1</span>, padding=<span class=\"number\">1</span>),</span><br><span class=\"line\">    nn.AdaptiveAvgPool2d((<span class=\"number\">1</span>,<span class=\"number\">1</span>)),</span><br><span class=\"line\">    <span class=\"comment\"># 将四维输出转换成二维输出，其形状为（batch_size, 10）</span></span><br><span class=\"line\">    nn.Flatten()</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>查看形状</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = torch.rand(size=(<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">224</span>,<span class=\"number\">224</span>))</span><br><span class=\"line\"><span class=\"keyword\">for</span> layer <span class=\"keyword\">in</span> net:</span><br><span class=\"line\">    X = layer(X)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(layer.__class__.__name__,<span class=\"string\">&#x27;output shape:\\t&#x27;</span>,X.shape)</span><br></pre></td></tr></table></figure>\n\n<pre><code>Sequential output shape:\t torch.Size([1, 96, 54, 54])\nMaxPool2d output shape:\t torch.Size([1, 96, 26, 26])\nSequential output shape:\t torch.Size([1, 256, 26, 26])\nMaxPool2d output shape:\t torch.Size([1, 256, 12, 12])\nSequential output shape:\t torch.Size([1, 384, 12, 12])\nMaxPool2d output shape:\t torch.Size([1, 384, 5, 5])\nDropout output shape:\t torch.Size([1, 384, 5, 5])\nSequential output shape:\t torch.Size([1, 10, 5, 5])\nAdaptiveAvgPool2d output shape:\t torch.Size([1, 10, 1, 1])\nFlatten output shape:\t torch.Size([1, 10])\n</code></pre>\n<h2 id=\"3-3-训练模型\"><a href=\"#3-3-训练模型\" class=\"headerlink\" title=\"3.3 训练模型\"></a>3.3 训练模型</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>] = <span class=\"string\">&#x27;TRUE&#x27;</span></span><br><span class=\"line\">lr, num_epochs, batch_size = <span class=\"number\">0.1</span>, <span class=\"number\">10</span>, <span class=\"number\">128</span></span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class=\"number\">224</span>)</span><br><span class=\"line\">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/3_NiN_files/3_NiN_7_0.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/3_NiN_files/3_NiN_7_0.svg\"></p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"3-网络中的网络（NiN）\"><a href=\"#3-网络中的网络（NiN）\" class=\"headerlink\" title=\"3 网络中的网络（NiN）\"></a>3 网络中的网络（NiN）</h1><ul>\n<li>LeNet、AlexNet和VGG都有一个共同设计模式：<ul>\n<li>通过一系列的卷积层与汇聚层来提取空间结构特征；然后通过全连接层对特征的表征进行处理。AlexNet和VGG对LeNet的改进主要在于如何扩大和加深这两个模块。</li>\n</ul>\n</li>\n<li>如果使用了全连接层，可能会完全放弃表征的空间结构。网络中的网络（NiN）提供了一个非常简单的解决方案：在每个像素的通道上分别使用多层感知机</li>\n</ul>\n<h2 id=\"3-1-NiN块\"><a href=\"#3-1-NiN块\" class=\"headerlink\" title=\"3.1 NiN块\"></a>3.1 NiN块</h2><ul>\n<li>NiN的想法是在每个像素位置（针对每个高度和宽度）应用一个全连接层。如果我们将权重连接到每个空间位置，我们可以将其视为1 × 1卷积层，或作为在每个像素位置上独立作用的全连接层。</li>\n<li>从另一个角度看，即将空间维度中的每个像素视为单个样本，将通道维度视为不同特征（feature）。<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/3_NiN/1.png\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/3_NiN/1.png\"></li>\n<li>NiN块以一个普通卷积层开始，后面是两个1 × 1的卷积层。这两个1 × 1卷积层充当带有ReLU激活函数的逐像素全连接层。第一层的卷积窗口形状通常由用户设置。随后的卷积窗口形状固定为1 × 1。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">nin_block</span>(<span class=\"params\">in_channels, out_channels, kernel_size, strides, padding</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nn.Sequential(</span><br><span class=\"line\">        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),</span><br><span class=\"line\">        nn.ReLU(),</span><br><span class=\"line\">        nn.Conv2d(out_channels, out_channels, kernel_size=<span class=\"number\">1</span>), nn.ReLU(),</span><br><span class=\"line\">        nn.Conv2d(out_channels, out_channels, kernel_size=<span class=\"number\">1</span>), nn.ReLU()</span><br><span class=\"line\">    )</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"3-2-NiN模型\"><a href=\"#3-2-NiN模型\" class=\"headerlink\" title=\"3.2 NiN模型\"></a>3.2 NiN模型</h2><ul>\n<li>NiN使用窗口形状为11×11、5×5和3×3的卷积层，输出通道数量与AlexNet中的相同。每个NiN块后有一个最大汇聚层，汇聚窗口形状为3 × 3，步幅为2。</li>\n<li>NiN和AlexNet之间的一个显著区别是NiN完全取消了全连接层。相反，NiN使用一个NiN块，其输出通道数等于标签类别的数量。最后放一个全局平均汇聚层（global average pooling layer），生成一个对数几率（logits）。</li>\n<li>NiN设计的一个优点是，它显著减少了模型所需参数的数量。然而，在实践中，这种设计有时会增加训练模型的时间。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net = nn.Sequential(</span><br><span class=\"line\">    nin_block(<span class=\"number\">1</span>, <span class=\"number\">96</span>, kernel_size=<span class=\"number\">11</span>, strides=<span class=\"number\">4</span>, padding=<span class=\"number\">0</span>),</span><br><span class=\"line\">    nn.MaxPool2d(<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>),</span><br><span class=\"line\">    nin_block(<span class=\"number\">96</span>, <span class=\"number\">256</span>, kernel_size=<span class=\"number\">5</span>, strides=<span class=\"number\">1</span>, padding=<span class=\"number\">2</span>),</span><br><span class=\"line\">    nn.MaxPool2d(<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>),</span><br><span class=\"line\">    nin_block(<span class=\"number\">256</span>, <span class=\"number\">384</span>, kernel_size=<span class=\"number\">3</span>, strides=<span class=\"number\">1</span>, padding=<span class=\"number\">1</span>),</span><br><span class=\"line\">    nn.MaxPool2d(<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>),</span><br><span class=\"line\">    nn.Dropout(<span class=\"number\">0.5</span>),</span><br><span class=\"line\">    <span class=\"comment\"># 标签类别数是10</span></span><br><span class=\"line\">    nin_block(<span class=\"number\">384</span>, <span class=\"number\">10</span>, kernel_size=<span class=\"number\">3</span>, strides=<span class=\"number\">1</span>, padding=<span class=\"number\">1</span>),</span><br><span class=\"line\">    nn.AdaptiveAvgPool2d((<span class=\"number\">1</span>,<span class=\"number\">1</span>)),</span><br><span class=\"line\">    <span class=\"comment\"># 将四维输出转换成二维输出，其形状为（batch_size, 10）</span></span><br><span class=\"line\">    nn.Flatten()</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>查看形状</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = torch.rand(size=(<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">224</span>,<span class=\"number\">224</span>))</span><br><span class=\"line\"><span class=\"keyword\">for</span> layer <span class=\"keyword\">in</span> net:</span><br><span class=\"line\">    X = layer(X)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(layer.__class__.__name__,<span class=\"string\">&#x27;output shape:\\t&#x27;</span>,X.shape)</span><br></pre></td></tr></table></figure>\n\n<pre><code>Sequential output shape:\t torch.Size([1, 96, 54, 54])\nMaxPool2d output shape:\t torch.Size([1, 96, 26, 26])\nSequential output shape:\t torch.Size([1, 256, 26, 26])\nMaxPool2d output shape:\t torch.Size([1, 256, 12, 12])\nSequential output shape:\t torch.Size([1, 384, 12, 12])\nMaxPool2d output shape:\t torch.Size([1, 384, 5, 5])\nDropout output shape:\t torch.Size([1, 384, 5, 5])\nSequential output shape:\t torch.Size([1, 10, 5, 5])\nAdaptiveAvgPool2d output shape:\t torch.Size([1, 10, 1, 1])\nFlatten output shape:\t torch.Size([1, 10])\n</code></pre>\n<h2 id=\"3-3-训练模型\"><a href=\"#3-3-训练模型\" class=\"headerlink\" title=\"3.3 训练模型\"></a>3.3 训练模型</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>] = <span class=\"string\">&#x27;TRUE&#x27;</span></span><br><span class=\"line\">lr, num_epochs, batch_size = <span class=\"number\">0.1</span>, <span class=\"number\">10</span>, <span class=\"number\">128</span></span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class=\"number\">224</span>)</span><br><span class=\"line\">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/3_NiN_files/3_NiN_7_0.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/3_NiN_files/3_NiN_7_0.svg\"></p>"},{"title":"6.5 批量规范化","date":"2024-04-22T06:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 5 批量规范化\n- 训练深层神经网络十分困难。批量规范化（batchnormalization）可持续加速深层网络的收敛速度。\n## 5.1 训练深层网络\n- 训练神经网络时出现的一些挑战：\n    - 数据预处理的方式通常会对最终结果产生巨大影响。例如使用真实数据时，我们的第一步是标准化输入特征，使其平均值为0，方差为1。直观地说，这种标准化可以很好地与我们的优化器配合使用，因为它可以将参数的量级进行统一。\n    - 中间层中的变量（例如，多层感知机中的仿射变换输出）可能具有更广的变化范围\n    - 更深层的网络很复杂，容易过拟合。这意味着正则化变得更加重要。\n- 批量规范化应用于单个可选层（也可以应用到所有层），其原理如下：在每次训练迭代中，我们首先规范化输入，即通过减去其均值并除以其标准差，其中两者均基于当前小批量处理。接下来，我们应用比例系数和比例偏移。正是由于这个基于批量统计的标准化，才有了批量规范化的名称。\n- 如果使用大小为1的小批量应用批量规范化，我们将无法学到任何东西。这是因为在减去均值之后，每个隐藏单元将为0。所以，只有使用足够大的小批量，批量规范化这种方法才是有效且稳定的。请注意，在应用批量规范化时，批量大小的选择可能比没有批量规范化时更重要。\n- 用$\\mathbf{x} \\in \\mathcal{B}$表示一个来自小批量$\\mathcal{B}$的输入，批量规范化定义为\n\n$$\\mathrm{BN} ( \\mathbf{x})= \\boldsymbol{ \\gamma} \\odot \\frac{ \\mathbf{x}- \\hat{ \\boldsymbol{ \\mu}}_ \\mathcal{B}}{ \\hat{ \\boldsymbol{ \\sigma}}_ \\mathcal{B}} + \\boldsymbol{ \\beta}$$\n\n$$\\begin{aligned}\n&\\hat{\\boldsymbol{\\mu}}_ {\\mathcal{B}} = \\frac1{| \\mathcal{B}|} \\sum_{ \\mathbf{x} \\in \\mathcal{B}} \\mathbf{x}, \\\\ \n&\\hat{\\boldsymbol{ \\sigma}}_ { \\mathcal{B}}^ 2 = \\frac1{| \\mathcal{B}|} \\sum_ { \\mathbf{x} \\in \\mathcal{B}}( \\mathbf{x} - \\hat{ \\boldsymbol{ \\mu}}_ \\mathcal{B})^ 2+ \\epsilon. \n\\end{aligned}$$\n- $\\gamma$和$\\beta$是可以学习的参数向量, $\\epsilon$防止除0\n- 在训练过程中，我们无法得知使用整个数据集来估计平均值和方差，所以只能根据每个小批次的平均值和方差不断训练模型。而在预测模式下，可以根据整个数据集精确计算批量规范化所需的平均值和方差。\n\n## 5.2 批量规范化层\n- 讨论两种情况：全连接层和卷积层，他们的批量规范化实现略有不同。\n### 5.2.1 全连接层\n- 通常，我们将批量规范化层置于全连接层中的仿射变换和激活函数之间。设全连接层的输入为x，权重参数为$\\mathbf{W}$，偏差参数为$ \\mathbf{b}$，激活函数为$\\phi$。设批量规范化的运算符为$\\mathrm{BN}$。那么，使用批量规范化的全连接层的输出的计算表达式为：\n$$\\mathbf{h}= \\phi( \\mathrm{BN}( \\mathbf{W} \\mathbf{x} + \\mathbf{b}))$$\n### 5.2.2 卷积层\n- 对于卷积层，我们可以在卷积层之后和非线性激活函数之前应用批量规范化。对每个通道的输出执行批量规范化，每个通道都有自己的拉伸（scale）和偏移（shift）参数，这两个参数都是标量。\n- 假设我们的小批量包含m个样本，并且对于每个通道，卷积的输出具有高度p和宽度q。那么对于卷积层，我们在每个输出通道的m · p · q个元素上同时执行每个批量规范化。\n### 5.2.3 预测过程中的批量规范化\n- 批量规范化在训练模式和预测模式下的行为通常不同。\n- 预测时，我们不再需要样本均值中的噪声以及在微批次上估计每个小批次产生的样本方差了。\n- 其次，例如，我们可能需要使用我们的模型对逐个样本进行预测。一种常用的方法是通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们得到确定的输出。可见，和暂退法一样，批量规范化层在训练模式和预测模式下的计算结果也是不一样的。\n\n## 5.3 从零实现\n- 基础设计模式：\n    - 用一个单独的函数定义其数学原理\n    - 然后将次功能集成到一个自定义层中，其代码主要处理数据移动到训练设备，初始化变量、跟踪移动平均线。\n\n\n```python\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\ndef batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n    # 通过is_grad_enabled来判断当前模型是训练模式还是预测模式\n    if not torch.is_grad_enabled():\n        # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差\n        X_hat= (X-moving_mean) / torch.sqrt(moving_var +eps)\n    else:\n        assert len(X.shape) in (2,4)\n        if len(X.shape) == 2:\n            mean = X.mean(dim=0)\n            var = ((X-mean)**2).mean(dim=0)\n        else:\n            # 使用二维卷积层的情况，计算通道维度上（axis = 1)的均值和方差\n            # 这里我们需要保持X的形状以便后面可以做广播运算\n            mean = X.mean(dim=(0,2,3), keepdim = True)\n            var = ((X - mean)**2).mean(dim=(0,2,3), keepdim=True)\n        # 训练模式下，用当前的均值和方差做标准化\n        X_hat = (X-mean) / torch.sqrt(var + eps)\n        # 更新移动平均的均值和方差\n        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n        moving_var = momentum * moving_var + (1.0 - momentum) * var\n    Y = gamma * X_hat + beta #缩放和位移\n    return Y, moving_mean, moving_var\n\n```\n\n- 创建BatchNorm层。\n- 参数：gamma,beta，这两个参数将在训练过程中更新。此外，我们的层将保存均值和方差的平均移动值，以便在模型预测期间随后使用。\n\n\n```python\nclass BatchNorm(nn.Module):\n    # num_features: 完全连接层的输出数量或卷积层的输出通道数\n    # num_dim: 2表示完全连接，4表示卷积层\n    def __init__(self, num_features, num_dims):\n        super().__init__()\n        if num_dims == 2:\n            shape = (1, num_features)\n        else:\n            shape = (1, num_features, 1,1)\n        # 参与求梯度和迭代的拉伸和偏移参数， 分别初始化成1和0\n        self.gamma = nn.Parameter(torch.ones(shape))\n        self.beta = nn.Parameter(torch.zeros(shape))\n        # 非模型参数的变量初始化为0和1\n        self.moving_mean = torch.zeros(shape)\n        self.moving_var = torch.ones(shape)\n    def forward(self, X):\n        # 如果X不在内存上， 将moving_mean和moving_var复制到X所在显卡上\n        if self.moving_mean.device != X.device:\n            self.moving_mean = self.moving_mean.to(X.device)\n            self.moving_var = self.moving_var.to(X.device)\n        # 保存更新过的moving_mean和moving_var\n        Y, self.moving_mean, self.moving_var = batch_norm(\n            X, self.gamma, self.beta, self.moving_mean, self.moving_var, eps=1e-5, momentum=0.9\n        )\n        return Y\n```\n\n## 5.4 使用批量规范化层的LeNet\n- 将BatchNorm应用于LeNet模型\n\n\n```python\nnet = nn.Sequential(\n    nn.Conv2d(1, 6, kernel_size=5), BatchNorm(6, num_dims=4), nn.Sigmoid(),\n    nn.AvgPool2d(kernel_size=2, stride=2),\n    nn.Conv2d(6, 16, kernel_size=5), BatchNorm(16, num_dims=4), nn.Sigmoid(),\n    nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(),\n    nn.Linear(16*4*4, 120), BatchNorm(120, num_dims=2), nn.Sigmoid(),\n    nn.Linear(120, 84), BatchNorm(84, num_dims=2), nn.Sigmoid(),\n    nn.Linear(84, 10)\n)\n\nimport os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\" \nlr, num_epochs, batch_size = 1.0, 10, 256\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\nd2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())\n```\n\n    loss 0.267, train acc 0.900, test acc 0.812\n    4058.4 examples/sec on cpu\n    \n\n\n    \n![svg](5_batchnormal_files/5_batchnormal_5_1.svg)\n![](img/deeplearning/code/pytorch/6_moden_conv/5_batchnormal_files/5_batchnormal_5_1.svg)\n\n\n\n## 5.5 简明实现\n- 通常高级API变体运行速度快得多，因为它的代码已编译为C++或CUDA，而我们的自定义代码由Python实现。\n\n\n```python\nnet = nn.Sequential(\n    nn.Conv2d(1,6, kernel_size=5), nn.BatchNorm2d(6), nn.Sigmoid(),\n    nn.AvgPool2d(kernel_size=2, stride=2),\n    nn.Conv2d(6, 16, kernel_size=5),nn.BatchNorm2d(16), nn.Sigmoid(),\n    nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(),\n    nn.Linear(256, 120), nn.BatchNorm1d(120), nn.Sigmoid(),\n    nn.Linear(120, 84), nn.BatchNorm1d(84), nn.Sigmoid(),\n    nn.Linear(84, 10)\n)\nd2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())\n```\n\n    loss 0.271, train acc 0.899, test acc 0.845\n    7140.4 examples/sec on cpu\n    \n\n\n    \n![svg](5_batchnormal_files/5_batchnormal_7_1.svg)\n![](img/deeplearning/code/pytorch/6_moden_conv/5_batchnormal_files/5_batchnormal_7_1.svg)\n\n\n## 5.6 争议\n- 批量规范化被认为可以使优化更加平滑。我们甚至不知道简单的神经网络（多层感知机和传统的卷积神经网络）为什么如此有效。即使在暂退法和权重衰减的情况下，它们仍然非常灵活，因此无法通过常规的学习理论泛化保证来解释它们是否能够泛化到看不见的数据。\n- 其原理：通过减少内部协变量偏移（internal covariate shift）。据推测，作者所说的内部协变量转移类似于上述的投机直觉，即变量值的分布在训练过程中会发生变化。然而，这种解释有两个问题：1、这种偏移与严格定义的协变量偏移（covariate shift）非常不同，所以这个名字用词不当；2、这种解释只提供了一种不明确的直觉，但留下了一个有待后续挖掘的问题：为什么这项技术如此有效？\n\n","source":"_posts/deeplearning/code/pytorch/6_moden_convolution/5_batchnormal.md","raw":"---\ntitle: 6.5 批量规范化\ndate: 2024-4-22 14:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 5 批量规范化\n- 训练深层神经网络十分困难。批量规范化（batchnormalization）可持续加速深层网络的收敛速度。\n## 5.1 训练深层网络\n- 训练神经网络时出现的一些挑战：\n    - 数据预处理的方式通常会对最终结果产生巨大影响。例如使用真实数据时，我们的第一步是标准化输入特征，使其平均值为0，方差为1。直观地说，这种标准化可以很好地与我们的优化器配合使用，因为它可以将参数的量级进行统一。\n    - 中间层中的变量（例如，多层感知机中的仿射变换输出）可能具有更广的变化范围\n    - 更深层的网络很复杂，容易过拟合。这意味着正则化变得更加重要。\n- 批量规范化应用于单个可选层（也可以应用到所有层），其原理如下：在每次训练迭代中，我们首先规范化输入，即通过减去其均值并除以其标准差，其中两者均基于当前小批量处理。接下来，我们应用比例系数和比例偏移。正是由于这个基于批量统计的标准化，才有了批量规范化的名称。\n- 如果使用大小为1的小批量应用批量规范化，我们将无法学到任何东西。这是因为在减去均值之后，每个隐藏单元将为0。所以，只有使用足够大的小批量，批量规范化这种方法才是有效且稳定的。请注意，在应用批量规范化时，批量大小的选择可能比没有批量规范化时更重要。\n- 用$\\mathbf{x} \\in \\mathcal{B}$表示一个来自小批量$\\mathcal{B}$的输入，批量规范化定义为\n\n$$\\mathrm{BN} ( \\mathbf{x})= \\boldsymbol{ \\gamma} \\odot \\frac{ \\mathbf{x}- \\hat{ \\boldsymbol{ \\mu}}_ \\mathcal{B}}{ \\hat{ \\boldsymbol{ \\sigma}}_ \\mathcal{B}} + \\boldsymbol{ \\beta}$$\n\n$$\\begin{aligned}\n&\\hat{\\boldsymbol{\\mu}}_ {\\mathcal{B}} = \\frac1{| \\mathcal{B}|} \\sum_{ \\mathbf{x} \\in \\mathcal{B}} \\mathbf{x}, \\\\ \n&\\hat{\\boldsymbol{ \\sigma}}_ { \\mathcal{B}}^ 2 = \\frac1{| \\mathcal{B}|} \\sum_ { \\mathbf{x} \\in \\mathcal{B}}( \\mathbf{x} - \\hat{ \\boldsymbol{ \\mu}}_ \\mathcal{B})^ 2+ \\epsilon. \n\\end{aligned}$$\n- $\\gamma$和$\\beta$是可以学习的参数向量, $\\epsilon$防止除0\n- 在训练过程中，我们无法得知使用整个数据集来估计平均值和方差，所以只能根据每个小批次的平均值和方差不断训练模型。而在预测模式下，可以根据整个数据集精确计算批量规范化所需的平均值和方差。\n\n## 5.2 批量规范化层\n- 讨论两种情况：全连接层和卷积层，他们的批量规范化实现略有不同。\n### 5.2.1 全连接层\n- 通常，我们将批量规范化层置于全连接层中的仿射变换和激活函数之间。设全连接层的输入为x，权重参数为$\\mathbf{W}$，偏差参数为$ \\mathbf{b}$，激活函数为$\\phi$。设批量规范化的运算符为$\\mathrm{BN}$。那么，使用批量规范化的全连接层的输出的计算表达式为：\n$$\\mathbf{h}= \\phi( \\mathrm{BN}( \\mathbf{W} \\mathbf{x} + \\mathbf{b}))$$\n### 5.2.2 卷积层\n- 对于卷积层，我们可以在卷积层之后和非线性激活函数之前应用批量规范化。对每个通道的输出执行批量规范化，每个通道都有自己的拉伸（scale）和偏移（shift）参数，这两个参数都是标量。\n- 假设我们的小批量包含m个样本，并且对于每个通道，卷积的输出具有高度p和宽度q。那么对于卷积层，我们在每个输出通道的m · p · q个元素上同时执行每个批量规范化。\n### 5.2.3 预测过程中的批量规范化\n- 批量规范化在训练模式和预测模式下的行为通常不同。\n- 预测时，我们不再需要样本均值中的噪声以及在微批次上估计每个小批次产生的样本方差了。\n- 其次，例如，我们可能需要使用我们的模型对逐个样本进行预测。一种常用的方法是通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们得到确定的输出。可见，和暂退法一样，批量规范化层在训练模式和预测模式下的计算结果也是不一样的。\n\n## 5.3 从零实现\n- 基础设计模式：\n    - 用一个单独的函数定义其数学原理\n    - 然后将次功能集成到一个自定义层中，其代码主要处理数据移动到训练设备，初始化变量、跟踪移动平均线。\n\n\n```python\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\ndef batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n    # 通过is_grad_enabled来判断当前模型是训练模式还是预测模式\n    if not torch.is_grad_enabled():\n        # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差\n        X_hat= (X-moving_mean) / torch.sqrt(moving_var +eps)\n    else:\n        assert len(X.shape) in (2,4)\n        if len(X.shape) == 2:\n            mean = X.mean(dim=0)\n            var = ((X-mean)**2).mean(dim=0)\n        else:\n            # 使用二维卷积层的情况，计算通道维度上（axis = 1)的均值和方差\n            # 这里我们需要保持X的形状以便后面可以做广播运算\n            mean = X.mean(dim=(0,2,3), keepdim = True)\n            var = ((X - mean)**2).mean(dim=(0,2,3), keepdim=True)\n        # 训练模式下，用当前的均值和方差做标准化\n        X_hat = (X-mean) / torch.sqrt(var + eps)\n        # 更新移动平均的均值和方差\n        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n        moving_var = momentum * moving_var + (1.0 - momentum) * var\n    Y = gamma * X_hat + beta #缩放和位移\n    return Y, moving_mean, moving_var\n\n```\n\n- 创建BatchNorm层。\n- 参数：gamma,beta，这两个参数将在训练过程中更新。此外，我们的层将保存均值和方差的平均移动值，以便在模型预测期间随后使用。\n\n\n```python\nclass BatchNorm(nn.Module):\n    # num_features: 完全连接层的输出数量或卷积层的输出通道数\n    # num_dim: 2表示完全连接，4表示卷积层\n    def __init__(self, num_features, num_dims):\n        super().__init__()\n        if num_dims == 2:\n            shape = (1, num_features)\n        else:\n            shape = (1, num_features, 1,1)\n        # 参与求梯度和迭代的拉伸和偏移参数， 分别初始化成1和0\n        self.gamma = nn.Parameter(torch.ones(shape))\n        self.beta = nn.Parameter(torch.zeros(shape))\n        # 非模型参数的变量初始化为0和1\n        self.moving_mean = torch.zeros(shape)\n        self.moving_var = torch.ones(shape)\n    def forward(self, X):\n        # 如果X不在内存上， 将moving_mean和moving_var复制到X所在显卡上\n        if self.moving_mean.device != X.device:\n            self.moving_mean = self.moving_mean.to(X.device)\n            self.moving_var = self.moving_var.to(X.device)\n        # 保存更新过的moving_mean和moving_var\n        Y, self.moving_mean, self.moving_var = batch_norm(\n            X, self.gamma, self.beta, self.moving_mean, self.moving_var, eps=1e-5, momentum=0.9\n        )\n        return Y\n```\n\n## 5.4 使用批量规范化层的LeNet\n- 将BatchNorm应用于LeNet模型\n\n\n```python\nnet = nn.Sequential(\n    nn.Conv2d(1, 6, kernel_size=5), BatchNorm(6, num_dims=4), nn.Sigmoid(),\n    nn.AvgPool2d(kernel_size=2, stride=2),\n    nn.Conv2d(6, 16, kernel_size=5), BatchNorm(16, num_dims=4), nn.Sigmoid(),\n    nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(),\n    nn.Linear(16*4*4, 120), BatchNorm(120, num_dims=2), nn.Sigmoid(),\n    nn.Linear(120, 84), BatchNorm(84, num_dims=2), nn.Sigmoid(),\n    nn.Linear(84, 10)\n)\n\nimport os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\" \nlr, num_epochs, batch_size = 1.0, 10, 256\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\nd2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())\n```\n\n    loss 0.267, train acc 0.900, test acc 0.812\n    4058.4 examples/sec on cpu\n    \n\n\n    \n![svg](5_batchnormal_files/5_batchnormal_5_1.svg)\n![](img/deeplearning/code/pytorch/6_moden_conv/5_batchnormal_files/5_batchnormal_5_1.svg)\n\n\n\n## 5.5 简明实现\n- 通常高级API变体运行速度快得多，因为它的代码已编译为C++或CUDA，而我们的自定义代码由Python实现。\n\n\n```python\nnet = nn.Sequential(\n    nn.Conv2d(1,6, kernel_size=5), nn.BatchNorm2d(6), nn.Sigmoid(),\n    nn.AvgPool2d(kernel_size=2, stride=2),\n    nn.Conv2d(6, 16, kernel_size=5),nn.BatchNorm2d(16), nn.Sigmoid(),\n    nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(),\n    nn.Linear(256, 120), nn.BatchNorm1d(120), nn.Sigmoid(),\n    nn.Linear(120, 84), nn.BatchNorm1d(84), nn.Sigmoid(),\n    nn.Linear(84, 10)\n)\nd2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())\n```\n\n    loss 0.271, train acc 0.899, test acc 0.845\n    7140.4 examples/sec on cpu\n    \n\n\n    \n![svg](5_batchnormal_files/5_batchnormal_7_1.svg)\n![](img/deeplearning/code/pytorch/6_moden_conv/5_batchnormal_files/5_batchnormal_7_1.svg)\n\n\n## 5.6 争议\n- 批量规范化被认为可以使优化更加平滑。我们甚至不知道简单的神经网络（多层感知机和传统的卷积神经网络）为什么如此有效。即使在暂退法和权重衰减的情况下，它们仍然非常灵活，因此无法通过常规的学习理论泛化保证来解释它们是否能够泛化到看不见的数据。\n- 其原理：通过减少内部协变量偏移（internal covariate shift）。据推测，作者所说的内部协变量转移类似于上述的投机直觉，即变量值的分布在训练过程中会发生变化。然而，这种解释有两个问题：1、这种偏移与严格定义的协变量偏移（covariate shift）非常不同，所以这个名字用词不当；2、这种解释只提供了一种不明确的直觉，但留下了一个有待后续挖掘的问题：为什么这项技术如此有效？\n\n","slug":"deeplearning/code/pytorch/6_moden_convolution/5_batchnormal","published":1,"updated":"2024-05-05T14:07:06.469Z","comments":1,"layout":"post","photos":[],"_id":"clvuli6dm0049zsvwgcy026hz","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"5-批量规范化\"><a href=\"#5-批量规范化\" class=\"headerlink\" title=\"5 批量规范化\"></a>5 批量规范化</h1><ul>\n<li>训练深层神经网络十分困难。批量规范化（batchnormalization）可持续加速深层网络的收敛速度。</li>\n</ul>\n<h2 id=\"5-1-训练深层网络\"><a href=\"#5-1-训练深层网络\" class=\"headerlink\" title=\"5.1 训练深层网络\"></a>5.1 训练深层网络</h2><ul>\n<li>训练神经网络时出现的一些挑战：<ul>\n<li>数据预处理的方式通常会对最终结果产生巨大影响。例如使用真实数据时，我们的第一步是标准化输入特征，使其平均值为0，方差为1。直观地说，这种标准化可以很好地与我们的优化器配合使用，因为它可以将参数的量级进行统一。</li>\n<li>中间层中的变量（例如，多层感知机中的仿射变换输出）可能具有更广的变化范围</li>\n<li>更深层的网络很复杂，容易过拟合。这意味着正则化变得更加重要。</li>\n</ul>\n</li>\n<li>批量规范化应用于单个可选层（也可以应用到所有层），其原理如下：在每次训练迭代中，我们首先规范化输入，即通过减去其均值并除以其标准差，其中两者均基于当前小批量处理。接下来，我们应用比例系数和比例偏移。正是由于这个基于批量统计的标准化，才有了批量规范化的名称。</li>\n<li>如果使用大小为1的小批量应用批量规范化，我们将无法学到任何东西。这是因为在减去均值之后，每个隐藏单元将为0。所以，只有使用足够大的小批量，批量规范化这种方法才是有效且稳定的。请注意，在应用批量规范化时，批量大小的选择可能比没有批量规范化时更重要。</li>\n<li>用$\\mathbf{x} \\in \\mathcal{B}$表示一个来自小批量$\\mathcal{B}$的输入，批量规范化定义为</li>\n</ul>\n<p>$$\\mathrm{BN} ( \\mathbf{x})&#x3D; \\boldsymbol{ \\gamma} \\odot \\frac{ \\mathbf{x}- \\hat{ \\boldsymbol{ \\mu}}_ \\mathcal{B}}{ \\hat{ \\boldsymbol{ \\sigma}}_ \\mathcal{B}} + \\boldsymbol{ \\beta}$$</p>\n<p>$$\\begin{aligned}<br>&amp;\\hat{\\boldsymbol{\\mu}}_ {\\mathcal{B}} &#x3D; \\frac1{| \\mathcal{B}|} \\sum_{ \\mathbf{x} \\in \\mathcal{B}} \\mathbf{x}, \\<br>&amp;\\hat{\\boldsymbol{ \\sigma}}_ { \\mathcal{B}}^ 2 &#x3D; \\frac1{| \\mathcal{B}|} \\sum_ { \\mathbf{x} \\in \\mathcal{B}}( \\mathbf{x} - \\hat{ \\boldsymbol{ \\mu}}_ \\mathcal{B})^ 2+ \\epsilon.<br>\\end{aligned}$$</p>\n<ul>\n<li>$\\gamma$和$\\beta$是可以学习的参数向量, $\\epsilon$防止除0</li>\n<li>在训练过程中，我们无法得知使用整个数据集来估计平均值和方差，所以只能根据每个小批次的平均值和方差不断训练模型。而在预测模式下，可以根据整个数据集精确计算批量规范化所需的平均值和方差。</li>\n</ul>\n<h2 id=\"5-2-批量规范化层\"><a href=\"#5-2-批量规范化层\" class=\"headerlink\" title=\"5.2 批量规范化层\"></a>5.2 批量规范化层</h2><ul>\n<li>讨论两种情况：全连接层和卷积层，他们的批量规范化实现略有不同。</li>\n</ul>\n<h3 id=\"5-2-1-全连接层\"><a href=\"#5-2-1-全连接层\" class=\"headerlink\" title=\"5.2.1 全连接层\"></a>5.2.1 全连接层</h3><ul>\n<li>通常，我们将批量规范化层置于全连接层中的仿射变换和激活函数之间。设全连接层的输入为x，权重参数为$\\mathbf{W}$，偏差参数为$ \\mathbf{b}$，激活函数为$\\phi$。设批量规范化的运算符为$\\mathrm{BN}$。那么，使用批量规范化的全连接层的输出的计算表达式为：<br>$$\\mathbf{h}&#x3D; \\phi( \\mathrm{BN}( \\mathbf{W} \\mathbf{x} + \\mathbf{b}))$$</li>\n</ul>\n<h3 id=\"5-2-2-卷积层\"><a href=\"#5-2-2-卷积层\" class=\"headerlink\" title=\"5.2.2 卷积层\"></a>5.2.2 卷积层</h3><ul>\n<li>对于卷积层，我们可以在卷积层之后和非线性激活函数之前应用批量规范化。对每个通道的输出执行批量规范化，每个通道都有自己的拉伸（scale）和偏移（shift）参数，这两个参数都是标量。</li>\n<li>假设我们的小批量包含m个样本，并且对于每个通道，卷积的输出具有高度p和宽度q。那么对于卷积层，我们在每个输出通道的m · p · q个元素上同时执行每个批量规范化。</li>\n</ul>\n<h3 id=\"5-2-3-预测过程中的批量规范化\"><a href=\"#5-2-3-预测过程中的批量规范化\" class=\"headerlink\" title=\"5.2.3 预测过程中的批量规范化\"></a>5.2.3 预测过程中的批量规范化</h3><ul>\n<li>批量规范化在训练模式和预测模式下的行为通常不同。</li>\n<li>预测时，我们不再需要样本均值中的噪声以及在微批次上估计每个小批次产生的样本方差了。</li>\n<li>其次，例如，我们可能需要使用我们的模型对逐个样本进行预测。一种常用的方法是通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们得到确定的输出。可见，和暂退法一样，批量规范化层在训练模式和预测模式下的计算结果也是不一样的。</li>\n</ul>\n<h2 id=\"5-3-从零实现\"><a href=\"#5-3-从零实现\" class=\"headerlink\" title=\"5.3 从零实现\"></a>5.3 从零实现</h2><ul>\n<li>基础设计模式：<ul>\n<li>用一个单独的函数定义其数学原理</li>\n<li>然后将次功能集成到一个自定义层中，其代码主要处理数据移动到训练设备，初始化变量、跟踪移动平均线。</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">batch_norm</span>(<span class=\"params\">X, gamma, beta, moving_mean, moving_var, eps, momentum</span>):</span><br><span class=\"line\">    <span class=\"comment\"># 通过is_grad_enabled来判断当前模型是训练模式还是预测模式</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> torch.is_grad_enabled():</span><br><span class=\"line\">        <span class=\"comment\"># 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差</span></span><br><span class=\"line\">        X_hat= (X-moving_mean) / torch.sqrt(moving_var +eps)</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">assert</span> <span class=\"built_in\">len</span>(X.shape) <span class=\"keyword\">in</span> (<span class=\"number\">2</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(X.shape) == <span class=\"number\">2</span>:</span><br><span class=\"line\">            mean = X.mean(dim=<span class=\"number\">0</span>)</span><br><span class=\"line\">            var = ((X-mean)**<span class=\"number\">2</span>).mean(dim=<span class=\"number\">0</span>)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"comment\"># 使用二维卷积层的情况，计算通道维度上（axis = 1)的均值和方差</span></span><br><span class=\"line\">            <span class=\"comment\"># 这里我们需要保持X的形状以便后面可以做广播运算</span></span><br><span class=\"line\">            mean = X.mean(dim=(<span class=\"number\">0</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>), keepdim = <span class=\"literal\">True</span>)</span><br><span class=\"line\">            var = ((X - mean)**<span class=\"number\">2</span>).mean(dim=(<span class=\"number\">0</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>), keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\">        <span class=\"comment\"># 训练模式下，用当前的均值和方差做标准化</span></span><br><span class=\"line\">        X_hat = (X-mean) / torch.sqrt(var + eps)</span><br><span class=\"line\">        <span class=\"comment\"># 更新移动平均的均值和方差</span></span><br><span class=\"line\">        moving_mean = momentum * moving_mean + (<span class=\"number\">1.0</span> - momentum) * mean</span><br><span class=\"line\">        moving_var = momentum * moving_var + (<span class=\"number\">1.0</span> - momentum) * var</span><br><span class=\"line\">    Y = gamma * X_hat + beta <span class=\"comment\">#缩放和位移</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> Y, moving_mean, moving_var</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>创建BatchNorm层。</li>\n<li>参数：gamma,beta，这两个参数将在训练过程中更新。此外，我们的层将保存均值和方差的平均移动值，以便在模型预测期间随后使用。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">BatchNorm</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"comment\"># num_features: 完全连接层的输出数量或卷积层的输出通道数</span></span><br><span class=\"line\">    <span class=\"comment\"># num_dim: 2表示完全连接，4表示卷积层</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, num_features, num_dims</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> num_dims == <span class=\"number\">2</span>:</span><br><span class=\"line\">            shape = (<span class=\"number\">1</span>, num_features)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            shape = (<span class=\"number\">1</span>, num_features, <span class=\"number\">1</span>,<span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"comment\"># 参与求梯度和迭代的拉伸和偏移参数， 分别初始化成1和0</span></span><br><span class=\"line\">        self.gamma = nn.Parameter(torch.ones(shape))</span><br><span class=\"line\">        self.beta = nn.Parameter(torch.zeros(shape))</span><br><span class=\"line\">        <span class=\"comment\"># 非模型参数的变量初始化为0和1</span></span><br><span class=\"line\">        self.moving_mean = torch.zeros(shape)</span><br><span class=\"line\">        self.moving_var = torch.ones(shape)</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 如果X不在内存上， 将moving_mean和moving_var复制到X所在显卡上</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.moving_mean.device != X.device:</span><br><span class=\"line\">            self.moving_mean = self.moving_mean.to(X.device)</span><br><span class=\"line\">            self.moving_var = self.moving_var.to(X.device)</span><br><span class=\"line\">        <span class=\"comment\"># 保存更新过的moving_mean和moving_var</span></span><br><span class=\"line\">        Y, self.moving_mean, self.moving_var = batch_norm(</span><br><span class=\"line\">            X, self.gamma, self.beta, self.moving_mean, self.moving_var, eps=<span class=\"number\">1e-5</span>, momentum=<span class=\"number\">0.9</span></span><br><span class=\"line\">        )</span><br><span class=\"line\">        <span class=\"keyword\">return</span> Y</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"5-4-使用批量规范化层的LeNet\"><a href=\"#5-4-使用批量规范化层的LeNet\" class=\"headerlink\" title=\"5.4 使用批量规范化层的LeNet\"></a>5.4 使用批量规范化层的LeNet</h2><ul>\n<li>将BatchNorm应用于LeNet模型</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net = nn.Sequential(</span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">6</span>, kernel_size=<span class=\"number\">5</span>), BatchNorm(<span class=\"number\">6</span>, num_dims=<span class=\"number\">4</span>), nn.Sigmoid(),</span><br><span class=\"line\">    nn.AvgPool2d(kernel_size=<span class=\"number\">2</span>, stride=<span class=\"number\">2</span>),</span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">6</span>, <span class=\"number\">16</span>, kernel_size=<span class=\"number\">5</span>), BatchNorm(<span class=\"number\">16</span>, num_dims=<span class=\"number\">4</span>), nn.Sigmoid(),</span><br><span class=\"line\">    nn.AvgPool2d(kernel_size=<span class=\"number\">2</span>, stride=<span class=\"number\">2</span>), nn.Flatten(),</span><br><span class=\"line\">    nn.Linear(<span class=\"number\">16</span>*<span class=\"number\">4</span>*<span class=\"number\">4</span>, <span class=\"number\">120</span>), BatchNorm(<span class=\"number\">120</span>, num_dims=<span class=\"number\">2</span>), nn.Sigmoid(),</span><br><span class=\"line\">    nn.Linear(<span class=\"number\">120</span>, <span class=\"number\">84</span>), BatchNorm(<span class=\"number\">84</span>, num_dims=<span class=\"number\">2</span>), nn.Sigmoid(),</span><br><span class=\"line\">    nn.Linear(<span class=\"number\">84</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>] = <span class=\"string\">&quot;TRUE&quot;</span> </span><br><span class=\"line\">lr, num_epochs, batch_size = <span class=\"number\">1.0</span>, <span class=\"number\">10</span>, <span class=\"number\">256</span></span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class=\"line\">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure>\n\n<pre><code>loss 0.267, train acc 0.900, test acc 0.812\n4058.4 examples/sec on cpu\n</code></pre>\n<p><img src=\"/5_batchnormal_files/5_batchnormal_5_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/5_batchnormal_files/5_batchnormal_5_1.svg\"></p>\n<h2 id=\"5-5-简明实现\"><a href=\"#5-5-简明实现\" class=\"headerlink\" title=\"5.5 简明实现\"></a>5.5 简明实现</h2><ul>\n<li>通常高级API变体运行速度快得多，因为它的代码已编译为C++或CUDA，而我们的自定义代码由Python实现。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net = nn.Sequential(</span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">1</span>,<span class=\"number\">6</span>, kernel_size=<span class=\"number\">5</span>), nn.BatchNorm2d(<span class=\"number\">6</span>), nn.Sigmoid(),</span><br><span class=\"line\">    nn.AvgPool2d(kernel_size=<span class=\"number\">2</span>, stride=<span class=\"number\">2</span>),</span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">6</span>, <span class=\"number\">16</span>, kernel_size=<span class=\"number\">5</span>),nn.BatchNorm2d(<span class=\"number\">16</span>), nn.Sigmoid(),</span><br><span class=\"line\">    nn.AvgPool2d(kernel_size=<span class=\"number\">2</span>, stride=<span class=\"number\">2</span>), nn.Flatten(),</span><br><span class=\"line\">    nn.Linear(<span class=\"number\">256</span>, <span class=\"number\">120</span>), nn.BatchNorm1d(<span class=\"number\">120</span>), nn.Sigmoid(),</span><br><span class=\"line\">    nn.Linear(<span class=\"number\">120</span>, <span class=\"number\">84</span>), nn.BatchNorm1d(<span class=\"number\">84</span>), nn.Sigmoid(),</span><br><span class=\"line\">    nn.Linear(<span class=\"number\">84</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\">)</span><br><span class=\"line\">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure>\n\n<pre><code>loss 0.271, train acc 0.899, test acc 0.845\n7140.4 examples/sec on cpu\n</code></pre>\n<p><img src=\"/5_batchnormal_files/5_batchnormal_7_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/5_batchnormal_files/5_batchnormal_7_1.svg\"></p>\n<h2 id=\"5-6-争议\"><a href=\"#5-6-争议\" class=\"headerlink\" title=\"5.6 争议\"></a>5.6 争议</h2><ul>\n<li>批量规范化被认为可以使优化更加平滑。我们甚至不知道简单的神经网络（多层感知机和传统的卷积神经网络）为什么如此有效。即使在暂退法和权重衰减的情况下，它们仍然非常灵活，因此无法通过常规的学习理论泛化保证来解释它们是否能够泛化到看不见的数据。</li>\n<li>其原理：通过减少内部协变量偏移（internal covariate shift）。据推测，作者所说的内部协变量转移类似于上述的投机直觉，即变量值的分布在训练过程中会发生变化。然而，这种解释有两个问题：1、这种偏移与严格定义的协变量偏移（covariate shift）非常不同，所以这个名字用词不当；2、这种解释只提供了一种不明确的直觉，但留下了一个有待后续挖掘的问题：为什么这项技术如此有效？</li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"5-批量规范化\"><a href=\"#5-批量规范化\" class=\"headerlink\" title=\"5 批量规范化\"></a>5 批量规范化</h1><ul>\n<li>训练深层神经网络十分困难。批量规范化（batchnormalization）可持续加速深层网络的收敛速度。</li>\n</ul>\n<h2 id=\"5-1-训练深层网络\"><a href=\"#5-1-训练深层网络\" class=\"headerlink\" title=\"5.1 训练深层网络\"></a>5.1 训练深层网络</h2><ul>\n<li>训练神经网络时出现的一些挑战：<ul>\n<li>数据预处理的方式通常会对最终结果产生巨大影响。例如使用真实数据时，我们的第一步是标准化输入特征，使其平均值为0，方差为1。直观地说，这种标准化可以很好地与我们的优化器配合使用，因为它可以将参数的量级进行统一。</li>\n<li>中间层中的变量（例如，多层感知机中的仿射变换输出）可能具有更广的变化范围</li>\n<li>更深层的网络很复杂，容易过拟合。这意味着正则化变得更加重要。</li>\n</ul>\n</li>\n<li>批量规范化应用于单个可选层（也可以应用到所有层），其原理如下：在每次训练迭代中，我们首先规范化输入，即通过减去其均值并除以其标准差，其中两者均基于当前小批量处理。接下来，我们应用比例系数和比例偏移。正是由于这个基于批量统计的标准化，才有了批量规范化的名称。</li>\n<li>如果使用大小为1的小批量应用批量规范化，我们将无法学到任何东西。这是因为在减去均值之后，每个隐藏单元将为0。所以，只有使用足够大的小批量，批量规范化这种方法才是有效且稳定的。请注意，在应用批量规范化时，批量大小的选择可能比没有批量规范化时更重要。</li>\n<li>用$\\mathbf{x} \\in \\mathcal{B}$表示一个来自小批量$\\mathcal{B}$的输入，批量规范化定义为</li>\n</ul>\n<p>$$\\mathrm{BN} ( \\mathbf{x})&#x3D; \\boldsymbol{ \\gamma} \\odot \\frac{ \\mathbf{x}- \\hat{ \\boldsymbol{ \\mu}}_ \\mathcal{B}}{ \\hat{ \\boldsymbol{ \\sigma}}_ \\mathcal{B}} + \\boldsymbol{ \\beta}$$</p>\n<p>$$\\begin{aligned}<br>&amp;\\hat{\\boldsymbol{\\mu}}_ {\\mathcal{B}} &#x3D; \\frac1{| \\mathcal{B}|} \\sum_{ \\mathbf{x} \\in \\mathcal{B}} \\mathbf{x}, \\<br>&amp;\\hat{\\boldsymbol{ \\sigma}}_ { \\mathcal{B}}^ 2 &#x3D; \\frac1{| \\mathcal{B}|} \\sum_ { \\mathbf{x} \\in \\mathcal{B}}( \\mathbf{x} - \\hat{ \\boldsymbol{ \\mu}}_ \\mathcal{B})^ 2+ \\epsilon.<br>\\end{aligned}$$</p>\n<ul>\n<li>$\\gamma$和$\\beta$是可以学习的参数向量, $\\epsilon$防止除0</li>\n<li>在训练过程中，我们无法得知使用整个数据集来估计平均值和方差，所以只能根据每个小批次的平均值和方差不断训练模型。而在预测模式下，可以根据整个数据集精确计算批量规范化所需的平均值和方差。</li>\n</ul>\n<h2 id=\"5-2-批量规范化层\"><a href=\"#5-2-批量规范化层\" class=\"headerlink\" title=\"5.2 批量规范化层\"></a>5.2 批量规范化层</h2><ul>\n<li>讨论两种情况：全连接层和卷积层，他们的批量规范化实现略有不同。</li>\n</ul>\n<h3 id=\"5-2-1-全连接层\"><a href=\"#5-2-1-全连接层\" class=\"headerlink\" title=\"5.2.1 全连接层\"></a>5.2.1 全连接层</h3><ul>\n<li>通常，我们将批量规范化层置于全连接层中的仿射变换和激活函数之间。设全连接层的输入为x，权重参数为$\\mathbf{W}$，偏差参数为$ \\mathbf{b}$，激活函数为$\\phi$。设批量规范化的运算符为$\\mathrm{BN}$。那么，使用批量规范化的全连接层的输出的计算表达式为：<br>$$\\mathbf{h}&#x3D; \\phi( \\mathrm{BN}( \\mathbf{W} \\mathbf{x} + \\mathbf{b}))$$</li>\n</ul>\n<h3 id=\"5-2-2-卷积层\"><a href=\"#5-2-2-卷积层\" class=\"headerlink\" title=\"5.2.2 卷积层\"></a>5.2.2 卷积层</h3><ul>\n<li>对于卷积层，我们可以在卷积层之后和非线性激活函数之前应用批量规范化。对每个通道的输出执行批量规范化，每个通道都有自己的拉伸（scale）和偏移（shift）参数，这两个参数都是标量。</li>\n<li>假设我们的小批量包含m个样本，并且对于每个通道，卷积的输出具有高度p和宽度q。那么对于卷积层，我们在每个输出通道的m · p · q个元素上同时执行每个批量规范化。</li>\n</ul>\n<h3 id=\"5-2-3-预测过程中的批量规范化\"><a href=\"#5-2-3-预测过程中的批量规范化\" class=\"headerlink\" title=\"5.2.3 预测过程中的批量规范化\"></a>5.2.3 预测过程中的批量规范化</h3><ul>\n<li>批量规范化在训练模式和预测模式下的行为通常不同。</li>\n<li>预测时，我们不再需要样本均值中的噪声以及在微批次上估计每个小批次产生的样本方差了。</li>\n<li>其次，例如，我们可能需要使用我们的模型对逐个样本进行预测。一种常用的方法是通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们得到确定的输出。可见，和暂退法一样，批量规范化层在训练模式和预测模式下的计算结果也是不一样的。</li>\n</ul>\n<h2 id=\"5-3-从零实现\"><a href=\"#5-3-从零实现\" class=\"headerlink\" title=\"5.3 从零实现\"></a>5.3 从零实现</h2><ul>\n<li>基础设计模式：<ul>\n<li>用一个单独的函数定义其数学原理</li>\n<li>然后将次功能集成到一个自定义层中，其代码主要处理数据移动到训练设备，初始化变量、跟踪移动平均线。</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">batch_norm</span>(<span class=\"params\">X, gamma, beta, moving_mean, moving_var, eps, momentum</span>):</span><br><span class=\"line\">    <span class=\"comment\"># 通过is_grad_enabled来判断当前模型是训练模式还是预测模式</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> torch.is_grad_enabled():</span><br><span class=\"line\">        <span class=\"comment\"># 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差</span></span><br><span class=\"line\">        X_hat= (X-moving_mean) / torch.sqrt(moving_var +eps)</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">assert</span> <span class=\"built_in\">len</span>(X.shape) <span class=\"keyword\">in</span> (<span class=\"number\">2</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(X.shape) == <span class=\"number\">2</span>:</span><br><span class=\"line\">            mean = X.mean(dim=<span class=\"number\">0</span>)</span><br><span class=\"line\">            var = ((X-mean)**<span class=\"number\">2</span>).mean(dim=<span class=\"number\">0</span>)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"comment\"># 使用二维卷积层的情况，计算通道维度上（axis = 1)的均值和方差</span></span><br><span class=\"line\">            <span class=\"comment\"># 这里我们需要保持X的形状以便后面可以做广播运算</span></span><br><span class=\"line\">            mean = X.mean(dim=(<span class=\"number\">0</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>), keepdim = <span class=\"literal\">True</span>)</span><br><span class=\"line\">            var = ((X - mean)**<span class=\"number\">2</span>).mean(dim=(<span class=\"number\">0</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>), keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\">        <span class=\"comment\"># 训练模式下，用当前的均值和方差做标准化</span></span><br><span class=\"line\">        X_hat = (X-mean) / torch.sqrt(var + eps)</span><br><span class=\"line\">        <span class=\"comment\"># 更新移动平均的均值和方差</span></span><br><span class=\"line\">        moving_mean = momentum * moving_mean + (<span class=\"number\">1.0</span> - momentum) * mean</span><br><span class=\"line\">        moving_var = momentum * moving_var + (<span class=\"number\">1.0</span> - momentum) * var</span><br><span class=\"line\">    Y = gamma * X_hat + beta <span class=\"comment\">#缩放和位移</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> Y, moving_mean, moving_var</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>创建BatchNorm层。</li>\n<li>参数：gamma,beta，这两个参数将在训练过程中更新。此外，我们的层将保存均值和方差的平均移动值，以便在模型预测期间随后使用。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">BatchNorm</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"comment\"># num_features: 完全连接层的输出数量或卷积层的输出通道数</span></span><br><span class=\"line\">    <span class=\"comment\"># num_dim: 2表示完全连接，4表示卷积层</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, num_features, num_dims</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> num_dims == <span class=\"number\">2</span>:</span><br><span class=\"line\">            shape = (<span class=\"number\">1</span>, num_features)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            shape = (<span class=\"number\">1</span>, num_features, <span class=\"number\">1</span>,<span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"comment\"># 参与求梯度和迭代的拉伸和偏移参数， 分别初始化成1和0</span></span><br><span class=\"line\">        self.gamma = nn.Parameter(torch.ones(shape))</span><br><span class=\"line\">        self.beta = nn.Parameter(torch.zeros(shape))</span><br><span class=\"line\">        <span class=\"comment\"># 非模型参数的变量初始化为0和1</span></span><br><span class=\"line\">        self.moving_mean = torch.zeros(shape)</span><br><span class=\"line\">        self.moving_var = torch.ones(shape)</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 如果X不在内存上， 将moving_mean和moving_var复制到X所在显卡上</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.moving_mean.device != X.device:</span><br><span class=\"line\">            self.moving_mean = self.moving_mean.to(X.device)</span><br><span class=\"line\">            self.moving_var = self.moving_var.to(X.device)</span><br><span class=\"line\">        <span class=\"comment\"># 保存更新过的moving_mean和moving_var</span></span><br><span class=\"line\">        Y, self.moving_mean, self.moving_var = batch_norm(</span><br><span class=\"line\">            X, self.gamma, self.beta, self.moving_mean, self.moving_var, eps=<span class=\"number\">1e-5</span>, momentum=<span class=\"number\">0.9</span></span><br><span class=\"line\">        )</span><br><span class=\"line\">        <span class=\"keyword\">return</span> Y</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"5-4-使用批量规范化层的LeNet\"><a href=\"#5-4-使用批量规范化层的LeNet\" class=\"headerlink\" title=\"5.4 使用批量规范化层的LeNet\"></a>5.4 使用批量规范化层的LeNet</h2><ul>\n<li>将BatchNorm应用于LeNet模型</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net = nn.Sequential(</span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">6</span>, kernel_size=<span class=\"number\">5</span>), BatchNorm(<span class=\"number\">6</span>, num_dims=<span class=\"number\">4</span>), nn.Sigmoid(),</span><br><span class=\"line\">    nn.AvgPool2d(kernel_size=<span class=\"number\">2</span>, stride=<span class=\"number\">2</span>),</span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">6</span>, <span class=\"number\">16</span>, kernel_size=<span class=\"number\">5</span>), BatchNorm(<span class=\"number\">16</span>, num_dims=<span class=\"number\">4</span>), nn.Sigmoid(),</span><br><span class=\"line\">    nn.AvgPool2d(kernel_size=<span class=\"number\">2</span>, stride=<span class=\"number\">2</span>), nn.Flatten(),</span><br><span class=\"line\">    nn.Linear(<span class=\"number\">16</span>*<span class=\"number\">4</span>*<span class=\"number\">4</span>, <span class=\"number\">120</span>), BatchNorm(<span class=\"number\">120</span>, num_dims=<span class=\"number\">2</span>), nn.Sigmoid(),</span><br><span class=\"line\">    nn.Linear(<span class=\"number\">120</span>, <span class=\"number\">84</span>), BatchNorm(<span class=\"number\">84</span>, num_dims=<span class=\"number\">2</span>), nn.Sigmoid(),</span><br><span class=\"line\">    nn.Linear(<span class=\"number\">84</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>] = <span class=\"string\">&quot;TRUE&quot;</span> </span><br><span class=\"line\">lr, num_epochs, batch_size = <span class=\"number\">1.0</span>, <span class=\"number\">10</span>, <span class=\"number\">256</span></span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class=\"line\">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure>\n\n<pre><code>loss 0.267, train acc 0.900, test acc 0.812\n4058.4 examples/sec on cpu\n</code></pre>\n<p><img src=\"/5_batchnormal_files/5_batchnormal_5_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/5_batchnormal_files/5_batchnormal_5_1.svg\"></p>\n<h2 id=\"5-5-简明实现\"><a href=\"#5-5-简明实现\" class=\"headerlink\" title=\"5.5 简明实现\"></a>5.5 简明实现</h2><ul>\n<li>通常高级API变体运行速度快得多，因为它的代码已编译为C++或CUDA，而我们的自定义代码由Python实现。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net = nn.Sequential(</span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">1</span>,<span class=\"number\">6</span>, kernel_size=<span class=\"number\">5</span>), nn.BatchNorm2d(<span class=\"number\">6</span>), nn.Sigmoid(),</span><br><span class=\"line\">    nn.AvgPool2d(kernel_size=<span class=\"number\">2</span>, stride=<span class=\"number\">2</span>),</span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">6</span>, <span class=\"number\">16</span>, kernel_size=<span class=\"number\">5</span>),nn.BatchNorm2d(<span class=\"number\">16</span>), nn.Sigmoid(),</span><br><span class=\"line\">    nn.AvgPool2d(kernel_size=<span class=\"number\">2</span>, stride=<span class=\"number\">2</span>), nn.Flatten(),</span><br><span class=\"line\">    nn.Linear(<span class=\"number\">256</span>, <span class=\"number\">120</span>), nn.BatchNorm1d(<span class=\"number\">120</span>), nn.Sigmoid(),</span><br><span class=\"line\">    nn.Linear(<span class=\"number\">120</span>, <span class=\"number\">84</span>), nn.BatchNorm1d(<span class=\"number\">84</span>), nn.Sigmoid(),</span><br><span class=\"line\">    nn.Linear(<span class=\"number\">84</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\">)</span><br><span class=\"line\">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure>\n\n<pre><code>loss 0.271, train acc 0.899, test acc 0.845\n7140.4 examples/sec on cpu\n</code></pre>\n<p><img src=\"/5_batchnormal_files/5_batchnormal_7_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/5_batchnormal_files/5_batchnormal_7_1.svg\"></p>\n<h2 id=\"5-6-争议\"><a href=\"#5-6-争议\" class=\"headerlink\" title=\"5.6 争议\"></a>5.6 争议</h2><ul>\n<li>批量规范化被认为可以使优化更加平滑。我们甚至不知道简单的神经网络（多层感知机和传统的卷积神经网络）为什么如此有效。即使在暂退法和权重衰减的情况下，它们仍然非常灵活，因此无法通过常规的学习理论泛化保证来解释它们是否能够泛化到看不见的数据。</li>\n<li>其原理：通过减少内部协变量偏移（internal covariate shift）。据推测，作者所说的内部协变量转移类似于上述的投机直觉，即变量值的分布在训练过程中会发生变化。然而，这种解释有两个问题：1、这种偏移与严格定义的协变量偏移（covariate shift）非常不同，所以这个名字用词不当；2、这种解释只提供了一种不明确的直觉，但留下了一个有待后续挖掘的问题：为什么这项技术如此有效？</li>\n</ul>"},{"title":"6.7 稠密连接网络（DenseNet）","date":"2024-04-28T06:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 7 稠密连接网络（DenseNet）\n- ResNet极大地改变了如何参数化深层网络中函数的观点。稠密连接网络（DenseNet）在某种程度上是ResNet的逻辑扩展。\n## 7.1 从ResNet到DenseNet\n- 泰勒展开式（Taylor expansion），把这个函数分解成越来越高阶的项。在x接近0时:\n$$f(x) = f(0) + f'(0)x + \\frac{f''(0)}{2!}x^ 2 + \\frac{f'''(0)}{3!}x^ 3 + \\cdots$$\n- ResNet将函数展开为：一个简单的线性项和一个复杂的非线性项。\n$$f(x) = x + g(x)$$\n- DenseNet将$f$拓展成超过两部分: ResNet和DenseNet的关键区别在于，DenseNet输出是连接（用图中的[, ]表示）而不是如ResNet的简单相加。\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/7_densenet/1.png)\n![](img/deeplearning/code/pytorch/6_moden_conv/7_densenet/1.png)\n$$ x \\rightarrow [x, f_ 1(x), f_ 2([x, f_ 1(x)]), f_ 3([x, f_ 1(x), f_ 2([x, f_ 1(x)])]), \\ldots]$$\n- 最后，将这些展开式结合到多层感知机中，再次减少特征的数量。我们不需要添加术语，而是将它们连接起来。DenseNet这个名字由变量之间的“稠密连接”而得来，最后一层与之前的所有层紧密相连。\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/7_densenet/2.png)\n![](img/deeplearning/code/pytorch/6_moden_conv/7_densenet/2.png)\n- DenseNet主要由2部分构成：稠密块（dense block）和过渡层（transition layer）。前者定义如何连接输入和输出，而后者则控制通道数量，使其不会太复杂。\n## 7.2 稠密块\n- DenseNet使用了ResNet改良版的“批量规范化、激活和卷积”架构\n\n\n```python\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\n# 卷积块\ndef conv_block(input_channels, num_channels):\n    return nn.Sequential(\n                        nn.BatchNorm2d(input_channels), nn.ReLU(),\n                        nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1)\n                        )\n```\n\n- 一个稠密块由多个卷积块组成，每个卷积块使用相同数量的输出通道。然而，在前向传播中，我们将每个卷积块的输入和输出在通道维上连结。\n\n\n```python\nclass DenseBlock(nn.Module):\n    def __init__(self, num_convs, input_channels, num_channels):\n        super(DenseBlock, self).__init__()\n        layer = []\n        for i in range(num_convs):\n            layer.append(conv_block(\n                num_channels * i + input_channels, num_channels))\n        self.net = nn.Sequential(*layer)\n    def forward(self, X):\n        for blk in self.net:\n            Y = blk(X)\n            # 连接通道维度上每个块的输入和输出\n            X = torch.cat((X, Y), dim=1)\n        return X\n\n```\n\n- 定义一个有2个输出通道数为10的DenseBlock。使用通道数为3的输入时，我们会得到通道数为3 + 2 × 10 = 23的输出。卷积块的通道数控制了输出通道数相对于输入通道数的增长，因此也被称为增长率（growth rate）。\n\n\n```python\nblk = DenseBlock(2, 3, 10)\nX = torch.randn(4, 3, 8, 8)\nY = blk(X)\nY.shape\n```\n\n\n\n\n    torch.Size([4, 23, 8, 8])\n\n\n\n## 7.3 过渡层\n- 由于每个稠密块都会带来通道数的增加，使用过多则会过于复杂化模型。过渡层可以用来控制模型复杂度。它通过1 × 1卷积层来减小通道数，并使用步幅为2的平均汇聚层减半高和宽，从而进一步降低模型复杂度。\n\n\n```python\ndef transition_block(input_channels, num_channels):\n    return nn.Sequential(\n                        nn.BatchNorm2d(input_channels), nn.ReLU(),\n                        nn.Conv2d(input_channels, num_channels, kernel_size=1),\n                        nn.AvgPool2d(kernel_size=2, stride=2)\n                        )\n\n```\n\n- 使用通道数为10的过渡层。此时输出的通道数减为10，高和宽均减半。\n\n\n```python\nblk = transition_block(23, 10)\nprint(blk(Y).shape)\n```\n\n    torch.Size([4, 10, 4, 4])\n    \n\n## 7.4 DenseNet模型\n- DenseNet首先使用同ResNet一样的单卷积层和最大汇聚层。\n\n\n```python\nb1 = nn.Sequential(\n                    nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n                    nn.BatchNorm2d(64), nn.ReLU(),\n                    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n                    )\n\n```\n\n- 类似于ResNet使用的4个残差块，DenseNet使用的是4个稠密块。与ResNet类似，我们可以设置每个稠密块使用多少个卷积层。这里我们设成4，从而与ResNet‐18保持一致。稠密块里的卷积层通道数（即增长率）设为32，所以每个稠密块将增加128个通道。\n- 在每个模块之间，ResNet通过步幅为2的残差块减小高和宽，DenseNet则使用过渡层来减半高和宽，并减半通道数。\n\n\n```python\n# num_channels为当前的通道数\nnum_channels, growth_rate = 64, 32\nnum_convs_in_dense_blocks = [4, 4, 4, 4]\nblks = []\nfor i, num_convs in enumerate(num_convs_in_dense_blocks):\n    blks.append(DenseBlock(num_convs, num_channels, growth_rate))\n    # 上一个稠密块的输出通道数\n    num_channels += num_convs * growth_rate\n    # 在稠密块之间添加一个转换层，使通道数量减半\n    if i != len(num_convs_in_dense_blocks) - 1:\n        blks.append(transition_block(num_channels, num_channels // 2))\n        num_channels = num_channels // 2\n```\n\n- 与ResNet类似，最后接上全局汇聚层和全连接层来输出结果。\n\n\n```python\nnet = nn.Sequential(\n                    b1, *blks,\n                    nn.BatchNorm2d(num_channels), nn.ReLU(),\n                    nn.AdaptiveAvgPool2d((1, 1)),\n                    nn.Flatten(),\n                    nn.Linear(num_channels, 10)\n                    )\n```\n\n## 7.5 训练\n- 由于这里使用了比较深的网络，本节里我们将输入高和宽从224降到96来简化计算。\n\n\n```python\nimport os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\nlr, num_epochs, batch_size = 0.1, 10, 256\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)\nd2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())\n\n```\n\n    loss 0.140, train acc 0.948, test acc 0.835\n    34.7 examples/sec on cpu\n    \n\n\n    \n![svg](7_DenseNet_files/7_DenseNet_17_1.svg)\n![](img/deeplearning/code/pytorch/6_moden_conv/7_DenseNet_files/7_DenseNet_17_1.svg)\n\n","source":"_posts/deeplearning/code/pytorch/6_moden_convolution/7_DenseNet.md","raw":"---\ntitle: 6.7 稠密连接网络（DenseNet）\ndate: 2024-4-28 14:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 7 稠密连接网络（DenseNet）\n- ResNet极大地改变了如何参数化深层网络中函数的观点。稠密连接网络（DenseNet）在某种程度上是ResNet的逻辑扩展。\n## 7.1 从ResNet到DenseNet\n- 泰勒展开式（Taylor expansion），把这个函数分解成越来越高阶的项。在x接近0时:\n$$f(x) = f(0) + f'(0)x + \\frac{f''(0)}{2!}x^ 2 + \\frac{f'''(0)}{3!}x^ 3 + \\cdots$$\n- ResNet将函数展开为：一个简单的线性项和一个复杂的非线性项。\n$$f(x) = x + g(x)$$\n- DenseNet将$f$拓展成超过两部分: ResNet和DenseNet的关键区别在于，DenseNet输出是连接（用图中的[, ]表示）而不是如ResNet的简单相加。\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/7_densenet/1.png)\n![](img/deeplearning/code/pytorch/6_moden_conv/7_densenet/1.png)\n$$ x \\rightarrow [x, f_ 1(x), f_ 2([x, f_ 1(x)]), f_ 3([x, f_ 1(x), f_ 2([x, f_ 1(x)])]), \\ldots]$$\n- 最后，将这些展开式结合到多层感知机中，再次减少特征的数量。我们不需要添加术语，而是将它们连接起来。DenseNet这个名字由变量之间的“稠密连接”而得来，最后一层与之前的所有层紧密相连。\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/7_densenet/2.png)\n![](img/deeplearning/code/pytorch/6_moden_conv/7_densenet/2.png)\n- DenseNet主要由2部分构成：稠密块（dense block）和过渡层（transition layer）。前者定义如何连接输入和输出，而后者则控制通道数量，使其不会太复杂。\n## 7.2 稠密块\n- DenseNet使用了ResNet改良版的“批量规范化、激活和卷积”架构\n\n\n```python\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\n# 卷积块\ndef conv_block(input_channels, num_channels):\n    return nn.Sequential(\n                        nn.BatchNorm2d(input_channels), nn.ReLU(),\n                        nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1)\n                        )\n```\n\n- 一个稠密块由多个卷积块组成，每个卷积块使用相同数量的输出通道。然而，在前向传播中，我们将每个卷积块的输入和输出在通道维上连结。\n\n\n```python\nclass DenseBlock(nn.Module):\n    def __init__(self, num_convs, input_channels, num_channels):\n        super(DenseBlock, self).__init__()\n        layer = []\n        for i in range(num_convs):\n            layer.append(conv_block(\n                num_channels * i + input_channels, num_channels))\n        self.net = nn.Sequential(*layer)\n    def forward(self, X):\n        for blk in self.net:\n            Y = blk(X)\n            # 连接通道维度上每个块的输入和输出\n            X = torch.cat((X, Y), dim=1)\n        return X\n\n```\n\n- 定义一个有2个输出通道数为10的DenseBlock。使用通道数为3的输入时，我们会得到通道数为3 + 2 × 10 = 23的输出。卷积块的通道数控制了输出通道数相对于输入通道数的增长，因此也被称为增长率（growth rate）。\n\n\n```python\nblk = DenseBlock(2, 3, 10)\nX = torch.randn(4, 3, 8, 8)\nY = blk(X)\nY.shape\n```\n\n\n\n\n    torch.Size([4, 23, 8, 8])\n\n\n\n## 7.3 过渡层\n- 由于每个稠密块都会带来通道数的增加，使用过多则会过于复杂化模型。过渡层可以用来控制模型复杂度。它通过1 × 1卷积层来减小通道数，并使用步幅为2的平均汇聚层减半高和宽，从而进一步降低模型复杂度。\n\n\n```python\ndef transition_block(input_channels, num_channels):\n    return nn.Sequential(\n                        nn.BatchNorm2d(input_channels), nn.ReLU(),\n                        nn.Conv2d(input_channels, num_channels, kernel_size=1),\n                        nn.AvgPool2d(kernel_size=2, stride=2)\n                        )\n\n```\n\n- 使用通道数为10的过渡层。此时输出的通道数减为10，高和宽均减半。\n\n\n```python\nblk = transition_block(23, 10)\nprint(blk(Y).shape)\n```\n\n    torch.Size([4, 10, 4, 4])\n    \n\n## 7.4 DenseNet模型\n- DenseNet首先使用同ResNet一样的单卷积层和最大汇聚层。\n\n\n```python\nb1 = nn.Sequential(\n                    nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n                    nn.BatchNorm2d(64), nn.ReLU(),\n                    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n                    )\n\n```\n\n- 类似于ResNet使用的4个残差块，DenseNet使用的是4个稠密块。与ResNet类似，我们可以设置每个稠密块使用多少个卷积层。这里我们设成4，从而与ResNet‐18保持一致。稠密块里的卷积层通道数（即增长率）设为32，所以每个稠密块将增加128个通道。\n- 在每个模块之间，ResNet通过步幅为2的残差块减小高和宽，DenseNet则使用过渡层来减半高和宽，并减半通道数。\n\n\n```python\n# num_channels为当前的通道数\nnum_channels, growth_rate = 64, 32\nnum_convs_in_dense_blocks = [4, 4, 4, 4]\nblks = []\nfor i, num_convs in enumerate(num_convs_in_dense_blocks):\n    blks.append(DenseBlock(num_convs, num_channels, growth_rate))\n    # 上一个稠密块的输出通道数\n    num_channels += num_convs * growth_rate\n    # 在稠密块之间添加一个转换层，使通道数量减半\n    if i != len(num_convs_in_dense_blocks) - 1:\n        blks.append(transition_block(num_channels, num_channels // 2))\n        num_channels = num_channels // 2\n```\n\n- 与ResNet类似，最后接上全局汇聚层和全连接层来输出结果。\n\n\n```python\nnet = nn.Sequential(\n                    b1, *blks,\n                    nn.BatchNorm2d(num_channels), nn.ReLU(),\n                    nn.AdaptiveAvgPool2d((1, 1)),\n                    nn.Flatten(),\n                    nn.Linear(num_channels, 10)\n                    )\n```\n\n## 7.5 训练\n- 由于这里使用了比较深的网络，本节里我们将输入高和宽从224降到96来简化计算。\n\n\n```python\nimport os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\nlr, num_epochs, batch_size = 0.1, 10, 256\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)\nd2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())\n\n```\n\n    loss 0.140, train acc 0.948, test acc 0.835\n    34.7 examples/sec on cpu\n    \n\n\n    \n![svg](7_DenseNet_files/7_DenseNet_17_1.svg)\n![](img/deeplearning/code/pytorch/6_moden_conv/7_DenseNet_files/7_DenseNet_17_1.svg)\n\n","slug":"deeplearning/code/pytorch/6_moden_convolution/7_DenseNet","published":1,"updated":"2024-05-05T14:09:01.508Z","comments":1,"layout":"post","photos":[],"_id":"clvuli6dn004czsvwerumem2c","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"7-稠密连接网络（DenseNet）\"><a href=\"#7-稠密连接网络（DenseNet）\" class=\"headerlink\" title=\"7 稠密连接网络（DenseNet）\"></a>7 稠密连接网络（DenseNet）</h1><ul>\n<li>ResNet极大地改变了如何参数化深层网络中函数的观点。稠密连接网络（DenseNet）在某种程度上是ResNet的逻辑扩展。</li>\n</ul>\n<h2 id=\"7-1-从ResNet到DenseNet\"><a href=\"#7-1-从ResNet到DenseNet\" class=\"headerlink\" title=\"7.1 从ResNet到DenseNet\"></a>7.1 从ResNet到DenseNet</h2><ul>\n<li>泰勒展开式（Taylor expansion），把这个函数分解成越来越高阶的项。在x接近0时:<br>$$f(x) &#x3D; f(0) + f’(0)x + \\frac{f’’(0)}{2!}x^ 2 + \\frac{f’’’(0)}{3!}x^ 3 + \\cdots$$</li>\n<li>ResNet将函数展开为：一个简单的线性项和一个复杂的非线性项。<br>$$f(x) &#x3D; x + g(x)$$</li>\n<li>DenseNet将$f$拓展成超过两部分: ResNet和DenseNet的关键区别在于，DenseNet输出是连接（用图中的[, ]表示）而不是如ResNet的简单相加。<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/7_densenet/1.png\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/7_densenet/1.png\"><br>$$ x \\rightarrow [x, f_ 1(x), f_ 2([x, f_ 1(x)]), f_ 3([x, f_ 1(x), f_ 2([x, f_ 1(x)])]), \\ldots]$$</li>\n<li>最后，将这些展开式结合到多层感知机中，再次减少特征的数量。我们不需要添加术语，而是将它们连接起来。DenseNet这个名字由变量之间的“稠密连接”而得来，最后一层与之前的所有层紧密相连。<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/7_densenet/2.png\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/7_densenet/2.png\"></li>\n<li>DenseNet主要由2部分构成：稠密块（dense block）和过渡层（transition layer）。前者定义如何连接输入和输出，而后者则控制通道数量，使其不会太复杂。</li>\n</ul>\n<h2 id=\"7-2-稠密块\"><a href=\"#7-2-稠密块\" class=\"headerlink\" title=\"7.2 稠密块\"></a>7.2 稠密块</h2><ul>\n<li>DenseNet使用了ResNet改良版的“批量规范化、激活和卷积”架构</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 卷积块</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">conv_block</span>(<span class=\"params\">input_channels, num_channels</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nn.Sequential(</span><br><span class=\"line\">                        nn.BatchNorm2d(input_channels), nn.ReLU(),</span><br><span class=\"line\">                        nn.Conv2d(input_channels, num_channels, kernel_size=<span class=\"number\">3</span>, padding=<span class=\"number\">1</span>)</span><br><span class=\"line\">                        )</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>一个稠密块由多个卷积块组成，每个卷积块使用相同数量的输出通道。然而，在前向传播中，我们将每个卷积块的输入和输出在通道维上连结。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">DenseBlock</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, num_convs, input_channels, num_channels</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(DenseBlock, self).__init__()</span><br><span class=\"line\">        layer = []</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_convs):</span><br><span class=\"line\">            layer.append(conv_block(</span><br><span class=\"line\">                num_channels * i + input_channels, num_channels))</span><br><span class=\"line\">        self.net = nn.Sequential(*layer)</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> blk <span class=\"keyword\">in</span> self.net:</span><br><span class=\"line\">            Y = blk(X)</span><br><span class=\"line\">            <span class=\"comment\"># 连接通道维度上每个块的输入和输出</span></span><br><span class=\"line\">            X = torch.cat((X, Y), dim=<span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> X</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>定义一个有2个输出通道数为10的DenseBlock。使用通道数为3的输入时，我们会得到通道数为3 + 2 × 10 &#x3D; 23的输出。卷积块的通道数控制了输出通道数相对于输入通道数的增长，因此也被称为增长率（growth rate）。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">blk = DenseBlock(<span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\">X = torch.randn(<span class=\"number\">4</span>, <span class=\"number\">3</span>, <span class=\"number\">8</span>, <span class=\"number\">8</span>)</span><br><span class=\"line\">Y = blk(X)</span><br><span class=\"line\">Y.shape</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>torch.Size([4, 23, 8, 8])\n</code></pre>\n<h2 id=\"7-3-过渡层\"><a href=\"#7-3-过渡层\" class=\"headerlink\" title=\"7.3 过渡层\"></a>7.3 过渡层</h2><ul>\n<li>由于每个稠密块都会带来通道数的增加，使用过多则会过于复杂化模型。过渡层可以用来控制模型复杂度。它通过1 × 1卷积层来减小通道数，并使用步幅为2的平均汇聚层减半高和宽，从而进一步降低模型复杂度。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">transition_block</span>(<span class=\"params\">input_channels, num_channels</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nn.Sequential(</span><br><span class=\"line\">                        nn.BatchNorm2d(input_channels), nn.ReLU(),</span><br><span class=\"line\">                        nn.Conv2d(input_channels, num_channels, kernel_size=<span class=\"number\">1</span>),</span><br><span class=\"line\">                        nn.AvgPool2d(kernel_size=<span class=\"number\">2</span>, stride=<span class=\"number\">2</span>)</span><br><span class=\"line\">                        )</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>使用通道数为10的过渡层。此时输出的通道数减为10，高和宽均减半。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">blk = transition_block(<span class=\"number\">23</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(blk(Y).shape)</span><br></pre></td></tr></table></figure>\n\n<pre><code>torch.Size([4, 10, 4, 4])\n</code></pre>\n<h2 id=\"7-4-DenseNet模型\"><a href=\"#7-4-DenseNet模型\" class=\"headerlink\" title=\"7.4 DenseNet模型\"></a>7.4 DenseNet模型</h2><ul>\n<li>DenseNet首先使用同ResNet一样的单卷积层和最大汇聚层。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">b1 = nn.Sequential(</span><br><span class=\"line\">                    nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">64</span>, kernel_size=<span class=\"number\">7</span>, stride=<span class=\"number\">2</span>, padding=<span class=\"number\">3</span>),</span><br><span class=\"line\">                    nn.BatchNorm2d(<span class=\"number\">64</span>), nn.ReLU(),</span><br><span class=\"line\">                    nn.MaxPool2d(kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>, padding=<span class=\"number\">1</span>)</span><br><span class=\"line\">                    )</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>类似于ResNet使用的4个残差块，DenseNet使用的是4个稠密块。与ResNet类似，我们可以设置每个稠密块使用多少个卷积层。这里我们设成4，从而与ResNet‐18保持一致。稠密块里的卷积层通道数（即增长率）设为32，所以每个稠密块将增加128个通道。</li>\n<li>在每个模块之间，ResNet通过步幅为2的残差块减小高和宽，DenseNet则使用过渡层来减半高和宽，并减半通道数。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># num_channels为当前的通道数</span></span><br><span class=\"line\">num_channels, growth_rate = <span class=\"number\">64</span>, <span class=\"number\">32</span></span><br><span class=\"line\">num_convs_in_dense_blocks = [<span class=\"number\">4</span>, <span class=\"number\">4</span>, <span class=\"number\">4</span>, <span class=\"number\">4</span>]</span><br><span class=\"line\">blks = []</span><br><span class=\"line\"><span class=\"keyword\">for</span> i, num_convs <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(num_convs_in_dense_blocks):</span><br><span class=\"line\">    blks.append(DenseBlock(num_convs, num_channels, growth_rate))</span><br><span class=\"line\">    <span class=\"comment\"># 上一个稠密块的输出通道数</span></span><br><span class=\"line\">    num_channels += num_convs * growth_rate</span><br><span class=\"line\">    <span class=\"comment\"># 在稠密块之间添加一个转换层，使通道数量减半</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> i != <span class=\"built_in\">len</span>(num_convs_in_dense_blocks) - <span class=\"number\">1</span>:</span><br><span class=\"line\">        blks.append(transition_block(num_channels, num_channels // <span class=\"number\">2</span>))</span><br><span class=\"line\">        num_channels = num_channels // <span class=\"number\">2</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>与ResNet类似，最后接上全局汇聚层和全连接层来输出结果。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net = nn.Sequential(</span><br><span class=\"line\">                    b1, *blks,</span><br><span class=\"line\">                    nn.BatchNorm2d(num_channels), nn.ReLU(),</span><br><span class=\"line\">                    nn.AdaptiveAvgPool2d((<span class=\"number\">1</span>, <span class=\"number\">1</span>)),</span><br><span class=\"line\">                    nn.Flatten(),</span><br><span class=\"line\">                    nn.Linear(num_channels, <span class=\"number\">10</span>)</span><br><span class=\"line\">                    )</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"7-5-训练\"><a href=\"#7-5-训练\" class=\"headerlink\" title=\"7.5 训练\"></a>7.5 训练</h2><ul>\n<li>由于这里使用了比较深的网络，本节里我们将输入高和宽从224降到96来简化计算。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>] = <span class=\"string\">&quot;TRUE&quot;</span></span><br><span class=\"line\">lr, num_epochs, batch_size = <span class=\"number\">0.1</span>, <span class=\"number\">10</span>, <span class=\"number\">256</span></span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class=\"number\">96</span>)</span><br><span class=\"line\">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<pre><code>loss 0.140, train acc 0.948, test acc 0.835\n34.7 examples/sec on cpu\n</code></pre>\n<p><img src=\"/7_DenseNet_files/7_DenseNet_17_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/7_DenseNet_files/7_DenseNet_17_1.svg\"></p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"7-稠密连接网络（DenseNet）\"><a href=\"#7-稠密连接网络（DenseNet）\" class=\"headerlink\" title=\"7 稠密连接网络（DenseNet）\"></a>7 稠密连接网络（DenseNet）</h1><ul>\n<li>ResNet极大地改变了如何参数化深层网络中函数的观点。稠密连接网络（DenseNet）在某种程度上是ResNet的逻辑扩展。</li>\n</ul>\n<h2 id=\"7-1-从ResNet到DenseNet\"><a href=\"#7-1-从ResNet到DenseNet\" class=\"headerlink\" title=\"7.1 从ResNet到DenseNet\"></a>7.1 从ResNet到DenseNet</h2><ul>\n<li>泰勒展开式（Taylor expansion），把这个函数分解成越来越高阶的项。在x接近0时:<br>$$f(x) &#x3D; f(0) + f’(0)x + \\frac{f’’(0)}{2!}x^ 2 + \\frac{f’’’(0)}{3!}x^ 3 + \\cdots$$</li>\n<li>ResNet将函数展开为：一个简单的线性项和一个复杂的非线性项。<br>$$f(x) &#x3D; x + g(x)$$</li>\n<li>DenseNet将$f$拓展成超过两部分: ResNet和DenseNet的关键区别在于，DenseNet输出是连接（用图中的[, ]表示）而不是如ResNet的简单相加。<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/7_densenet/1.png\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/7_densenet/1.png\"><br>$$ x \\rightarrow [x, f_ 1(x), f_ 2([x, f_ 1(x)]), f_ 3([x, f_ 1(x), f_ 2([x, f_ 1(x)])]), \\ldots]$$</li>\n<li>最后，将这些展开式结合到多层感知机中，再次减少特征的数量。我们不需要添加术语，而是将它们连接起来。DenseNet这个名字由变量之间的“稠密连接”而得来，最后一层与之前的所有层紧密相连。<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/7_densenet/2.png\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/7_densenet/2.png\"></li>\n<li>DenseNet主要由2部分构成：稠密块（dense block）和过渡层（transition layer）。前者定义如何连接输入和输出，而后者则控制通道数量，使其不会太复杂。</li>\n</ul>\n<h2 id=\"7-2-稠密块\"><a href=\"#7-2-稠密块\" class=\"headerlink\" title=\"7.2 稠密块\"></a>7.2 稠密块</h2><ul>\n<li>DenseNet使用了ResNet改良版的“批量规范化、激活和卷积”架构</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 卷积块</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">conv_block</span>(<span class=\"params\">input_channels, num_channels</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nn.Sequential(</span><br><span class=\"line\">                        nn.BatchNorm2d(input_channels), nn.ReLU(),</span><br><span class=\"line\">                        nn.Conv2d(input_channels, num_channels, kernel_size=<span class=\"number\">3</span>, padding=<span class=\"number\">1</span>)</span><br><span class=\"line\">                        )</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>一个稠密块由多个卷积块组成，每个卷积块使用相同数量的输出通道。然而，在前向传播中，我们将每个卷积块的输入和输出在通道维上连结。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">DenseBlock</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, num_convs, input_channels, num_channels</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(DenseBlock, self).__init__()</span><br><span class=\"line\">        layer = []</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_convs):</span><br><span class=\"line\">            layer.append(conv_block(</span><br><span class=\"line\">                num_channels * i + input_channels, num_channels))</span><br><span class=\"line\">        self.net = nn.Sequential(*layer)</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> blk <span class=\"keyword\">in</span> self.net:</span><br><span class=\"line\">            Y = blk(X)</span><br><span class=\"line\">            <span class=\"comment\"># 连接通道维度上每个块的输入和输出</span></span><br><span class=\"line\">            X = torch.cat((X, Y), dim=<span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> X</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>定义一个有2个输出通道数为10的DenseBlock。使用通道数为3的输入时，我们会得到通道数为3 + 2 × 10 &#x3D; 23的输出。卷积块的通道数控制了输出通道数相对于输入通道数的增长，因此也被称为增长率（growth rate）。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">blk = DenseBlock(<span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\">X = torch.randn(<span class=\"number\">4</span>, <span class=\"number\">3</span>, <span class=\"number\">8</span>, <span class=\"number\">8</span>)</span><br><span class=\"line\">Y = blk(X)</span><br><span class=\"line\">Y.shape</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>torch.Size([4, 23, 8, 8])\n</code></pre>\n<h2 id=\"7-3-过渡层\"><a href=\"#7-3-过渡层\" class=\"headerlink\" title=\"7.3 过渡层\"></a>7.3 过渡层</h2><ul>\n<li>由于每个稠密块都会带来通道数的增加，使用过多则会过于复杂化模型。过渡层可以用来控制模型复杂度。它通过1 × 1卷积层来减小通道数，并使用步幅为2的平均汇聚层减半高和宽，从而进一步降低模型复杂度。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">transition_block</span>(<span class=\"params\">input_channels, num_channels</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nn.Sequential(</span><br><span class=\"line\">                        nn.BatchNorm2d(input_channels), nn.ReLU(),</span><br><span class=\"line\">                        nn.Conv2d(input_channels, num_channels, kernel_size=<span class=\"number\">1</span>),</span><br><span class=\"line\">                        nn.AvgPool2d(kernel_size=<span class=\"number\">2</span>, stride=<span class=\"number\">2</span>)</span><br><span class=\"line\">                        )</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>使用通道数为10的过渡层。此时输出的通道数减为10，高和宽均减半。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">blk = transition_block(<span class=\"number\">23</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(blk(Y).shape)</span><br></pre></td></tr></table></figure>\n\n<pre><code>torch.Size([4, 10, 4, 4])\n</code></pre>\n<h2 id=\"7-4-DenseNet模型\"><a href=\"#7-4-DenseNet模型\" class=\"headerlink\" title=\"7.4 DenseNet模型\"></a>7.4 DenseNet模型</h2><ul>\n<li>DenseNet首先使用同ResNet一样的单卷积层和最大汇聚层。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">b1 = nn.Sequential(</span><br><span class=\"line\">                    nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">64</span>, kernel_size=<span class=\"number\">7</span>, stride=<span class=\"number\">2</span>, padding=<span class=\"number\">3</span>),</span><br><span class=\"line\">                    nn.BatchNorm2d(<span class=\"number\">64</span>), nn.ReLU(),</span><br><span class=\"line\">                    nn.MaxPool2d(kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>, padding=<span class=\"number\">1</span>)</span><br><span class=\"line\">                    )</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>类似于ResNet使用的4个残差块，DenseNet使用的是4个稠密块。与ResNet类似，我们可以设置每个稠密块使用多少个卷积层。这里我们设成4，从而与ResNet‐18保持一致。稠密块里的卷积层通道数（即增长率）设为32，所以每个稠密块将增加128个通道。</li>\n<li>在每个模块之间，ResNet通过步幅为2的残差块减小高和宽，DenseNet则使用过渡层来减半高和宽，并减半通道数。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># num_channels为当前的通道数</span></span><br><span class=\"line\">num_channels, growth_rate = <span class=\"number\">64</span>, <span class=\"number\">32</span></span><br><span class=\"line\">num_convs_in_dense_blocks = [<span class=\"number\">4</span>, <span class=\"number\">4</span>, <span class=\"number\">4</span>, <span class=\"number\">4</span>]</span><br><span class=\"line\">blks = []</span><br><span class=\"line\"><span class=\"keyword\">for</span> i, num_convs <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(num_convs_in_dense_blocks):</span><br><span class=\"line\">    blks.append(DenseBlock(num_convs, num_channels, growth_rate))</span><br><span class=\"line\">    <span class=\"comment\"># 上一个稠密块的输出通道数</span></span><br><span class=\"line\">    num_channels += num_convs * growth_rate</span><br><span class=\"line\">    <span class=\"comment\"># 在稠密块之间添加一个转换层，使通道数量减半</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> i != <span class=\"built_in\">len</span>(num_convs_in_dense_blocks) - <span class=\"number\">1</span>:</span><br><span class=\"line\">        blks.append(transition_block(num_channels, num_channels // <span class=\"number\">2</span>))</span><br><span class=\"line\">        num_channels = num_channels // <span class=\"number\">2</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>与ResNet类似，最后接上全局汇聚层和全连接层来输出结果。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net = nn.Sequential(</span><br><span class=\"line\">                    b1, *blks,</span><br><span class=\"line\">                    nn.BatchNorm2d(num_channels), nn.ReLU(),</span><br><span class=\"line\">                    nn.AdaptiveAvgPool2d((<span class=\"number\">1</span>, <span class=\"number\">1</span>)),</span><br><span class=\"line\">                    nn.Flatten(),</span><br><span class=\"line\">                    nn.Linear(num_channels, <span class=\"number\">10</span>)</span><br><span class=\"line\">                    )</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"7-5-训练\"><a href=\"#7-5-训练\" class=\"headerlink\" title=\"7.5 训练\"></a>7.5 训练</h2><ul>\n<li>由于这里使用了比较深的网络，本节里我们将输入高和宽从224降到96来简化计算。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>] = <span class=\"string\">&quot;TRUE&quot;</span></span><br><span class=\"line\">lr, num_epochs, batch_size = <span class=\"number\">0.1</span>, <span class=\"number\">10</span>, <span class=\"number\">256</span></span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class=\"number\">96</span>)</span><br><span class=\"line\">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<pre><code>loss 0.140, train acc 0.948, test acc 0.835\n34.7 examples/sec on cpu\n</code></pre>\n<p><img src=\"/7_DenseNet_files/7_DenseNet_17_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/7_DenseNet_files/7_DenseNet_17_1.svg\"></p>"},{"title":"6.4 含并行连结的网络（GoogLeNet）","date":"2024-04-22T05:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 4 含并行连结的网络（GoogLeNet）\n- GoogLeNet吸收了NiN中串联网络的思想，解决了什么样大小的卷积核最合适的问题。有时使用不同大小的卷积核组合是有利的。\n## 4.1 Inception块\n- 在GoogLeNet中，基本的卷积块被称为Inception块（Inception block）。这很可能得名于电影《盗梦空间》（Inception），因为电影中的一句话“我们需要走得更深”（“We need to go deeper”）。\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/4_googlenet/1.png)\n![](img/deeplearning/code/pytorch/6_moden_conv/4_googlenet/1.png)\n- Inception块由四条并行路径组成。前三条路径使用窗口大小为1 × 1、3 × 3和5 × 5的卷积层，从不同空间大小中提取信息。\n\n\n```python\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom d2l import torch as d2l\n\nclass Inception(nn.Module):\n    # c1--c4是每条路径的输出通道数\n    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs) -> None:\n        super(Inception, self).__init__(**kwargs)\n        # 线路1， 单1 x 1卷积层\n        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\n        # 线路2， 1 x 1卷积层后接3 x 3卷积层\n        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n        # 线路3， 1 x 1卷积层后接5 x 5卷积层\n        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)\n        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n        # 线路4， 3 x 3最大池化层后接1 x 1卷积层\n        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)\n    def forward(self, X):\n        p1 = F.relu(self.p1_1(X))\n        p2 = F.relu(self.p2_2(F.relu(self.p2_1(X))))\n        p3 = F.relu(self.p3_2(F.relu(self.p3_1(X))))\n        p4 = F.relu(self.p4_2(self.p4_1(X)))\n        # 在通道维度上连接输出\n        return torch.cat((p1, p2, p3, p4), dim=1)\n```\n\n- GoogLeNet可以用各种滤波器尺寸探索图像，这意味着不同大小的滤波器可以有效地识别不同范围的图像细节。\n## 4.2 GoogLeNet模型\n- GoogLeNet使用9个Inception块和全局平均汇聚层的堆叠来生成其估计值。Inception块之间的最大汇聚层可降低维度。\n- 第一个模块类似于AlexNet和LeNet，Inception块的组合从VGG继承，全局平均汇聚层避免了在最后使用全连接层。\n- GoogLeNet架构：\n\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/4_googlenet/2.png)\n![](img/deeplearning/code/pytorch/6_moden_conv/4_googlenet/2.png)\n\n\n```python\n# 1. 第一个模块使用64个通道、7 × 7卷积层。\nb1 = nn.Sequential(\n    nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n)\n# 2. 第二个模块使用两个卷积层：64个通道1 × 1卷积层、通道数量增加三倍的3 × 3卷积层\nb2 = nn.Sequential(\n    nn.Conv2d(64,64,kernel_size=1),\n    nn.ReLU(),\n    nn.Conv2d(64, 192, kernel_size=3, padding=1),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n)\n# 3. 第三个模块串联两个完整的Inception块。\n# Inception_1的输出通道为64+128+32+32=256，第2和第3路径将输入通道数量分别减少到96/192 = 1/2和16/192 = 1/12，然后连接第二个卷积层。\n# Inception_2的输出通道为128+192+96+64=480，第二条和第三条路径首先将输入通道的数量分别减少到128/256 = 1/2和32/256 = 1/8\nb3 = nn.Sequential(\n    Inception(192, 64, (96, 128), (16, 32), 32),\n    Inception(256, 128, (128, 192), (32, 96), 64),\n    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n)\n# 4. 第四个模块串联五个Inception块\nb4 = nn.Sequential(\n    Inception(480, 192, (96, 208), (16, 48), 64),\n    Inception(512, 160, (112, 224), (24, 64), 64),\n    Inception(512, 128, (128, 256), (24, 64), 64),\n    Inception(512, 112, (144, 288), (32, 64), 64),\n    Inception(528, 256, (160, 320), (32, 128), 128),\n    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n)\n# 5. 第五个模块串联两个Inception块\nb5 = nn.Sequential(\n    Inception(832, 256, (160, 320), (32, 128), 128),\n    Inception(832, 384, (192, 384), (48, 128), 128),\n    nn.AdaptiveAvgPool2d((1,1)),\n    nn.Flatten()\n)\n\nnet = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024, 10))\n```\n\n- 查看形状\n\n\n```python\nX = torch.rand(size=(1,1,96,96))\nfor layer in net:\n    X = layer(X)\n    print(layer.__class__.__name__,'output shape:\\t', X.shape)\n```\n\n    Sequential output shape:\t torch.Size([1, 64, 24, 24])\n    Sequential output shape:\t torch.Size([1, 192, 12, 12])\n    Sequential output shape:\t torch.Size([1, 480, 6, 6])\n    Sequential output shape:\t torch.Size([1, 832, 3, 3])\n    Sequential output shape:\t torch.Size([1, 1024])\n    Linear output shape:\t torch.Size([1, 10])\n    \n\n## 4.3 训练模型\n- 训练之前，我们将图片转换为96x96大小，然后进行训练。\n\n\n```python\nimport os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\nlr, num_epochs, batch_size = 0.1, 10, 128\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)\nd2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())\n```\n\n    loss 0.248, train acc 0.906, test acc 0.894\n    21.5 examples/sec on cpu\n    \n\n\n    \n![svg](4_googleNet_files/4_googleNet_7_1.svg)\n![](img/deeplearning/code/pytorch/6_moden_conv/4_googleNet_files/4_googleNet_7_1.svg)\n\n","source":"_posts/deeplearning/code/pytorch/6_moden_convolution/4_googleNet.md","raw":"---\ntitle: 6.4 含并行连结的网络（GoogLeNet）\ndate: 2024-4-22 13:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 4 含并行连结的网络（GoogLeNet）\n- GoogLeNet吸收了NiN中串联网络的思想，解决了什么样大小的卷积核最合适的问题。有时使用不同大小的卷积核组合是有利的。\n## 4.1 Inception块\n- 在GoogLeNet中，基本的卷积块被称为Inception块（Inception block）。这很可能得名于电影《盗梦空间》（Inception），因为电影中的一句话“我们需要走得更深”（“We need to go deeper”）。\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/4_googlenet/1.png)\n![](img/deeplearning/code/pytorch/6_moden_conv/4_googlenet/1.png)\n- Inception块由四条并行路径组成。前三条路径使用窗口大小为1 × 1、3 × 3和5 × 5的卷积层，从不同空间大小中提取信息。\n\n\n```python\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom d2l import torch as d2l\n\nclass Inception(nn.Module):\n    # c1--c4是每条路径的输出通道数\n    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs) -> None:\n        super(Inception, self).__init__(**kwargs)\n        # 线路1， 单1 x 1卷积层\n        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\n        # 线路2， 1 x 1卷积层后接3 x 3卷积层\n        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n        # 线路3， 1 x 1卷积层后接5 x 5卷积层\n        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)\n        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n        # 线路4， 3 x 3最大池化层后接1 x 1卷积层\n        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)\n    def forward(self, X):\n        p1 = F.relu(self.p1_1(X))\n        p2 = F.relu(self.p2_2(F.relu(self.p2_1(X))))\n        p3 = F.relu(self.p3_2(F.relu(self.p3_1(X))))\n        p4 = F.relu(self.p4_2(self.p4_1(X)))\n        # 在通道维度上连接输出\n        return torch.cat((p1, p2, p3, p4), dim=1)\n```\n\n- GoogLeNet可以用各种滤波器尺寸探索图像，这意味着不同大小的滤波器可以有效地识别不同范围的图像细节。\n## 4.2 GoogLeNet模型\n- GoogLeNet使用9个Inception块和全局平均汇聚层的堆叠来生成其估计值。Inception块之间的最大汇聚层可降低维度。\n- 第一个模块类似于AlexNet和LeNet，Inception块的组合从VGG继承，全局平均汇聚层避免了在最后使用全连接层。\n- GoogLeNet架构：\n\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/4_googlenet/2.png)\n![](img/deeplearning/code/pytorch/6_moden_conv/4_googlenet/2.png)\n\n\n```python\n# 1. 第一个模块使用64个通道、7 × 7卷积层。\nb1 = nn.Sequential(\n    nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n)\n# 2. 第二个模块使用两个卷积层：64个通道1 × 1卷积层、通道数量增加三倍的3 × 3卷积层\nb2 = nn.Sequential(\n    nn.Conv2d(64,64,kernel_size=1),\n    nn.ReLU(),\n    nn.Conv2d(64, 192, kernel_size=3, padding=1),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n)\n# 3. 第三个模块串联两个完整的Inception块。\n# Inception_1的输出通道为64+128+32+32=256，第2和第3路径将输入通道数量分别减少到96/192 = 1/2和16/192 = 1/12，然后连接第二个卷积层。\n# Inception_2的输出通道为128+192+96+64=480，第二条和第三条路径首先将输入通道的数量分别减少到128/256 = 1/2和32/256 = 1/8\nb3 = nn.Sequential(\n    Inception(192, 64, (96, 128), (16, 32), 32),\n    Inception(256, 128, (128, 192), (32, 96), 64),\n    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n)\n# 4. 第四个模块串联五个Inception块\nb4 = nn.Sequential(\n    Inception(480, 192, (96, 208), (16, 48), 64),\n    Inception(512, 160, (112, 224), (24, 64), 64),\n    Inception(512, 128, (128, 256), (24, 64), 64),\n    Inception(512, 112, (144, 288), (32, 64), 64),\n    Inception(528, 256, (160, 320), (32, 128), 128),\n    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n)\n# 5. 第五个模块串联两个Inception块\nb5 = nn.Sequential(\n    Inception(832, 256, (160, 320), (32, 128), 128),\n    Inception(832, 384, (192, 384), (48, 128), 128),\n    nn.AdaptiveAvgPool2d((1,1)),\n    nn.Flatten()\n)\n\nnet = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024, 10))\n```\n\n- 查看形状\n\n\n```python\nX = torch.rand(size=(1,1,96,96))\nfor layer in net:\n    X = layer(X)\n    print(layer.__class__.__name__,'output shape:\\t', X.shape)\n```\n\n    Sequential output shape:\t torch.Size([1, 64, 24, 24])\n    Sequential output shape:\t torch.Size([1, 192, 12, 12])\n    Sequential output shape:\t torch.Size([1, 480, 6, 6])\n    Sequential output shape:\t torch.Size([1, 832, 3, 3])\n    Sequential output shape:\t torch.Size([1, 1024])\n    Linear output shape:\t torch.Size([1, 10])\n    \n\n## 4.3 训练模型\n- 训练之前，我们将图片转换为96x96大小，然后进行训练。\n\n\n```python\nimport os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\nlr, num_epochs, batch_size = 0.1, 10, 128\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)\nd2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())\n```\n\n    loss 0.248, train acc 0.906, test acc 0.894\n    21.5 examples/sec on cpu\n    \n\n\n    \n![svg](4_googleNet_files/4_googleNet_7_1.svg)\n![](img/deeplearning/code/pytorch/6_moden_conv/4_googleNet_files/4_googleNet_7_1.svg)\n\n","slug":"deeplearning/code/pytorch/6_moden_convolution/4_googleNet","published":1,"updated":"2024-05-05T14:05:47.805Z","comments":1,"layout":"post","photos":[],"_id":"clvuli6dp004fzsvw3ra045jb","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"4-含并行连结的网络（GoogLeNet）\"><a href=\"#4-含并行连结的网络（GoogLeNet）\" class=\"headerlink\" title=\"4 含并行连结的网络（GoogLeNet）\"></a>4 含并行连结的网络（GoogLeNet）</h1><ul>\n<li>GoogLeNet吸收了NiN中串联网络的思想，解决了什么样大小的卷积核最合适的问题。有时使用不同大小的卷积核组合是有利的。</li>\n</ul>\n<h2 id=\"4-1-Inception块\"><a href=\"#4-1-Inception块\" class=\"headerlink\" title=\"4.1 Inception块\"></a>4.1 Inception块</h2><ul>\n<li>在GoogLeNet中，基本的卷积块被称为Inception块（Inception block）。这很可能得名于电影《盗梦空间》（Inception），因为电影中的一句话“我们需要走得更深”（“We need to go deeper”）。<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/4_googlenet/1.png\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/4_googlenet/1.png\"></li>\n<li>Inception块由四条并行路径组成。前三条路径使用窗口大小为1 × 1、3 × 3和5 × 5的卷积层，从不同空间大小中提取信息。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.nn <span class=\"keyword\">import</span> functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Inception</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"comment\"># c1--c4是每条路径的输出通道数</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, in_channels, c1, c2, c3, c4, **kwargs</span>) -&gt; <span class=\"literal\">None</span>:</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Inception, self).__init__(**kwargs)</span><br><span class=\"line\">        <span class=\"comment\"># 线路1， 单1 x 1卷积层</span></span><br><span class=\"line\">        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=<span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"comment\"># 线路2， 1 x 1卷积层后接3 x 3卷积层</span></span><br><span class=\"line\">        self.p2_1 = nn.Conv2d(in_channels, c2[<span class=\"number\">0</span>], kernel_size=<span class=\"number\">1</span>)</span><br><span class=\"line\">        self.p2_2 = nn.Conv2d(c2[<span class=\"number\">0</span>], c2[<span class=\"number\">1</span>], kernel_size=<span class=\"number\">3</span>, padding=<span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"comment\"># 线路3， 1 x 1卷积层后接5 x 5卷积层</span></span><br><span class=\"line\">        self.p3_1 = nn.Conv2d(in_channels, c3[<span class=\"number\">0</span>], kernel_size=<span class=\"number\">1</span>)</span><br><span class=\"line\">        self.p3_2 = nn.Conv2d(c3[<span class=\"number\">0</span>], c3[<span class=\"number\">1</span>], kernel_size=<span class=\"number\">5</span>, padding=<span class=\"number\">2</span>)</span><br><span class=\"line\">        <span class=\"comment\"># 线路4， 3 x 3最大池化层后接1 x 1卷积层</span></span><br><span class=\"line\">        self.p4_1 = nn.MaxPool2d(kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">1</span>, padding=<span class=\"number\">1</span>)</span><br><span class=\"line\">        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        p1 = F.relu(self.p1_1(X))</span><br><span class=\"line\">        p2 = F.relu(self.p2_2(F.relu(self.p2_1(X))))</span><br><span class=\"line\">        p3 = F.relu(self.p3_2(F.relu(self.p3_1(X))))</span><br><span class=\"line\">        p4 = F.relu(self.p4_2(self.p4_1(X)))</span><br><span class=\"line\">        <span class=\"comment\"># 在通道维度上连接输出</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> torch.cat((p1, p2, p3, p4), dim=<span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>GoogLeNet可以用各种滤波器尺寸探索图像，这意味着不同大小的滤波器可以有效地识别不同范围的图像细节。</li>\n</ul>\n<h2 id=\"4-2-GoogLeNet模型\"><a href=\"#4-2-GoogLeNet模型\" class=\"headerlink\" title=\"4.2 GoogLeNet模型\"></a>4.2 GoogLeNet模型</h2><ul>\n<li>GoogLeNet使用9个Inception块和全局平均汇聚层的堆叠来生成其估计值。Inception块之间的最大汇聚层可降低维度。</li>\n<li>第一个模块类似于AlexNet和LeNet，Inception块的组合从VGG继承，全局平均汇聚层避免了在最后使用全连接层。</li>\n<li>GoogLeNet架构：</li>\n</ul>\n<p><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/4_googlenet/2.png\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/4_googlenet/2.png\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 1. 第一个模块使用64个通道、7 × 7卷积层。</span></span><br><span class=\"line\">b1 = nn.Sequential(</span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">64</span>, kernel_size=<span class=\"number\">7</span>, stride=<span class=\"number\">2</span>, padding=<span class=\"number\">3</span>),</span><br><span class=\"line\">    nn.ReLU(),</span><br><span class=\"line\">    nn.MaxPool2d(kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>, padding=<span class=\"number\">1</span>)</span><br><span class=\"line\">)</span><br><span class=\"line\"><span class=\"comment\"># 2. 第二个模块使用两个卷积层：64个通道1 × 1卷积层、通道数量增加三倍的3 × 3卷积层</span></span><br><span class=\"line\">b2 = nn.Sequential(</span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">64</span>,<span class=\"number\">64</span>,kernel_size=<span class=\"number\">1</span>),</span><br><span class=\"line\">    nn.ReLU(),</span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">64</span>, <span class=\"number\">192</span>, kernel_size=<span class=\"number\">3</span>, padding=<span class=\"number\">1</span>),</span><br><span class=\"line\">    nn.ReLU(),</span><br><span class=\"line\">    nn.MaxPool2d(kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>, padding=<span class=\"number\">1</span>)</span><br><span class=\"line\">)</span><br><span class=\"line\"><span class=\"comment\"># 3. 第三个模块串联两个完整的Inception块。</span></span><br><span class=\"line\"><span class=\"comment\"># Inception_1的输出通道为64+128+32+32=256，第2和第3路径将输入通道数量分别减少到96/192 = 1/2和16/192 = 1/12，然后连接第二个卷积层。</span></span><br><span class=\"line\"><span class=\"comment\"># Inception_2的输出通道为128+192+96+64=480，第二条和第三条路径首先将输入通道的数量分别减少到128/256 = 1/2和32/256 = 1/8</span></span><br><span class=\"line\">b3 = nn.Sequential(</span><br><span class=\"line\">    Inception(<span class=\"number\">192</span>, <span class=\"number\">64</span>, (<span class=\"number\">96</span>, <span class=\"number\">128</span>), (<span class=\"number\">16</span>, <span class=\"number\">32</span>), <span class=\"number\">32</span>),</span><br><span class=\"line\">    Inception(<span class=\"number\">256</span>, <span class=\"number\">128</span>, (<span class=\"number\">128</span>, <span class=\"number\">192</span>), (<span class=\"number\">32</span>, <span class=\"number\">96</span>), <span class=\"number\">64</span>),</span><br><span class=\"line\">    nn.MaxPool2d(kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>, padding=<span class=\"number\">1</span>)</span><br><span class=\"line\">)</span><br><span class=\"line\"><span class=\"comment\"># 4. 第四个模块串联五个Inception块</span></span><br><span class=\"line\">b4 = nn.Sequential(</span><br><span class=\"line\">    Inception(<span class=\"number\">480</span>, <span class=\"number\">192</span>, (<span class=\"number\">96</span>, <span class=\"number\">208</span>), (<span class=\"number\">16</span>, <span class=\"number\">48</span>), <span class=\"number\">64</span>),</span><br><span class=\"line\">    Inception(<span class=\"number\">512</span>, <span class=\"number\">160</span>, (<span class=\"number\">112</span>, <span class=\"number\">224</span>), (<span class=\"number\">24</span>, <span class=\"number\">64</span>), <span class=\"number\">64</span>),</span><br><span class=\"line\">    Inception(<span class=\"number\">512</span>, <span class=\"number\">128</span>, (<span class=\"number\">128</span>, <span class=\"number\">256</span>), (<span class=\"number\">24</span>, <span class=\"number\">64</span>), <span class=\"number\">64</span>),</span><br><span class=\"line\">    Inception(<span class=\"number\">512</span>, <span class=\"number\">112</span>, (<span class=\"number\">144</span>, <span class=\"number\">288</span>), (<span class=\"number\">32</span>, <span class=\"number\">64</span>), <span class=\"number\">64</span>),</span><br><span class=\"line\">    Inception(<span class=\"number\">528</span>, <span class=\"number\">256</span>, (<span class=\"number\">160</span>, <span class=\"number\">320</span>), (<span class=\"number\">32</span>, <span class=\"number\">128</span>), <span class=\"number\">128</span>),</span><br><span class=\"line\">    nn.MaxPool2d(kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>, padding=<span class=\"number\">1</span>)</span><br><span class=\"line\">)</span><br><span class=\"line\"><span class=\"comment\"># 5. 第五个模块串联两个Inception块</span></span><br><span class=\"line\">b5 = nn.Sequential(</span><br><span class=\"line\">    Inception(<span class=\"number\">832</span>, <span class=\"number\">256</span>, (<span class=\"number\">160</span>, <span class=\"number\">320</span>), (<span class=\"number\">32</span>, <span class=\"number\">128</span>), <span class=\"number\">128</span>),</span><br><span class=\"line\">    Inception(<span class=\"number\">832</span>, <span class=\"number\">384</span>, (<span class=\"number\">192</span>, <span class=\"number\">384</span>), (<span class=\"number\">48</span>, <span class=\"number\">128</span>), <span class=\"number\">128</span>),</span><br><span class=\"line\">    nn.AdaptiveAvgPool2d((<span class=\"number\">1</span>,<span class=\"number\">1</span>)),</span><br><span class=\"line\">    nn.Flatten()</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(<span class=\"number\">1024</span>, <span class=\"number\">10</span>))</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>查看形状</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = torch.rand(size=(<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">96</span>,<span class=\"number\">96</span>))</span><br><span class=\"line\"><span class=\"keyword\">for</span> layer <span class=\"keyword\">in</span> net:</span><br><span class=\"line\">    X = layer(X)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(layer.__class__.__name__,<span class=\"string\">&#x27;output shape:\\t&#x27;</span>, X.shape)</span><br></pre></td></tr></table></figure>\n\n<pre><code>Sequential output shape:\t torch.Size([1, 64, 24, 24])\nSequential output shape:\t torch.Size([1, 192, 12, 12])\nSequential output shape:\t torch.Size([1, 480, 6, 6])\nSequential output shape:\t torch.Size([1, 832, 3, 3])\nSequential output shape:\t torch.Size([1, 1024])\nLinear output shape:\t torch.Size([1, 10])\n</code></pre>\n<h2 id=\"4-3-训练模型\"><a href=\"#4-3-训练模型\" class=\"headerlink\" title=\"4.3 训练模型\"></a>4.3 训练模型</h2><ul>\n<li>训练之前，我们将图片转换为96x96大小，然后进行训练。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>] = <span class=\"string\">&quot;TRUE&quot;</span></span><br><span class=\"line\">lr, num_epochs, batch_size = <span class=\"number\">0.1</span>, <span class=\"number\">10</span>, <span class=\"number\">128</span></span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class=\"number\">96</span>)</span><br><span class=\"line\">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure>\n\n<pre><code>loss 0.248, train acc 0.906, test acc 0.894\n21.5 examples/sec on cpu\n</code></pre>\n<p><img src=\"/4_googleNet_files/4_googleNet_7_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/4_googleNet_files/4_googleNet_7_1.svg\"></p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"4-含并行连结的网络（GoogLeNet）\"><a href=\"#4-含并行连结的网络（GoogLeNet）\" class=\"headerlink\" title=\"4 含并行连结的网络（GoogLeNet）\"></a>4 含并行连结的网络（GoogLeNet）</h1><ul>\n<li>GoogLeNet吸收了NiN中串联网络的思想，解决了什么样大小的卷积核最合适的问题。有时使用不同大小的卷积核组合是有利的。</li>\n</ul>\n<h2 id=\"4-1-Inception块\"><a href=\"#4-1-Inception块\" class=\"headerlink\" title=\"4.1 Inception块\"></a>4.1 Inception块</h2><ul>\n<li>在GoogLeNet中，基本的卷积块被称为Inception块（Inception block）。这很可能得名于电影《盗梦空间》（Inception），因为电影中的一句话“我们需要走得更深”（“We need to go deeper”）。<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/4_googlenet/1.png\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/4_googlenet/1.png\"></li>\n<li>Inception块由四条并行路径组成。前三条路径使用窗口大小为1 × 1、3 × 3和5 × 5的卷积层，从不同空间大小中提取信息。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.nn <span class=\"keyword\">import</span> functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Inception</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"comment\"># c1--c4是每条路径的输出通道数</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, in_channels, c1, c2, c3, c4, **kwargs</span>) -&gt; <span class=\"literal\">None</span>:</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Inception, self).__init__(**kwargs)</span><br><span class=\"line\">        <span class=\"comment\"># 线路1， 单1 x 1卷积层</span></span><br><span class=\"line\">        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=<span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"comment\"># 线路2， 1 x 1卷积层后接3 x 3卷积层</span></span><br><span class=\"line\">        self.p2_1 = nn.Conv2d(in_channels, c2[<span class=\"number\">0</span>], kernel_size=<span class=\"number\">1</span>)</span><br><span class=\"line\">        self.p2_2 = nn.Conv2d(c2[<span class=\"number\">0</span>], c2[<span class=\"number\">1</span>], kernel_size=<span class=\"number\">3</span>, padding=<span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"comment\"># 线路3， 1 x 1卷积层后接5 x 5卷积层</span></span><br><span class=\"line\">        self.p3_1 = nn.Conv2d(in_channels, c3[<span class=\"number\">0</span>], kernel_size=<span class=\"number\">1</span>)</span><br><span class=\"line\">        self.p3_2 = nn.Conv2d(c3[<span class=\"number\">0</span>], c3[<span class=\"number\">1</span>], kernel_size=<span class=\"number\">5</span>, padding=<span class=\"number\">2</span>)</span><br><span class=\"line\">        <span class=\"comment\"># 线路4， 3 x 3最大池化层后接1 x 1卷积层</span></span><br><span class=\"line\">        self.p4_1 = nn.MaxPool2d(kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">1</span>, padding=<span class=\"number\">1</span>)</span><br><span class=\"line\">        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        p1 = F.relu(self.p1_1(X))</span><br><span class=\"line\">        p2 = F.relu(self.p2_2(F.relu(self.p2_1(X))))</span><br><span class=\"line\">        p3 = F.relu(self.p3_2(F.relu(self.p3_1(X))))</span><br><span class=\"line\">        p4 = F.relu(self.p4_2(self.p4_1(X)))</span><br><span class=\"line\">        <span class=\"comment\"># 在通道维度上连接输出</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> torch.cat((p1, p2, p3, p4), dim=<span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>GoogLeNet可以用各种滤波器尺寸探索图像，这意味着不同大小的滤波器可以有效地识别不同范围的图像细节。</li>\n</ul>\n<h2 id=\"4-2-GoogLeNet模型\"><a href=\"#4-2-GoogLeNet模型\" class=\"headerlink\" title=\"4.2 GoogLeNet模型\"></a>4.2 GoogLeNet模型</h2><ul>\n<li>GoogLeNet使用9个Inception块和全局平均汇聚层的堆叠来生成其估计值。Inception块之间的最大汇聚层可降低维度。</li>\n<li>第一个模块类似于AlexNet和LeNet，Inception块的组合从VGG继承，全局平均汇聚层避免了在最后使用全连接层。</li>\n<li>GoogLeNet架构：</li>\n</ul>\n<p><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/4_googlenet/2.png\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/4_googlenet/2.png\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 1. 第一个模块使用64个通道、7 × 7卷积层。</span></span><br><span class=\"line\">b1 = nn.Sequential(</span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">64</span>, kernel_size=<span class=\"number\">7</span>, stride=<span class=\"number\">2</span>, padding=<span class=\"number\">3</span>),</span><br><span class=\"line\">    nn.ReLU(),</span><br><span class=\"line\">    nn.MaxPool2d(kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>, padding=<span class=\"number\">1</span>)</span><br><span class=\"line\">)</span><br><span class=\"line\"><span class=\"comment\"># 2. 第二个模块使用两个卷积层：64个通道1 × 1卷积层、通道数量增加三倍的3 × 3卷积层</span></span><br><span class=\"line\">b2 = nn.Sequential(</span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">64</span>,<span class=\"number\">64</span>,kernel_size=<span class=\"number\">1</span>),</span><br><span class=\"line\">    nn.ReLU(),</span><br><span class=\"line\">    nn.Conv2d(<span class=\"number\">64</span>, <span class=\"number\">192</span>, kernel_size=<span class=\"number\">3</span>, padding=<span class=\"number\">1</span>),</span><br><span class=\"line\">    nn.ReLU(),</span><br><span class=\"line\">    nn.MaxPool2d(kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>, padding=<span class=\"number\">1</span>)</span><br><span class=\"line\">)</span><br><span class=\"line\"><span class=\"comment\"># 3. 第三个模块串联两个完整的Inception块。</span></span><br><span class=\"line\"><span class=\"comment\"># Inception_1的输出通道为64+128+32+32=256，第2和第3路径将输入通道数量分别减少到96/192 = 1/2和16/192 = 1/12，然后连接第二个卷积层。</span></span><br><span class=\"line\"><span class=\"comment\"># Inception_2的输出通道为128+192+96+64=480，第二条和第三条路径首先将输入通道的数量分别减少到128/256 = 1/2和32/256 = 1/8</span></span><br><span class=\"line\">b3 = nn.Sequential(</span><br><span class=\"line\">    Inception(<span class=\"number\">192</span>, <span class=\"number\">64</span>, (<span class=\"number\">96</span>, <span class=\"number\">128</span>), (<span class=\"number\">16</span>, <span class=\"number\">32</span>), <span class=\"number\">32</span>),</span><br><span class=\"line\">    Inception(<span class=\"number\">256</span>, <span class=\"number\">128</span>, (<span class=\"number\">128</span>, <span class=\"number\">192</span>), (<span class=\"number\">32</span>, <span class=\"number\">96</span>), <span class=\"number\">64</span>),</span><br><span class=\"line\">    nn.MaxPool2d(kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>, padding=<span class=\"number\">1</span>)</span><br><span class=\"line\">)</span><br><span class=\"line\"><span class=\"comment\"># 4. 第四个模块串联五个Inception块</span></span><br><span class=\"line\">b4 = nn.Sequential(</span><br><span class=\"line\">    Inception(<span class=\"number\">480</span>, <span class=\"number\">192</span>, (<span class=\"number\">96</span>, <span class=\"number\">208</span>), (<span class=\"number\">16</span>, <span class=\"number\">48</span>), <span class=\"number\">64</span>),</span><br><span class=\"line\">    Inception(<span class=\"number\">512</span>, <span class=\"number\">160</span>, (<span class=\"number\">112</span>, <span class=\"number\">224</span>), (<span class=\"number\">24</span>, <span class=\"number\">64</span>), <span class=\"number\">64</span>),</span><br><span class=\"line\">    Inception(<span class=\"number\">512</span>, <span class=\"number\">128</span>, (<span class=\"number\">128</span>, <span class=\"number\">256</span>), (<span class=\"number\">24</span>, <span class=\"number\">64</span>), <span class=\"number\">64</span>),</span><br><span class=\"line\">    Inception(<span class=\"number\">512</span>, <span class=\"number\">112</span>, (<span class=\"number\">144</span>, <span class=\"number\">288</span>), (<span class=\"number\">32</span>, <span class=\"number\">64</span>), <span class=\"number\">64</span>),</span><br><span class=\"line\">    Inception(<span class=\"number\">528</span>, <span class=\"number\">256</span>, (<span class=\"number\">160</span>, <span class=\"number\">320</span>), (<span class=\"number\">32</span>, <span class=\"number\">128</span>), <span class=\"number\">128</span>),</span><br><span class=\"line\">    nn.MaxPool2d(kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>, padding=<span class=\"number\">1</span>)</span><br><span class=\"line\">)</span><br><span class=\"line\"><span class=\"comment\"># 5. 第五个模块串联两个Inception块</span></span><br><span class=\"line\">b5 = nn.Sequential(</span><br><span class=\"line\">    Inception(<span class=\"number\">832</span>, <span class=\"number\">256</span>, (<span class=\"number\">160</span>, <span class=\"number\">320</span>), (<span class=\"number\">32</span>, <span class=\"number\">128</span>), <span class=\"number\">128</span>),</span><br><span class=\"line\">    Inception(<span class=\"number\">832</span>, <span class=\"number\">384</span>, (<span class=\"number\">192</span>, <span class=\"number\">384</span>), (<span class=\"number\">48</span>, <span class=\"number\">128</span>), <span class=\"number\">128</span>),</span><br><span class=\"line\">    nn.AdaptiveAvgPool2d((<span class=\"number\">1</span>,<span class=\"number\">1</span>)),</span><br><span class=\"line\">    nn.Flatten()</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(<span class=\"number\">1024</span>, <span class=\"number\">10</span>))</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>查看形状</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = torch.rand(size=(<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">96</span>,<span class=\"number\">96</span>))</span><br><span class=\"line\"><span class=\"keyword\">for</span> layer <span class=\"keyword\">in</span> net:</span><br><span class=\"line\">    X = layer(X)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(layer.__class__.__name__,<span class=\"string\">&#x27;output shape:\\t&#x27;</span>, X.shape)</span><br></pre></td></tr></table></figure>\n\n<pre><code>Sequential output shape:\t torch.Size([1, 64, 24, 24])\nSequential output shape:\t torch.Size([1, 192, 12, 12])\nSequential output shape:\t torch.Size([1, 480, 6, 6])\nSequential output shape:\t torch.Size([1, 832, 3, 3])\nSequential output shape:\t torch.Size([1, 1024])\nLinear output shape:\t torch.Size([1, 10])\n</code></pre>\n<h2 id=\"4-3-训练模型\"><a href=\"#4-3-训练模型\" class=\"headerlink\" title=\"4.3 训练模型\"></a>4.3 训练模型</h2><ul>\n<li>训练之前，我们将图片转换为96x96大小，然后进行训练。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>] = <span class=\"string\">&quot;TRUE&quot;</span></span><br><span class=\"line\">lr, num_epochs, batch_size = <span class=\"number\">0.1</span>, <span class=\"number\">10</span>, <span class=\"number\">128</span></span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class=\"number\">96</span>)</span><br><span class=\"line\">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure>\n\n<pre><code>loss 0.248, train acc 0.906, test acc 0.894\n21.5 examples/sec on cpu\n</code></pre>\n<p><img src=\"/4_googleNet_files/4_googleNet_7_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/4_googleNet_files/4_googleNet_7_1.svg\"></p>"},{"title":"6.6 残差网络（ResNet）","date":"2024-04-27T06:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 6 残差网络（ResNet）\n- 随着我们设计越来越深的网络，深刻理解“新添加的层如何提升神经网络的性能”变得至关重要。更重要的是设计网络的能力，在这种网络中，添加层会使网络更具表现力，为了取得质的突破，我们需要一些数学基础知识。\n## 6.1 函数类\n- 假设神经网络：$\\mathcal{F}$，它包括学习率和其他超参数设置。对于所有$f \\in \\mathcal{F}$，存在一些参数集（如权重和偏置），这些参数可以通过在合适的数据集上进行训练而获得。\n- 假设$f^ *$是我们的目标函数，我们希望找到一个函数$f_ {\\mathcal{F}}^ * \\in \\mathcal{F}$，使$f_ {\\mathcal{F}}^ * \\approx f^ *$。\n- 给定一个具有$\\mathbf{X}$特征和$\\mathbf{y}$标签的数据集，我们可以通过解决以下优化问题来找到他：\n$$f_ {\\mathcal{F}}^ * = \\arg \\min _{f \\in \\mathcal{F}} L(\\mathbf{X}, \\mathbf{y}, f)$$\n- 如何找到一个$f_ {\\mathcal{F}^ `}^ *$，使比$f_ {\\mathcal{F}}^ *$更接近$f^ *$？只有当较复杂的函数类包含较小的函数类时，我们才能确保提高它们的性能。左边，虽然F3比F1更接近f∗，但F6却离的更远了。右侧的嵌套函数（nested function）类F1 ⊆ . . . ⊆ F6，我们可以避免上述问题。\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/6_resnet/1.png)\n![](img/deeplearning/code/pytorch/6_moden_conv/6_resnet/1.png)\n- 对于深度神经网络，如果我们能将新添加的层训练成恒等映射（identity function）f(x) = x，新模型和原模型将同样有效。同时，由于新模型可能得出更优的解来拟合训练数据集，因此添加层似乎更容易降低训练误差。\n- 残差网络的核心思想是：每个附加层都应该更容易地包含原始函数作为其元素之一。\n## 6.2 残差块\n- 输入为x，希望学出的理想映射为f(x)。左图虚线框中的部分需要直接拟合出该映射f(x)，而右图虚线框中的部分则需要拟合出残差映射f(x) − x。我们只需将 图右图虚线框内上方的加权运算（如仿射）的权重和偏置参数设成0，那么f(x)即为恒等映射。当理想映射f(x)极接近于恒等映射时，残差映射也易于捕捉恒等映射的细微波动。\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/6_resnet/2.png)\n![](img/deeplearning/code/pytorch/6_moden_conv/6_resnet/2.png)\n- resnet沿用vgg完整的3 x 3卷积层设计。残差块里首先有2个有相同输出通道数的3 × 3卷积层。每个卷积层后接一个批量规范化层和ReLU激活函数。然后我们通过跨层数据通路，跳过这2个卷积运算，将输入直接加在最后的ReLU激活函数前。这样的设计要求2个卷积层的输出与输入形状一样，从而使它们可以相加。如果想改变通道数，就需要引入一个额外的1 × 1卷积层来将输入变换成需要的形状后再做相加运算。\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/6_resnet/3.png)\n![](img/deeplearning/code/pytorch/6_moden_conv/6_resnet/3.png)\n\n\n```python\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom d2l import torch as d2l\n\nclass Residual(nn.Module): #@save\n    def __init__(self, input_channels, num_channels, use_1x1conv=False, strides=1):\n        super().__init__()\n        self.conv1 = nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1, stride=strides)\n        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n        if use_1x1conv: #通道数有改变\n            self.conv3 = nn.Conv2d(input_channels, num_channels, kernel_size=1, stride=strides)\n        else:\n            self.conv3 = None\n        self.bn1 = nn.BatchNorm2d(num_channels)\n        self.bn2 = nn.BatchNorm2d(num_channels)\n    def forward(self, X):\n        Y = F.relu(self.bn1(self.conv1(X)))\n        Y = self.bn2(self.conv2(Y))\n        if self.conv3:\n            X = self.conv3(X)\n        Y += X\n        return F.relu(Y)\n    \n# 输入和输出形状一致的情况\nblk = Residual(3,3)\nX = torch.rand(4,3,6,6)\nY = blk(X)\nprint(Y.shape) #(4,3,6,6)\n\n# 增加输出通道，减少输出的高和宽\nblk = Residual(3,6, use_1x1conv=True, strides=2)\nprint(blk(X).shape) #(4,6,3,3)\n\n```\n\n    torch.Size([4, 3, 6, 6])\n    torch.Size([4, 6, 3, 3])\n    \n\n## 6.3 ResNet模型\n- 前两层跟GoogLeNet中的一样：在输出通道数为64、步幅为2的7 × 7卷积层后，接步幅为2的3 × 3的最大汇聚层。不同之处在于ResNet每个卷积层后增加了批量规范化层。\n\n\n```python\nb1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n                    nn.BatchNorm2d(64), nn.ReLU(),\n                    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n                    )\n```\n\n- GoogLeNet在后面接了4个由Inception块组成的模块。ResNet则使用4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。第一个模块的通道数同输入通道数一致。由于之前已经使用了步幅为2的最大汇聚层，所以无须减小高和宽。之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。\n\n\n\n```python\ndef resnet_block(input_channels, num_channels, num_residuals, first_block=False):\n    blk = []\n    for i in range(num_residuals):\n        if i == 0 and not first_block:\n            blk.append(Residual(input_channels, num_channels, use_1x1conv=True, strides=2))\n        else:\n            blk.append(Residual(num_channels, num_channels))\n    return blk\n```\n\n- 接着在ResNet加入所有残差块，这里每个模块使用2个残差块。\n\n\n```python\nb2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))\nb3 = nn.Sequential(*resnet_block(64, 128, 2))\nb4 = nn.Sequential(*resnet_block(128, 256, 2))\nb5 = nn.Sequential(*resnet_block(256, 512, 2))\n```\n\n- 最后，与GoogLeNet一样，在ResNet中加入全局平均汇聚层，以及全连接层输出。\n- 每个模块有4个卷积层（不包括恒等映射的1 × 1卷积层）。加上第一个7 × 7卷积层和最后一个全连接层，共有18层。因此，这种模型通常被称为ResNet‐18。\n- 虽然ResNet的主体架构跟GoogLeNet类似，但ResNet架构更简单，修改也更方便。这些因素都导致了ResNet迅速被广泛使用。\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/6_resnet/4.png)\n![](img/deeplearning/code/pytorch/6_moden_conv/6_resnet/4.png)\n\n\n```python\nnet = nn.Sequential(b1, b2, b3, b4, b5,\n                    nn.AdaptiveAvgPool2d((1,1)),\n                    nn.Flatten(), nn.Linear(512, 10)\n                    )\n\n```\n\n- 查看形状。分辨率降低，通道数量增加，直到全局平均汇聚层聚集所有特征。\n\n\n```python\nX = torch.rand(size=(1, 1, 224, 224))\nfor layer in net:\n    X = layer(X)\n    print(layer.__class__.__name__,'output shape:\\t', X.shape)\n\n```\n\n    Sequential output shape:\t torch.Size([1, 64, 56, 56])\n    Sequential output shape:\t torch.Size([1, 64, 56, 56])\n    Sequential output shape:\t torch.Size([1, 128, 28, 28])\n    Sequential output shape:\t torch.Size([1, 256, 14, 14])\n    Sequential output shape:\t torch.Size([1, 512, 7, 7])\n    AdaptiveAvgPool2d output shape:\t torch.Size([1, 512, 1, 1])\n    Flatten output shape:\t torch.Size([1, 512])\n    Linear output shape:\t torch.Size([1, 10])\n    \n\n## 6.4 训练\n- 在Fashion‐MNIST数据集上训练ResNet。\n\n\n```python\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n\nlr, num_epochs, batch_size = 0.05, 10, 256\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)\nd2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())\n```\n\n    loss 0.011, train acc 0.997, test acc 0.919\n    52.9 examples/sec on cpu\n    \n\n\n    \n![svg](6_resnet_files/6_resnet_13_1.svg)\n![](img/deeplearning/code/pytorch/6_moden_conv/6_resnet_files/6_resnet_13_1.svg)\n\n","source":"_posts/deeplearning/code/pytorch/6_moden_convolution/6_resnet.md","raw":"---\ntitle: 6.6 残差网络（ResNet）\ndate: 2024-4-27 14:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 6 残差网络（ResNet）\n- 随着我们设计越来越深的网络，深刻理解“新添加的层如何提升神经网络的性能”变得至关重要。更重要的是设计网络的能力，在这种网络中，添加层会使网络更具表现力，为了取得质的突破，我们需要一些数学基础知识。\n## 6.1 函数类\n- 假设神经网络：$\\mathcal{F}$，它包括学习率和其他超参数设置。对于所有$f \\in \\mathcal{F}$，存在一些参数集（如权重和偏置），这些参数可以通过在合适的数据集上进行训练而获得。\n- 假设$f^ *$是我们的目标函数，我们希望找到一个函数$f_ {\\mathcal{F}}^ * \\in \\mathcal{F}$，使$f_ {\\mathcal{F}}^ * \\approx f^ *$。\n- 给定一个具有$\\mathbf{X}$特征和$\\mathbf{y}$标签的数据集，我们可以通过解决以下优化问题来找到他：\n$$f_ {\\mathcal{F}}^ * = \\arg \\min _{f \\in \\mathcal{F}} L(\\mathbf{X}, \\mathbf{y}, f)$$\n- 如何找到一个$f_ {\\mathcal{F}^ `}^ *$，使比$f_ {\\mathcal{F}}^ *$更接近$f^ *$？只有当较复杂的函数类包含较小的函数类时，我们才能确保提高它们的性能。左边，虽然F3比F1更接近f∗，但F6却离的更远了。右侧的嵌套函数（nested function）类F1 ⊆ . . . ⊆ F6，我们可以避免上述问题。\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/6_resnet/1.png)\n![](img/deeplearning/code/pytorch/6_moden_conv/6_resnet/1.png)\n- 对于深度神经网络，如果我们能将新添加的层训练成恒等映射（identity function）f(x) = x，新模型和原模型将同样有效。同时，由于新模型可能得出更优的解来拟合训练数据集，因此添加层似乎更容易降低训练误差。\n- 残差网络的核心思想是：每个附加层都应该更容易地包含原始函数作为其元素之一。\n## 6.2 残差块\n- 输入为x，希望学出的理想映射为f(x)。左图虚线框中的部分需要直接拟合出该映射f(x)，而右图虚线框中的部分则需要拟合出残差映射f(x) − x。我们只需将 图右图虚线框内上方的加权运算（如仿射）的权重和偏置参数设成0，那么f(x)即为恒等映射。当理想映射f(x)极接近于恒等映射时，残差映射也易于捕捉恒等映射的细微波动。\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/6_resnet/2.png)\n![](img/deeplearning/code/pytorch/6_moden_conv/6_resnet/2.png)\n- resnet沿用vgg完整的3 x 3卷积层设计。残差块里首先有2个有相同输出通道数的3 × 3卷积层。每个卷积层后接一个批量规范化层和ReLU激活函数。然后我们通过跨层数据通路，跳过这2个卷积运算，将输入直接加在最后的ReLU激活函数前。这样的设计要求2个卷积层的输出与输入形状一样，从而使它们可以相加。如果想改变通道数，就需要引入一个额外的1 × 1卷积层来将输入变换成需要的形状后再做相加运算。\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/6_resnet/3.png)\n![](img/deeplearning/code/pytorch/6_moden_conv/6_resnet/3.png)\n\n\n```python\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom d2l import torch as d2l\n\nclass Residual(nn.Module): #@save\n    def __init__(self, input_channels, num_channels, use_1x1conv=False, strides=1):\n        super().__init__()\n        self.conv1 = nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1, stride=strides)\n        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n        if use_1x1conv: #通道数有改变\n            self.conv3 = nn.Conv2d(input_channels, num_channels, kernel_size=1, stride=strides)\n        else:\n            self.conv3 = None\n        self.bn1 = nn.BatchNorm2d(num_channels)\n        self.bn2 = nn.BatchNorm2d(num_channels)\n    def forward(self, X):\n        Y = F.relu(self.bn1(self.conv1(X)))\n        Y = self.bn2(self.conv2(Y))\n        if self.conv3:\n            X = self.conv3(X)\n        Y += X\n        return F.relu(Y)\n    \n# 输入和输出形状一致的情况\nblk = Residual(3,3)\nX = torch.rand(4,3,6,6)\nY = blk(X)\nprint(Y.shape) #(4,3,6,6)\n\n# 增加输出通道，减少输出的高和宽\nblk = Residual(3,6, use_1x1conv=True, strides=2)\nprint(blk(X).shape) #(4,6,3,3)\n\n```\n\n    torch.Size([4, 3, 6, 6])\n    torch.Size([4, 6, 3, 3])\n    \n\n## 6.3 ResNet模型\n- 前两层跟GoogLeNet中的一样：在输出通道数为64、步幅为2的7 × 7卷积层后，接步幅为2的3 × 3的最大汇聚层。不同之处在于ResNet每个卷积层后增加了批量规范化层。\n\n\n```python\nb1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n                    nn.BatchNorm2d(64), nn.ReLU(),\n                    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n                    )\n```\n\n- GoogLeNet在后面接了4个由Inception块组成的模块。ResNet则使用4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。第一个模块的通道数同输入通道数一致。由于之前已经使用了步幅为2的最大汇聚层，所以无须减小高和宽。之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。\n\n\n\n```python\ndef resnet_block(input_channels, num_channels, num_residuals, first_block=False):\n    blk = []\n    for i in range(num_residuals):\n        if i == 0 and not first_block:\n            blk.append(Residual(input_channels, num_channels, use_1x1conv=True, strides=2))\n        else:\n            blk.append(Residual(num_channels, num_channels))\n    return blk\n```\n\n- 接着在ResNet加入所有残差块，这里每个模块使用2个残差块。\n\n\n```python\nb2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))\nb3 = nn.Sequential(*resnet_block(64, 128, 2))\nb4 = nn.Sequential(*resnet_block(128, 256, 2))\nb5 = nn.Sequential(*resnet_block(256, 512, 2))\n```\n\n- 最后，与GoogLeNet一样，在ResNet中加入全局平均汇聚层，以及全连接层输出。\n- 每个模块有4个卷积层（不包括恒等映射的1 × 1卷积层）。加上第一个7 × 7卷积层和最后一个全连接层，共有18层。因此，这种模型通常被称为ResNet‐18。\n- 虽然ResNet的主体架构跟GoogLeNet类似，但ResNet架构更简单，修改也更方便。这些因素都导致了ResNet迅速被广泛使用。\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/6_resnet/4.png)\n![](img/deeplearning/code/pytorch/6_moden_conv/6_resnet/4.png)\n\n\n```python\nnet = nn.Sequential(b1, b2, b3, b4, b5,\n                    nn.AdaptiveAvgPool2d((1,1)),\n                    nn.Flatten(), nn.Linear(512, 10)\n                    )\n\n```\n\n- 查看形状。分辨率降低，通道数量增加，直到全局平均汇聚层聚集所有特征。\n\n\n```python\nX = torch.rand(size=(1, 1, 224, 224))\nfor layer in net:\n    X = layer(X)\n    print(layer.__class__.__name__,'output shape:\\t', X.shape)\n\n```\n\n    Sequential output shape:\t torch.Size([1, 64, 56, 56])\n    Sequential output shape:\t torch.Size([1, 64, 56, 56])\n    Sequential output shape:\t torch.Size([1, 128, 28, 28])\n    Sequential output shape:\t torch.Size([1, 256, 14, 14])\n    Sequential output shape:\t torch.Size([1, 512, 7, 7])\n    AdaptiveAvgPool2d output shape:\t torch.Size([1, 512, 1, 1])\n    Flatten output shape:\t torch.Size([1, 512])\n    Linear output shape:\t torch.Size([1, 10])\n    \n\n## 6.4 训练\n- 在Fashion‐MNIST数据集上训练ResNet。\n\n\n```python\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n\nlr, num_epochs, batch_size = 0.05, 10, 256\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)\nd2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())\n```\n\n    loss 0.011, train acc 0.997, test acc 0.919\n    52.9 examples/sec on cpu\n    \n\n\n    \n![svg](6_resnet_files/6_resnet_13_1.svg)\n![](img/deeplearning/code/pytorch/6_moden_conv/6_resnet_files/6_resnet_13_1.svg)\n\n","slug":"deeplearning/code/pytorch/6_moden_convolution/6_resnet","published":1,"updated":"2024-05-05T14:08:14.371Z","comments":1,"layout":"post","photos":[],"_id":"clvuli6dq004izsvwgu034mfn","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"6-残差网络（ResNet）\"><a href=\"#6-残差网络（ResNet）\" class=\"headerlink\" title=\"6 残差网络（ResNet）\"></a>6 残差网络（ResNet）</h1><ul>\n<li>随着我们设计越来越深的网络，深刻理解“新添加的层如何提升神经网络的性能”变得至关重要。更重要的是设计网络的能力，在这种网络中，添加层会使网络更具表现力，为了取得质的突破，我们需要一些数学基础知识。</li>\n</ul>\n<h2 id=\"6-1-函数类\"><a href=\"#6-1-函数类\" class=\"headerlink\" title=\"6.1 函数类\"></a>6.1 函数类</h2><ul>\n<li>假设神经网络：$\\mathcal{F}$，它包括学习率和其他超参数设置。对于所有$f \\in \\mathcal{F}$，存在一些参数集（如权重和偏置），这些参数可以通过在合适的数据集上进行训练而获得。</li>\n<li>假设$f^ *$是我们的目标函数，我们希望找到一个函数$f_ {\\mathcal{F}}^ * \\in \\mathcal{F}$，使$f_ {\\mathcal{F}}^ * \\approx f^ *$。</li>\n<li>给定一个具有$\\mathbf{X}$特征和$\\mathbf{y}$标签的数据集，我们可以通过解决以下优化问题来找到他：<br>$$f_ {\\mathcal{F}}^ * &#x3D; \\arg \\min _{f \\in \\mathcal{F}} L(\\mathbf{X}, \\mathbf{y}, f)$$</li>\n<li>如何找到一个$f_ {\\mathcal{F}^ &#96;}^ *$，使比$f_ {\\mathcal{F}}^ *$更接近$f^ *$？只有当较复杂的函数类包含较小的函数类时，我们才能确保提高它们的性能。左边，虽然F3比F1更接近f∗，但F6却离的更远了。右侧的嵌套函数（nested function）类F1 ⊆ . . . ⊆ F6，我们可以避免上述问题。<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/6_resnet/1.png\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/6_resnet/1.png\"></li>\n<li>对于深度神经网络，如果我们能将新添加的层训练成恒等映射（identity function）f(x) &#x3D; x，新模型和原模型将同样有效。同时，由于新模型可能得出更优的解来拟合训练数据集，因此添加层似乎更容易降低训练误差。</li>\n<li>残差网络的核心思想是：每个附加层都应该更容易地包含原始函数作为其元素之一。</li>\n</ul>\n<h2 id=\"6-2-残差块\"><a href=\"#6-2-残差块\" class=\"headerlink\" title=\"6.2 残差块\"></a>6.2 残差块</h2><ul>\n<li>输入为x，希望学出的理想映射为f(x)。左图虚线框中的部分需要直接拟合出该映射f(x)，而右图虚线框中的部分则需要拟合出残差映射f(x) − x。我们只需将 图右图虚线框内上方的加权运算（如仿射）的权重和偏置参数设成0，那么f(x)即为恒等映射。当理想映射f(x)极接近于恒等映射时，残差映射也易于捕捉恒等映射的细微波动。<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/6_resnet/2.png\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/6_resnet/2.png\"></li>\n<li>resnet沿用vgg完整的3 x 3卷积层设计。残差块里首先有2个有相同输出通道数的3 × 3卷积层。每个卷积层后接一个批量规范化层和ReLU激活函数。然后我们通过跨层数据通路，跳过这2个卷积运算，将输入直接加在最后的ReLU激活函数前。这样的设计要求2个卷积层的输出与输入形状一样，从而使它们可以相加。如果想改变通道数，就需要引入一个额外的1 × 1卷积层来将输入变换成需要的形状后再做相加运算。<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/6_resnet/3.png\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/6_resnet/3.png\"></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.nn <span class=\"keyword\">import</span> functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Residual</span>(nn.Module): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, input_channels, num_channels, use_1x1conv=<span class=\"literal\">False</span>, strides=<span class=\"number\">1</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        self.conv1 = nn.Conv2d(input_channels, num_channels, kernel_size=<span class=\"number\">3</span>, padding=<span class=\"number\">1</span>, stride=strides)</span><br><span class=\"line\">        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=<span class=\"number\">3</span>, padding=<span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> use_1x1conv: <span class=\"comment\">#通道数有改变</span></span><br><span class=\"line\">            self.conv3 = nn.Conv2d(input_channels, num_channels, kernel_size=<span class=\"number\">1</span>, stride=strides)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            self.conv3 = <span class=\"literal\">None</span></span><br><span class=\"line\">        self.bn1 = nn.BatchNorm2d(num_channels)</span><br><span class=\"line\">        self.bn2 = nn.BatchNorm2d(num_channels)</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        Y = F.relu(self.bn1(self.conv1(X)))</span><br><span class=\"line\">        Y = self.bn2(self.conv2(Y))</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.conv3:</span><br><span class=\"line\">            X = self.conv3(X)</span><br><span class=\"line\">        Y += X</span><br><span class=\"line\">        <span class=\"keyword\">return</span> F.relu(Y)</span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"comment\"># 输入和输出形状一致的情况</span></span><br><span class=\"line\">blk = Residual(<span class=\"number\">3</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">X = torch.rand(<span class=\"number\">4</span>,<span class=\"number\">3</span>,<span class=\"number\">6</span>,<span class=\"number\">6</span>)</span><br><span class=\"line\">Y = blk(X)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(Y.shape) <span class=\"comment\">#(4,3,6,6)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 增加输出通道，减少输出的高和宽</span></span><br><span class=\"line\">blk = Residual(<span class=\"number\">3</span>,<span class=\"number\">6</span>, use_1x1conv=<span class=\"literal\">True</span>, strides=<span class=\"number\">2</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(blk(X).shape) <span class=\"comment\">#(4,6,3,3)</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<pre><code>torch.Size([4, 3, 6, 6])\ntorch.Size([4, 6, 3, 3])\n</code></pre>\n<h2 id=\"6-3-ResNet模型\"><a href=\"#6-3-ResNet模型\" class=\"headerlink\" title=\"6.3 ResNet模型\"></a>6.3 ResNet模型</h2><ul>\n<li>前两层跟GoogLeNet中的一样：在输出通道数为64、步幅为2的7 × 7卷积层后，接步幅为2的3 × 3的最大汇聚层。不同之处在于ResNet每个卷积层后增加了批量规范化层。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">b1 = nn.Sequential(nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">64</span>, kernel_size=<span class=\"number\">7</span>, stride=<span class=\"number\">2</span>, padding=<span class=\"number\">3</span>),</span><br><span class=\"line\">                    nn.BatchNorm2d(<span class=\"number\">64</span>), nn.ReLU(),</span><br><span class=\"line\">                    nn.MaxPool2d(kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>, padding=<span class=\"number\">1</span>)</span><br><span class=\"line\">                    )</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>GoogLeNet在后面接了4个由Inception块组成的模块。ResNet则使用4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。第一个模块的通道数同输入通道数一致。由于之前已经使用了步幅为2的最大汇聚层，所以无须减小高和宽。之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">resnet_block</span>(<span class=\"params\">input_channels, num_channels, num_residuals, first_block=<span class=\"literal\">False</span></span>):</span><br><span class=\"line\">    blk = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_residuals):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> i == <span class=\"number\">0</span> <span class=\"keyword\">and</span> <span class=\"keyword\">not</span> first_block:</span><br><span class=\"line\">            blk.append(Residual(input_channels, num_channels, use_1x1conv=<span class=\"literal\">True</span>, strides=<span class=\"number\">2</span>))</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            blk.append(Residual(num_channels, num_channels))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> blk</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>接着在ResNet加入所有残差块，这里每个模块使用2个残差块。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">b2 = nn.Sequential(*resnet_block(<span class=\"number\">64</span>, <span class=\"number\">64</span>, <span class=\"number\">2</span>, first_block=<span class=\"literal\">True</span>))</span><br><span class=\"line\">b3 = nn.Sequential(*resnet_block(<span class=\"number\">64</span>, <span class=\"number\">128</span>, <span class=\"number\">2</span>))</span><br><span class=\"line\">b4 = nn.Sequential(*resnet_block(<span class=\"number\">128</span>, <span class=\"number\">256</span>, <span class=\"number\">2</span>))</span><br><span class=\"line\">b5 = nn.Sequential(*resnet_block(<span class=\"number\">256</span>, <span class=\"number\">512</span>, <span class=\"number\">2</span>))</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>最后，与GoogLeNet一样，在ResNet中加入全局平均汇聚层，以及全连接层输出。</li>\n<li>每个模块有4个卷积层（不包括恒等映射的1 × 1卷积层）。加上第一个7 × 7卷积层和最后一个全连接层，共有18层。因此，这种模型通常被称为ResNet‐18。</li>\n<li>虽然ResNet的主体架构跟GoogLeNet类似，但ResNet架构更简单，修改也更方便。这些因素都导致了ResNet迅速被广泛使用。<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/6_resnet/4.png\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/6_resnet/4.png\"></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net = nn.Sequential(b1, b2, b3, b4, b5,</span><br><span class=\"line\">                    nn.AdaptiveAvgPool2d((<span class=\"number\">1</span>,<span class=\"number\">1</span>)),</span><br><span class=\"line\">                    nn.Flatten(), nn.Linear(<span class=\"number\">512</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\">                    )</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>查看形状。分辨率降低，通道数量增加，直到全局平均汇聚层聚集所有特征。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = torch.rand(size=(<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">224</span>, <span class=\"number\">224</span>))</span><br><span class=\"line\"><span class=\"keyword\">for</span> layer <span class=\"keyword\">in</span> net:</span><br><span class=\"line\">    X = layer(X)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(layer.__class__.__name__,<span class=\"string\">&#x27;output shape:\\t&#x27;</span>, X.shape)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<pre><code>Sequential output shape:\t torch.Size([1, 64, 56, 56])\nSequential output shape:\t torch.Size([1, 64, 56, 56])\nSequential output shape:\t torch.Size([1, 128, 28, 28])\nSequential output shape:\t torch.Size([1, 256, 14, 14])\nSequential output shape:\t torch.Size([1, 512, 7, 7])\nAdaptiveAvgPool2d output shape:\t torch.Size([1, 512, 1, 1])\nFlatten output shape:\t torch.Size([1, 512])\nLinear output shape:\t torch.Size([1, 10])\n</code></pre>\n<h2 id=\"6-4-训练\"><a href=\"#6-4-训练\" class=\"headerlink\" title=\"6.4 训练\"></a>6.4 训练</h2><ul>\n<li>在Fashion‐MNIST数据集上训练ResNet。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>] = <span class=\"string\">&#x27;TRUE&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\">lr, num_epochs, batch_size = <span class=\"number\">0.05</span>, <span class=\"number\">10</span>, <span class=\"number\">256</span></span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class=\"number\">96</span>)</span><br><span class=\"line\">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure>\n\n<pre><code>loss 0.011, train acc 0.997, test acc 0.919\n52.9 examples/sec on cpu\n</code></pre>\n<p><img src=\"/6_resnet_files/6_resnet_13_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/6_resnet_files/6_resnet_13_1.svg\"></p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"6-残差网络（ResNet）\"><a href=\"#6-残差网络（ResNet）\" class=\"headerlink\" title=\"6 残差网络（ResNet）\"></a>6 残差网络（ResNet）</h1><ul>\n<li>随着我们设计越来越深的网络，深刻理解“新添加的层如何提升神经网络的性能”变得至关重要。更重要的是设计网络的能力，在这种网络中，添加层会使网络更具表现力，为了取得质的突破，我们需要一些数学基础知识。</li>\n</ul>\n<h2 id=\"6-1-函数类\"><a href=\"#6-1-函数类\" class=\"headerlink\" title=\"6.1 函数类\"></a>6.1 函数类</h2><ul>\n<li>假设神经网络：$\\mathcal{F}$，它包括学习率和其他超参数设置。对于所有$f \\in \\mathcal{F}$，存在一些参数集（如权重和偏置），这些参数可以通过在合适的数据集上进行训练而获得。</li>\n<li>假设$f^ *$是我们的目标函数，我们希望找到一个函数$f_ {\\mathcal{F}}^ * \\in \\mathcal{F}$，使$f_ {\\mathcal{F}}^ * \\approx f^ *$。</li>\n<li>给定一个具有$\\mathbf{X}$特征和$\\mathbf{y}$标签的数据集，我们可以通过解决以下优化问题来找到他：<br>$$f_ {\\mathcal{F}}^ * &#x3D; \\arg \\min _{f \\in \\mathcal{F}} L(\\mathbf{X}, \\mathbf{y}, f)$$</li>\n<li>如何找到一个$f_ {\\mathcal{F}^ &#96;}^ *$，使比$f_ {\\mathcal{F}}^ *$更接近$f^ *$？只有当较复杂的函数类包含较小的函数类时，我们才能确保提高它们的性能。左边，虽然F3比F1更接近f∗，但F6却离的更远了。右侧的嵌套函数（nested function）类F1 ⊆ . . . ⊆ F6，我们可以避免上述问题。<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/6_resnet/1.png\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/6_resnet/1.png\"></li>\n<li>对于深度神经网络，如果我们能将新添加的层训练成恒等映射（identity function）f(x) &#x3D; x，新模型和原模型将同样有效。同时，由于新模型可能得出更优的解来拟合训练数据集，因此添加层似乎更容易降低训练误差。</li>\n<li>残差网络的核心思想是：每个附加层都应该更容易地包含原始函数作为其元素之一。</li>\n</ul>\n<h2 id=\"6-2-残差块\"><a href=\"#6-2-残差块\" class=\"headerlink\" title=\"6.2 残差块\"></a>6.2 残差块</h2><ul>\n<li>输入为x，希望学出的理想映射为f(x)。左图虚线框中的部分需要直接拟合出该映射f(x)，而右图虚线框中的部分则需要拟合出残差映射f(x) − x。我们只需将 图右图虚线框内上方的加权运算（如仿射）的权重和偏置参数设成0，那么f(x)即为恒等映射。当理想映射f(x)极接近于恒等映射时，残差映射也易于捕捉恒等映射的细微波动。<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/6_resnet/2.png\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/6_resnet/2.png\"></li>\n<li>resnet沿用vgg完整的3 x 3卷积层设计。残差块里首先有2个有相同输出通道数的3 × 3卷积层。每个卷积层后接一个批量规范化层和ReLU激活函数。然后我们通过跨层数据通路，跳过这2个卷积运算，将输入直接加在最后的ReLU激活函数前。这样的设计要求2个卷积层的输出与输入形状一样，从而使它们可以相加。如果想改变通道数，就需要引入一个额外的1 × 1卷积层来将输入变换成需要的形状后再做相加运算。<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/6_resnet/3.png\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/6_resnet/3.png\"></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.nn <span class=\"keyword\">import</span> functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Residual</span>(nn.Module): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, input_channels, num_channels, use_1x1conv=<span class=\"literal\">False</span>, strides=<span class=\"number\">1</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        self.conv1 = nn.Conv2d(input_channels, num_channels, kernel_size=<span class=\"number\">3</span>, padding=<span class=\"number\">1</span>, stride=strides)</span><br><span class=\"line\">        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=<span class=\"number\">3</span>, padding=<span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> use_1x1conv: <span class=\"comment\">#通道数有改变</span></span><br><span class=\"line\">            self.conv3 = nn.Conv2d(input_channels, num_channels, kernel_size=<span class=\"number\">1</span>, stride=strides)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            self.conv3 = <span class=\"literal\">None</span></span><br><span class=\"line\">        self.bn1 = nn.BatchNorm2d(num_channels)</span><br><span class=\"line\">        self.bn2 = nn.BatchNorm2d(num_channels)</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        Y = F.relu(self.bn1(self.conv1(X)))</span><br><span class=\"line\">        Y = self.bn2(self.conv2(Y))</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.conv3:</span><br><span class=\"line\">            X = self.conv3(X)</span><br><span class=\"line\">        Y += X</span><br><span class=\"line\">        <span class=\"keyword\">return</span> F.relu(Y)</span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"comment\"># 输入和输出形状一致的情况</span></span><br><span class=\"line\">blk = Residual(<span class=\"number\">3</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">X = torch.rand(<span class=\"number\">4</span>,<span class=\"number\">3</span>,<span class=\"number\">6</span>,<span class=\"number\">6</span>)</span><br><span class=\"line\">Y = blk(X)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(Y.shape) <span class=\"comment\">#(4,3,6,6)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 增加输出通道，减少输出的高和宽</span></span><br><span class=\"line\">blk = Residual(<span class=\"number\">3</span>,<span class=\"number\">6</span>, use_1x1conv=<span class=\"literal\">True</span>, strides=<span class=\"number\">2</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(blk(X).shape) <span class=\"comment\">#(4,6,3,3)</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<pre><code>torch.Size([4, 3, 6, 6])\ntorch.Size([4, 6, 3, 3])\n</code></pre>\n<h2 id=\"6-3-ResNet模型\"><a href=\"#6-3-ResNet模型\" class=\"headerlink\" title=\"6.3 ResNet模型\"></a>6.3 ResNet模型</h2><ul>\n<li>前两层跟GoogLeNet中的一样：在输出通道数为64、步幅为2的7 × 7卷积层后，接步幅为2的3 × 3的最大汇聚层。不同之处在于ResNet每个卷积层后增加了批量规范化层。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">b1 = nn.Sequential(nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">64</span>, kernel_size=<span class=\"number\">7</span>, stride=<span class=\"number\">2</span>, padding=<span class=\"number\">3</span>),</span><br><span class=\"line\">                    nn.BatchNorm2d(<span class=\"number\">64</span>), nn.ReLU(),</span><br><span class=\"line\">                    nn.MaxPool2d(kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>, padding=<span class=\"number\">1</span>)</span><br><span class=\"line\">                    )</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>GoogLeNet在后面接了4个由Inception块组成的模块。ResNet则使用4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。第一个模块的通道数同输入通道数一致。由于之前已经使用了步幅为2的最大汇聚层，所以无须减小高和宽。之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">resnet_block</span>(<span class=\"params\">input_channels, num_channels, num_residuals, first_block=<span class=\"literal\">False</span></span>):</span><br><span class=\"line\">    blk = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_residuals):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> i == <span class=\"number\">0</span> <span class=\"keyword\">and</span> <span class=\"keyword\">not</span> first_block:</span><br><span class=\"line\">            blk.append(Residual(input_channels, num_channels, use_1x1conv=<span class=\"literal\">True</span>, strides=<span class=\"number\">2</span>))</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            blk.append(Residual(num_channels, num_channels))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> blk</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>接着在ResNet加入所有残差块，这里每个模块使用2个残差块。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">b2 = nn.Sequential(*resnet_block(<span class=\"number\">64</span>, <span class=\"number\">64</span>, <span class=\"number\">2</span>, first_block=<span class=\"literal\">True</span>))</span><br><span class=\"line\">b3 = nn.Sequential(*resnet_block(<span class=\"number\">64</span>, <span class=\"number\">128</span>, <span class=\"number\">2</span>))</span><br><span class=\"line\">b4 = nn.Sequential(*resnet_block(<span class=\"number\">128</span>, <span class=\"number\">256</span>, <span class=\"number\">2</span>))</span><br><span class=\"line\">b5 = nn.Sequential(*resnet_block(<span class=\"number\">256</span>, <span class=\"number\">512</span>, <span class=\"number\">2</span>))</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>最后，与GoogLeNet一样，在ResNet中加入全局平均汇聚层，以及全连接层输出。</li>\n<li>每个模块有4个卷积层（不包括恒等映射的1 × 1卷积层）。加上第一个7 × 7卷积层和最后一个全连接层，共有18层。因此，这种模型通常被称为ResNet‐18。</li>\n<li>虽然ResNet的主体架构跟GoogLeNet类似，但ResNet架构更简单，修改也更方便。这些因素都导致了ResNet迅速被广泛使用。<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/6_moden_conv/6_resnet/4.png\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/6_resnet/4.png\"></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net = nn.Sequential(b1, b2, b3, b4, b5,</span><br><span class=\"line\">                    nn.AdaptiveAvgPool2d((<span class=\"number\">1</span>,<span class=\"number\">1</span>)),</span><br><span class=\"line\">                    nn.Flatten(), nn.Linear(<span class=\"number\">512</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\">                    )</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>查看形状。分辨率降低，通道数量增加，直到全局平均汇聚层聚集所有特征。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = torch.rand(size=(<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">224</span>, <span class=\"number\">224</span>))</span><br><span class=\"line\"><span class=\"keyword\">for</span> layer <span class=\"keyword\">in</span> net:</span><br><span class=\"line\">    X = layer(X)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(layer.__class__.__name__,<span class=\"string\">&#x27;output shape:\\t&#x27;</span>, X.shape)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<pre><code>Sequential output shape:\t torch.Size([1, 64, 56, 56])\nSequential output shape:\t torch.Size([1, 64, 56, 56])\nSequential output shape:\t torch.Size([1, 128, 28, 28])\nSequential output shape:\t torch.Size([1, 256, 14, 14])\nSequential output shape:\t torch.Size([1, 512, 7, 7])\nAdaptiveAvgPool2d output shape:\t torch.Size([1, 512, 1, 1])\nFlatten output shape:\t torch.Size([1, 512])\nLinear output shape:\t torch.Size([1, 10])\n</code></pre>\n<h2 id=\"6-4-训练\"><a href=\"#6-4-训练\" class=\"headerlink\" title=\"6.4 训练\"></a>6.4 训练</h2><ul>\n<li>在Fashion‐MNIST数据集上训练ResNet。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>] = <span class=\"string\">&#x27;TRUE&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\">lr, num_epochs, batch_size = <span class=\"number\">0.05</span>, <span class=\"number\">10</span>, <span class=\"number\">256</span></span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class=\"number\">96</span>)</span><br><span class=\"line\">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure>\n\n<pre><code>loss 0.011, train acc 0.997, test acc 0.919\n52.9 examples/sec on cpu\n</code></pre>\n<p><img src=\"/6_resnet_files/6_resnet_13_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/6_moden_conv/6_resnet_files/6_resnet_13_1.svg\"></p>"},{"title":"鼠标侧键更改","date":"2024-03-18T00:00:00.000Z","comment":true,"toc":true,"_content":"\n#\n<!--more-->\n\n# 更改鼠标侧键\n\n- 原始的鼠标侧键是在浏览器/文件资源管理器中上一页下一页，想将其修改为其他快捷键。\n\n## 1. X-Mouse Button Control\n\n- [下载X-Mouse Button Control](https://www.highrez.co.uk/scripts/download.asp?package=XMouse)\n\n- 找到侧键位置，更改即可\n\n  ![](D:/blog/themes/yilia/source/img/experience/app/mouse/1.png)\n\n  ![](img/experience/app/mouse/1.png)\n\n  - 此外还可以自己设置不同的方案等。\n\n## 2. autohotkey\n\n- [下载](https://www.autohotkey.com/)(两个都要下载)\n\n   ![](D:/blog/themes/yilia/source/img/experience/app/mouse/2.png)\n\n  ![](img/experience/app/mouse/2.png) \n\n- 新建一个文件：\n\n     ![](D:/blog/themes/yilia/source/img/experience/app/mouse/3.png)\n\n    ![](img/experience/app/mouse/3.png) \n\n- 写入如下内容，实现短按、长按不同功能：\n\n  ```ahk\n  #Persistent\n  #UseHook\n  \n  XButton1::\n  KeyWait, XButton1, T0.2\n  If ErrorLevel ; 表示按键被长按\n      Send, #{v} ; 模拟按下 Win+V\n  Else\n      Send, ^{z} ; 模拟按下 Ctrl+Z\n  Return\n  \n  XButton2::\n  KeyWait, XButton2, T0.2\n  If ErrorLevel ; 表示按键被长按\n      Send, ^{v} ; 模拟按下 Ctrl+V\n  Else\n      Send, ^{c} ; 模拟按下 Ctrl+C\n  Return\n  ```\n\n  双击运行即可。\n  \n- 将该文件放到：`C:\\Users\\123\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Startup`开机自启动。","source":"_posts/experience/app/mouse.md","raw":"---\ntitle: 鼠标侧键更改\n\ndate: 2024-3-18 08:00:00\n\ntags: [鼠标]\n\ncategories: [经验]\n\ncomment: true\n\ntoc: true\n\n\n---\n\n#\n<!--more-->\n\n# 更改鼠标侧键\n\n- 原始的鼠标侧键是在浏览器/文件资源管理器中上一页下一页，想将其修改为其他快捷键。\n\n## 1. X-Mouse Button Control\n\n- [下载X-Mouse Button Control](https://www.highrez.co.uk/scripts/download.asp?package=XMouse)\n\n- 找到侧键位置，更改即可\n\n  ![](D:/blog/themes/yilia/source/img/experience/app/mouse/1.png)\n\n  ![](img/experience/app/mouse/1.png)\n\n  - 此外还可以自己设置不同的方案等。\n\n## 2. autohotkey\n\n- [下载](https://www.autohotkey.com/)(两个都要下载)\n\n   ![](D:/blog/themes/yilia/source/img/experience/app/mouse/2.png)\n\n  ![](img/experience/app/mouse/2.png) \n\n- 新建一个文件：\n\n     ![](D:/blog/themes/yilia/source/img/experience/app/mouse/3.png)\n\n    ![](img/experience/app/mouse/3.png) \n\n- 写入如下内容，实现短按、长按不同功能：\n\n  ```ahk\n  #Persistent\n  #UseHook\n  \n  XButton1::\n  KeyWait, XButton1, T0.2\n  If ErrorLevel ; 表示按键被长按\n      Send, #{v} ; 模拟按下 Win+V\n  Else\n      Send, ^{z} ; 模拟按下 Ctrl+Z\n  Return\n  \n  XButton2::\n  KeyWait, XButton2, T0.2\n  If ErrorLevel ; 表示按键被长按\n      Send, ^{v} ; 模拟按下 Ctrl+V\n  Else\n      Send, ^{c} ; 模拟按下 Ctrl+C\n  Return\n  ```\n\n  双击运行即可。\n  \n- 将该文件放到：`C:\\Users\\123\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Startup`开机自启动。","slug":"experience/app/mouse","published":1,"updated":"2024-03-18T08:55:28.271Z","_id":"clvuli6dq004lzsvwafq0ey9i","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h1 id=\"更改鼠标侧键\"><a href=\"#更改鼠标侧键\" class=\"headerlink\" title=\"更改鼠标侧键\"></a>更改鼠标侧键</h1><ul>\n<li>原始的鼠标侧键是在浏览器&#x2F;文件资源管理器中上一页下一页，想将其修改为其他快捷键。</li>\n</ul>\n<h2 id=\"1-X-Mouse-Button-Control\"><a href=\"#1-X-Mouse-Button-Control\" class=\"headerlink\" title=\"1. X-Mouse Button Control\"></a>1. X-Mouse Button Control</h2><ul>\n<li><p><a href=\"https://www.highrez.co.uk/scripts/download.asp?package=XMouse\">下载X-Mouse Button Control</a></p>\n</li>\n<li><p>找到侧键位置，更改即可</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/experience/app/mouse/1.png\"></p>\n<p><img src=\"/img/experience/app/mouse/1.png\"></p>\n<ul>\n<li>此外还可以自己设置不同的方案等。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"2-autohotkey\"><a href=\"#2-autohotkey\" class=\"headerlink\" title=\"2. autohotkey\"></a>2. autohotkey</h2><ul>\n<li><p><a href=\"https://www.autohotkey.com/\">下载</a>(两个都要下载)</p>\n<p> <img src=\"D:/blog/themes/yilia/source/img/experience/app/mouse/2.png\"></p>\n<p><img src=\"/img/experience/app/mouse/2.png\"> </p>\n</li>\n<li><p>新建一个文件：</p>\n<p>   <img src=\"D:/blog/themes/yilia/source/img/experience/app/mouse/3.png\"></p>\n<p>  <img src=\"/img/experience/app/mouse/3.png\"> </p>\n</li>\n<li><p>写入如下内容，实现短按、长按不同功能：</p>\n<figure class=\"highlight ahk\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#Persistent</span></span><br><span class=\"line\"><span class=\"meta\">#UseHook</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"title\">XButton1::</span></span><br><span class=\"line\"><span class=\"built_in\">KeyWait,</span> XButton1, T0.<span class=\"number\">2</span></span><br><span class=\"line\">If <span class=\"built_in\">ErrorLevel</span> <span class=\"comment\">; 表示按键被长按</span></span><br><span class=\"line\"><span class=\"built_in\">    Send,</span> #&#123;v&#125; <span class=\"comment\">; 模拟按下 Win+V</span></span><br><span class=\"line\">Else</span><br><span class=\"line\"><span class=\"built_in\">    Send,</span> ^&#123;z&#125; <span class=\"comment\">; 模拟按下 Ctrl+Z</span></span><br><span class=\"line\"><span class=\"keyword\">Return</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"title\">XButton2::</span></span><br><span class=\"line\"><span class=\"built_in\">KeyWait,</span> XButton2, T0.<span class=\"number\">2</span></span><br><span class=\"line\">If <span class=\"built_in\">ErrorLevel</span> <span class=\"comment\">; 表示按键被长按</span></span><br><span class=\"line\"><span class=\"built_in\">    Send,</span> ^&#123;v&#125; <span class=\"comment\">; 模拟按下 Ctrl+V</span></span><br><span class=\"line\">Else</span><br><span class=\"line\"><span class=\"built_in\">    Send,</span> ^&#123;c&#125; <span class=\"comment\">; 模拟按下 Ctrl+C</span></span><br><span class=\"line\"><span class=\"keyword\">Return</span></span><br></pre></td></tr></table></figure>\n\n<p>双击运行即可。</p>\n</li>\n<li><p>将该文件放到：<code>C:\\Users\\123\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Startup</code>开机自启动。</p>\n</li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"更改鼠标侧键\"><a href=\"#更改鼠标侧键\" class=\"headerlink\" title=\"更改鼠标侧键\"></a>更改鼠标侧键</h1><ul>\n<li>原始的鼠标侧键是在浏览器&#x2F;文件资源管理器中上一页下一页，想将其修改为其他快捷键。</li>\n</ul>\n<h2 id=\"1-X-Mouse-Button-Control\"><a href=\"#1-X-Mouse-Button-Control\" class=\"headerlink\" title=\"1. X-Mouse Button Control\"></a>1. X-Mouse Button Control</h2><ul>\n<li><p><a href=\"https://www.highrez.co.uk/scripts/download.asp?package=XMouse\">下载X-Mouse Button Control</a></p>\n</li>\n<li><p>找到侧键位置，更改即可</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/experience/app/mouse/1.png\"></p>\n<p><img src=\"/img/experience/app/mouse/1.png\"></p>\n<ul>\n<li>此外还可以自己设置不同的方案等。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"2-autohotkey\"><a href=\"#2-autohotkey\" class=\"headerlink\" title=\"2. autohotkey\"></a>2. autohotkey</h2><ul>\n<li><p><a href=\"https://www.autohotkey.com/\">下载</a>(两个都要下载)</p>\n<p> <img src=\"D:/blog/themes/yilia/source/img/experience/app/mouse/2.png\"></p>\n<p><img src=\"/img/experience/app/mouse/2.png\"> </p>\n</li>\n<li><p>新建一个文件：</p>\n<p>   <img src=\"D:/blog/themes/yilia/source/img/experience/app/mouse/3.png\"></p>\n<p>  <img src=\"/img/experience/app/mouse/3.png\"> </p>\n</li>\n<li><p>写入如下内容，实现短按、长按不同功能：</p>\n<figure class=\"highlight ahk\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#Persistent</span></span><br><span class=\"line\"><span class=\"meta\">#UseHook</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"title\">XButton1::</span></span><br><span class=\"line\"><span class=\"built_in\">KeyWait,</span> XButton1, T0.<span class=\"number\">2</span></span><br><span class=\"line\">If <span class=\"built_in\">ErrorLevel</span> <span class=\"comment\">; 表示按键被长按</span></span><br><span class=\"line\"><span class=\"built_in\">    Send,</span> #&#123;v&#125; <span class=\"comment\">; 模拟按下 Win+V</span></span><br><span class=\"line\">Else</span><br><span class=\"line\"><span class=\"built_in\">    Send,</span> ^&#123;z&#125; <span class=\"comment\">; 模拟按下 Ctrl+Z</span></span><br><span class=\"line\"><span class=\"keyword\">Return</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"title\">XButton2::</span></span><br><span class=\"line\"><span class=\"built_in\">KeyWait,</span> XButton2, T0.<span class=\"number\">2</span></span><br><span class=\"line\">If <span class=\"built_in\">ErrorLevel</span> <span class=\"comment\">; 表示按键被长按</span></span><br><span class=\"line\"><span class=\"built_in\">    Send,</span> ^&#123;v&#125; <span class=\"comment\">; 模拟按下 Ctrl+V</span></span><br><span class=\"line\">Else</span><br><span class=\"line\"><span class=\"built_in\">    Send,</span> ^&#123;c&#125; <span class=\"comment\">; 模拟按下 Ctrl+C</span></span><br><span class=\"line\"><span class=\"keyword\">Return</span></span><br></pre></td></tr></table></figure>\n\n<p>双击运行即可。</p>\n</li>\n<li><p>将该文件放到：<code>C:\\Users\\123\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Startup</code>开机自启动。</p>\n</li>\n</ul>"},{"title":"word笔记","date":"2024-04-13T16:00:00.000Z","comment":true,"toc":true,"_content":"\n#\n<!--more-->\n\n# 1. 公式\n\n- 使用latex公式\n\n  ![](D:/blog/themes/yilia/source/img/experience/app/word/1.png)\n\n  ![](img/experience/app/word/1.png)\n\n- 写latex公式\n  \n  - 写/ alt + = / ctrl + =\n\n\n\n# 2. 保存pdf\n\n## 2.1 保存pdf的时候添加导航栏\n\n- word里面ctrl+f打开导航栏\n\n- 另存为 ， 选择类型为pdf, 选项\n\n  ![](D:/blog/themes/yilia/source/img/experience/app/word/to_pdf/1.png)\n\n  ![](img/experience/app/word/to_pdf/1.png)\n\n- 选择标题\n\n  ![](D:/blog/themes/yilia/source/img/experience/app/word/to_pdf/2.png)\n  \n  ![](img/experience/app/word/to_pdf/2.png)\n\n\n\n# 3. 页码\n\n## 3.1 分割页码\n\n- 如何将摘要编页码，目录无页码，正文编页码\n\n  ![](D:/blog/themes/yilia/source/img/experience/app/word/page/0.png)\n\n  ![](img/experience/app/word/page/0.png)\n\n- 分别在上图箭头处（每一节的结尾插入分节符）（选择分节符/下一页，不要选连续）\n\n  ![](D:/blog/themes/yilia/source/img/experience/app/word/page/1.png)\n\n  ![](img/experience/app/word/page/1.png)\n\n- 重复n次插入后，你获得了四个区块。每个区块里面的分页随便你插入何种类型的分页符。\n\n  ![](D:/blog/themes/yilia/source/img/experience/app/word/page/2.png)\n\n  ![](img/experience/app/word/page/2.png)\n\n- 双击页脚（假设你在页脚插入页码），在**每**个区块的的第一页**取消链接到前一节**，插入页码。（每个区块的第一页都要点击**取消链接到前一节**斩断联系。）\n\n   ![](D:/blog/themes/yilia/source/img/experience/app/word/page/3.png)\n\n   ![](img/experience/app/word/page/3.png)  \n\n- 此时页码不是1，不要慌，点击页码，设置格式。就ok了\n\n   ![](D:/blog/themes/yilia/source/img/experience/app/word/page/4.png)\n\n   ![](img/experience/app/word/page/4.png)  \n\n   ![](D:/blog/themes/yilia/source/img/experience/app/word/page/5.png)\n\n    ![](img/experience/app/word/page/5.png) \n\n- 不需要页码的区块直接删除 \n\n\n\n# 4. ","source":"_posts/experience/app/word.md","raw":"---\ntitle: word笔记\n\ndate: 2024-4-14\n\ntags: [经验]\n\ncategories: [经验]\n\ncomment: true\n\ntoc: true\n\n\n\n---\n\n#\n<!--more-->\n\n# 1. 公式\n\n- 使用latex公式\n\n  ![](D:/blog/themes/yilia/source/img/experience/app/word/1.png)\n\n  ![](img/experience/app/word/1.png)\n\n- 写latex公式\n  \n  - 写/ alt + = / ctrl + =\n\n\n\n# 2. 保存pdf\n\n## 2.1 保存pdf的时候添加导航栏\n\n- word里面ctrl+f打开导航栏\n\n- 另存为 ， 选择类型为pdf, 选项\n\n  ![](D:/blog/themes/yilia/source/img/experience/app/word/to_pdf/1.png)\n\n  ![](img/experience/app/word/to_pdf/1.png)\n\n- 选择标题\n\n  ![](D:/blog/themes/yilia/source/img/experience/app/word/to_pdf/2.png)\n  \n  ![](img/experience/app/word/to_pdf/2.png)\n\n\n\n# 3. 页码\n\n## 3.1 分割页码\n\n- 如何将摘要编页码，目录无页码，正文编页码\n\n  ![](D:/blog/themes/yilia/source/img/experience/app/word/page/0.png)\n\n  ![](img/experience/app/word/page/0.png)\n\n- 分别在上图箭头处（每一节的结尾插入分节符）（选择分节符/下一页，不要选连续）\n\n  ![](D:/blog/themes/yilia/source/img/experience/app/word/page/1.png)\n\n  ![](img/experience/app/word/page/1.png)\n\n- 重复n次插入后，你获得了四个区块。每个区块里面的分页随便你插入何种类型的分页符。\n\n  ![](D:/blog/themes/yilia/source/img/experience/app/word/page/2.png)\n\n  ![](img/experience/app/word/page/2.png)\n\n- 双击页脚（假设你在页脚插入页码），在**每**个区块的的第一页**取消链接到前一节**，插入页码。（每个区块的第一页都要点击**取消链接到前一节**斩断联系。）\n\n   ![](D:/blog/themes/yilia/source/img/experience/app/word/page/3.png)\n\n   ![](img/experience/app/word/page/3.png)  \n\n- 此时页码不是1，不要慌，点击页码，设置格式。就ok了\n\n   ![](D:/blog/themes/yilia/source/img/experience/app/word/page/4.png)\n\n   ![](img/experience/app/word/page/4.png)  \n\n   ![](D:/blog/themes/yilia/source/img/experience/app/word/page/5.png)\n\n    ![](img/experience/app/word/page/5.png) \n\n- 不需要页码的区块直接删除 \n\n\n\n# 4. ","slug":"experience/app/word","published":1,"updated":"2024-04-14T15:22:42.247Z","_id":"clvuli6e80051zsvwd8go50hm","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h1 id=\"1-公式\"><a href=\"#1-公式\" class=\"headerlink\" title=\"1. 公式\"></a>1. 公式</h1><ul>\n<li><p>使用latex公式</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/experience/app/word/1.png\"></p>\n<p><img src=\"/img/experience/app/word/1.png\"></p>\n</li>\n<li><p>写latex公式</p>\n<ul>\n<li>写&#x2F; alt + &#x3D; &#x2F; ctrl + &#x3D;</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"2-保存pdf\"><a href=\"#2-保存pdf\" class=\"headerlink\" title=\"2. 保存pdf\"></a>2. 保存pdf</h1><h2 id=\"2-1-保存pdf的时候添加导航栏\"><a href=\"#2-1-保存pdf的时候添加导航栏\" class=\"headerlink\" title=\"2.1 保存pdf的时候添加导航栏\"></a>2.1 保存pdf的时候添加导航栏</h2><ul>\n<li><p>word里面ctrl+f打开导航栏</p>\n</li>\n<li><p>另存为 ， 选择类型为pdf, 选项</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/experience/app/word/to_pdf/1.png\"></p>\n<p><img src=\"/img/experience/app/word/to_pdf/1.png\"></p>\n</li>\n<li><p>选择标题</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/experience/app/word/to_pdf/2.png\"></p>\n<p><img src=\"/img/experience/app/word/to_pdf/2.png\"></p>\n</li>\n</ul>\n<h1 id=\"3-页码\"><a href=\"#3-页码\" class=\"headerlink\" title=\"3. 页码\"></a>3. 页码</h1><h2 id=\"3-1-分割页码\"><a href=\"#3-1-分割页码\" class=\"headerlink\" title=\"3.1 分割页码\"></a>3.1 分割页码</h2><ul>\n<li><p>如何将摘要编页码，目录无页码，正文编页码</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/experience/app/word/page/0.png\"></p>\n<p><img src=\"/img/experience/app/word/page/0.png\"></p>\n</li>\n<li><p>分别在上图箭头处（每一节的结尾插入分节符）（选择分节符&#x2F;下一页，不要选连续）</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/experience/app/word/page/1.png\"></p>\n<p><img src=\"/img/experience/app/word/page/1.png\"></p>\n</li>\n<li><p>重复n次插入后，你获得了四个区块。每个区块里面的分页随便你插入何种类型的分页符。</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/experience/app/word/page/2.png\"></p>\n<p><img src=\"/img/experience/app/word/page/2.png\"></p>\n</li>\n<li><p>双击页脚（假设你在页脚插入页码），在<strong>每</strong>个区块的的第一页<strong>取消链接到前一节</strong>，插入页码。（每个区块的第一页都要点击<strong>取消链接到前一节</strong>斩断联系。）</p>\n<p> <img src=\"D:/blog/themes/yilia/source/img/experience/app/word/page/3.png\"></p>\n<p> <img src=\"/img/experience/app/word/page/3.png\">  </p>\n</li>\n<li><p>此时页码不是1，不要慌，点击页码，设置格式。就ok了</p>\n<p> <img src=\"D:/blog/themes/yilia/source/img/experience/app/word/page/4.png\"></p>\n<p> <img src=\"/img/experience/app/word/page/4.png\">  </p>\n<p> <img src=\"D:/blog/themes/yilia/source/img/experience/app/word/page/5.png\"></p>\n<p>  <img src=\"/img/experience/app/word/page/5.png\"> </p>\n</li>\n<li><p>不需要页码的区块直接删除</p>\n</li>\n</ul>\n<h1 id=\"4\"><a href=\"#4\" class=\"headerlink\" title=\"4.\"></a>4.</h1>","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"1-公式\"><a href=\"#1-公式\" class=\"headerlink\" title=\"1. 公式\"></a>1. 公式</h1><ul>\n<li><p>使用latex公式</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/experience/app/word/1.png\"></p>\n<p><img src=\"/img/experience/app/word/1.png\"></p>\n</li>\n<li><p>写latex公式</p>\n<ul>\n<li>写&#x2F; alt + &#x3D; &#x2F; ctrl + &#x3D;</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"2-保存pdf\"><a href=\"#2-保存pdf\" class=\"headerlink\" title=\"2. 保存pdf\"></a>2. 保存pdf</h1><h2 id=\"2-1-保存pdf的时候添加导航栏\"><a href=\"#2-1-保存pdf的时候添加导航栏\" class=\"headerlink\" title=\"2.1 保存pdf的时候添加导航栏\"></a>2.1 保存pdf的时候添加导航栏</h2><ul>\n<li><p>word里面ctrl+f打开导航栏</p>\n</li>\n<li><p>另存为 ， 选择类型为pdf, 选项</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/experience/app/word/to_pdf/1.png\"></p>\n<p><img src=\"/img/experience/app/word/to_pdf/1.png\"></p>\n</li>\n<li><p>选择标题</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/experience/app/word/to_pdf/2.png\"></p>\n<p><img src=\"/img/experience/app/word/to_pdf/2.png\"></p>\n</li>\n</ul>\n<h1 id=\"3-页码\"><a href=\"#3-页码\" class=\"headerlink\" title=\"3. 页码\"></a>3. 页码</h1><h2 id=\"3-1-分割页码\"><a href=\"#3-1-分割页码\" class=\"headerlink\" title=\"3.1 分割页码\"></a>3.1 分割页码</h2><ul>\n<li><p>如何将摘要编页码，目录无页码，正文编页码</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/experience/app/word/page/0.png\"></p>\n<p><img src=\"/img/experience/app/word/page/0.png\"></p>\n</li>\n<li><p>分别在上图箭头处（每一节的结尾插入分节符）（选择分节符&#x2F;下一页，不要选连续）</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/experience/app/word/page/1.png\"></p>\n<p><img src=\"/img/experience/app/word/page/1.png\"></p>\n</li>\n<li><p>重复n次插入后，你获得了四个区块。每个区块里面的分页随便你插入何种类型的分页符。</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/experience/app/word/page/2.png\"></p>\n<p><img src=\"/img/experience/app/word/page/2.png\"></p>\n</li>\n<li><p>双击页脚（假设你在页脚插入页码），在<strong>每</strong>个区块的的第一页<strong>取消链接到前一节</strong>，插入页码。（每个区块的第一页都要点击<strong>取消链接到前一节</strong>斩断联系。）</p>\n<p> <img src=\"D:/blog/themes/yilia/source/img/experience/app/word/page/3.png\"></p>\n<p> <img src=\"/img/experience/app/word/page/3.png\">  </p>\n</li>\n<li><p>此时页码不是1，不要慌，点击页码，设置格式。就ok了</p>\n<p> <img src=\"D:/blog/themes/yilia/source/img/experience/app/word/page/4.png\"></p>\n<p> <img src=\"/img/experience/app/word/page/4.png\">  </p>\n<p> <img src=\"D:/blog/themes/yilia/source/img/experience/app/word/page/5.png\"></p>\n<p>  <img src=\"/img/experience/app/word/page/5.png\"> </p>\n</li>\n<li><p>不需要页码的区块直接删除</p>\n</li>\n</ul>\n<h1 id=\"4\"><a href=\"#4\" class=\"headerlink\" title=\"4.\"></a>4.</h1>"},{"title":"Linux入门-0配置","date":"2021-03-01T12:00:00.000Z","toc":true,"_content":"#\n\n<!-- more -->\n\n## 0. 安装\n\n\n\n### 0.1 虚拟机\n\n### 0.2 [下载finalshell](https://www.hostbuf.com/downloads/finalshell_install.exe)\n\n### 0.3 连接finalshell\n\n1. 在linux中右键，打开终端\n\n2. 输入：ifconfig ,找到ens下的inet后面的数字，复制![](./img/linux/install/1.png)\n\n   ![](D:\\blog\\themes\\yilia\\source\\img\\linux\\install\\1.png)\n1. 打开finalshell点击左上文件图标，点击新串口左上带加号的文件图标，点击ssh连接，输入自定义的名称，主机输入刚刚的复制的一串数字，用户名、密码为linux用户的，点击确定\n  ![](./img/linux/install/2.png)\n\n  ![](D:\\blog\\themes\\yilia\\source\\img\\linux\\install\\2.png)\n### 0.4（windows中使用ubuntu）\n\n1. windows中搜索：windows功能，点击：启动或关闭windows功能，将：适用于Linux的子系统勾选并重启\n\n2. 微软应用商店搜索ubuntu并下载，win10还需下载terminal\n\n3. 在terminal中点击向下箭头，选择Ubuntu\n  ![](./img/linux/install/3.png)\n\n  ![](D:\\blog\\themes\\yilia\\source\\img\\linux\\install\\3.png)\n\n4. 如果显示Error: 0x800701bc WSL 2 ?????????????????? 则返回windows命令行输入wsl --update\n\n5. 完成后打开Ubuntu窗口，输入用户名，密码，密码\n\n","source":"_posts/linux/Linux_0install.md","raw":"---\ntitle: Linux入门-0配置\ndate: 2021-03-01 20:00:00\ntoc: true\ntags: [Linux]\ncategories: [Linux]\n\n---\n#\n\n<!-- more -->\n\n## 0. 安装\n\n\n\n### 0.1 虚拟机\n\n### 0.2 [下载finalshell](https://www.hostbuf.com/downloads/finalshell_install.exe)\n\n### 0.3 连接finalshell\n\n1. 在linux中右键，打开终端\n\n2. 输入：ifconfig ,找到ens下的inet后面的数字，复制![](./img/linux/install/1.png)\n\n   ![](D:\\blog\\themes\\yilia\\source\\img\\linux\\install\\1.png)\n1. 打开finalshell点击左上文件图标，点击新串口左上带加号的文件图标，点击ssh连接，输入自定义的名称，主机输入刚刚的复制的一串数字，用户名、密码为linux用户的，点击确定\n  ![](./img/linux/install/2.png)\n\n  ![](D:\\blog\\themes\\yilia\\source\\img\\linux\\install\\2.png)\n### 0.4（windows中使用ubuntu）\n\n1. windows中搜索：windows功能，点击：启动或关闭windows功能，将：适用于Linux的子系统勾选并重启\n\n2. 微软应用商店搜索ubuntu并下载，win10还需下载terminal\n\n3. 在terminal中点击向下箭头，选择Ubuntu\n  ![](./img/linux/install/3.png)\n\n  ![](D:\\blog\\themes\\yilia\\source\\img\\linux\\install\\3.png)\n\n4. 如果显示Error: 0x800701bc WSL 2 ?????????????????? 则返回windows命令行输入wsl --update\n\n5. 完成后打开Ubuntu窗口，输入用户名，密码，密码\n\n","slug":"linux/Linux_0install","published":1,"updated":"2024-03-02T02:44:18.204Z","_id":"clvuli6hj0055zsvweonu523m","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h2 id=\"0-安装\"><a href=\"#0-安装\" class=\"headerlink\" title=\"0. 安装\"></a>0. 安装</h2><h3 id=\"0-1-虚拟机\"><a href=\"#0-1-虚拟机\" class=\"headerlink\" title=\"0.1 虚拟机\"></a>0.1 虚拟机</h3><h3 id=\"0-2-下载finalshell\"><a href=\"#0-2-下载finalshell\" class=\"headerlink\" title=\"0.2 下载finalshell\"></a>0.2 <a href=\"https://www.hostbuf.com/downloads/finalshell_install.exe\">下载finalshell</a></h3><h3 id=\"0-3-连接finalshell\"><a href=\"#0-3-连接finalshell\" class=\"headerlink\" title=\"0.3 连接finalshell\"></a>0.3 连接finalshell</h3><ol>\n<li><p>在linux中右键，打开终端</p>\n</li>\n<li><p>输入：ifconfig ,找到ens下的inet后面的数字，复制<img src=\"/./img/linux/install/1.png\"></p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\linux\\install\\1.png\"></p>\n</li>\n<li><p>打开finalshell点击左上文件图标，点击新串口左上带加号的文件图标，点击ssh连接，输入自定义的名称，主机输入刚刚的复制的一串数字，用户名、密码为linux用户的，点击确定<br>  <img src=\"/./img/linux/install/2.png\"></p>\n</li>\n</ol>\n<p>  <img src=\"D:\\blog\\themes\\yilia\\source\\img\\linux\\install\\2.png\"></p>\n<h3 id=\"0-4（windows中使用ubuntu）\"><a href=\"#0-4（windows中使用ubuntu）\" class=\"headerlink\" title=\"0.4（windows中使用ubuntu）\"></a>0.4（windows中使用ubuntu）</h3><ol>\n<li><p>windows中搜索：windows功能，点击：启动或关闭windows功能，将：适用于Linux的子系统勾选并重启</p>\n</li>\n<li><p>微软应用商店搜索ubuntu并下载，win10还需下载terminal</p>\n</li>\n<li><p>在terminal中点击向下箭头，选择Ubuntu<br>  <img src=\"/./img/linux/install/3.png\"></p>\n</li>\n</ol>\n<p>  <img src=\"D:\\blog\\themes\\yilia\\source\\img\\linux\\install\\3.png\"></p>\n<ol start=\"4\">\n<li><p>如果显示Error: 0x800701bc WSL 2 ?????????????????? 则返回windows命令行输入wsl –update</p>\n</li>\n<li><p>完成后打开Ubuntu窗口，输入用户名，密码，密码</p>\n</li>\n</ol>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h2 id=\"0-安装\"><a href=\"#0-安装\" class=\"headerlink\" title=\"0. 安装\"></a>0. 安装</h2><h3 id=\"0-1-虚拟机\"><a href=\"#0-1-虚拟机\" class=\"headerlink\" title=\"0.1 虚拟机\"></a>0.1 虚拟机</h3><h3 id=\"0-2-下载finalshell\"><a href=\"#0-2-下载finalshell\" class=\"headerlink\" title=\"0.2 下载finalshell\"></a>0.2 <a href=\"https://www.hostbuf.com/downloads/finalshell_install.exe\">下载finalshell</a></h3><h3 id=\"0-3-连接finalshell\"><a href=\"#0-3-连接finalshell\" class=\"headerlink\" title=\"0.3 连接finalshell\"></a>0.3 连接finalshell</h3><ol>\n<li><p>在linux中右键，打开终端</p>\n</li>\n<li><p>输入：ifconfig ,找到ens下的inet后面的数字，复制<img src=\"/./img/linux/install/1.png\"></p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\linux\\install\\1.png\"></p>\n</li>\n<li><p>打开finalshell点击左上文件图标，点击新串口左上带加号的文件图标，点击ssh连接，输入自定义的名称，主机输入刚刚的复制的一串数字，用户名、密码为linux用户的，点击确定<br>  <img src=\"/./img/linux/install/2.png\"></p>\n</li>\n</ol>\n<p>  <img src=\"D:\\blog\\themes\\yilia\\source\\img\\linux\\install\\2.png\"></p>\n<h3 id=\"0-4（windows中使用ubuntu）\"><a href=\"#0-4（windows中使用ubuntu）\" class=\"headerlink\" title=\"0.4（windows中使用ubuntu）\"></a>0.4（windows中使用ubuntu）</h3><ol>\n<li><p>windows中搜索：windows功能，点击：启动或关闭windows功能，将：适用于Linux的子系统勾选并重启</p>\n</li>\n<li><p>微软应用商店搜索ubuntu并下载，win10还需下载terminal</p>\n</li>\n<li><p>在terminal中点击向下箭头，选择Ubuntu<br>  <img src=\"/./img/linux/install/3.png\"></p>\n</li>\n</ol>\n<p>  <img src=\"D:\\blog\\themes\\yilia\\source\\img\\linux\\install\\3.png\"></p>\n<ol start=\"4\">\n<li><p>如果显示Error: 0x800701bc WSL 2 ?????????????????? 则返回windows命令行输入wsl –update</p>\n</li>\n<li><p>完成后打开Ubuntu窗口，输入用户名，密码，密码</p>\n</li>\n</ol>"},{"title":"git部署博客源码","date":"2024-01-29T16:00:00.000Z","comment":true,"toc":true,"_content":"\n#\n\n <!--more-->\n\n# git部署博客源码\n\n- 在blog/config文件里面确定了hexo s将博客部署到仓库的main分支，但是部署的是渲染后的代码，如何保存源码进行备份。\n\n\n\n## 1 创建分支\n\n在blog下打开gitbash\n\n```\ngit init \n```\n\n去掉blog/.gitignore文件中这三项：\n\n```\ndb.json,\nThumbs.db\nnode-modules/,\n```\n\n运行\n\n```\ngit add *\ngit commit -m \"first commit\"\n```\n\n创建一个source分支，上传源码（注意此处仓库地址，参考blog/config文件里面的部署部分，是带@符号的那个名字，不是简单的博客地址）\n\n```\ngit checkout -b source\ngit remote add github 远程仓库地址\ngit push -u github source\n```\n\n\n\n## 2 设置默认分支\n\n建议将source分支设置成默认分支，防止误删\n\n![](D:\\blog\\themes\\yilia\\source\\img\\hexo\\config\\1.png)\n\n![](img/hexo/config/1.png)\n\n![](D:\\blog\\themes\\yilia\\source\\img\\hexo\\config\\2.png)\n\n![](img/hexo/config/2.png)\n\n\n\n- 以后只需要执行：\n\n  ```bash\n  git add .\n  git commit -m \"Your commit message\"\n  git push -u github source\n  \n  ```\n\n## 3 在新设备上编写博客\n1. 在设备上生成ssh密钥\n\n    ```bash\n    ssh-keygen\n    ```\n\n- 然后一路回车，会在:user/.ssh下面有id_rsa和id_rsa.pub。\n\n2. 将id_rsa.pub内容复制到github中：\n- 在github点击右上角自己的图像\n- settings\n- 左侧SSH and GPG keys\n- 右侧：New SSH key\n\n3. 在设备上提供github用户名和邮箱\n- 用户名可以在右上角头像获得\n- 又像可以通过setting -> email获得\n  ```bash\n  git config --global user.name \"Your Name\"\n  git config --global user.email \"you@example.com\"\n  ```\n\n4. 上传博客\n    ```bash\n    git push\n    ```\n\n5. 在别的设别更新\n    ```bash\n    git pull\n    ```","source":"_posts/hexo/sourcecode.md","raw":"---\ntitle: git部署博客源码\n\ndate: 2024-1-30\n\ntags: [博客]\n\ncategories: [博客,git]\n\ncomment: true\n\ntoc: true\n\n\n---\n\n#\n\n <!--more-->\n\n# git部署博客源码\n\n- 在blog/config文件里面确定了hexo s将博客部署到仓库的main分支，但是部署的是渲染后的代码，如何保存源码进行备份。\n\n\n\n## 1 创建分支\n\n在blog下打开gitbash\n\n```\ngit init \n```\n\n去掉blog/.gitignore文件中这三项：\n\n```\ndb.json,\nThumbs.db\nnode-modules/,\n```\n\n运行\n\n```\ngit add *\ngit commit -m \"first commit\"\n```\n\n创建一个source分支，上传源码（注意此处仓库地址，参考blog/config文件里面的部署部分，是带@符号的那个名字，不是简单的博客地址）\n\n```\ngit checkout -b source\ngit remote add github 远程仓库地址\ngit push -u github source\n```\n\n\n\n## 2 设置默认分支\n\n建议将source分支设置成默认分支，防止误删\n\n![](D:\\blog\\themes\\yilia\\source\\img\\hexo\\config\\1.png)\n\n![](img/hexo/config/1.png)\n\n![](D:\\blog\\themes\\yilia\\source\\img\\hexo\\config\\2.png)\n\n![](img/hexo/config/2.png)\n\n\n\n- 以后只需要执行：\n\n  ```bash\n  git add .\n  git commit -m \"Your commit message\"\n  git push -u github source\n  \n  ```\n\n## 3 在新设备上编写博客\n1. 在设备上生成ssh密钥\n\n    ```bash\n    ssh-keygen\n    ```\n\n- 然后一路回车，会在:user/.ssh下面有id_rsa和id_rsa.pub。\n\n2. 将id_rsa.pub内容复制到github中：\n- 在github点击右上角自己的图像\n- settings\n- 左侧SSH and GPG keys\n- 右侧：New SSH key\n\n3. 在设备上提供github用户名和邮箱\n- 用户名可以在右上角头像获得\n- 又像可以通过setting -> email获得\n  ```bash\n  git config --global user.name \"Your Name\"\n  git config --global user.email \"you@example.com\"\n  ```\n\n4. 上传博客\n    ```bash\n    git push\n    ```\n\n5. 在别的设别更新\n    ```bash\n    git pull\n    ```","slug":"hexo/sourcecode","published":1,"updated":"2024-04-22T15:08:04.000Z","_id":"clvuli6hj0056zsvwceyv70az","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1> <span id=\"more\"></span>\n\n<h1 id=\"git部署博客源码\"><a href=\"#git部署博客源码\" class=\"headerlink\" title=\"git部署博客源码\"></a>git部署博客源码</h1><ul>\n<li>在blog&#x2F;config文件里面确定了hexo s将博客部署到仓库的main分支，但是部署的是渲染后的代码，如何保存源码进行备份。</li>\n</ul>\n<h2 id=\"1-创建分支\"><a href=\"#1-创建分支\" class=\"headerlink\" title=\"1 创建分支\"></a>1 创建分支</h2><p>在blog下打开gitbash</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git init </span><br></pre></td></tr></table></figure>\n\n<p>去掉blog&#x2F;.gitignore文件中这三项：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">db.json,</span><br><span class=\"line\">Thumbs.db</span><br><span class=\"line\">node-modules/,</span><br></pre></td></tr></table></figure>\n\n<p>运行</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git add *</span><br><span class=\"line\">git commit -m &quot;first commit&quot;</span><br></pre></td></tr></table></figure>\n\n<p>创建一个source分支，上传源码（注意此处仓库地址，参考blog&#x2F;config文件里面的部署部分，是带@符号的那个名字，不是简单的博客地址）</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git checkout -b source</span><br><span class=\"line\">git remote add github 远程仓库地址</span><br><span class=\"line\">git push -u github source</span><br></pre></td></tr></table></figure>\n\n\n\n<h2 id=\"2-设置默认分支\"><a href=\"#2-设置默认分支\" class=\"headerlink\" title=\"2 设置默认分支\"></a>2 设置默认分支</h2><p>建议将source分支设置成默认分支，防止误删</p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\hexo\\config\\1.png\"></p>\n<p><img src=\"/img/hexo/config/1.png\"></p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\hexo\\config\\2.png\"></p>\n<p><img src=\"/img/hexo/config/2.png\"></p>\n<ul>\n<li><p>以后只需要执行：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git add .</span><br><span class=\"line\">git commit -m <span class=\"string\">&quot;Your commit message&quot;</span></span><br><span class=\"line\">git push -u github <span class=\"built_in\">source</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></li>\n</ul>\n<h2 id=\"3-在新设备上编写博客\"><a href=\"#3-在新设备上编写博客\" class=\"headerlink\" title=\"3 在新设备上编写博客\"></a>3 在新设备上编写博客</h2><ol>\n<li><p>在设备上生成ssh密钥</p>\n <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh-keygen</span><br></pre></td></tr></table></figure></li>\n</ol>\n<ul>\n<li>然后一路回车，会在:user&#x2F;.ssh下面有id_rsa和id_rsa.pub。</li>\n</ul>\n<ol start=\"2\">\n<li>将id_rsa.pub内容复制到github中：</li>\n</ol>\n<ul>\n<li>在github点击右上角自己的图像</li>\n<li>settings</li>\n<li>左侧SSH and GPG keys</li>\n<li>右侧：New SSH key</li>\n</ul>\n<ol start=\"3\">\n<li>在设备上提供github用户名和邮箱</li>\n</ol>\n<ul>\n<li>用户名可以在右上角头像获得</li>\n<li>又像可以通过setting -&gt; email获得<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git config --global user.name <span class=\"string\">&quot;Your Name&quot;</span></span><br><span class=\"line\">git config --global user.email <span class=\"string\">&quot;you@example.com&quot;</span></span><br></pre></td></tr></table></figure></li>\n</ul>\n<ol start=\"4\">\n<li><p>上传博客</p>\n <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git push</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>在别的设别更新</p>\n <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git pull</span><br></pre></td></tr></table></figure></li>\n</ol>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"git部署博客源码\"><a href=\"#git部署博客源码\" class=\"headerlink\" title=\"git部署博客源码\"></a>git部署博客源码</h1><ul>\n<li>在blog&#x2F;config文件里面确定了hexo s将博客部署到仓库的main分支，但是部署的是渲染后的代码，如何保存源码进行备份。</li>\n</ul>\n<h2 id=\"1-创建分支\"><a href=\"#1-创建分支\" class=\"headerlink\" title=\"1 创建分支\"></a>1 创建分支</h2><p>在blog下打开gitbash</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git init </span><br></pre></td></tr></table></figure>\n\n<p>去掉blog&#x2F;.gitignore文件中这三项：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">db.json,</span><br><span class=\"line\">Thumbs.db</span><br><span class=\"line\">node-modules/,</span><br></pre></td></tr></table></figure>\n\n<p>运行</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git add *</span><br><span class=\"line\">git commit -m &quot;first commit&quot;</span><br></pre></td></tr></table></figure>\n\n<p>创建一个source分支，上传源码（注意此处仓库地址，参考blog&#x2F;config文件里面的部署部分，是带@符号的那个名字，不是简单的博客地址）</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git checkout -b source</span><br><span class=\"line\">git remote add github 远程仓库地址</span><br><span class=\"line\">git push -u github source</span><br></pre></td></tr></table></figure>\n\n\n\n<h2 id=\"2-设置默认分支\"><a href=\"#2-设置默认分支\" class=\"headerlink\" title=\"2 设置默认分支\"></a>2 设置默认分支</h2><p>建议将source分支设置成默认分支，防止误删</p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\hexo\\config\\1.png\"></p>\n<p><img src=\"/img/hexo/config/1.png\"></p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\hexo\\config\\2.png\"></p>\n<p><img src=\"/img/hexo/config/2.png\"></p>\n<ul>\n<li><p>以后只需要执行：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git add .</span><br><span class=\"line\">git commit -m <span class=\"string\">&quot;Your commit message&quot;</span></span><br><span class=\"line\">git push -u github <span class=\"built_in\">source</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></li>\n</ul>\n<h2 id=\"3-在新设备上编写博客\"><a href=\"#3-在新设备上编写博客\" class=\"headerlink\" title=\"3 在新设备上编写博客\"></a>3 在新设备上编写博客</h2><ol>\n<li><p>在设备上生成ssh密钥</p>\n <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh-keygen</span><br></pre></td></tr></table></figure></li>\n</ol>\n<ul>\n<li>然后一路回车，会在:user&#x2F;.ssh下面有id_rsa和id_rsa.pub。</li>\n</ul>\n<ol start=\"2\">\n<li>将id_rsa.pub内容复制到github中：</li>\n</ol>\n<ul>\n<li>在github点击右上角自己的图像</li>\n<li>settings</li>\n<li>左侧SSH and GPG keys</li>\n<li>右侧：New SSH key</li>\n</ul>\n<ol start=\"3\">\n<li>在设备上提供github用户名和邮箱</li>\n</ol>\n<ul>\n<li>用户名可以在右上角头像获得</li>\n<li>又像可以通过setting -&gt; email获得<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git config --global user.name <span class=\"string\">&quot;Your Name&quot;</span></span><br><span class=\"line\">git config --global user.email <span class=\"string\">&quot;you@example.com&quot;</span></span><br></pre></td></tr></table></figure></li>\n</ul>\n<ol start=\"4\">\n<li><p>上传博客</p>\n <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git push</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>在别的设别更新</p>\n <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git pull</span><br></pre></td></tr></table></figure></li>\n</ol>"},{"title":"hexo博客关于latex公式","date":"2024-01-29T16:00:00.000Z","comment":true,"toc":true,"_content":"\n#\n\n<!--more-->\n\n- 在yilia主题文件夹下的config文件中设置\n\n  `mathjax: true`\n\n- 一些网页无法渲染的公式\n\n  在\\前_后^后都放个空格\n\n|                                                              |                                                              |\n| ------------------------------------------------------------ | ------------------------------------------------------------ |\n| 网页上无法渲染：$$ softmax(\\mathbf{X})_{ij} = \\frac{ \\exp(\\mathbf{X}_{ij}) } { \\sum_k \\exp(\\mathbf{X}_{ik}) } $$ | 在后面的下标ij的下划线前加上空格$$ softmax(\\mathbf{X})_{ij} = \\frac{ \\exp(\\mathbf{X} _{ij}) } { \\sum_k \\exp(\\mathbf{X} _{ik}) } $$ |\n| 用左括号并列的公式无法渲染：$h ^\\prime =\\left\\{ \\begin{matrix}0 &amp;p \\\\\\frac{h}{1-p} &amp;1-p \\\\\\end{matrix}\\right.$ | 使用cases，并用html换行：<span style=\"display:block\"> $h ^\\prime =\\begin{cases}0 &p \\\\ \\frac{h}{1-p} &1-p \\\\ \\end{cases}.$   </span> |\n\n- 公式无法换行：\n\n  - ```bash\n    cd blog\n    npm uninstall hexo-renderer-marked\n    npm install hexo-renderer-marked@1.0.0\n    ```\n\n  - 编辑 blog/node_modules/marked/lib/marked.js \n\n    - 第539行\n\n    - ```bash\n      escape: /^\\\\([!\"#$%&'()*+,\\-./:;<=>?@\\[\\]\\\\^_`{|}~])/,\n      改成\n      escape: /^\\\\([!\"#$%&'()*+,\\-./:;<=>?@\\[\\]^_`{|}~])/,\n      ```\n\n    - 第564行\n\n    - ```bash\n      inline._escapes = /\\\\([!\"#$%&'()*+,\\-./:;<=>?@\\[\\]\\\\^_`{|}~])/g;\n      改成\n      inline._escapes = /\\\\([!\"#$%&'()*+,\\-./:;<=>?@\\[\\]^_`{|}~])/g;\n      ```\n\n  - \n\n","source":"_posts/hexo/latex.md","raw":"---\ntitle: hexo博客关于latex公式\n\ndate: 2024-1-30\n\ntags: [博客]\n\ncategories: [博客,hexo]\n\ncomment: true\n\ntoc: true\n\n\n\n---\n\n#\n\n<!--more-->\n\n- 在yilia主题文件夹下的config文件中设置\n\n  `mathjax: true`\n\n- 一些网页无法渲染的公式\n\n  在\\前_后^后都放个空格\n\n|                                                              |                                                              |\n| ------------------------------------------------------------ | ------------------------------------------------------------ |\n| 网页上无法渲染：$$ softmax(\\mathbf{X})_{ij} = \\frac{ \\exp(\\mathbf{X}_{ij}) } { \\sum_k \\exp(\\mathbf{X}_{ik}) } $$ | 在后面的下标ij的下划线前加上空格$$ softmax(\\mathbf{X})_{ij} = \\frac{ \\exp(\\mathbf{X} _{ij}) } { \\sum_k \\exp(\\mathbf{X} _{ik}) } $$ |\n| 用左括号并列的公式无法渲染：$h ^\\prime =\\left\\{ \\begin{matrix}0 &amp;p \\\\\\frac{h}{1-p} &amp;1-p \\\\\\end{matrix}\\right.$ | 使用cases，并用html换行：<span style=\"display:block\"> $h ^\\prime =\\begin{cases}0 &p \\\\ \\frac{h}{1-p} &1-p \\\\ \\end{cases}.$   </span> |\n\n- 公式无法换行：\n\n  - ```bash\n    cd blog\n    npm uninstall hexo-renderer-marked\n    npm install hexo-renderer-marked@1.0.0\n    ```\n\n  - 编辑 blog/node_modules/marked/lib/marked.js \n\n    - 第539行\n\n    - ```bash\n      escape: /^\\\\([!\"#$%&'()*+,\\-./:;<=>?@\\[\\]\\\\^_`{|}~])/,\n      改成\n      escape: /^\\\\([!\"#$%&'()*+,\\-./:;<=>?@\\[\\]^_`{|}~])/,\n      ```\n\n    - 第564行\n\n    - ```bash\n      inline._escapes = /\\\\([!\"#$%&'()*+,\\-./:;<=>?@\\[\\]\\\\^_`{|}~])/g;\n      改成\n      inline._escapes = /\\\\([!\"#$%&'()*+,\\-./:;<=>?@\\[\\]^_`{|}~])/g;\n      ```\n\n  - \n\n","slug":"hexo/latex","published":1,"updated":"2024-04-24T13:39:13.226Z","_id":"clvuli6hk0058zsvw5rac1qsn","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<ul>\n<li><p>在yilia主题文件夹下的config文件中设置</p>\n<p><code>mathjax: true</code></p>\n</li>\n<li><p>一些网页无法渲染的公式</p>\n<p>在\\前_后^后都放个空格</p>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th></th>\n<th></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>网页上无法渲染：$$ softmax(\\mathbf{X})<em>{ij} &#x3D; \\frac{ \\exp(\\mathbf{X}</em>{ij}) } { \\sum_k \\exp(\\mathbf{X}_{ik}) } $$</td>\n<td>在后面的下标ij的下划线前加上空格$$ softmax(\\mathbf{X})_{ij} &#x3D; \\frac{ \\exp(\\mathbf{X} _{ij}) } { \\sum_k \\exp(\\mathbf{X} _{ik}) } $$</td>\n</tr>\n<tr>\n<td>用左括号并列的公式无法渲染：$h ^\\prime &#x3D;\\left{ \\begin{matrix}0 &amp;p \\\\frac{h}{1-p} &amp;1-p \\\\end{matrix}\\right.$</td>\n<td>使用cases，并用html换行：<span style=\"display:block\"> $h ^\\prime &#x3D;\\begin{cases}0 &amp;p \\ \\frac{h}{1-p} &amp;1-p \\ \\end{cases}.$   </span></td>\n</tr>\n</tbody></table>\n<ul>\n<li><p>公式无法换行：</p>\n<ul>\n<li><pre><code class=\"bash\">cd blog\nnpm uninstall hexo-renderer-marked\nnpm install hexo-renderer-marked@1.0.0\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">- 编辑 blog/node_modules/marked/lib/marked.js </span><br><span class=\"line\"></span><br><span class=\"line\">  - 第539行</span><br><span class=\"line\"></span><br><span class=\"line\">  - ```bash</span><br><span class=\"line\">    escape: /^\\\\([!&quot;#$%&amp;&#x27;()*+,\\-./:;&lt;=&gt;?@\\[\\]\\\\^_`&#123;|&#125;~])/,</span><br><span class=\"line\">    改成</span><br><span class=\"line\">    escape: /^\\\\([!&quot;#$%&amp;&#x27;()*+,\\-./:;&lt;=&gt;?@\\[\\]^_`&#123;|&#125;~])/,</span><br></pre></td></tr></table></figure>\n\n- 第564行\n\n- ```bash\n  inline._escapes = /\\\\([!&quot;#$%&amp;&#39;()*+,\\-./:;&lt;=&gt;?@\\[\\]\\\\^_`&#123;|&#125;~])/g;\n  改成\n  inline._escapes = /\\\\([!&quot;#$%&amp;&#39;()*+,\\-./:;&lt;=&gt;?@\\[\\]^_`&#123;|&#125;~])/g;\n</code></pre>\n</li>\n<li></li>\n</ul>\n</li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<ul>\n<li><p>在yilia主题文件夹下的config文件中设置</p>\n<p><code>mathjax: true</code></p>\n</li>\n<li><p>一些网页无法渲染的公式</p>\n<p>在\\前_后^后都放个空格</p>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th></th>\n<th></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>网页上无法渲染：$$ softmax(\\mathbf{X})<em>{ij} &#x3D; \\frac{ \\exp(\\mathbf{X}</em>{ij}) } { \\sum_k \\exp(\\mathbf{X}_{ik}) } $$</td>\n<td>在后面的下标ij的下划线前加上空格$$ softmax(\\mathbf{X})_{ij} &#x3D; \\frac{ \\exp(\\mathbf{X} _{ij}) } { \\sum_k \\exp(\\mathbf{X} _{ik}) } $$</td>\n</tr>\n<tr>\n<td>用左括号并列的公式无法渲染：$h ^\\prime &#x3D;\\left{ \\begin{matrix}0 &amp;p \\\\frac{h}{1-p} &amp;1-p \\\\end{matrix}\\right.$</td>\n<td>使用cases，并用html换行：<span style=\"display:block\"> $h ^\\prime &#x3D;\\begin{cases}0 &amp;p \\ \\frac{h}{1-p} &amp;1-p \\ \\end{cases}.$   </span></td>\n</tr>\n</tbody></table>\n<ul>\n<li><p>公式无法换行：</p>\n<ul>\n<li><pre><code class=\"bash\">cd blog\nnpm uninstall hexo-renderer-marked\nnpm install hexo-renderer-marked@1.0.0\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">- 编辑 blog/node_modules/marked/lib/marked.js </span><br><span class=\"line\"></span><br><span class=\"line\">  - 第539行</span><br><span class=\"line\"></span><br><span class=\"line\">  - ```bash</span><br><span class=\"line\">    escape: /^\\\\([!&quot;#$%&amp;&#x27;()*+,\\-./:;&lt;=&gt;?@\\[\\]\\\\^_`&#123;|&#125;~])/,</span><br><span class=\"line\">    改成</span><br><span class=\"line\">    escape: /^\\\\([!&quot;#$%&amp;&#x27;()*+,\\-./:;&lt;=&gt;?@\\[\\]^_`&#123;|&#125;~])/,</span><br></pre></td></tr></table></figure>\n\n- 第564行\n\n- ```bash\n  inline._escapes = /\\\\([!&quot;#$%&amp;&#39;()*+,\\-./:;&lt;=&gt;?@\\[\\]\\\\^_`&#123;|&#125;~])/g;\n  改成\n  inline._escapes = /\\\\([!&quot;#$%&amp;&#39;()*+,\\-./:;&lt;=&gt;?@\\[\\]^_`&#123;|&#125;~])/g;\n</code></pre>\n</li>\n<li></li>\n</ul>\n</li>\n</ul>"},{"title":"Linux入门-2用户和权限","date":"2021-03-03T12:00:00.000Z","toc":true,"_content":"#\n\n<!--more-->\n\n# 2 用户和权限\n\n| <a href='#su'>切换用户</a>             | su 用户名                                 |\n| -------------------------------------- | ----------------------------------------- |\n| <a href='#sudo'>管理员运行</a>         | sudo 命令                                 |\n| <a href='#groupadd'>创建用户组</a>     | groupadd 组名                             |\n| <a href='#groupdel'>删除用户组</a>     | groupdel 组名                             |\n| <a href='#useradd'>创建用户</a>        | useradd 用户名 -g 组名 -d 用户家目录      |\n| <a href='#userdel'>删除用户</a>        | userdel -r 用户名                         |\n| <a href='#id'>查看用户所属组</a>       | id 用户名                                 |\n| <a href='#usermod'>修改用户所属组</a>  | usermod -aG 组名 用户名                   |\n| <a href='#getent'>查看系统用户(组)</a> | getent passwd或group                      |\n| <a href='#chmod'>修改权限</a>          | chmod [-R] 权限 文件或文件夹              |\n| <a href='#chown'>修改所属用户</a>      | `chown [-R] [用户][:用户组] 文件或文件夹` |\n\n\n\n### 1. su（switch user）<span id='su'>命令</span>\n\nsu [-] [用户名]\n\n- 退出用户：exit或ctrl+d\n\n### 2. sudo<span id='sudo'>命令</span>（以管理员身份运行）\n\nsudo Linux命令\n\n- 需要先为用户配置sudo认证  \n  - 先进入root用户\n  - 再visudo，G到最后一行，o到末尾，再末尾输入：用户名 ALL=(ALL)      NOPASSWD: ALL\n  - 再esc，:wq退出 \n\n### 3. 用户组\n- 创建：<span id='groupadd'>groupadd</span> 组名\n- 删除：<span id='groupdel'>groupdel</span> 组名\n\n### 4. 用户\n#### 4-1创建用户\n<span id='useradd'>useradd [-g -d] 用户名</span>\n（useradd 用户名 -g 组名 -d 目录）\n\n- g 指定用户的组，不写则组与用户名同\n- d指定用户home目录位置\n#### 4-2 删除用户\n<span id='userdel'>userdel [-r] 用户名</span>\n\n- r 删除home目录\n#### 4-3 查看用户所属组<span id='id'> </span>\n\nid [用户名]\n\n#### 4-4 修改用户所属组<span id='usermod'> </span>\n\nusermod -aG 组名 用户名\n\n#### 4-5 查看系统用户(组)<span id='getent'></span>\n\ngetent passwd或group\n- 用户显示的信息：用户名:密码:用户组:描述信息:HOME目录:执行终端（默认bash)\n- 组显示信息：组名:组认证:组ID\n\n### 5 权限\n\n#### 5.1 权限信息\n\n![](./img/linux/user/8.png)\n\n![](D:\\blog\\themes\\yilia\\source\\img\\linux\\user\\8.png)\n\n- 权限信息（序号1）\n\n  总共10位：\n\n  | 类型                              | 所属用户权限 |      |      | 所属用户组权限 |      |      | 其他用户权限 |      |      |\n  | --------------------------------- | ------------ | ---- | ---- | -------------- | ---- | ---- | ------------ | ---- | ---- |\n  | -/d/l                             | r/-          | w/-  | x/-  | r/-            | w/-  | x/-  | r/-          | w/-  | x/-  |\n  | -：文件<br>d：文件夹<br>l：软连接 | 读           | 写   | 执行 |                |      |      |              |      |      |\n\n  对于文件夹：\n\n  - r：可ls里面的内容\n  - w：可以在里面创建、删除、改名\n  - x：可cd到此\n\n#### 5.2 <span id='chmod'>修改权限</span>\n\n- chmod命令\n\n  ```bash\n  chmod [-R] 权限 文件或文件夹\n  ```\n\n  - -R：递归\n\n  - 例子：\n\n    ```bash\n    chmod u=rwx,g=rx,o=x test.txt\n    chmod 751 test.txt\n    ```\n    \n    \n\n#### 5.3 <span id='chown'>修改所属用户</span>\n\n- chown命令\n\n```bash\nchown [-R] [用户][:用户组] 文件或文件夹\n```\n\n  \n\n","source":"_posts/linux/Linux_2.user.md","raw":"---\ntitle: Linux入门-2用户和权限\ndate: 2021-03-03 20:00:00\ntoc: true\ntags: [Linux]\ncategories: [Linux]\n\n---\n#\n\n<!--more-->\n\n# 2 用户和权限\n\n| <a href='#su'>切换用户</a>             | su 用户名                                 |\n| -------------------------------------- | ----------------------------------------- |\n| <a href='#sudo'>管理员运行</a>         | sudo 命令                                 |\n| <a href='#groupadd'>创建用户组</a>     | groupadd 组名                             |\n| <a href='#groupdel'>删除用户组</a>     | groupdel 组名                             |\n| <a href='#useradd'>创建用户</a>        | useradd 用户名 -g 组名 -d 用户家目录      |\n| <a href='#userdel'>删除用户</a>        | userdel -r 用户名                         |\n| <a href='#id'>查看用户所属组</a>       | id 用户名                                 |\n| <a href='#usermod'>修改用户所属组</a>  | usermod -aG 组名 用户名                   |\n| <a href='#getent'>查看系统用户(组)</a> | getent passwd或group                      |\n| <a href='#chmod'>修改权限</a>          | chmod [-R] 权限 文件或文件夹              |\n| <a href='#chown'>修改所属用户</a>      | `chown [-R] [用户][:用户组] 文件或文件夹` |\n\n\n\n### 1. su（switch user）<span id='su'>命令</span>\n\nsu [-] [用户名]\n\n- 退出用户：exit或ctrl+d\n\n### 2. sudo<span id='sudo'>命令</span>（以管理员身份运行）\n\nsudo Linux命令\n\n- 需要先为用户配置sudo认证  \n  - 先进入root用户\n  - 再visudo，G到最后一行，o到末尾，再末尾输入：用户名 ALL=(ALL)      NOPASSWD: ALL\n  - 再esc，:wq退出 \n\n### 3. 用户组\n- 创建：<span id='groupadd'>groupadd</span> 组名\n- 删除：<span id='groupdel'>groupdel</span> 组名\n\n### 4. 用户\n#### 4-1创建用户\n<span id='useradd'>useradd [-g -d] 用户名</span>\n（useradd 用户名 -g 组名 -d 目录）\n\n- g 指定用户的组，不写则组与用户名同\n- d指定用户home目录位置\n#### 4-2 删除用户\n<span id='userdel'>userdel [-r] 用户名</span>\n\n- r 删除home目录\n#### 4-3 查看用户所属组<span id='id'> </span>\n\nid [用户名]\n\n#### 4-4 修改用户所属组<span id='usermod'> </span>\n\nusermod -aG 组名 用户名\n\n#### 4-5 查看系统用户(组)<span id='getent'></span>\n\ngetent passwd或group\n- 用户显示的信息：用户名:密码:用户组:描述信息:HOME目录:执行终端（默认bash)\n- 组显示信息：组名:组认证:组ID\n\n### 5 权限\n\n#### 5.1 权限信息\n\n![](./img/linux/user/8.png)\n\n![](D:\\blog\\themes\\yilia\\source\\img\\linux\\user\\8.png)\n\n- 权限信息（序号1）\n\n  总共10位：\n\n  | 类型                              | 所属用户权限 |      |      | 所属用户组权限 |      |      | 其他用户权限 |      |      |\n  | --------------------------------- | ------------ | ---- | ---- | -------------- | ---- | ---- | ------------ | ---- | ---- |\n  | -/d/l                             | r/-          | w/-  | x/-  | r/-            | w/-  | x/-  | r/-          | w/-  | x/-  |\n  | -：文件<br>d：文件夹<br>l：软连接 | 读           | 写   | 执行 |                |      |      |              |      |      |\n\n  对于文件夹：\n\n  - r：可ls里面的内容\n  - w：可以在里面创建、删除、改名\n  - x：可cd到此\n\n#### 5.2 <span id='chmod'>修改权限</span>\n\n- chmod命令\n\n  ```bash\n  chmod [-R] 权限 文件或文件夹\n  ```\n\n  - -R：递归\n\n  - 例子：\n\n    ```bash\n    chmod u=rwx,g=rx,o=x test.txt\n    chmod 751 test.txt\n    ```\n    \n    \n\n#### 5.3 <span id='chown'>修改所属用户</span>\n\n- chown命令\n\n```bash\nchown [-R] [用户][:用户组] 文件或文件夹\n```\n\n  \n\n","slug":"linux/Linux_2.user","published":1,"updated":"2024-03-02T06:15:44.848Z","_id":"clvuli6hm005ezsvw5v1a9105","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h1 id=\"2-用户和权限\"><a href=\"#2-用户和权限\" class=\"headerlink\" title=\"2 用户和权限\"></a>2 用户和权限</h1><table>\n<thead>\n<tr>\n<th><a href='#su'>切换用户</a></th>\n<th>su 用户名</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><a href='#sudo'>管理员运行</a></td>\n<td>sudo 命令</td>\n</tr>\n<tr>\n<td><a href='#groupadd'>创建用户组</a></td>\n<td>groupadd 组名</td>\n</tr>\n<tr>\n<td><a href='#groupdel'>删除用户组</a></td>\n<td>groupdel 组名</td>\n</tr>\n<tr>\n<td><a href='#useradd'>创建用户</a></td>\n<td>useradd 用户名 -g 组名 -d 用户家目录</td>\n</tr>\n<tr>\n<td><a href='#userdel'>删除用户</a></td>\n<td>userdel -r 用户名</td>\n</tr>\n<tr>\n<td><a href='#id'>查看用户所属组</a></td>\n<td>id 用户名</td>\n</tr>\n<tr>\n<td><a href='#usermod'>修改用户所属组</a></td>\n<td>usermod -aG 组名 用户名</td>\n</tr>\n<tr>\n<td><a href='#getent'>查看系统用户(组)</a></td>\n<td>getent passwd或group</td>\n</tr>\n<tr>\n<td><a href='#chmod'>修改权限</a></td>\n<td>chmod [-R] 权限 文件或文件夹</td>\n</tr>\n<tr>\n<td><a href='#chown'>修改所属用户</a></td>\n<td><code>chown [-R] [用户][:用户组] 文件或文件夹</code></td>\n</tr>\n</tbody></table>\n<h3 id=\"1-su（switch-user）命令\"><a href=\"#1-su（switch-user）命令\" class=\"headerlink\" title=\"1. su（switch user）命令\"></a>1. su（switch user）<span id='su'>命令</span></h3><p>su [-] [用户名]</p>\n<ul>\n<li>退出用户：exit或ctrl+d</li>\n</ul>\n<h3 id=\"2-sudo命令（以管理员身份运行）\"><a href=\"#2-sudo命令（以管理员身份运行）\" class=\"headerlink\" title=\"2. sudo命令（以管理员身份运行）\"></a>2. sudo<span id='sudo'>命令</span>（以管理员身份运行）</h3><p>sudo Linux命令</p>\n<ul>\n<li>需要先为用户配置sudo认证  <ul>\n<li>先进入root用户</li>\n<li>再visudo，G到最后一行，o到末尾，再末尾输入：用户名 ALL&#x3D;(ALL)      NOPASSWD: ALL</li>\n<li>再esc，:wq退出</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"3-用户组\"><a href=\"#3-用户组\" class=\"headerlink\" title=\"3. 用户组\"></a>3. 用户组</h3><ul>\n<li>创建：<span id='groupadd'>groupadd</span> 组名</li>\n<li>删除：<span id='groupdel'>groupdel</span> 组名</li>\n</ul>\n<h3 id=\"4-用户\"><a href=\"#4-用户\" class=\"headerlink\" title=\"4. 用户\"></a>4. 用户</h3><h4 id=\"4-1创建用户\"><a href=\"#4-1创建用户\" class=\"headerlink\" title=\"4-1创建用户\"></a>4-1创建用户</h4><p><span id='useradd'>useradd [-g -d] 用户名</span><br>（useradd 用户名 -g 组名 -d 目录）</p>\n<ul>\n<li>g 指定用户的组，不写则组与用户名同</li>\n<li>d指定用户home目录位置</li>\n</ul>\n<h4 id=\"4-2-删除用户\"><a href=\"#4-2-删除用户\" class=\"headerlink\" title=\"4-2 删除用户\"></a>4-2 删除用户</h4><p><span id='userdel'>userdel [-r] 用户名</span></p>\n<ul>\n<li>r 删除home目录</li>\n</ul>\n<h4 id=\"4-3-查看用户所属组\"><a href=\"#4-3-查看用户所属组\" class=\"headerlink\" title=\"4-3 查看用户所属组 \"></a>4-3 查看用户所属组<span id='id'> </span></h4><p>id [用户名]</p>\n<h4 id=\"4-4-修改用户所属组\"><a href=\"#4-4-修改用户所属组\" class=\"headerlink\" title=\"4-4 修改用户所属组 \"></a>4-4 修改用户所属组<span id='usermod'> </span></h4><p>usermod -aG 组名 用户名</p>\n<h4 id=\"4-5-查看系统用户-组\"><a href=\"#4-5-查看系统用户-组\" class=\"headerlink\" title=\"4-5 查看系统用户(组)\"></a>4-5 查看系统用户(组)<span id='getent'></span></h4><p>getent passwd或group</p>\n<ul>\n<li>用户显示的信息：用户名:密码:用户组:描述信息:HOME目录:执行终端（默认bash)</li>\n<li>组显示信息：组名:组认证:组ID</li>\n</ul>\n<h3 id=\"5-权限\"><a href=\"#5-权限\" class=\"headerlink\" title=\"5 权限\"></a>5 权限</h3><h4 id=\"5-1-权限信息\"><a href=\"#5-1-权限信息\" class=\"headerlink\" title=\"5.1 权限信息\"></a>5.1 权限信息</h4><p><img src=\"/./img/linux/user/8.png\"></p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\linux\\user\\8.png\"></p>\n<ul>\n<li><p>权限信息（序号1）</p>\n<p>总共10位：</p>\n<table>\n<thead>\n<tr>\n<th>类型</th>\n<th>所属用户权限</th>\n<th></th>\n<th></th>\n<th>所属用户组权限</th>\n<th></th>\n<th></th>\n<th>其他用户权限</th>\n<th></th>\n<th></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>-&#x2F;d&#x2F;l</td>\n<td>r&#x2F;-</td>\n<td>w&#x2F;-</td>\n<td>x&#x2F;-</td>\n<td>r&#x2F;-</td>\n<td>w&#x2F;-</td>\n<td>x&#x2F;-</td>\n<td>r&#x2F;-</td>\n<td>w&#x2F;-</td>\n<td>x&#x2F;-</td>\n</tr>\n<tr>\n<td>-：文件<br>d：文件夹<br>l：软连接</td>\n<td>读</td>\n<td>写</td>\n<td>执行</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<p>对于文件夹：</p>\n<ul>\n<li>r：可ls里面的内容</li>\n<li>w：可以在里面创建、删除、改名</li>\n<li>x：可cd到此</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"5-2-修改权限\"><a href=\"#5-2-修改权限\" class=\"headerlink\" title=\"5.2 修改权限\"></a>5.2 <span id='chmod'>修改权限</span></h4><ul>\n<li><p>chmod命令</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">chmod</span> [-R] 权限 文件或文件夹</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p>-R：递归</p>\n</li>\n<li><p>例子：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">chmod</span> u=rwx,g=rx,o=x test.txt</span><br><span class=\"line\"><span class=\"built_in\">chmod</span> 751 test.txt</span><br></pre></td></tr></table></figure></li>\n</ul>\n</li>\n</ul>\n<h4 id=\"5-3-修改所属用户\"><a href=\"#5-3-修改所属用户\" class=\"headerlink\" title=\"5.3 修改所属用户\"></a>5.3 <span id='chown'>修改所属用户</span></h4><ul>\n<li>chown命令</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">chown</span> [-R] [用户][:用户组] 文件或文件夹</span><br></pre></td></tr></table></figure>\n\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"2-用户和权限\"><a href=\"#2-用户和权限\" class=\"headerlink\" title=\"2 用户和权限\"></a>2 用户和权限</h1><table>\n<thead>\n<tr>\n<th><a href='#su'>切换用户</a></th>\n<th>su 用户名</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><a href='#sudo'>管理员运行</a></td>\n<td>sudo 命令</td>\n</tr>\n<tr>\n<td><a href='#groupadd'>创建用户组</a></td>\n<td>groupadd 组名</td>\n</tr>\n<tr>\n<td><a href='#groupdel'>删除用户组</a></td>\n<td>groupdel 组名</td>\n</tr>\n<tr>\n<td><a href='#useradd'>创建用户</a></td>\n<td>useradd 用户名 -g 组名 -d 用户家目录</td>\n</tr>\n<tr>\n<td><a href='#userdel'>删除用户</a></td>\n<td>userdel -r 用户名</td>\n</tr>\n<tr>\n<td><a href='#id'>查看用户所属组</a></td>\n<td>id 用户名</td>\n</tr>\n<tr>\n<td><a href='#usermod'>修改用户所属组</a></td>\n<td>usermod -aG 组名 用户名</td>\n</tr>\n<tr>\n<td><a href='#getent'>查看系统用户(组)</a></td>\n<td>getent passwd或group</td>\n</tr>\n<tr>\n<td><a href='#chmod'>修改权限</a></td>\n<td>chmod [-R] 权限 文件或文件夹</td>\n</tr>\n<tr>\n<td><a href='#chown'>修改所属用户</a></td>\n<td><code>chown [-R] [用户][:用户组] 文件或文件夹</code></td>\n</tr>\n</tbody></table>\n<h3 id=\"1-su（switch-user）命令\"><a href=\"#1-su（switch-user）命令\" class=\"headerlink\" title=\"1. su（switch user）命令\"></a>1. su（switch user）<span id='su'>命令</span></h3><p>su [-] [用户名]</p>\n<ul>\n<li>退出用户：exit或ctrl+d</li>\n</ul>\n<h3 id=\"2-sudo命令（以管理员身份运行）\"><a href=\"#2-sudo命令（以管理员身份运行）\" class=\"headerlink\" title=\"2. sudo命令（以管理员身份运行）\"></a>2. sudo<span id='sudo'>命令</span>（以管理员身份运行）</h3><p>sudo Linux命令</p>\n<ul>\n<li>需要先为用户配置sudo认证  <ul>\n<li>先进入root用户</li>\n<li>再visudo，G到最后一行，o到末尾，再末尾输入：用户名 ALL&#x3D;(ALL)      NOPASSWD: ALL</li>\n<li>再esc，:wq退出</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"3-用户组\"><a href=\"#3-用户组\" class=\"headerlink\" title=\"3. 用户组\"></a>3. 用户组</h3><ul>\n<li>创建：<span id='groupadd'>groupadd</span> 组名</li>\n<li>删除：<span id='groupdel'>groupdel</span> 组名</li>\n</ul>\n<h3 id=\"4-用户\"><a href=\"#4-用户\" class=\"headerlink\" title=\"4. 用户\"></a>4. 用户</h3><h4 id=\"4-1创建用户\"><a href=\"#4-1创建用户\" class=\"headerlink\" title=\"4-1创建用户\"></a>4-1创建用户</h4><p><span id='useradd'>useradd [-g -d] 用户名</span><br>（useradd 用户名 -g 组名 -d 目录）</p>\n<ul>\n<li>g 指定用户的组，不写则组与用户名同</li>\n<li>d指定用户home目录位置</li>\n</ul>\n<h4 id=\"4-2-删除用户\"><a href=\"#4-2-删除用户\" class=\"headerlink\" title=\"4-2 删除用户\"></a>4-2 删除用户</h4><p><span id='userdel'>userdel [-r] 用户名</span></p>\n<ul>\n<li>r 删除home目录</li>\n</ul>\n<h4 id=\"4-3-查看用户所属组\"><a href=\"#4-3-查看用户所属组\" class=\"headerlink\" title=\"4-3 查看用户所属组 \"></a>4-3 查看用户所属组<span id='id'> </span></h4><p>id [用户名]</p>\n<h4 id=\"4-4-修改用户所属组\"><a href=\"#4-4-修改用户所属组\" class=\"headerlink\" title=\"4-4 修改用户所属组 \"></a>4-4 修改用户所属组<span id='usermod'> </span></h4><p>usermod -aG 组名 用户名</p>\n<h4 id=\"4-5-查看系统用户-组\"><a href=\"#4-5-查看系统用户-组\" class=\"headerlink\" title=\"4-5 查看系统用户(组)\"></a>4-5 查看系统用户(组)<span id='getent'></span></h4><p>getent passwd或group</p>\n<ul>\n<li>用户显示的信息：用户名:密码:用户组:描述信息:HOME目录:执行终端（默认bash)</li>\n<li>组显示信息：组名:组认证:组ID</li>\n</ul>\n<h3 id=\"5-权限\"><a href=\"#5-权限\" class=\"headerlink\" title=\"5 权限\"></a>5 权限</h3><h4 id=\"5-1-权限信息\"><a href=\"#5-1-权限信息\" class=\"headerlink\" title=\"5.1 权限信息\"></a>5.1 权限信息</h4><p><img src=\"/./img/linux/user/8.png\"></p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\linux\\user\\8.png\"></p>\n<ul>\n<li><p>权限信息（序号1）</p>\n<p>总共10位：</p>\n<table>\n<thead>\n<tr>\n<th>类型</th>\n<th>所属用户权限</th>\n<th></th>\n<th></th>\n<th>所属用户组权限</th>\n<th></th>\n<th></th>\n<th>其他用户权限</th>\n<th></th>\n<th></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>-&#x2F;d&#x2F;l</td>\n<td>r&#x2F;-</td>\n<td>w&#x2F;-</td>\n<td>x&#x2F;-</td>\n<td>r&#x2F;-</td>\n<td>w&#x2F;-</td>\n<td>x&#x2F;-</td>\n<td>r&#x2F;-</td>\n<td>w&#x2F;-</td>\n<td>x&#x2F;-</td>\n</tr>\n<tr>\n<td>-：文件<br>d：文件夹<br>l：软连接</td>\n<td>读</td>\n<td>写</td>\n<td>执行</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<p>对于文件夹：</p>\n<ul>\n<li>r：可ls里面的内容</li>\n<li>w：可以在里面创建、删除、改名</li>\n<li>x：可cd到此</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"5-2-修改权限\"><a href=\"#5-2-修改权限\" class=\"headerlink\" title=\"5.2 修改权限\"></a>5.2 <span id='chmod'>修改权限</span></h4><ul>\n<li><p>chmod命令</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">chmod</span> [-R] 权限 文件或文件夹</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p>-R：递归</p>\n</li>\n<li><p>例子：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">chmod</span> u=rwx,g=rx,o=x test.txt</span><br><span class=\"line\"><span class=\"built_in\">chmod</span> 751 test.txt</span><br></pre></td></tr></table></figure></li>\n</ul>\n</li>\n</ul>\n<h4 id=\"5-3-修改所属用户\"><a href=\"#5-3-修改所属用户\" class=\"headerlink\" title=\"5.3 修改所属用户\"></a>5.3 <span id='chown'>修改所属用户</span></h4><ul>\n<li>chown命令</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">chown</span> [-R] [用户][:用户组] 文件或文件夹</span><br></pre></td></tr></table></figure>"},{"title":"Linux入门-3使用操作2","date":"2024-03-02T13:00:00.000Z","toc":true,"_content":"\n#\n\n<!--more-->\n\n\n\n# 3 Linux实用操作\n\n## 3.6 ip与主机\n\n- 查看ip\n\n```bash\nifconfig #查看ip\nyum -y install net-tools #如果无法使用ifconfig\n```\n\n- 查看主机名\n\n```bash\nhostname #查看主机名\nhostnamectl set-hostname 主机名 #修改主机名\n```\n\n通过修改`     C:\\Windows\\System32\\drivers\\etc\\hosts  `（linux在`/etc/hosts`）中的配置就可以实现通过主机名访问-\n\n- 在VMware中设置固定ip\n\n  1. 在vmware中配置ip地址网关和网段\n\n     ![](D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\1.png)\n\n     ![](img/linux/usage/1.png)\n\n     ![](D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\2.png)\n\n     ![](img/linux/usage/2.png)\n\n     ![](D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\3.png)\n\n     ![](img/linux/usage/3.png)\n\n     ![](D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\4.png)\n\n     ![](img/linux/usage/4.png)\n\n  2. 在linux中手动修改配置文件\n\n     ```bash\n     vim /etc/sysconfig/network-scripts/ifcfg-ens33\n     ```\n\n     做如下修改：\n\n     ![](D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\5.png)\n\n     ![](img/linux/usage/5.png)\n\n     ```bash\n     systemctl restart network #重启网卡\n     ```\n\n## 3.7 网络传输\n\n  - ping命令\n\n    ```bash\n    ping [-c 数字] ip或主机名 #检查指定服务器是否可联通\n    ```\n\n    - -c：检查次数\n\n  - wget命令\n\n    ```bash\n    wget [-b] url #文件下载\n    tail -f wget-log #监控后台下载进度\n    ```\n\n    - -b：后台下载，会将日志写入当前工作目录的wget-log文件\n\n    - 如果下载未完成，请及时清理未完成的不可用文件\n\n  - curl命令\n\n    发送http网络请求，可用于下载文件、获取信息等\n\n    ```bash\n    curl [-O] url\n    ```\n\n    - -O：下载文件\n\n  - 端口\n\n    | 端口        | 作用                              |\n    | ----------- | --------------------------------- |\n    | 1~1023      | 公认端口                          |\n    | 1024~49151  | 注册端口，松散的绑定一些程序\\服务 |\n    | 49152~65535 | 不绑定固定程序，临时使用          |\n    |             |                                   |\n\n    - 查看端口占用\n\n      - nmap命令\n\n        ```bash\n        yum -y install nmap #安装nmap\n        nmap ip #查看端口占用\n        ```\n\n      - netstat命令\n\n        ```bash\n        yum -y install net-tools #安装netstat\n        netstat -anp | grep 端口号 #查看端口被谁占用\n        ```\n\n## 3.8 进程管理\n\n- 查看进程\n\n  ```bash\n  ps [-e -f] #查看进程信息\n  ```\n\n  - 例子\n\n    ```bash\n    ps -ef | grep tail #查找tail命令的进程信息\n    ```\n\n    \n\n  - -e：全部进程\n\n  - -f：以完全格式化的形式展示信息\n\n    | 列名  | 解释                                          |\n    | ----- | --------------------------------------------- |\n    | UID   | 用户ID                                        |\n    | PID   | 进程号                                        |\n    | PPID  | 父进程                                        |\n    | C     | cpu占用率                                     |\n    | STIME | 启动时间                                      |\n    | TTY   | 启动此进程的终端序号，如显示?，表示非终端启动 |\n    | TIME  | 占用cup时间                                   |\n    | CMD   | 进程名<br>启动路径<br>启动命令                |\n\n  - 关闭进程\n\n    ```bash\n    kill [-9] 进程ID #关闭进程\n    ```\n\n    - -9：强制关闭，不询问进程\n\n  \n\n## 3.9 主机状态\n\n- 查看系统资源占用\n\n  ```bash\n  top [-p -d -c -n -b -i -u] #查看系统资源占用\n  ```\n\n  ![](D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\6.png)\n\n  ![](img/linux/usage/6.png)\n\n  | 行   | 解释                                                         |\n  | ---- | ------------------------------------------------------------ |\n  | 1    | 时间<br>up：启动了6分钟<br>users：2个用户登录<br>load：1、5、15分钟负载 |\n  | 2    | tasks：175个进程<br>running：1个进程在运行<br>sleeping：174个在睡眠<brL>0个停止进行，0个僵尸进程 |\n  | 3    | cpu：cpu使用率<br>us：用户cpu使用率<br>sy：系统cpu使用率<br>ni：高优先级进程占用CPU时间百分比<br>id：空闲CPU率<br>wa：IO等待CPU占用率<br>hi：CPU硬件中断率<br>si：CPU软件中断率<br>st：强制等待占用CPU率 |\n  | 4    | Kib Mem：物理内存<br>total：总量<br>free：空闲<br/>used：使用<br/>buff/cache：buff和cache占用 |\n  | 5    | KibSwap：虚拟内存（交换空间）<br/>total：总量<br/>free：空闲<br/>used：使用<br/>buff/cache：buff和cache占用 |\n  | 6    | PR：进程优先级，越小越高<br>NI：负值表示高优先级，正表示低优先级<br>VIRT：进程使用虚拟内存，单位KB<br>RES：进程使用物理内存，单位KB<br/>SHR：进程使用共享内存，单位KB<br/>S：进程状态（S休眠，R运行，Z僵死状态，N负数优先级，I空闲状态）<br/>%MEM：进程占用内存率<br/>TIME+：进程使用CPU时间总计，单位10毫秒<br/>COMMAND：进程的命令或名称或程序文件路径 |\n\n  ![](D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\7.png)\n\n  ![](img/linux/usage/7.png)\n\n  - 在top界面下：\n\n    ![](D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\8.png)\n\n    ![](img/linux/usage/8.png)\n\n- 磁盘信息监控\n\n  - df命令查看磁盘使用\n\n    ```bash\n    df [-h]\n    ```\n\n    - -h显示单位\n\n  - iostat查看cpu、磁盘信息\n\n    ```bash\n    iostat [-x][数字1][数字2]\n    ```\n\n    - -x：显示更多信息\n\n    - 数字1：刷新间隔\n\n    - 数字2：刷新次数\n\n      ![](D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\9.png)\n\n      ![](img/linux/usage/9.png)\n\n      - rrqm/s： 每秒这个设备相关的读取请求有多少被Merge了（当系统调用需要读取数据的时候，VFS将请求发到各个FS，如果FS发现不同的读取请求读取的是相同Block的数据，FS会将这个请求合并Merge, 提高IO利用率, 避免重复调用）；\n\n      - wrqm/s： 每秒这个设备相关的写入请求有多少被Merge了。\n\n      - rsec/s： 每秒读取的扇区数；sectors\n\n      - wsec/： 每秒写入的扇区数。\n\n      - rKB/s： 每秒发送到设备的读取请求数\n\n      - wKB/s： 每秒发送到设备的写入请求数\n\n      - avgrq-sz  平均请求扇区的大小\n\n      - avgqu-sz  平均请求队列的长度。毫无疑问，队列长度越短越好。  \n\n      - await：  每一个IO请求的处理的平均时间（单位是微秒毫秒）。\n\n      - svctm   表示平均每次设备I/O操作的服务时间（以毫秒为单位）\n\n      - %util：  磁盘利用率\n\n- 网络状态监控\n\n  ```bash\n  sar -n DEV 数字1 数字2\n  ```\n\n  - -n：查看网络\n\n  - DEV：查看网络接口\n\n  - 数字1：刷新间隔\n\n  - 数字2：查看次数\n\n    ![](D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\10.png)\n\n    ![](img/linux/usage/10.png)\n\n    - IFACE 本地网卡接口的名称\n\n    - rxpck/s 每秒钟接受的数据包\n\n    - txpck/s 每秒钟发送的数据包\n\n    - rxKB/S 每秒钟接受的数据包大小，单位为KB\n\n    - txKB/S 每秒钟发送的数据包大小，单位为KB\n\n    - rxcmp/s 每秒钟接受的压缩数据包\n\n    - txcmp/s 每秒钟发送的压缩包\n\n    - rxmcst/s 每秒钟接收的多播数据包","source":"_posts/linux/Linux_4.usage2.md","raw":"---\ntitle: Linux入门-3使用操作2\ndate: 2024-03-02 21:00:00\ntoc: true\ntags: [Linux]\ncategories: [Linux]\n\n---\n\n#\n\n<!--more-->\n\n\n\n# 3 Linux实用操作\n\n## 3.6 ip与主机\n\n- 查看ip\n\n```bash\nifconfig #查看ip\nyum -y install net-tools #如果无法使用ifconfig\n```\n\n- 查看主机名\n\n```bash\nhostname #查看主机名\nhostnamectl set-hostname 主机名 #修改主机名\n```\n\n通过修改`     C:\\Windows\\System32\\drivers\\etc\\hosts  `（linux在`/etc/hosts`）中的配置就可以实现通过主机名访问-\n\n- 在VMware中设置固定ip\n\n  1. 在vmware中配置ip地址网关和网段\n\n     ![](D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\1.png)\n\n     ![](img/linux/usage/1.png)\n\n     ![](D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\2.png)\n\n     ![](img/linux/usage/2.png)\n\n     ![](D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\3.png)\n\n     ![](img/linux/usage/3.png)\n\n     ![](D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\4.png)\n\n     ![](img/linux/usage/4.png)\n\n  2. 在linux中手动修改配置文件\n\n     ```bash\n     vim /etc/sysconfig/network-scripts/ifcfg-ens33\n     ```\n\n     做如下修改：\n\n     ![](D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\5.png)\n\n     ![](img/linux/usage/5.png)\n\n     ```bash\n     systemctl restart network #重启网卡\n     ```\n\n## 3.7 网络传输\n\n  - ping命令\n\n    ```bash\n    ping [-c 数字] ip或主机名 #检查指定服务器是否可联通\n    ```\n\n    - -c：检查次数\n\n  - wget命令\n\n    ```bash\n    wget [-b] url #文件下载\n    tail -f wget-log #监控后台下载进度\n    ```\n\n    - -b：后台下载，会将日志写入当前工作目录的wget-log文件\n\n    - 如果下载未完成，请及时清理未完成的不可用文件\n\n  - curl命令\n\n    发送http网络请求，可用于下载文件、获取信息等\n\n    ```bash\n    curl [-O] url\n    ```\n\n    - -O：下载文件\n\n  - 端口\n\n    | 端口        | 作用                              |\n    | ----------- | --------------------------------- |\n    | 1~1023      | 公认端口                          |\n    | 1024~49151  | 注册端口，松散的绑定一些程序\\服务 |\n    | 49152~65535 | 不绑定固定程序，临时使用          |\n    |             |                                   |\n\n    - 查看端口占用\n\n      - nmap命令\n\n        ```bash\n        yum -y install nmap #安装nmap\n        nmap ip #查看端口占用\n        ```\n\n      - netstat命令\n\n        ```bash\n        yum -y install net-tools #安装netstat\n        netstat -anp | grep 端口号 #查看端口被谁占用\n        ```\n\n## 3.8 进程管理\n\n- 查看进程\n\n  ```bash\n  ps [-e -f] #查看进程信息\n  ```\n\n  - 例子\n\n    ```bash\n    ps -ef | grep tail #查找tail命令的进程信息\n    ```\n\n    \n\n  - -e：全部进程\n\n  - -f：以完全格式化的形式展示信息\n\n    | 列名  | 解释                                          |\n    | ----- | --------------------------------------------- |\n    | UID   | 用户ID                                        |\n    | PID   | 进程号                                        |\n    | PPID  | 父进程                                        |\n    | C     | cpu占用率                                     |\n    | STIME | 启动时间                                      |\n    | TTY   | 启动此进程的终端序号，如显示?，表示非终端启动 |\n    | TIME  | 占用cup时间                                   |\n    | CMD   | 进程名<br>启动路径<br>启动命令                |\n\n  - 关闭进程\n\n    ```bash\n    kill [-9] 进程ID #关闭进程\n    ```\n\n    - -9：强制关闭，不询问进程\n\n  \n\n## 3.9 主机状态\n\n- 查看系统资源占用\n\n  ```bash\n  top [-p -d -c -n -b -i -u] #查看系统资源占用\n  ```\n\n  ![](D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\6.png)\n\n  ![](img/linux/usage/6.png)\n\n  | 行   | 解释                                                         |\n  | ---- | ------------------------------------------------------------ |\n  | 1    | 时间<br>up：启动了6分钟<br>users：2个用户登录<br>load：1、5、15分钟负载 |\n  | 2    | tasks：175个进程<br>running：1个进程在运行<br>sleeping：174个在睡眠<brL>0个停止进行，0个僵尸进程 |\n  | 3    | cpu：cpu使用率<br>us：用户cpu使用率<br>sy：系统cpu使用率<br>ni：高优先级进程占用CPU时间百分比<br>id：空闲CPU率<br>wa：IO等待CPU占用率<br>hi：CPU硬件中断率<br>si：CPU软件中断率<br>st：强制等待占用CPU率 |\n  | 4    | Kib Mem：物理内存<br>total：总量<br>free：空闲<br/>used：使用<br/>buff/cache：buff和cache占用 |\n  | 5    | KibSwap：虚拟内存（交换空间）<br/>total：总量<br/>free：空闲<br/>used：使用<br/>buff/cache：buff和cache占用 |\n  | 6    | PR：进程优先级，越小越高<br>NI：负值表示高优先级，正表示低优先级<br>VIRT：进程使用虚拟内存，单位KB<br>RES：进程使用物理内存，单位KB<br/>SHR：进程使用共享内存，单位KB<br/>S：进程状态（S休眠，R运行，Z僵死状态，N负数优先级，I空闲状态）<br/>%MEM：进程占用内存率<br/>TIME+：进程使用CPU时间总计，单位10毫秒<br/>COMMAND：进程的命令或名称或程序文件路径 |\n\n  ![](D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\7.png)\n\n  ![](img/linux/usage/7.png)\n\n  - 在top界面下：\n\n    ![](D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\8.png)\n\n    ![](img/linux/usage/8.png)\n\n- 磁盘信息监控\n\n  - df命令查看磁盘使用\n\n    ```bash\n    df [-h]\n    ```\n\n    - -h显示单位\n\n  - iostat查看cpu、磁盘信息\n\n    ```bash\n    iostat [-x][数字1][数字2]\n    ```\n\n    - -x：显示更多信息\n\n    - 数字1：刷新间隔\n\n    - 数字2：刷新次数\n\n      ![](D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\9.png)\n\n      ![](img/linux/usage/9.png)\n\n      - rrqm/s： 每秒这个设备相关的读取请求有多少被Merge了（当系统调用需要读取数据的时候，VFS将请求发到各个FS，如果FS发现不同的读取请求读取的是相同Block的数据，FS会将这个请求合并Merge, 提高IO利用率, 避免重复调用）；\n\n      - wrqm/s： 每秒这个设备相关的写入请求有多少被Merge了。\n\n      - rsec/s： 每秒读取的扇区数；sectors\n\n      - wsec/： 每秒写入的扇区数。\n\n      - rKB/s： 每秒发送到设备的读取请求数\n\n      - wKB/s： 每秒发送到设备的写入请求数\n\n      - avgrq-sz  平均请求扇区的大小\n\n      - avgqu-sz  平均请求队列的长度。毫无疑问，队列长度越短越好。  \n\n      - await：  每一个IO请求的处理的平均时间（单位是微秒毫秒）。\n\n      - svctm   表示平均每次设备I/O操作的服务时间（以毫秒为单位）\n\n      - %util：  磁盘利用率\n\n- 网络状态监控\n\n  ```bash\n  sar -n DEV 数字1 数字2\n  ```\n\n  - -n：查看网络\n\n  - DEV：查看网络接口\n\n  - 数字1：刷新间隔\n\n  - 数字2：查看次数\n\n    ![](D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\10.png)\n\n    ![](img/linux/usage/10.png)\n\n    - IFACE 本地网卡接口的名称\n\n    - rxpck/s 每秒钟接受的数据包\n\n    - txpck/s 每秒钟发送的数据包\n\n    - rxKB/S 每秒钟接受的数据包大小，单位为KB\n\n    - txKB/S 每秒钟发送的数据包大小，单位为KB\n\n    - rxcmp/s 每秒钟接受的压缩数据包\n\n    - txcmp/s 每秒钟发送的压缩包\n\n    - rxmcst/s 每秒钟接收的多播数据包","slug":"linux/Linux_4.usage2","published":1,"updated":"2024-03-02T12:47:26.719Z","_id":"clvuli6hn005fzsvwba8z83va","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n\n\n<h1 id=\"3-Linux实用操作\"><a href=\"#3-Linux实用操作\" class=\"headerlink\" title=\"3 Linux实用操作\"></a>3 Linux实用操作</h1><h2 id=\"3-6-ip与主机\"><a href=\"#3-6-ip与主机\" class=\"headerlink\" title=\"3.6 ip与主机\"></a>3.6 ip与主机</h2><ul>\n<li>查看ip</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ifconfig <span class=\"comment\">#查看ip</span></span><br><span class=\"line\">yum -y install net-tools <span class=\"comment\">#如果无法使用ifconfig</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>查看主机名</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hostname <span class=\"comment\">#查看主机名</span></span><br><span class=\"line\">hostnamectl set-hostname 主机名 <span class=\"comment\">#修改主机名</span></span><br></pre></td></tr></table></figure>\n\n<p>通过修改<code>    C:\\Windows\\System32\\drivers\\etc\\hosts </code>（linux在<code>/etc/hosts</code>）中的配置就可以实现通过主机名访问-</p>\n<ul>\n<li><p>在VMware中设置固定ip</p>\n<ol>\n<li><p>在vmware中配置ip地址网关和网段</p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\1.png\"></p>\n<p><img src=\"/img/linux/usage/1.png\"></p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\2.png\"></p>\n<p><img src=\"/img/linux/usage/2.png\"></p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\3.png\"></p>\n<p><img src=\"/img/linux/usage/3.png\"></p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\4.png\"></p>\n<p><img src=\"/img/linux/usage/4.png\"></p>\n</li>\n<li><p>在linux中手动修改配置文件</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vim /etc/sysconfig/network-scripts/ifcfg-ens33</span><br></pre></td></tr></table></figure>\n\n<p>做如下修改：</p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\5.png\"></p>\n<p><img src=\"/img/linux/usage/5.png\"></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl restart network <span class=\"comment\">#重启网卡</span></span><br></pre></td></tr></table></figure></li>\n</ol>\n</li>\n</ul>\n<h2 id=\"3-7-网络传输\"><a href=\"#3-7-网络传输\" class=\"headerlink\" title=\"3.7 网络传输\"></a>3.7 网络传输</h2><ul>\n<li><p>ping命令</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ping [-c 数字] ip或主机名 <span class=\"comment\">#检查指定服务器是否可联通</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>-c：检查次数</li>\n</ul>\n</li>\n<li><p>wget命令</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget [-b] url <span class=\"comment\">#文件下载</span></span><br><span class=\"line\"><span class=\"built_in\">tail</span> -f wget-log <span class=\"comment\">#监控后台下载进度</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p>-b：后台下载，会将日志写入当前工作目录的wget-log文件</p>\n</li>\n<li><p>如果下载未完成，请及时清理未完成的不可用文件</p>\n</li>\n</ul>\n</li>\n<li><p>curl命令</p>\n<p>发送http网络请求，可用于下载文件、获取信息等</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl [-O] url</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>-O：下载文件</li>\n</ul>\n</li>\n<li><p>端口</p>\n<table>\n<thead>\n<tr>\n<th>端口</th>\n<th>作用</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1~1023</td>\n<td>公认端口</td>\n</tr>\n<tr>\n<td>1024~49151</td>\n<td>注册端口，松散的绑定一些程序\\服务</td>\n</tr>\n<tr>\n<td>49152~65535</td>\n<td>不绑定固定程序，临时使用</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<ul>\n<li><p>查看端口占用</p>\n<ul>\n<li><p>nmap命令</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum -y install nmap <span class=\"comment\">#安装nmap</span></span><br><span class=\"line\">nmap ip <span class=\"comment\">#查看端口占用</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>netstat命令</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum -y install net-tools <span class=\"comment\">#安装netstat</span></span><br><span class=\"line\">netstat -anp | grep 端口号 <span class=\"comment\">#查看端口被谁占用</span></span><br></pre></td></tr></table></figure></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"3-8-进程管理\"><a href=\"#3-8-进程管理\" class=\"headerlink\" title=\"3.8 进程管理\"></a>3.8 进程管理</h2><ul>\n<li><p>查看进程</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ps [-e -f] <span class=\"comment\">#查看进程信息</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p>例子</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ps -ef | grep <span class=\"built_in\">tail</span> <span class=\"comment\">#查找tail命令的进程信息</span></span><br></pre></td></tr></table></figure>\n\n\n</li>\n<li><p>-e：全部进程</p>\n</li>\n<li><p>-f：以完全格式化的形式展示信息</p>\n<table>\n<thead>\n<tr>\n<th>列名</th>\n<th>解释</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>UID</td>\n<td>用户ID</td>\n</tr>\n<tr>\n<td>PID</td>\n<td>进程号</td>\n</tr>\n<tr>\n<td>PPID</td>\n<td>父进程</td>\n</tr>\n<tr>\n<td>C</td>\n<td>cpu占用率</td>\n</tr>\n<tr>\n<td>STIME</td>\n<td>启动时间</td>\n</tr>\n<tr>\n<td>TTY</td>\n<td>启动此进程的终端序号，如显示?，表示非终端启动</td>\n</tr>\n<tr>\n<td>TIME</td>\n<td>占用cup时间</td>\n</tr>\n<tr>\n<td>CMD</td>\n<td>进程名<br>启动路径<br>启动命令</td>\n</tr>\n</tbody></table>\n</li>\n<li><p>关闭进程</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">kill</span> [-9] 进程ID <span class=\"comment\">#关闭进程</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>-9：强制关闭，不询问进程</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"3-9-主机状态\"><a href=\"#3-9-主机状态\" class=\"headerlink\" title=\"3.9 主机状态\"></a>3.9 主机状态</h2><ul>\n<li><p>查看系统资源占用</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">top [-p -d -c -n -b -i -u] <span class=\"comment\">#查看系统资源占用</span></span><br></pre></td></tr></table></figure>\n\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\6.png\"></p>\n<p><img src=\"/img/linux/usage/6.png\"></p>\n<table>\n<thead>\n<tr>\n<th>行</th>\n<th>解释</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1</td>\n<td>时间<br>up：启动了6分钟<br>users：2个用户登录<br>load：1、5、15分钟负载</td>\n</tr>\n<tr>\n<td>2</td>\n<td>tasks：175个进程<br>running：1个进程在运行<br>sleeping：174个在睡眠<brL>0个停止进行，0个僵尸进程</td>\n</tr>\n<tr>\n<td>3</td>\n<td>cpu：cpu使用率<br>us：用户cpu使用率<br>sy：系统cpu使用率<br>ni：高优先级进程占用CPU时间百分比<br>id：空闲CPU率<br>wa：IO等待CPU占用率<br>hi：CPU硬件中断率<br>si：CPU软件中断率<br>st：强制等待占用CPU率</td>\n</tr>\n<tr>\n<td>4</td>\n<td>Kib Mem：物理内存<br>total：总量<br>free：空闲<br/>used：使用<br/>buff&#x2F;cache：buff和cache占用</td>\n</tr>\n<tr>\n<td>5</td>\n<td>KibSwap：虚拟内存（交换空间）<br/>total：总量<br/>free：空闲<br/>used：使用<br/>buff&#x2F;cache：buff和cache占用</td>\n</tr>\n<tr>\n<td>6</td>\n<td>PR：进程优先级，越小越高<br>NI：负值表示高优先级，正表示低优先级<br>VIRT：进程使用虚拟内存，单位KB<br>RES：进程使用物理内存，单位KB<br/>SHR：进程使用共享内存，单位KB<br/>S：进程状态（S休眠，R运行，Z僵死状态，N负数优先级，I空闲状态）<br/>%MEM：进程占用内存率<br/>TIME+：进程使用CPU时间总计，单位10毫秒<br/>COMMAND：进程的命令或名称或程序文件路径</td>\n</tr>\n</tbody></table>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\7.png\"></p>\n<p><img src=\"/img/linux/usage/7.png\"></p>\n<ul>\n<li><p>在top界面下：</p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\8.png\"></p>\n<p><img src=\"/img/linux/usage/8.png\"></p>\n</li>\n</ul>\n</li>\n<li><p>磁盘信息监控</p>\n<ul>\n<li><p>df命令查看磁盘使用</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">df</span> [-h]</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>-h显示单位</li>\n</ul>\n</li>\n<li><p>iostat查看cpu、磁盘信息</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">iostat [-x][数字1][数字2]</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p>-x：显示更多信息</p>\n</li>\n<li><p>数字1：刷新间隔</p>\n</li>\n<li><p>数字2：刷新次数</p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\9.png\"></p>\n<p><img src=\"/img/linux/usage/9.png\"></p>\n<ul>\n<li><p>rrqm&#x2F;s： 每秒这个设备相关的读取请求有多少被Merge了（当系统调用需要读取数据的时候，VFS将请求发到各个FS，如果FS发现不同的读取请求读取的是相同Block的数据，FS会将这个请求合并Merge, 提高IO利用率, 避免重复调用）；</p>\n</li>\n<li><p>wrqm&#x2F;s： 每秒这个设备相关的写入请求有多少被Merge了。</p>\n</li>\n<li><p>rsec&#x2F;s： 每秒读取的扇区数；sectors</p>\n</li>\n<li><p>wsec&#x2F;： 每秒写入的扇区数。</p>\n</li>\n<li><p>rKB&#x2F;s： 每秒发送到设备的读取请求数</p>\n</li>\n<li><p>wKB&#x2F;s： 每秒发送到设备的写入请求数</p>\n</li>\n<li><p>avgrq-sz  平均请求扇区的大小</p>\n</li>\n<li><p>avgqu-sz  平均请求队列的长度。毫无疑问，队列长度越短越好。  </p>\n</li>\n<li><p>await：  每一个IO请求的处理的平均时间（单位是微秒毫秒）。</p>\n</li>\n<li><p>svctm   表示平均每次设备I&#x2F;O操作的服务时间（以毫秒为单位）</p>\n</li>\n<li><p>%util：  磁盘利用率</p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>网络状态监控</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sar -n DEV 数字1 数字2</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p>-n：查看网络</p>\n</li>\n<li><p>DEV：查看网络接口</p>\n</li>\n<li><p>数字1：刷新间隔</p>\n</li>\n<li><p>数字2：查看次数</p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\10.png\"></p>\n<p><img src=\"/img/linux/usage/10.png\"></p>\n<ul>\n<li><p>IFACE 本地网卡接口的名称</p>\n</li>\n<li><p>rxpck&#x2F;s 每秒钟接受的数据包</p>\n</li>\n<li><p>txpck&#x2F;s 每秒钟发送的数据包</p>\n</li>\n<li><p>rxKB&#x2F;S 每秒钟接受的数据包大小，单位为KB</p>\n</li>\n<li><p>txKB&#x2F;S 每秒钟发送的数据包大小，单位为KB</p>\n</li>\n<li><p>rxcmp&#x2F;s 每秒钟接受的压缩数据包</p>\n</li>\n<li><p>txcmp&#x2F;s 每秒钟发送的压缩包</p>\n</li>\n<li><p>rxmcst&#x2F;s 每秒钟接收的多播数据包</p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"3-Linux实用操作\"><a href=\"#3-Linux实用操作\" class=\"headerlink\" title=\"3 Linux实用操作\"></a>3 Linux实用操作</h1><h2 id=\"3-6-ip与主机\"><a href=\"#3-6-ip与主机\" class=\"headerlink\" title=\"3.6 ip与主机\"></a>3.6 ip与主机</h2><ul>\n<li>查看ip</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ifconfig <span class=\"comment\">#查看ip</span></span><br><span class=\"line\">yum -y install net-tools <span class=\"comment\">#如果无法使用ifconfig</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>查看主机名</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hostname <span class=\"comment\">#查看主机名</span></span><br><span class=\"line\">hostnamectl set-hostname 主机名 <span class=\"comment\">#修改主机名</span></span><br></pre></td></tr></table></figure>\n\n<p>通过修改<code>    C:\\Windows\\System32\\drivers\\etc\\hosts </code>（linux在<code>/etc/hosts</code>）中的配置就可以实现通过主机名访问-</p>\n<ul>\n<li><p>在VMware中设置固定ip</p>\n<ol>\n<li><p>在vmware中配置ip地址网关和网段</p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\1.png\"></p>\n<p><img src=\"/img/linux/usage/1.png\"></p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\2.png\"></p>\n<p><img src=\"/img/linux/usage/2.png\"></p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\3.png\"></p>\n<p><img src=\"/img/linux/usage/3.png\"></p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\4.png\"></p>\n<p><img src=\"/img/linux/usage/4.png\"></p>\n</li>\n<li><p>在linux中手动修改配置文件</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vim /etc/sysconfig/network-scripts/ifcfg-ens33</span><br></pre></td></tr></table></figure>\n\n<p>做如下修改：</p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\5.png\"></p>\n<p><img src=\"/img/linux/usage/5.png\"></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl restart network <span class=\"comment\">#重启网卡</span></span><br></pre></td></tr></table></figure></li>\n</ol>\n</li>\n</ul>\n<h2 id=\"3-7-网络传输\"><a href=\"#3-7-网络传输\" class=\"headerlink\" title=\"3.7 网络传输\"></a>3.7 网络传输</h2><ul>\n<li><p>ping命令</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ping [-c 数字] ip或主机名 <span class=\"comment\">#检查指定服务器是否可联通</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>-c：检查次数</li>\n</ul>\n</li>\n<li><p>wget命令</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget [-b] url <span class=\"comment\">#文件下载</span></span><br><span class=\"line\"><span class=\"built_in\">tail</span> -f wget-log <span class=\"comment\">#监控后台下载进度</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p>-b：后台下载，会将日志写入当前工作目录的wget-log文件</p>\n</li>\n<li><p>如果下载未完成，请及时清理未完成的不可用文件</p>\n</li>\n</ul>\n</li>\n<li><p>curl命令</p>\n<p>发送http网络请求，可用于下载文件、获取信息等</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl [-O] url</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>-O：下载文件</li>\n</ul>\n</li>\n<li><p>端口</p>\n<table>\n<thead>\n<tr>\n<th>端口</th>\n<th>作用</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1~1023</td>\n<td>公认端口</td>\n</tr>\n<tr>\n<td>1024~49151</td>\n<td>注册端口，松散的绑定一些程序\\服务</td>\n</tr>\n<tr>\n<td>49152~65535</td>\n<td>不绑定固定程序，临时使用</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<ul>\n<li><p>查看端口占用</p>\n<ul>\n<li><p>nmap命令</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum -y install nmap <span class=\"comment\">#安装nmap</span></span><br><span class=\"line\">nmap ip <span class=\"comment\">#查看端口占用</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>netstat命令</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum -y install net-tools <span class=\"comment\">#安装netstat</span></span><br><span class=\"line\">netstat -anp | grep 端口号 <span class=\"comment\">#查看端口被谁占用</span></span><br></pre></td></tr></table></figure></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"3-8-进程管理\"><a href=\"#3-8-进程管理\" class=\"headerlink\" title=\"3.8 进程管理\"></a>3.8 进程管理</h2><ul>\n<li><p>查看进程</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ps [-e -f] <span class=\"comment\">#查看进程信息</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p>例子</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ps -ef | grep <span class=\"built_in\">tail</span> <span class=\"comment\">#查找tail命令的进程信息</span></span><br></pre></td></tr></table></figure>\n\n\n</li>\n<li><p>-e：全部进程</p>\n</li>\n<li><p>-f：以完全格式化的形式展示信息</p>\n<table>\n<thead>\n<tr>\n<th>列名</th>\n<th>解释</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>UID</td>\n<td>用户ID</td>\n</tr>\n<tr>\n<td>PID</td>\n<td>进程号</td>\n</tr>\n<tr>\n<td>PPID</td>\n<td>父进程</td>\n</tr>\n<tr>\n<td>C</td>\n<td>cpu占用率</td>\n</tr>\n<tr>\n<td>STIME</td>\n<td>启动时间</td>\n</tr>\n<tr>\n<td>TTY</td>\n<td>启动此进程的终端序号，如显示?，表示非终端启动</td>\n</tr>\n<tr>\n<td>TIME</td>\n<td>占用cup时间</td>\n</tr>\n<tr>\n<td>CMD</td>\n<td>进程名<br>启动路径<br>启动命令</td>\n</tr>\n</tbody></table>\n</li>\n<li><p>关闭进程</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">kill</span> [-9] 进程ID <span class=\"comment\">#关闭进程</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>-9：强制关闭，不询问进程</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"3-9-主机状态\"><a href=\"#3-9-主机状态\" class=\"headerlink\" title=\"3.9 主机状态\"></a>3.9 主机状态</h2><ul>\n<li><p>查看系统资源占用</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">top [-p -d -c -n -b -i -u] <span class=\"comment\">#查看系统资源占用</span></span><br></pre></td></tr></table></figure>\n\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\6.png\"></p>\n<p><img src=\"/img/linux/usage/6.png\"></p>\n<table>\n<thead>\n<tr>\n<th>行</th>\n<th>解释</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1</td>\n<td>时间<br>up：启动了6分钟<br>users：2个用户登录<br>load：1、5、15分钟负载</td>\n</tr>\n<tr>\n<td>2</td>\n<td>tasks：175个进程<br>running：1个进程在运行<br>sleeping：174个在睡眠<brL>0个停止进行，0个僵尸进程</td>\n</tr>\n<tr>\n<td>3</td>\n<td>cpu：cpu使用率<br>us：用户cpu使用率<br>sy：系统cpu使用率<br>ni：高优先级进程占用CPU时间百分比<br>id：空闲CPU率<br>wa：IO等待CPU占用率<br>hi：CPU硬件中断率<br>si：CPU软件中断率<br>st：强制等待占用CPU率</td>\n</tr>\n<tr>\n<td>4</td>\n<td>Kib Mem：物理内存<br>total：总量<br>free：空闲<br/>used：使用<br/>buff&#x2F;cache：buff和cache占用</td>\n</tr>\n<tr>\n<td>5</td>\n<td>KibSwap：虚拟内存（交换空间）<br/>total：总量<br/>free：空闲<br/>used：使用<br/>buff&#x2F;cache：buff和cache占用</td>\n</tr>\n<tr>\n<td>6</td>\n<td>PR：进程优先级，越小越高<br>NI：负值表示高优先级，正表示低优先级<br>VIRT：进程使用虚拟内存，单位KB<br>RES：进程使用物理内存，单位KB<br/>SHR：进程使用共享内存，单位KB<br/>S：进程状态（S休眠，R运行，Z僵死状态，N负数优先级，I空闲状态）<br/>%MEM：进程占用内存率<br/>TIME+：进程使用CPU时间总计，单位10毫秒<br/>COMMAND：进程的命令或名称或程序文件路径</td>\n</tr>\n</tbody></table>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\7.png\"></p>\n<p><img src=\"/img/linux/usage/7.png\"></p>\n<ul>\n<li><p>在top界面下：</p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\8.png\"></p>\n<p><img src=\"/img/linux/usage/8.png\"></p>\n</li>\n</ul>\n</li>\n<li><p>磁盘信息监控</p>\n<ul>\n<li><p>df命令查看磁盘使用</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">df</span> [-h]</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>-h显示单位</li>\n</ul>\n</li>\n<li><p>iostat查看cpu、磁盘信息</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">iostat [-x][数字1][数字2]</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p>-x：显示更多信息</p>\n</li>\n<li><p>数字1：刷新间隔</p>\n</li>\n<li><p>数字2：刷新次数</p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\9.png\"></p>\n<p><img src=\"/img/linux/usage/9.png\"></p>\n<ul>\n<li><p>rrqm&#x2F;s： 每秒这个设备相关的读取请求有多少被Merge了（当系统调用需要读取数据的时候，VFS将请求发到各个FS，如果FS发现不同的读取请求读取的是相同Block的数据，FS会将这个请求合并Merge, 提高IO利用率, 避免重复调用）；</p>\n</li>\n<li><p>wrqm&#x2F;s： 每秒这个设备相关的写入请求有多少被Merge了。</p>\n</li>\n<li><p>rsec&#x2F;s： 每秒读取的扇区数；sectors</p>\n</li>\n<li><p>wsec&#x2F;： 每秒写入的扇区数。</p>\n</li>\n<li><p>rKB&#x2F;s： 每秒发送到设备的读取请求数</p>\n</li>\n<li><p>wKB&#x2F;s： 每秒发送到设备的写入请求数</p>\n</li>\n<li><p>avgrq-sz  平均请求扇区的大小</p>\n</li>\n<li><p>avgqu-sz  平均请求队列的长度。毫无疑问，队列长度越短越好。  </p>\n</li>\n<li><p>await：  每一个IO请求的处理的平均时间（单位是微秒毫秒）。</p>\n</li>\n<li><p>svctm   表示平均每次设备I&#x2F;O操作的服务时间（以毫秒为单位）</p>\n</li>\n<li><p>%util：  磁盘利用率</p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>网络状态监控</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sar -n DEV 数字1 数字2</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p>-n：查看网络</p>\n</li>\n<li><p>DEV：查看网络接口</p>\n</li>\n<li><p>数字1：刷新间隔</p>\n</li>\n<li><p>数字2：查看次数</p>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\linux\\usage\\10.png\"></p>\n<p><img src=\"/img/linux/usage/10.png\"></p>\n<ul>\n<li><p>IFACE 本地网卡接口的名称</p>\n</li>\n<li><p>rxpck&#x2F;s 每秒钟接受的数据包</p>\n</li>\n<li><p>txpck&#x2F;s 每秒钟发送的数据包</p>\n</li>\n<li><p>rxKB&#x2F;S 每秒钟接受的数据包大小，单位为KB</p>\n</li>\n<li><p>txKB&#x2F;S 每秒钟发送的数据包大小，单位为KB</p>\n</li>\n<li><p>rxcmp&#x2F;s 每秒钟接受的压缩数据包</p>\n</li>\n<li><p>txcmp&#x2F;s 每秒钟发送的压缩包</p>\n</li>\n<li><p>rxmcst&#x2F;s 每秒钟接收的多播数据包</p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>"},{"title":"Linux入门-1基础命令","date":"2021-03-02T12:00:00.000Z","toc":true,"_content":"#\n\n<!--more-->\n\n# 1 基础命令\n\n| 显示                   | ls -alh 路径                             |\n| ---------------------- | ---------------------------------------- |\n| 打开路径               | <a href='#cd'>cd 路径</a>                |\n| 显示工作路径           | <a href='#pwd'>pwd</a>                    |\n| 创建文件夹             | <a href='#mkdir'>mkdir -p 路径</a>          |\n| 创建文件               | <a href='#touch'>touch 路径</a>             |\n| 查看文件               | <a href='#cat'>cat 路径</a>               |\n| 分页查看               | <a href='#more'>more 路径  </a>            |\n| 复制                   | <a href='#cp'>cp -r 源 目的  </a>        |\n| 移动文件               | <a href='#mv'>mv 源 目的（可重命名</a>） |\n| 删除文件               | <a href='#rm'>rm -rf 路径</a>            |\n| 文件中通过关键字搜索行 | <a href='#grep'>grep -n 关键字 路径 </a>   |\n| 文件内容统计           | <a href='#wc'>wc -cmlw 路径   </a>       |\n| 查看命令源文件         | <a href='#which'>which 命令 </a>            |\n| 搜索文件               | <a href='#find'>find 起始路径 -name \"test\"  </a> |\n| 打印                   | <a href='#echo'>echo</a>                   |\n| 从尾部查看             | <a href='#tail'>tail -f -数字 路径  </a>   |\n| 编辑文件               | <a href='#vi'>vi 路径</a>                |\n\n\n\n## 1. 目录结构\n\n根目录：/\n/home/hellow/test.txt\n\n## 2. 小tip\n\n### 2-1 特殊符号\n\n- . 当前目录\n\n- .. 上级目录\n\n- ~ home\n- `作为命令执行\n### 2-2 进入root用户\n\n- 进入\n\nsu - root\n\n123456\n\n- 退出\nexit\n### 2-3管道符\n\n|\ncat 1.txt | grep \"hello\"\n\n- 左边结果作为右边的输入\n\n### 2-4重定向符\n\\>：右边文件内容清空，左边内容写入文件\n\n\\>>：追加\n## 3. 命令\n命令 [选项] [参数]\n### <span id='ls'>1. ls命令（显示内容）</span>\n\nls [-a -l -h] [路径]\n- a: all，列出全部文件（包括隐藏文件）\n- l: list，以列表展现（权限，用户和用户组，大小，创建日期）\n- h: 以易于阅读的形式，列出文件大小（搭配-l)\n- 组合使用：如-la\n\n### 2. <span id='cd'>cd命令（change directory）</span>\n\ncd [路径]\n\n- cd 回到用户HOME目录\n- cd ../.. 返回上两级\n- cd 路径\n### <span id='pwd'>3. pwd命令（print work directory）</span>\n\npwd\n### 4. <span id='mkdir'>mkdir命令（make directory）</span>\n\nmkdir [-p] 路径\n\n- 多层创建\n### 5. <span id='touch'>touch命令（创建文件）</span>\n\ntouch 路径\n### 6. <span id='cat'>cat命令（查看文件）</span>\n\ncat 路径\n### <span id='more'>7. more命令（查看文件）</span>\n\nmore 路径（分页看，空格下一页，q退出）\n### 8. <span id='cp'>cp命令（copy）</span>\n\ncp [-r] 源路径 目的地\n- r 递归，用于文件夹的复制\n### 9. <span id='mv'>mv命令（move）</span>\n\nmv 源路径 目的地/（可重命名）\n### 10. <span id='rm'>rm命令（remove）</span>\n\nrm [-r -f] 参数1 参数2...\n\n- r 递归，用于删除文件夹\n- f (force)强制删除\n- 支持通配符：test* 表示以test开头\n### 11. <span id='grep'>grep命令（文件中通过关键字搜索行）</span>\n\ngrep [-n] \"关键字\" 文件路径\n\n- n：结果中显示行号\n- 关键字：用于查找\n- 路径：可做输入端口\n### 12. <span id='wc'>wc命令（文件内容统计）</span>\n\nwc [-cmlw] 文件路径\n- c：统计bytes数量\n- m：统计字符数量\n- l：统计行数\n- w：统计单词数\n- 路径：可做输入端口\n### 13. <span id='which'>which命令（查看命令的源文件）</span>\n\nwhich Linux命令\n### 14. <span id='find'>find命令（文件搜索）</span>\n\nfind 起始路径 -name \"test*\"\nfind 起始路径 -size +100k(大于100kb)\n\n### 15. <span id='echo'>echo命令（打印）</span>\n\necho 输出内容\n- echo `pwd：'的作用：将pwd作为命令执行\n### 16. <span id='tail'>tail命令（从尾部查看）</span>\n\ntail [-f -数字] 路径\n- f 持续跟踪(会实时刷新)\n- 数字 查看多少行（默认10）\n\n## 4. <span id='vi'>vim编辑器</span>\n\nvi 文件路径\n\nvim 文件路径\n\n- vim是vi的升级版  \n\n![](./img/linux/commend/4.png)\n![](./img/linux/commend/5.png)\n![](./img/linux/commend/6.png)\n![](./img/linux/commend/7.png)\n![](../../../themes/yilia/source/img/linux/commend/4.png)\n![](../../../themes/yilia/source/img/linux/commend/5.png)\n![](../../../themes/yilia/source/img/linux/commend/6.png)\n![](../../../themes/yilia/source/img/linux/commend/7.png)\n\n## 5. <span id='help'>帮助</span>\n\n命令 --help\n- 查看用法\n\nman 命令\n\n- 操作手册","source":"_posts/linux/Linux_1.command.md","raw":"---\ntitle: Linux入门-1基础命令\ndate: 2021-03-02 20:00:00\ntoc: true\ntags: [Linux]\ncategories: [Linux]\n---\n#\n\n<!--more-->\n\n# 1 基础命令\n\n| 显示                   | ls -alh 路径                             |\n| ---------------------- | ---------------------------------------- |\n| 打开路径               | <a href='#cd'>cd 路径</a>                |\n| 显示工作路径           | <a href='#pwd'>pwd</a>                    |\n| 创建文件夹             | <a href='#mkdir'>mkdir -p 路径</a>          |\n| 创建文件               | <a href='#touch'>touch 路径</a>             |\n| 查看文件               | <a href='#cat'>cat 路径</a>               |\n| 分页查看               | <a href='#more'>more 路径  </a>            |\n| 复制                   | <a href='#cp'>cp -r 源 目的  </a>        |\n| 移动文件               | <a href='#mv'>mv 源 目的（可重命名</a>） |\n| 删除文件               | <a href='#rm'>rm -rf 路径</a>            |\n| 文件中通过关键字搜索行 | <a href='#grep'>grep -n 关键字 路径 </a>   |\n| 文件内容统计           | <a href='#wc'>wc -cmlw 路径   </a>       |\n| 查看命令源文件         | <a href='#which'>which 命令 </a>            |\n| 搜索文件               | <a href='#find'>find 起始路径 -name \"test\"  </a> |\n| 打印                   | <a href='#echo'>echo</a>                   |\n| 从尾部查看             | <a href='#tail'>tail -f -数字 路径  </a>   |\n| 编辑文件               | <a href='#vi'>vi 路径</a>                |\n\n\n\n## 1. 目录结构\n\n根目录：/\n/home/hellow/test.txt\n\n## 2. 小tip\n\n### 2-1 特殊符号\n\n- . 当前目录\n\n- .. 上级目录\n\n- ~ home\n- `作为命令执行\n### 2-2 进入root用户\n\n- 进入\n\nsu - root\n\n123456\n\n- 退出\nexit\n### 2-3管道符\n\n|\ncat 1.txt | grep \"hello\"\n\n- 左边结果作为右边的输入\n\n### 2-4重定向符\n\\>：右边文件内容清空，左边内容写入文件\n\n\\>>：追加\n## 3. 命令\n命令 [选项] [参数]\n### <span id='ls'>1. ls命令（显示内容）</span>\n\nls [-a -l -h] [路径]\n- a: all，列出全部文件（包括隐藏文件）\n- l: list，以列表展现（权限，用户和用户组，大小，创建日期）\n- h: 以易于阅读的形式，列出文件大小（搭配-l)\n- 组合使用：如-la\n\n### 2. <span id='cd'>cd命令（change directory）</span>\n\ncd [路径]\n\n- cd 回到用户HOME目录\n- cd ../.. 返回上两级\n- cd 路径\n### <span id='pwd'>3. pwd命令（print work directory）</span>\n\npwd\n### 4. <span id='mkdir'>mkdir命令（make directory）</span>\n\nmkdir [-p] 路径\n\n- 多层创建\n### 5. <span id='touch'>touch命令（创建文件）</span>\n\ntouch 路径\n### 6. <span id='cat'>cat命令（查看文件）</span>\n\ncat 路径\n### <span id='more'>7. more命令（查看文件）</span>\n\nmore 路径（分页看，空格下一页，q退出）\n### 8. <span id='cp'>cp命令（copy）</span>\n\ncp [-r] 源路径 目的地\n- r 递归，用于文件夹的复制\n### 9. <span id='mv'>mv命令（move）</span>\n\nmv 源路径 目的地/（可重命名）\n### 10. <span id='rm'>rm命令（remove）</span>\n\nrm [-r -f] 参数1 参数2...\n\n- r 递归，用于删除文件夹\n- f (force)强制删除\n- 支持通配符：test* 表示以test开头\n### 11. <span id='grep'>grep命令（文件中通过关键字搜索行）</span>\n\ngrep [-n] \"关键字\" 文件路径\n\n- n：结果中显示行号\n- 关键字：用于查找\n- 路径：可做输入端口\n### 12. <span id='wc'>wc命令（文件内容统计）</span>\n\nwc [-cmlw] 文件路径\n- c：统计bytes数量\n- m：统计字符数量\n- l：统计行数\n- w：统计单词数\n- 路径：可做输入端口\n### 13. <span id='which'>which命令（查看命令的源文件）</span>\n\nwhich Linux命令\n### 14. <span id='find'>find命令（文件搜索）</span>\n\nfind 起始路径 -name \"test*\"\nfind 起始路径 -size +100k(大于100kb)\n\n### 15. <span id='echo'>echo命令（打印）</span>\n\necho 输出内容\n- echo `pwd：'的作用：将pwd作为命令执行\n### 16. <span id='tail'>tail命令（从尾部查看）</span>\n\ntail [-f -数字] 路径\n- f 持续跟踪(会实时刷新)\n- 数字 查看多少行（默认10）\n\n## 4. <span id='vi'>vim编辑器</span>\n\nvi 文件路径\n\nvim 文件路径\n\n- vim是vi的升级版  \n\n![](./img/linux/commend/4.png)\n![](./img/linux/commend/5.png)\n![](./img/linux/commend/6.png)\n![](./img/linux/commend/7.png)\n![](../../../themes/yilia/source/img/linux/commend/4.png)\n![](../../../themes/yilia/source/img/linux/commend/5.png)\n![](../../../themes/yilia/source/img/linux/commend/6.png)\n![](../../../themes/yilia/source/img/linux/commend/7.png)\n\n## 5. <span id='help'>帮助</span>\n\n命令 --help\n- 查看用法\n\nman 命令\n\n- 操作手册","slug":"linux/Linux_1.command","published":1,"updated":"2024-03-02T05:42:41.789Z","_id":"clvuli6ho005izsvweu4q4cli","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h1 id=\"1-基础命令\"><a href=\"#1-基础命令\" class=\"headerlink\" title=\"1 基础命令\"></a>1 基础命令</h1><table>\n<thead>\n<tr>\n<th>显示</th>\n<th>ls -alh 路径</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>打开路径</td>\n<td><a href='#cd'>cd 路径</a></td>\n</tr>\n<tr>\n<td>显示工作路径</td>\n<td><a href='#pwd'>pwd</a></td>\n</tr>\n<tr>\n<td>创建文件夹</td>\n<td><a href='#mkdir'>mkdir -p 路径</a></td>\n</tr>\n<tr>\n<td>创建文件</td>\n<td><a href='#touch'>touch 路径</a></td>\n</tr>\n<tr>\n<td>查看文件</td>\n<td><a href='#cat'>cat 路径</a></td>\n</tr>\n<tr>\n<td>分页查看</td>\n<td><a href='#more'>more 路径  </a></td>\n</tr>\n<tr>\n<td>复制</td>\n<td><a href='#cp'>cp -r 源 目的  </a></td>\n</tr>\n<tr>\n<td>移动文件</td>\n<td><a href='#mv'>mv 源 目的（可重命名</a>）</td>\n</tr>\n<tr>\n<td>删除文件</td>\n<td><a href='#rm'>rm -rf 路径</a></td>\n</tr>\n<tr>\n<td>文件中通过关键字搜索行</td>\n<td><a href='#grep'>grep -n 关键字 路径 </a></td>\n</tr>\n<tr>\n<td>文件内容统计</td>\n<td><a href='#wc'>wc -cmlw 路径   </a></td>\n</tr>\n<tr>\n<td>查看命令源文件</td>\n<td><a href='#which'>which 命令 </a></td>\n</tr>\n<tr>\n<td>搜索文件</td>\n<td><a href='#find'>find 起始路径 -name “test”  </a></td>\n</tr>\n<tr>\n<td>打印</td>\n<td><a href='#echo'>echo</a></td>\n</tr>\n<tr>\n<td>从尾部查看</td>\n<td><a href='#tail'>tail -f -数字 路径  </a></td>\n</tr>\n<tr>\n<td>编辑文件</td>\n<td><a href='#vi'>vi 路径</a></td>\n</tr>\n</tbody></table>\n<h2 id=\"1-目录结构\"><a href=\"#1-目录结构\" class=\"headerlink\" title=\"1. 目录结构\"></a>1. 目录结构</h2><p>根目录：&#x2F;<br>&#x2F;home&#x2F;hellow&#x2F;test.txt</p>\n<h2 id=\"2-小tip\"><a href=\"#2-小tip\" class=\"headerlink\" title=\"2. 小tip\"></a>2. 小tip</h2><h3 id=\"2-1-特殊符号\"><a href=\"#2-1-特殊符号\" class=\"headerlink\" title=\"2-1 特殊符号\"></a>2-1 特殊符号</h3><ul>\n<li><p>. 当前目录</p>\n</li>\n<li><p>.. 上级目录</p>\n</li>\n<li><p>~ home</p>\n</li>\n<li><p>&#96;作为命令执行</p>\n</li>\n</ul>\n<h3 id=\"2-2-进入root用户\"><a href=\"#2-2-进入root用户\" class=\"headerlink\" title=\"2-2 进入root用户\"></a>2-2 进入root用户</h3><ul>\n<li>进入</li>\n</ul>\n<p>su - root</p>\n<p>123456</p>\n<ul>\n<li>退出<br>exit</li>\n</ul>\n<h3 id=\"2-3管道符\"><a href=\"#2-3管道符\" class=\"headerlink\" title=\"2-3管道符\"></a>2-3管道符</h3><p>|<br>cat 1.txt | grep “hello”</p>\n<ul>\n<li>左边结果作为右边的输入</li>\n</ul>\n<h3 id=\"2-4重定向符\"><a href=\"#2-4重定向符\" class=\"headerlink\" title=\"2-4重定向符\"></a>2-4重定向符</h3><p>&gt;：右边文件内容清空，左边内容写入文件</p>\n<p>&gt;&gt;：追加</p>\n<h2 id=\"3-命令\"><a href=\"#3-命令\" class=\"headerlink\" title=\"3. 命令\"></a>3. 命令</h2><p>命令 [选项] [参数]</p>\n<h3 id=\"1-ls命令（显示内容）\"><a href=\"#1-ls命令（显示内容）\" class=\"headerlink\" title=\"1. ls命令（显示内容）\"></a><span id='ls'>1. ls命令（显示内容）</span></h3><p>ls [-a -l -h] [路径]</p>\n<ul>\n<li>a: all，列出全部文件（包括隐藏文件）</li>\n<li>l: list，以列表展现（权限，用户和用户组，大小，创建日期）</li>\n<li>h: 以易于阅读的形式，列出文件大小（搭配-l)</li>\n<li>组合使用：如-la</li>\n</ul>\n<h3 id=\"2-cd命令（change-directory）\"><a href=\"#2-cd命令（change-directory）\" class=\"headerlink\" title=\"2. cd命令（change directory）\"></a>2. <span id='cd'>cd命令（change directory）</span></h3><p>cd [路径]</p>\n<ul>\n<li>cd 回到用户HOME目录</li>\n<li>cd ..&#x2F;.. 返回上两级</li>\n<li>cd 路径</li>\n</ul>\n<h3 id=\"3-pwd命令（print-work-directory）\"><a href=\"#3-pwd命令（print-work-directory）\" class=\"headerlink\" title=\"3. pwd命令（print work directory）\"></a><span id='pwd'>3. pwd命令（print work directory）</span></h3><p>pwd</p>\n<h3 id=\"4-mkdir命令（make-directory）\"><a href=\"#4-mkdir命令（make-directory）\" class=\"headerlink\" title=\"4. mkdir命令（make directory）\"></a>4. <span id='mkdir'>mkdir命令（make directory）</span></h3><p>mkdir [-p] 路径</p>\n<ul>\n<li>多层创建</li>\n</ul>\n<h3 id=\"5-touch命令（创建文件）\"><a href=\"#5-touch命令（创建文件）\" class=\"headerlink\" title=\"5. touch命令（创建文件）\"></a>5. <span id='touch'>touch命令（创建文件）</span></h3><p>touch 路径</p>\n<h3 id=\"6-cat命令（查看文件）\"><a href=\"#6-cat命令（查看文件）\" class=\"headerlink\" title=\"6. cat命令（查看文件）\"></a>6. <span id='cat'>cat命令（查看文件）</span></h3><p>cat 路径</p>\n<h3 id=\"7-more命令（查看文件）\"><a href=\"#7-more命令（查看文件）\" class=\"headerlink\" title=\"7. more命令（查看文件）\"></a><span id='more'>7. more命令（查看文件）</span></h3><p>more 路径（分页看，空格下一页，q退出）</p>\n<h3 id=\"8-cp命令（copy）\"><a href=\"#8-cp命令（copy）\" class=\"headerlink\" title=\"8. cp命令（copy）\"></a>8. <span id='cp'>cp命令（copy）</span></h3><p>cp [-r] 源路径 目的地</p>\n<ul>\n<li>r 递归，用于文件夹的复制</li>\n</ul>\n<h3 id=\"9-mv命令（move）\"><a href=\"#9-mv命令（move）\" class=\"headerlink\" title=\"9. mv命令（move）\"></a>9. <span id='mv'>mv命令（move）</span></h3><p>mv 源路径 目的地&#x2F;（可重命名）</p>\n<h3 id=\"10-rm命令（remove）\"><a href=\"#10-rm命令（remove）\" class=\"headerlink\" title=\"10. rm命令（remove）\"></a>10. <span id='rm'>rm命令（remove）</span></h3><p>rm [-r -f] 参数1 参数2…</p>\n<ul>\n<li>r 递归，用于删除文件夹</li>\n<li>f (force)强制删除</li>\n<li>支持通配符：test* 表示以test开头</li>\n</ul>\n<h3 id=\"11-grep命令（文件中通过关键字搜索行）\"><a href=\"#11-grep命令（文件中通过关键字搜索行）\" class=\"headerlink\" title=\"11. grep命令（文件中通过关键字搜索行）\"></a>11. <span id='grep'>grep命令（文件中通过关键字搜索行）</span></h3><p>grep [-n] “关键字” 文件路径</p>\n<ul>\n<li>n：结果中显示行号</li>\n<li>关键字：用于查找</li>\n<li>路径：可做输入端口</li>\n</ul>\n<h3 id=\"12-wc命令（文件内容统计）\"><a href=\"#12-wc命令（文件内容统计）\" class=\"headerlink\" title=\"12. wc命令（文件内容统计）\"></a>12. <span id='wc'>wc命令（文件内容统计）</span></h3><p>wc [-cmlw] 文件路径</p>\n<ul>\n<li>c：统计bytes数量</li>\n<li>m：统计字符数量</li>\n<li>l：统计行数</li>\n<li>w：统计单词数</li>\n<li>路径：可做输入端口</li>\n</ul>\n<h3 id=\"13-which命令（查看命令的源文件）\"><a href=\"#13-which命令（查看命令的源文件）\" class=\"headerlink\" title=\"13. which命令（查看命令的源文件）\"></a>13. <span id='which'>which命令（查看命令的源文件）</span></h3><p>which Linux命令</p>\n<h3 id=\"14-find命令（文件搜索）\"><a href=\"#14-find命令（文件搜索）\" class=\"headerlink\" title=\"14. find命令（文件搜索）\"></a>14. <span id='find'>find命令（文件搜索）</span></h3><p>find 起始路径 -name “test*”<br>find 起始路径 -size +100k(大于100kb)</p>\n<h3 id=\"15-echo命令（打印）\"><a href=\"#15-echo命令（打印）\" class=\"headerlink\" title=\"15. echo命令（打印）\"></a>15. <span id='echo'>echo命令（打印）</span></h3><p>echo 输出内容</p>\n<ul>\n<li>echo &#96;pwd：’的作用：将pwd作为命令执行</li>\n</ul>\n<h3 id=\"16-tail命令（从尾部查看）\"><a href=\"#16-tail命令（从尾部查看）\" class=\"headerlink\" title=\"16. tail命令（从尾部查看）\"></a>16. <span id='tail'>tail命令（从尾部查看）</span></h3><p>tail [-f -数字] 路径</p>\n<ul>\n<li>f 持续跟踪(会实时刷新)</li>\n<li>数字 查看多少行（默认10）</li>\n</ul>\n<h2 id=\"4-vim编辑器\"><a href=\"#4-vim编辑器\" class=\"headerlink\" title=\"4. vim编辑器\"></a>4. <span id='vi'>vim编辑器</span></h2><p>vi 文件路径</p>\n<p>vim 文件路径</p>\n<ul>\n<li>vim是vi的升级版</li>\n</ul>\n<p><img src=\"/./img/linux/commend/4.png\"><br><img src=\"/./img/linux/commend/5.png\"><br><img src=\"/./img/linux/commend/6.png\"><br><img src=\"/./img/linux/commend/7.png\"><br><img src=\"/../../../themes/yilia/source/img/linux/commend/4.png\"><br><img src=\"/../../../themes/yilia/source/img/linux/commend/5.png\"><br><img src=\"/../../../themes/yilia/source/img/linux/commend/6.png\"><br><img src=\"/../../../themes/yilia/source/img/linux/commend/7.png\"></p>\n<h2 id=\"5-帮助\"><a href=\"#5-帮助\" class=\"headerlink\" title=\"5. 帮助\"></a>5. <span id='help'>帮助</span></h2><p>命令 –help</p>\n<ul>\n<li>查看用法</li>\n</ul>\n<p>man 命令</p>\n<ul>\n<li>操作手册</li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"1-基础命令\"><a href=\"#1-基础命令\" class=\"headerlink\" title=\"1 基础命令\"></a>1 基础命令</h1><table>\n<thead>\n<tr>\n<th>显示</th>\n<th>ls -alh 路径</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>打开路径</td>\n<td><a href='#cd'>cd 路径</a></td>\n</tr>\n<tr>\n<td>显示工作路径</td>\n<td><a href='#pwd'>pwd</a></td>\n</tr>\n<tr>\n<td>创建文件夹</td>\n<td><a href='#mkdir'>mkdir -p 路径</a></td>\n</tr>\n<tr>\n<td>创建文件</td>\n<td><a href='#touch'>touch 路径</a></td>\n</tr>\n<tr>\n<td>查看文件</td>\n<td><a href='#cat'>cat 路径</a></td>\n</tr>\n<tr>\n<td>分页查看</td>\n<td><a href='#more'>more 路径  </a></td>\n</tr>\n<tr>\n<td>复制</td>\n<td><a href='#cp'>cp -r 源 目的  </a></td>\n</tr>\n<tr>\n<td>移动文件</td>\n<td><a href='#mv'>mv 源 目的（可重命名</a>）</td>\n</tr>\n<tr>\n<td>删除文件</td>\n<td><a href='#rm'>rm -rf 路径</a></td>\n</tr>\n<tr>\n<td>文件中通过关键字搜索行</td>\n<td><a href='#grep'>grep -n 关键字 路径 </a></td>\n</tr>\n<tr>\n<td>文件内容统计</td>\n<td><a href='#wc'>wc -cmlw 路径   </a></td>\n</tr>\n<tr>\n<td>查看命令源文件</td>\n<td><a href='#which'>which 命令 </a></td>\n</tr>\n<tr>\n<td>搜索文件</td>\n<td><a href='#find'>find 起始路径 -name “test”  </a></td>\n</tr>\n<tr>\n<td>打印</td>\n<td><a href='#echo'>echo</a></td>\n</tr>\n<tr>\n<td>从尾部查看</td>\n<td><a href='#tail'>tail -f -数字 路径  </a></td>\n</tr>\n<tr>\n<td>编辑文件</td>\n<td><a href='#vi'>vi 路径</a></td>\n</tr>\n</tbody></table>\n<h2 id=\"1-目录结构\"><a href=\"#1-目录结构\" class=\"headerlink\" title=\"1. 目录结构\"></a>1. 目录结构</h2><p>根目录：&#x2F;<br>&#x2F;home&#x2F;hellow&#x2F;test.txt</p>\n<h2 id=\"2-小tip\"><a href=\"#2-小tip\" class=\"headerlink\" title=\"2. 小tip\"></a>2. 小tip</h2><h3 id=\"2-1-特殊符号\"><a href=\"#2-1-特殊符号\" class=\"headerlink\" title=\"2-1 特殊符号\"></a>2-1 特殊符号</h3><ul>\n<li><p>. 当前目录</p>\n</li>\n<li><p>.. 上级目录</p>\n</li>\n<li><p>~ home</p>\n</li>\n<li><p>&#96;作为命令执行</p>\n</li>\n</ul>\n<h3 id=\"2-2-进入root用户\"><a href=\"#2-2-进入root用户\" class=\"headerlink\" title=\"2-2 进入root用户\"></a>2-2 进入root用户</h3><ul>\n<li>进入</li>\n</ul>\n<p>su - root</p>\n<p>123456</p>\n<ul>\n<li>退出<br>exit</li>\n</ul>\n<h3 id=\"2-3管道符\"><a href=\"#2-3管道符\" class=\"headerlink\" title=\"2-3管道符\"></a>2-3管道符</h3><p>|<br>cat 1.txt | grep “hello”</p>\n<ul>\n<li>左边结果作为右边的输入</li>\n</ul>\n<h3 id=\"2-4重定向符\"><a href=\"#2-4重定向符\" class=\"headerlink\" title=\"2-4重定向符\"></a>2-4重定向符</h3><p>&gt;：右边文件内容清空，左边内容写入文件</p>\n<p>&gt;&gt;：追加</p>\n<h2 id=\"3-命令\"><a href=\"#3-命令\" class=\"headerlink\" title=\"3. 命令\"></a>3. 命令</h2><p>命令 [选项] [参数]</p>\n<h3 id=\"1-ls命令（显示内容）\"><a href=\"#1-ls命令（显示内容）\" class=\"headerlink\" title=\"1. ls命令（显示内容）\"></a><span id='ls'>1. ls命令（显示内容）</span></h3><p>ls [-a -l -h] [路径]</p>\n<ul>\n<li>a: all，列出全部文件（包括隐藏文件）</li>\n<li>l: list，以列表展现（权限，用户和用户组，大小，创建日期）</li>\n<li>h: 以易于阅读的形式，列出文件大小（搭配-l)</li>\n<li>组合使用：如-la</li>\n</ul>\n<h3 id=\"2-cd命令（change-directory）\"><a href=\"#2-cd命令（change-directory）\" class=\"headerlink\" title=\"2. cd命令（change directory）\"></a>2. <span id='cd'>cd命令（change directory）</span></h3><p>cd [路径]</p>\n<ul>\n<li>cd 回到用户HOME目录</li>\n<li>cd ..&#x2F;.. 返回上两级</li>\n<li>cd 路径</li>\n</ul>\n<h3 id=\"3-pwd命令（print-work-directory）\"><a href=\"#3-pwd命令（print-work-directory）\" class=\"headerlink\" title=\"3. pwd命令（print work directory）\"></a><span id='pwd'>3. pwd命令（print work directory）</span></h3><p>pwd</p>\n<h3 id=\"4-mkdir命令（make-directory）\"><a href=\"#4-mkdir命令（make-directory）\" class=\"headerlink\" title=\"4. mkdir命令（make directory）\"></a>4. <span id='mkdir'>mkdir命令（make directory）</span></h3><p>mkdir [-p] 路径</p>\n<ul>\n<li>多层创建</li>\n</ul>\n<h3 id=\"5-touch命令（创建文件）\"><a href=\"#5-touch命令（创建文件）\" class=\"headerlink\" title=\"5. touch命令（创建文件）\"></a>5. <span id='touch'>touch命令（创建文件）</span></h3><p>touch 路径</p>\n<h3 id=\"6-cat命令（查看文件）\"><a href=\"#6-cat命令（查看文件）\" class=\"headerlink\" title=\"6. cat命令（查看文件）\"></a>6. <span id='cat'>cat命令（查看文件）</span></h3><p>cat 路径</p>\n<h3 id=\"7-more命令（查看文件）\"><a href=\"#7-more命令（查看文件）\" class=\"headerlink\" title=\"7. more命令（查看文件）\"></a><span id='more'>7. more命令（查看文件）</span></h3><p>more 路径（分页看，空格下一页，q退出）</p>\n<h3 id=\"8-cp命令（copy）\"><a href=\"#8-cp命令（copy）\" class=\"headerlink\" title=\"8. cp命令（copy）\"></a>8. <span id='cp'>cp命令（copy）</span></h3><p>cp [-r] 源路径 目的地</p>\n<ul>\n<li>r 递归，用于文件夹的复制</li>\n</ul>\n<h3 id=\"9-mv命令（move）\"><a href=\"#9-mv命令（move）\" class=\"headerlink\" title=\"9. mv命令（move）\"></a>9. <span id='mv'>mv命令（move）</span></h3><p>mv 源路径 目的地&#x2F;（可重命名）</p>\n<h3 id=\"10-rm命令（remove）\"><a href=\"#10-rm命令（remove）\" class=\"headerlink\" title=\"10. rm命令（remove）\"></a>10. <span id='rm'>rm命令（remove）</span></h3><p>rm [-r -f] 参数1 参数2…</p>\n<ul>\n<li>r 递归，用于删除文件夹</li>\n<li>f (force)强制删除</li>\n<li>支持通配符：test* 表示以test开头</li>\n</ul>\n<h3 id=\"11-grep命令（文件中通过关键字搜索行）\"><a href=\"#11-grep命令（文件中通过关键字搜索行）\" class=\"headerlink\" title=\"11. grep命令（文件中通过关键字搜索行）\"></a>11. <span id='grep'>grep命令（文件中通过关键字搜索行）</span></h3><p>grep [-n] “关键字” 文件路径</p>\n<ul>\n<li>n：结果中显示行号</li>\n<li>关键字：用于查找</li>\n<li>路径：可做输入端口</li>\n</ul>\n<h3 id=\"12-wc命令（文件内容统计）\"><a href=\"#12-wc命令（文件内容统计）\" class=\"headerlink\" title=\"12. wc命令（文件内容统计）\"></a>12. <span id='wc'>wc命令（文件内容统计）</span></h3><p>wc [-cmlw] 文件路径</p>\n<ul>\n<li>c：统计bytes数量</li>\n<li>m：统计字符数量</li>\n<li>l：统计行数</li>\n<li>w：统计单词数</li>\n<li>路径：可做输入端口</li>\n</ul>\n<h3 id=\"13-which命令（查看命令的源文件）\"><a href=\"#13-which命令（查看命令的源文件）\" class=\"headerlink\" title=\"13. which命令（查看命令的源文件）\"></a>13. <span id='which'>which命令（查看命令的源文件）</span></h3><p>which Linux命令</p>\n<h3 id=\"14-find命令（文件搜索）\"><a href=\"#14-find命令（文件搜索）\" class=\"headerlink\" title=\"14. find命令（文件搜索）\"></a>14. <span id='find'>find命令（文件搜索）</span></h3><p>find 起始路径 -name “test*”<br>find 起始路径 -size +100k(大于100kb)</p>\n<h3 id=\"15-echo命令（打印）\"><a href=\"#15-echo命令（打印）\" class=\"headerlink\" title=\"15. echo命令（打印）\"></a>15. <span id='echo'>echo命令（打印）</span></h3><p>echo 输出内容</p>\n<ul>\n<li>echo &#96;pwd：’的作用：将pwd作为命令执行</li>\n</ul>\n<h3 id=\"16-tail命令（从尾部查看）\"><a href=\"#16-tail命令（从尾部查看）\" class=\"headerlink\" title=\"16. tail命令（从尾部查看）\"></a>16. <span id='tail'>tail命令（从尾部查看）</span></h3><p>tail [-f -数字] 路径</p>\n<ul>\n<li>f 持续跟踪(会实时刷新)</li>\n<li>数字 查看多少行（默认10）</li>\n</ul>\n<h2 id=\"4-vim编辑器\"><a href=\"#4-vim编辑器\" class=\"headerlink\" title=\"4. vim编辑器\"></a>4. <span id='vi'>vim编辑器</span></h2><p>vi 文件路径</p>\n<p>vim 文件路径</p>\n<ul>\n<li>vim是vi的升级版</li>\n</ul>\n<p><img src=\"/./img/linux/commend/4.png\"><br><img src=\"/./img/linux/commend/5.png\"><br><img src=\"/./img/linux/commend/6.png\"><br><img src=\"/./img/linux/commend/7.png\"><br><img src=\"/../../../themes/yilia/source/img/linux/commend/4.png\"><br><img src=\"/../../../themes/yilia/source/img/linux/commend/5.png\"><br><img src=\"/../../../themes/yilia/source/img/linux/commend/6.png\"><br><img src=\"/../../../themes/yilia/source/img/linux/commend/7.png\"></p>\n<h2 id=\"5-帮助\"><a href=\"#5-帮助\" class=\"headerlink\" title=\"5. 帮助\"></a>5. <span id='help'>帮助</span></h2><p>命令 –help</p>\n<ul>\n<li>查看用法</li>\n</ul>\n<p>man 命令</p>\n<ul>\n<li>操作手册</li>\n</ul>"},{"title":"Linux入门-3实用操作1","date":"2024-03-02T12:00:00.000Z","toc":true,"_content":"\n#\n\n<!--more-->\n\n# 3 Linux实用操作\n\n## 3.1 快捷键\n\n| 快捷键    | 说明                                   |\n| --------- | -------------------------------------- |\n| ctrl + c  | 停止程序<br>重新输入命令               |\n| ctrl + d  | 退出账户<br>退出程序界面               |\n| history   | 历史命令                               |\n| !命令前缀 | 执行上一次匹配前缀的命令               |\n| ctrl + r  | 搜索历史命令（回车执行，左右键不执行） |\n| ctrl + l  | 清空终端（clear）                      |\n\n| 移动光标  |              |\n| --------- | ------------ |\n| ctrl + a  | 跳到命令开头 |\n| ctrl + e  | 跳到命令结尾 |\n| ctrl + 左 | 左跳一个单词 |\n\n## 3.2 软件安装\n\n### 3.2.1 yum（centos）\n\n- RPM包软件管理器，用于自动化安装配置Linux软件，并可以自动解决依赖问题。相当于Linux应用商店\n\n```bash\nyum [-y] [install | remove |search] 软件名\n```\n\n| 选项    | 解释     |\n| ------- | -------- |\n| -y      | 自动确认 |\n| install | 安装     |\n| remove  | 卸载     |\n| search  | 搜索     |\n\n- ubuntu中用apt\n\n\n\n## 3.3 服务管理\n\n- systemctl命令\n\n```bash\nsystemctl start | stop | status | enable |disable 服务名\n```\n\n|                | 说明       |\n| -------------- | ---------- |\n| status         | 查看状态   |\n| enable         | 开机自启动 |\n| NetworkManager | 主网络服务 |\n| network        | 副网络服务 |\n| firewalld      | 防火墙服务 |\n| sshd，ssh      |            |\n\n- 部分软件安装后没有自动集成到systemctl中，我们可以手动添加  \n\n## 3.4 软链接\n\n- 类似快捷方式\n\n```bash\nln -s 起点 终点 \n```\n\n## 3.5 时间\n\n- date命令\n\n```bash\ndate [-d] [+格式化字符串]\n```\n\n| 格式 | 说明                               |\n| ---- | ---------------------------------- |\n| %Y   | 年                                 |\n| %y   | 两位数字的年                       |\n| %m   | 月                                 |\n| %d   | 日                                 |\n| %H   | 小时                               |\n| %M   | 分钟                               |\n| %S   | 秒                                 |\n| %s   | 从1970-01-01 0:0:0到现在过了多少秒 |\n\n​\t例子\n\n```bash\ndate \"+%Y-%m-%d %H:%M:%S\"\ndate -d \"+1day\" +%Y%m%d #后一天\n```\n\n- 修改时区\n\n```bash\nrm -f /etc/localtime #删除文件\nsudo ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime #将该文件链接为localtime\n```\n\n- ntp程序\n\n  通过ntp程序自动校准时间\n\n```bash\nyum -y install ntp #安装\nsystemctl start ntpd #启动服务\nsystemctl enable ntpd #开机自启动\n```\n\n​\t\t手动校准\n\n```bash\nntpdate -u ntp.aliyun.com\n```\n\n","source":"_posts/linux/Linux_3.usage.md","raw":"---\ntitle: Linux入门-3实用操作1\ndate: 2024-03-02 20:00:00\ntoc: true\ntags: [Linux]\ncategories: [Linux]\n---\n\n#\n\n<!--more-->\n\n# 3 Linux实用操作\n\n## 3.1 快捷键\n\n| 快捷键    | 说明                                   |\n| --------- | -------------------------------------- |\n| ctrl + c  | 停止程序<br>重新输入命令               |\n| ctrl + d  | 退出账户<br>退出程序界面               |\n| history   | 历史命令                               |\n| !命令前缀 | 执行上一次匹配前缀的命令               |\n| ctrl + r  | 搜索历史命令（回车执行，左右键不执行） |\n| ctrl + l  | 清空终端（clear）                      |\n\n| 移动光标  |              |\n| --------- | ------------ |\n| ctrl + a  | 跳到命令开头 |\n| ctrl + e  | 跳到命令结尾 |\n| ctrl + 左 | 左跳一个单词 |\n\n## 3.2 软件安装\n\n### 3.2.1 yum（centos）\n\n- RPM包软件管理器，用于自动化安装配置Linux软件，并可以自动解决依赖问题。相当于Linux应用商店\n\n```bash\nyum [-y] [install | remove |search] 软件名\n```\n\n| 选项    | 解释     |\n| ------- | -------- |\n| -y      | 自动确认 |\n| install | 安装     |\n| remove  | 卸载     |\n| search  | 搜索     |\n\n- ubuntu中用apt\n\n\n\n## 3.3 服务管理\n\n- systemctl命令\n\n```bash\nsystemctl start | stop | status | enable |disable 服务名\n```\n\n|                | 说明       |\n| -------------- | ---------- |\n| status         | 查看状态   |\n| enable         | 开机自启动 |\n| NetworkManager | 主网络服务 |\n| network        | 副网络服务 |\n| firewalld      | 防火墙服务 |\n| sshd，ssh      |            |\n\n- 部分软件安装后没有自动集成到systemctl中，我们可以手动添加  \n\n## 3.4 软链接\n\n- 类似快捷方式\n\n```bash\nln -s 起点 终点 \n```\n\n## 3.5 时间\n\n- date命令\n\n```bash\ndate [-d] [+格式化字符串]\n```\n\n| 格式 | 说明                               |\n| ---- | ---------------------------------- |\n| %Y   | 年                                 |\n| %y   | 两位数字的年                       |\n| %m   | 月                                 |\n| %d   | 日                                 |\n| %H   | 小时                               |\n| %M   | 分钟                               |\n| %S   | 秒                                 |\n| %s   | 从1970-01-01 0:0:0到现在过了多少秒 |\n\n​\t例子\n\n```bash\ndate \"+%Y-%m-%d %H:%M:%S\"\ndate -d \"+1day\" +%Y%m%d #后一天\n```\n\n- 修改时区\n\n```bash\nrm -f /etc/localtime #删除文件\nsudo ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime #将该文件链接为localtime\n```\n\n- ntp程序\n\n  通过ntp程序自动校准时间\n\n```bash\nyum -y install ntp #安装\nsystemctl start ntpd #启动服务\nsystemctl enable ntpd #开机自启动\n```\n\n​\t\t手动校准\n\n```bash\nntpdate -u ntp.aliyun.com\n```\n\n","slug":"linux/Linux_3.usage","published":1,"updated":"2024-03-02T11:54:59.742Z","_id":"clvuli6hp005kzsvw7nxu3q95","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h1 id=\"3-Linux实用操作\"><a href=\"#3-Linux实用操作\" class=\"headerlink\" title=\"3 Linux实用操作\"></a>3 Linux实用操作</h1><h2 id=\"3-1-快捷键\"><a href=\"#3-1-快捷键\" class=\"headerlink\" title=\"3.1 快捷键\"></a>3.1 快捷键</h2><table>\n<thead>\n<tr>\n<th>快捷键</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>ctrl + c</td>\n<td>停止程序<br>重新输入命令</td>\n</tr>\n<tr>\n<td>ctrl + d</td>\n<td>退出账户<br>退出程序界面</td>\n</tr>\n<tr>\n<td>history</td>\n<td>历史命令</td>\n</tr>\n<tr>\n<td>!命令前缀</td>\n<td>执行上一次匹配前缀的命令</td>\n</tr>\n<tr>\n<td>ctrl + r</td>\n<td>搜索历史命令（回车执行，左右键不执行）</td>\n</tr>\n<tr>\n<td>ctrl + l</td>\n<td>清空终端（clear）</td>\n</tr>\n</tbody></table>\n<table>\n<thead>\n<tr>\n<th>移动光标</th>\n<th></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>ctrl + a</td>\n<td>跳到命令开头</td>\n</tr>\n<tr>\n<td>ctrl + e</td>\n<td>跳到命令结尾</td>\n</tr>\n<tr>\n<td>ctrl + 左</td>\n<td>左跳一个单词</td>\n</tr>\n</tbody></table>\n<h2 id=\"3-2-软件安装\"><a href=\"#3-2-软件安装\" class=\"headerlink\" title=\"3.2 软件安装\"></a>3.2 软件安装</h2><h3 id=\"3-2-1-yum（centos）\"><a href=\"#3-2-1-yum（centos）\" class=\"headerlink\" title=\"3.2.1 yum（centos）\"></a>3.2.1 yum（centos）</h3><ul>\n<li>RPM包软件管理器，用于自动化安装配置Linux软件，并可以自动解决依赖问题。相当于Linux应用商店</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum [-y] [install | remove |search] 软件名</span><br></pre></td></tr></table></figure>\n\n<table>\n<thead>\n<tr>\n<th>选项</th>\n<th>解释</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>-y</td>\n<td>自动确认</td>\n</tr>\n<tr>\n<td>install</td>\n<td>安装</td>\n</tr>\n<tr>\n<td>remove</td>\n<td>卸载</td>\n</tr>\n<tr>\n<td>search</td>\n<td>搜索</td>\n</tr>\n</tbody></table>\n<ul>\n<li>ubuntu中用apt</li>\n</ul>\n<h2 id=\"3-3-服务管理\"><a href=\"#3-3-服务管理\" class=\"headerlink\" title=\"3.3 服务管理\"></a>3.3 服务管理</h2><ul>\n<li>systemctl命令</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl start | stop | status | <span class=\"built_in\">enable</span> |<span class=\"built_in\">disable</span> 服务名</span><br></pre></td></tr></table></figure>\n\n<table>\n<thead>\n<tr>\n<th></th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>status</td>\n<td>查看状态</td>\n</tr>\n<tr>\n<td>enable</td>\n<td>开机自启动</td>\n</tr>\n<tr>\n<td>NetworkManager</td>\n<td>主网络服务</td>\n</tr>\n<tr>\n<td>network</td>\n<td>副网络服务</td>\n</tr>\n<tr>\n<td>firewalld</td>\n<td>防火墙服务</td>\n</tr>\n<tr>\n<td>sshd，ssh</td>\n<td></td>\n</tr>\n</tbody></table>\n<ul>\n<li>部分软件安装后没有自动集成到systemctl中，我们可以手动添加</li>\n</ul>\n<h2 id=\"3-4-软链接\"><a href=\"#3-4-软链接\" class=\"headerlink\" title=\"3.4 软链接\"></a>3.4 软链接</h2><ul>\n<li>类似快捷方式</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">ln</span> -s 起点 终点 </span><br></pre></td></tr></table></figure>\n\n<h2 id=\"3-5-时间\"><a href=\"#3-5-时间\" class=\"headerlink\" title=\"3.5 时间\"></a>3.5 时间</h2><ul>\n<li>date命令</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">date</span> [-d] [+格式化字符串]</span><br></pre></td></tr></table></figure>\n\n<table>\n<thead>\n<tr>\n<th>格式</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>%Y</td>\n<td>年</td>\n</tr>\n<tr>\n<td>%y</td>\n<td>两位数字的年</td>\n</tr>\n<tr>\n<td>%m</td>\n<td>月</td>\n</tr>\n<tr>\n<td>%d</td>\n<td>日</td>\n</tr>\n<tr>\n<td>%H</td>\n<td>小时</td>\n</tr>\n<tr>\n<td>%M</td>\n<td>分钟</td>\n</tr>\n<tr>\n<td>%S</td>\n<td>秒</td>\n</tr>\n<tr>\n<td>%s</td>\n<td>从1970-01-01 0:0:0到现在过了多少秒</td>\n</tr>\n</tbody></table>\n<p>​\t例子</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">date</span> <span class=\"string\">&quot;+%Y-%m-%d %H:%M:%S&quot;</span></span><br><span class=\"line\"><span class=\"built_in\">date</span> -d <span class=\"string\">&quot;+1day&quot;</span> +%Y%m%d <span class=\"comment\">#后一天</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>修改时区</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">rm</span> -f /etc/localtime <span class=\"comment\">#删除文件</span></span><br><span class=\"line\">sudo <span class=\"built_in\">ln</span> -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime <span class=\"comment\">#将该文件链接为localtime</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p>ntp程序</p>\n<p>通过ntp程序自动校准时间</p>\n</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum -y install ntp <span class=\"comment\">#安装</span></span><br><span class=\"line\">systemctl start ntpd <span class=\"comment\">#启动服务</span></span><br><span class=\"line\">systemctl <span class=\"built_in\">enable</span> ntpd <span class=\"comment\">#开机自启动</span></span><br></pre></td></tr></table></figure>\n\n<p>​\t\t手动校准</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ntpdate -u ntp.aliyun.com</span><br></pre></td></tr></table></figure>\n\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"3-Linux实用操作\"><a href=\"#3-Linux实用操作\" class=\"headerlink\" title=\"3 Linux实用操作\"></a>3 Linux实用操作</h1><h2 id=\"3-1-快捷键\"><a href=\"#3-1-快捷键\" class=\"headerlink\" title=\"3.1 快捷键\"></a>3.1 快捷键</h2><table>\n<thead>\n<tr>\n<th>快捷键</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>ctrl + c</td>\n<td>停止程序<br>重新输入命令</td>\n</tr>\n<tr>\n<td>ctrl + d</td>\n<td>退出账户<br>退出程序界面</td>\n</tr>\n<tr>\n<td>history</td>\n<td>历史命令</td>\n</tr>\n<tr>\n<td>!命令前缀</td>\n<td>执行上一次匹配前缀的命令</td>\n</tr>\n<tr>\n<td>ctrl + r</td>\n<td>搜索历史命令（回车执行，左右键不执行）</td>\n</tr>\n<tr>\n<td>ctrl + l</td>\n<td>清空终端（clear）</td>\n</tr>\n</tbody></table>\n<table>\n<thead>\n<tr>\n<th>移动光标</th>\n<th></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>ctrl + a</td>\n<td>跳到命令开头</td>\n</tr>\n<tr>\n<td>ctrl + e</td>\n<td>跳到命令结尾</td>\n</tr>\n<tr>\n<td>ctrl + 左</td>\n<td>左跳一个单词</td>\n</tr>\n</tbody></table>\n<h2 id=\"3-2-软件安装\"><a href=\"#3-2-软件安装\" class=\"headerlink\" title=\"3.2 软件安装\"></a>3.2 软件安装</h2><h3 id=\"3-2-1-yum（centos）\"><a href=\"#3-2-1-yum（centos）\" class=\"headerlink\" title=\"3.2.1 yum（centos）\"></a>3.2.1 yum（centos）</h3><ul>\n<li>RPM包软件管理器，用于自动化安装配置Linux软件，并可以自动解决依赖问题。相当于Linux应用商店</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum [-y] [install | remove |search] 软件名</span><br></pre></td></tr></table></figure>\n\n<table>\n<thead>\n<tr>\n<th>选项</th>\n<th>解释</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>-y</td>\n<td>自动确认</td>\n</tr>\n<tr>\n<td>install</td>\n<td>安装</td>\n</tr>\n<tr>\n<td>remove</td>\n<td>卸载</td>\n</tr>\n<tr>\n<td>search</td>\n<td>搜索</td>\n</tr>\n</tbody></table>\n<ul>\n<li>ubuntu中用apt</li>\n</ul>\n<h2 id=\"3-3-服务管理\"><a href=\"#3-3-服务管理\" class=\"headerlink\" title=\"3.3 服务管理\"></a>3.3 服务管理</h2><ul>\n<li>systemctl命令</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl start | stop | status | <span class=\"built_in\">enable</span> |<span class=\"built_in\">disable</span> 服务名</span><br></pre></td></tr></table></figure>\n\n<table>\n<thead>\n<tr>\n<th></th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>status</td>\n<td>查看状态</td>\n</tr>\n<tr>\n<td>enable</td>\n<td>开机自启动</td>\n</tr>\n<tr>\n<td>NetworkManager</td>\n<td>主网络服务</td>\n</tr>\n<tr>\n<td>network</td>\n<td>副网络服务</td>\n</tr>\n<tr>\n<td>firewalld</td>\n<td>防火墙服务</td>\n</tr>\n<tr>\n<td>sshd，ssh</td>\n<td></td>\n</tr>\n</tbody></table>\n<ul>\n<li>部分软件安装后没有自动集成到systemctl中，我们可以手动添加</li>\n</ul>\n<h2 id=\"3-4-软链接\"><a href=\"#3-4-软链接\" class=\"headerlink\" title=\"3.4 软链接\"></a>3.4 软链接</h2><ul>\n<li>类似快捷方式</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">ln</span> -s 起点 终点 </span><br></pre></td></tr></table></figure>\n\n<h2 id=\"3-5-时间\"><a href=\"#3-5-时间\" class=\"headerlink\" title=\"3.5 时间\"></a>3.5 时间</h2><ul>\n<li>date命令</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">date</span> [-d] [+格式化字符串]</span><br></pre></td></tr></table></figure>\n\n<table>\n<thead>\n<tr>\n<th>格式</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>%Y</td>\n<td>年</td>\n</tr>\n<tr>\n<td>%y</td>\n<td>两位数字的年</td>\n</tr>\n<tr>\n<td>%m</td>\n<td>月</td>\n</tr>\n<tr>\n<td>%d</td>\n<td>日</td>\n</tr>\n<tr>\n<td>%H</td>\n<td>小时</td>\n</tr>\n<tr>\n<td>%M</td>\n<td>分钟</td>\n</tr>\n<tr>\n<td>%S</td>\n<td>秒</td>\n</tr>\n<tr>\n<td>%s</td>\n<td>从1970-01-01 0:0:0到现在过了多少秒</td>\n</tr>\n</tbody></table>\n<p>​\t例子</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">date</span> <span class=\"string\">&quot;+%Y-%m-%d %H:%M:%S&quot;</span></span><br><span class=\"line\"><span class=\"built_in\">date</span> -d <span class=\"string\">&quot;+1day&quot;</span> +%Y%m%d <span class=\"comment\">#后一天</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>修改时区</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">rm</span> -f /etc/localtime <span class=\"comment\">#删除文件</span></span><br><span class=\"line\">sudo <span class=\"built_in\">ln</span> -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime <span class=\"comment\">#将该文件链接为localtime</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p>ntp程序</p>\n<p>通过ntp程序自动校准时间</p>\n</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum -y install ntp <span class=\"comment\">#安装</span></span><br><span class=\"line\">systemctl start ntpd <span class=\"comment\">#启动服务</span></span><br><span class=\"line\">systemctl <span class=\"built_in\">enable</span> ntpd <span class=\"comment\">#开机自启动</span></span><br></pre></td></tr></table></figure>\n\n<p>​\t\t手动校准</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ntpdate -u ntp.aliyun.com</span><br></pre></td></tr></table></figure>"},{"title":"Linux入门-3使用操作3","date":"2024-03-02T14:00:00.000Z","toc":true,"_content":"\n#\n\n<!--more-->\n\n# 3 Linux实用操作\n\n## 3.9 环境变量\n\n### 3.9.1 查看环境变量\n\n```bash\nenv\n```\n\n- 查看PATH变量\n\n  ```bash\n  env | grep PATH\n  ```\n\n  - PATH记录了系统执行任何命令的搜索路径，当执行任何命令，都会按照顺序，从上述路径中搜索要执行的程序的本体。比如执行cd命令，就从第二个目录/usr/bin中搜索到了cd命令，并执行\n\n### 3.9.2 获取环境变量值\n\n```bash\n$变量名\n```\n\n- 例子\n\n  ```bash\n  echo ${PATH}abc\n  ```\n\n### 3.9.3 设置环境变量\n\n- 临时设置\n\n  ```bash\n  export 变量名=变量值\n  ```\n\n- 永久设置\n\n  | 作用         | 文件                          |\n  | ------------ | ----------------------------- |\n  | 针对当前用户 | ~/.bashrc                     |\n  | 所有用户     | /etc/profile                  |\n  |              | 使用`source 配置文件`立刻生效 |\n\n  - 例子\n\n    ```bash\n    export PATH=$PATH:目标文件\n    ```\n\n## 3.10 上传下载\n\n- rz、sz命令\n\n```bash\nyum -y install lrzsz #下载rz、sz\nrz #上传\nsz 文件 #下载\n```\n\n## 3.11 压缩解压\n\n### 1. tar命令\n\n```bash\ntar [-c -v -x -f -z -C] 参1 参2...\n```\n\n- 可解压\n\n  | 格式 | 说明                                                         |\n  | ---- | ------------------------------------------------------------ |\n  | .tar | tarball，归档文件，     简单的将文件组装到一个.tar的文件内，并没有太多文件体积的减少，仅仅是简单的封装 |\n  | .gz  | •也常见为.tar.gz，gzip格式压缩文件，即使用gzip压缩算法将文件压缩到一个文件内，可以极大的减少压缩后的体积 |\n\n- 参数\n\n  | 参数 | 说明                       |\n  | ---- | -------------------------- |\n  | c    | 创建压缩文件               |\n  | v    | 查看进度                   |\n  | x    | 解压模式                   |\n  | f    | 目标文件，必须放在最后一个 |\n  | z    | gzip模式，一般放在第一个   |\n  | C    | 解压目的地，单独使用       |\n\n- 压缩\n\n  ```bash\n  tar -cvf test.tar 文件1 文件2 #tar压缩\n  tar -zcvf test.tar.gz 文件1 文件2 #gzip压缩\n  ```\n\n- 解压\n\n  ```bash\n  tar -xvf 文件\n  tar -xvf 文件 -C 目标路径 #指定目录\n  tar -zxvf 文件 -C 目标路径 #解压.gz\n  ```\n\n### 2. zip命令\n\n```bash\nzip [-r] 文件1 文件2\n```\n\n### 3. unzip命令\n\n```bash\nunzip 文件 [-d 目标路径]\n```\n\n\n\n","source":"_posts/linux/Linux_5.usage3.md","raw":"---\ntitle: Linux入门-3使用操作3\ndate: 2024-03-02 22:00:00\ntoc: true\ntags: [Linux]\ncategories: [Linux]\n\n\n---\n\n#\n\n<!--more-->\n\n# 3 Linux实用操作\n\n## 3.9 环境变量\n\n### 3.9.1 查看环境变量\n\n```bash\nenv\n```\n\n- 查看PATH变量\n\n  ```bash\n  env | grep PATH\n  ```\n\n  - PATH记录了系统执行任何命令的搜索路径，当执行任何命令，都会按照顺序，从上述路径中搜索要执行的程序的本体。比如执行cd命令，就从第二个目录/usr/bin中搜索到了cd命令，并执行\n\n### 3.9.2 获取环境变量值\n\n```bash\n$变量名\n```\n\n- 例子\n\n  ```bash\n  echo ${PATH}abc\n  ```\n\n### 3.9.3 设置环境变量\n\n- 临时设置\n\n  ```bash\n  export 变量名=变量值\n  ```\n\n- 永久设置\n\n  | 作用         | 文件                          |\n  | ------------ | ----------------------------- |\n  | 针对当前用户 | ~/.bashrc                     |\n  | 所有用户     | /etc/profile                  |\n  |              | 使用`source 配置文件`立刻生效 |\n\n  - 例子\n\n    ```bash\n    export PATH=$PATH:目标文件\n    ```\n\n## 3.10 上传下载\n\n- rz、sz命令\n\n```bash\nyum -y install lrzsz #下载rz、sz\nrz #上传\nsz 文件 #下载\n```\n\n## 3.11 压缩解压\n\n### 1. tar命令\n\n```bash\ntar [-c -v -x -f -z -C] 参1 参2...\n```\n\n- 可解压\n\n  | 格式 | 说明                                                         |\n  | ---- | ------------------------------------------------------------ |\n  | .tar | tarball，归档文件，     简单的将文件组装到一个.tar的文件内，并没有太多文件体积的减少，仅仅是简单的封装 |\n  | .gz  | •也常见为.tar.gz，gzip格式压缩文件，即使用gzip压缩算法将文件压缩到一个文件内，可以极大的减少压缩后的体积 |\n\n- 参数\n\n  | 参数 | 说明                       |\n  | ---- | -------------------------- |\n  | c    | 创建压缩文件               |\n  | v    | 查看进度                   |\n  | x    | 解压模式                   |\n  | f    | 目标文件，必须放在最后一个 |\n  | z    | gzip模式，一般放在第一个   |\n  | C    | 解压目的地，单独使用       |\n\n- 压缩\n\n  ```bash\n  tar -cvf test.tar 文件1 文件2 #tar压缩\n  tar -zcvf test.tar.gz 文件1 文件2 #gzip压缩\n  ```\n\n- 解压\n\n  ```bash\n  tar -xvf 文件\n  tar -xvf 文件 -C 目标路径 #指定目录\n  tar -zxvf 文件 -C 目标路径 #解压.gz\n  ```\n\n### 2. zip命令\n\n```bash\nzip [-r] 文件1 文件2\n```\n\n### 3. unzip命令\n\n```bash\nunzip 文件 [-d 目标路径]\n```\n\n\n\n","slug":"linux/Linux_5.usage3","published":1,"updated":"2024-03-02T12:47:39.959Z","_id":"clvuli6hp005nzsvwdulj0ibg","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h1 id=\"3-Linux实用操作\"><a href=\"#3-Linux实用操作\" class=\"headerlink\" title=\"3 Linux实用操作\"></a>3 Linux实用操作</h1><h2 id=\"3-9-环境变量\"><a href=\"#3-9-环境变量\" class=\"headerlink\" title=\"3.9 环境变量\"></a>3.9 环境变量</h2><h3 id=\"3-9-1-查看环境变量\"><a href=\"#3-9-1-查看环境变量\" class=\"headerlink\" title=\"3.9.1 查看环境变量\"></a>3.9.1 查看环境变量</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">env</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p>查看PATH变量</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">env</span> | grep PATH</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>PATH记录了系统执行任何命令的搜索路径，当执行任何命令，都会按照顺序，从上述路径中搜索要执行的程序的本体。比如执行cd命令，就从第二个目录&#x2F;usr&#x2F;bin中搜索到了cd命令，并执行</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"3-9-2-获取环境变量值\"><a href=\"#3-9-2-获取环境变量值\" class=\"headerlink\" title=\"3.9.2 获取环境变量值\"></a>3.9.2 获取环境变量值</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$变量名</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p>例子</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">echo</span> <span class=\"variable\">$&#123;PATH&#125;</span>abc</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h3 id=\"3-9-3-设置环境变量\"><a href=\"#3-9-3-设置环境变量\" class=\"headerlink\" title=\"3.9.3 设置环境变量\"></a>3.9.3 设置环境变量</h3><ul>\n<li><p>临时设置</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">export</span> 变量名=变量值</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>永久设置</p>\n<table>\n<thead>\n<tr>\n<th>作用</th>\n<th>文件</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>针对当前用户</td>\n<td>~&#x2F;.bashrc</td>\n</tr>\n<tr>\n<td>所有用户</td>\n<td>&#x2F;etc&#x2F;profile</td>\n</tr>\n<tr>\n<td></td>\n<td>使用<code>source 配置文件</code>立刻生效</td>\n</tr>\n</tbody></table>\n<ul>\n<li><p>例子</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">export</span> PATH=<span class=\"variable\">$PATH</span>:目标文件</span><br></pre></td></tr></table></figure></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"3-10-上传下载\"><a href=\"#3-10-上传下载\" class=\"headerlink\" title=\"3.10 上传下载\"></a>3.10 上传下载</h2><ul>\n<li>rz、sz命令</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum -y install lrzsz <span class=\"comment\">#下载rz、sz</span></span><br><span class=\"line\">rz <span class=\"comment\">#上传</span></span><br><span class=\"line\">sz 文件 <span class=\"comment\">#下载</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"3-11-压缩解压\"><a href=\"#3-11-压缩解压\" class=\"headerlink\" title=\"3.11 压缩解压\"></a>3.11 压缩解压</h2><h3 id=\"1-tar命令\"><a href=\"#1-tar命令\" class=\"headerlink\" title=\"1. tar命令\"></a>1. tar命令</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tar [-c -v -x -f -z -C] 参1 参2...</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p>可解压</p>\n<table>\n<thead>\n<tr>\n<th>格式</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>.tar</td>\n<td>tarball，归档文件，     简单的将文件组装到一个.tar的文件内，并没有太多文件体积的减少，仅仅是简单的封装</td>\n</tr>\n<tr>\n<td>.gz</td>\n<td>•也常见为.tar.gz，gzip格式压缩文件，即使用gzip压缩算法将文件压缩到一个文件内，可以极大的减少压缩后的体积</td>\n</tr>\n</tbody></table>\n</li>\n<li><p>参数</p>\n<table>\n<thead>\n<tr>\n<th>参数</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>c</td>\n<td>创建压缩文件</td>\n</tr>\n<tr>\n<td>v</td>\n<td>查看进度</td>\n</tr>\n<tr>\n<td>x</td>\n<td>解压模式</td>\n</tr>\n<tr>\n<td>f</td>\n<td>目标文件，必须放在最后一个</td>\n</tr>\n<tr>\n<td>z</td>\n<td>gzip模式，一般放在第一个</td>\n</tr>\n<tr>\n<td>C</td>\n<td>解压目的地，单独使用</td>\n</tr>\n</tbody></table>\n</li>\n<li><p>压缩</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tar -cvf test.tar 文件1 文件2 <span class=\"comment\">#tar压缩</span></span><br><span class=\"line\">tar -zcvf test.tar.gz 文件1 文件2 <span class=\"comment\">#gzip压缩</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>解压</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tar -xvf 文件</span><br><span class=\"line\">tar -xvf 文件 -C 目标路径 <span class=\"comment\">#指定目录</span></span><br><span class=\"line\">tar -zxvf 文件 -C 目标路径 <span class=\"comment\">#解压.gz</span></span><br></pre></td></tr></table></figure></li>\n</ul>\n<h3 id=\"2-zip命令\"><a href=\"#2-zip命令\" class=\"headerlink\" title=\"2. zip命令\"></a>2. zip命令</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">zip [-r] 文件1 文件2</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"3-unzip命令\"><a href=\"#3-unzip命令\" class=\"headerlink\" title=\"3. unzip命令\"></a>3. unzip命令</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">unzip 文件 [-d 目标路径]</span><br></pre></td></tr></table></figure>\n\n\n\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"3-Linux实用操作\"><a href=\"#3-Linux实用操作\" class=\"headerlink\" title=\"3 Linux实用操作\"></a>3 Linux实用操作</h1><h2 id=\"3-9-环境变量\"><a href=\"#3-9-环境变量\" class=\"headerlink\" title=\"3.9 环境变量\"></a>3.9 环境变量</h2><h3 id=\"3-9-1-查看环境变量\"><a href=\"#3-9-1-查看环境变量\" class=\"headerlink\" title=\"3.9.1 查看环境变量\"></a>3.9.1 查看环境变量</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">env</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p>查看PATH变量</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">env</span> | grep PATH</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>PATH记录了系统执行任何命令的搜索路径，当执行任何命令，都会按照顺序，从上述路径中搜索要执行的程序的本体。比如执行cd命令，就从第二个目录&#x2F;usr&#x2F;bin中搜索到了cd命令，并执行</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"3-9-2-获取环境变量值\"><a href=\"#3-9-2-获取环境变量值\" class=\"headerlink\" title=\"3.9.2 获取环境变量值\"></a>3.9.2 获取环境变量值</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$变量名</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p>例子</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">echo</span> <span class=\"variable\">$&#123;PATH&#125;</span>abc</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h3 id=\"3-9-3-设置环境变量\"><a href=\"#3-9-3-设置环境变量\" class=\"headerlink\" title=\"3.9.3 设置环境变量\"></a>3.9.3 设置环境变量</h3><ul>\n<li><p>临时设置</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">export</span> 变量名=变量值</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>永久设置</p>\n<table>\n<thead>\n<tr>\n<th>作用</th>\n<th>文件</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>针对当前用户</td>\n<td>~&#x2F;.bashrc</td>\n</tr>\n<tr>\n<td>所有用户</td>\n<td>&#x2F;etc&#x2F;profile</td>\n</tr>\n<tr>\n<td></td>\n<td>使用<code>source 配置文件</code>立刻生效</td>\n</tr>\n</tbody></table>\n<ul>\n<li><p>例子</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">export</span> PATH=<span class=\"variable\">$PATH</span>:目标文件</span><br></pre></td></tr></table></figure></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"3-10-上传下载\"><a href=\"#3-10-上传下载\" class=\"headerlink\" title=\"3.10 上传下载\"></a>3.10 上传下载</h2><ul>\n<li>rz、sz命令</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum -y install lrzsz <span class=\"comment\">#下载rz、sz</span></span><br><span class=\"line\">rz <span class=\"comment\">#上传</span></span><br><span class=\"line\">sz 文件 <span class=\"comment\">#下载</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"3-11-压缩解压\"><a href=\"#3-11-压缩解压\" class=\"headerlink\" title=\"3.11 压缩解压\"></a>3.11 压缩解压</h2><h3 id=\"1-tar命令\"><a href=\"#1-tar命令\" class=\"headerlink\" title=\"1. tar命令\"></a>1. tar命令</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tar [-c -v -x -f -z -C] 参1 参2...</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p>可解压</p>\n<table>\n<thead>\n<tr>\n<th>格式</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>.tar</td>\n<td>tarball，归档文件，     简单的将文件组装到一个.tar的文件内，并没有太多文件体积的减少，仅仅是简单的封装</td>\n</tr>\n<tr>\n<td>.gz</td>\n<td>•也常见为.tar.gz，gzip格式压缩文件，即使用gzip压缩算法将文件压缩到一个文件内，可以极大的减少压缩后的体积</td>\n</tr>\n</tbody></table>\n</li>\n<li><p>参数</p>\n<table>\n<thead>\n<tr>\n<th>参数</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>c</td>\n<td>创建压缩文件</td>\n</tr>\n<tr>\n<td>v</td>\n<td>查看进度</td>\n</tr>\n<tr>\n<td>x</td>\n<td>解压模式</td>\n</tr>\n<tr>\n<td>f</td>\n<td>目标文件，必须放在最后一个</td>\n</tr>\n<tr>\n<td>z</td>\n<td>gzip模式，一般放在第一个</td>\n</tr>\n<tr>\n<td>C</td>\n<td>解压目的地，单独使用</td>\n</tr>\n</tbody></table>\n</li>\n<li><p>压缩</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tar -cvf test.tar 文件1 文件2 <span class=\"comment\">#tar压缩</span></span><br><span class=\"line\">tar -zcvf test.tar.gz 文件1 文件2 <span class=\"comment\">#gzip压缩</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>解压</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tar -xvf 文件</span><br><span class=\"line\">tar -xvf 文件 -C 目标路径 <span class=\"comment\">#指定目录</span></span><br><span class=\"line\">tar -zxvf 文件 -C 目标路径 <span class=\"comment\">#解压.gz</span></span><br></pre></td></tr></table></figure></li>\n</ul>\n<h3 id=\"2-zip命令\"><a href=\"#2-zip命令\" class=\"headerlink\" title=\"2. zip命令\"></a>2. zip命令</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">zip [-r] 文件1 文件2</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"3-unzip命令\"><a href=\"#3-unzip命令\" class=\"headerlink\" title=\"3. unzip命令\"></a>3. unzip命令</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">unzip 文件 [-d 目标路径]</span><br></pre></td></tr></table></figure>"},{"title":"Linux服务器免密登录","date":"2024-03-09T14:00:00.000Z","toc":true,"_content":"\n#\n\n<!--more-->\n\n# Linux服务器免密登录\n\n## 1 生成本地公钥\n\n- 本地运行（可以先到user/.ssh下查看有没有id_rsa.pub文件，有就不用生成了）：\n\n  ```bash\n  ssh-keygen\n  ```\n\n  一直回车就行。\n\n## 2 将生成的公钥上传到服务器\n\n- 在服务器端创建~/.ssh\n\n  ```bash\n  mkdir .ssh\n  ```\n\n  \n\n- 本地cd到：user/用户名/.ssh。将id_rsa.pub复制到服务器的~/.ssh/authorized_keys（root用户的根目录与普通用户的根目录不同）\n\n  ```bash\n  scp -P 端口 id_rsa.pub 用户名@IP:~/.ssh/authorized_keys\n  ```\n\n  ","source":"_posts/linux/Linux_nopassword.md","raw":"---\ntitle: Linux服务器免密登录\ndate: 2024-03-09 22:00:00\ntoc: true\ntags: [Linux]\ncategories: [Linux]\n\n---\n\n#\n\n<!--more-->\n\n# Linux服务器免密登录\n\n## 1 生成本地公钥\n\n- 本地运行（可以先到user/.ssh下查看有没有id_rsa.pub文件，有就不用生成了）：\n\n  ```bash\n  ssh-keygen\n  ```\n\n  一直回车就行。\n\n## 2 将生成的公钥上传到服务器\n\n- 在服务器端创建~/.ssh\n\n  ```bash\n  mkdir .ssh\n  ```\n\n  \n\n- 本地cd到：user/用户名/.ssh。将id_rsa.pub复制到服务器的~/.ssh/authorized_keys（root用户的根目录与普通用户的根目录不同）\n\n  ```bash\n  scp -P 端口 id_rsa.pub 用户名@IP:~/.ssh/authorized_keys\n  ```\n\n  ","slug":"linux/Linux_nopassword","published":1,"updated":"2024-03-11T12:31:15.983Z","_id":"clvuli6hq005qzsvw0ix2hp5m","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h1 id=\"Linux服务器免密登录\"><a href=\"#Linux服务器免密登录\" class=\"headerlink\" title=\"Linux服务器免密登录\"></a>Linux服务器免密登录</h1><h2 id=\"1-生成本地公钥\"><a href=\"#1-生成本地公钥\" class=\"headerlink\" title=\"1 生成本地公钥\"></a>1 生成本地公钥</h2><ul>\n<li><p>本地运行（可以先到user&#x2F;.ssh下查看有没有id_rsa.pub文件，有就不用生成了）：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh-keygen</span><br></pre></td></tr></table></figure>\n\n<p>一直回车就行。</p>\n</li>\n</ul>\n<h2 id=\"2-将生成的公钥上传到服务器\"><a href=\"#2-将生成的公钥上传到服务器\" class=\"headerlink\" title=\"2 将生成的公钥上传到服务器\"></a>2 将生成的公钥上传到服务器</h2><ul>\n<li><p>在服务器端创建~&#x2F;.ssh</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">mkdir</span> .ssh</span><br></pre></td></tr></table></figure>\n\n\n</li>\n<li><p>本地cd到：user&#x2F;用户名&#x2F;.ssh。将id_rsa.pub复制到服务器的~&#x2F;.ssh&#x2F;authorized_keys（root用户的根目录与普通用户的根目录不同）</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scp -P 端口 id_rsa.pub 用户名@IP:~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure></li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"Linux服务器免密登录\"><a href=\"#Linux服务器免密登录\" class=\"headerlink\" title=\"Linux服务器免密登录\"></a>Linux服务器免密登录</h1><h2 id=\"1-生成本地公钥\"><a href=\"#1-生成本地公钥\" class=\"headerlink\" title=\"1 生成本地公钥\"></a>1 生成本地公钥</h2><ul>\n<li><p>本地运行（可以先到user&#x2F;.ssh下查看有没有id_rsa.pub文件，有就不用生成了）：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh-keygen</span><br></pre></td></tr></table></figure>\n\n<p>一直回车就行。</p>\n</li>\n</ul>\n<h2 id=\"2-将生成的公钥上传到服务器\"><a href=\"#2-将生成的公钥上传到服务器\" class=\"headerlink\" title=\"2 将生成的公钥上传到服务器\"></a>2 将生成的公钥上传到服务器</h2><ul>\n<li><p>在服务器端创建~&#x2F;.ssh</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">mkdir</span> .ssh</span><br></pre></td></tr></table></figure>\n\n\n</li>\n<li><p>本地cd到：user&#x2F;用户名&#x2F;.ssh。将id_rsa.pub复制到服务器的~&#x2F;.ssh&#x2F;authorized_keys（root用户的根目录与普通用户的根目录不同）</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scp -P 端口 id_rsa.pub 用户名@IP:~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure></li>\n</ul>"},{"title":"tmux使用","date":"2024-03-11T14:00:00.000Z","toc":true,"_content":"\n#\n\n<!--more-->\n\n\n\n# Linux中使用tmux\n\n-  假设你在本地windows powershell中连接了远端服务器，然后打开了服务器的tmux，然后在tmux中运行了一个python代码，代码还没跑完就将本地的windows powershell关掉了，但是你的服务器还能接着跑代码。\n\n\n\n- \n\n  |              |                                 |\n  | ------------ | ------------------------------- |\n  | 查看会话     | tmux ls                         |\n  | 删除会话     | tmux kill-session -t 会话名或id |\n  | 激活会话     | tmux attach -t 会话名或id       |\n  | 退出当前会话 | ctrl+d                          |\n\n- \n\n  |                  |                         |\n  | ---------------- | ----------------------- |\n  | 向左打开一个窗口 | ctrl+b %                |\n  | 向下打开一个窗口 | ctrl+b \"                |\n  | 调整窗口大小     | ctrl+b+方向（狂按方向） |\n|                  |                         |\n  \n  ","source":"_posts/linux/tmux.md","raw":"---\ntitle: tmux使用\ndate: 2024-03-11 22:00:00\ntoc: true\ntags: [Linux]\ncategories: [Linux]\n\n\n---\n\n#\n\n<!--more-->\n\n\n\n# Linux中使用tmux\n\n-  假设你在本地windows powershell中连接了远端服务器，然后打开了服务器的tmux，然后在tmux中运行了一个python代码，代码还没跑完就将本地的windows powershell关掉了，但是你的服务器还能接着跑代码。\n\n\n\n- \n\n  |              |                                 |\n  | ------------ | ------------------------------- |\n  | 查看会话     | tmux ls                         |\n  | 删除会话     | tmux kill-session -t 会话名或id |\n  | 激活会话     | tmux attach -t 会话名或id       |\n  | 退出当前会话 | ctrl+d                          |\n\n- \n\n  |                  |                         |\n  | ---------------- | ----------------------- |\n  | 向左打开一个窗口 | ctrl+b %                |\n  | 向下打开一个窗口 | ctrl+b \"                |\n  | 调整窗口大小     | ctrl+b+方向（狂按方向） |\n|                  |                         |\n  \n  ","slug":"linux/tmux","published":1,"updated":"2024-03-11T14:19:08.051Z","_id":"clvuli6hq005tzsvwhk4x6227","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n\n\n<h1 id=\"Linux中使用tmux\"><a href=\"#Linux中使用tmux\" class=\"headerlink\" title=\"Linux中使用tmux\"></a>Linux中使用tmux</h1><ul>\n<li><p>假设你在本地windows powershell中连接了远端服务器，然后打开了服务器的tmux，然后在tmux中运行了一个python代码，代码还没跑完就将本地的windows powershell关掉了，但是你的服务器还能接着跑代码。</p>\n</li>\n<li><table>\n<thead>\n<tr>\n<th></th>\n<th></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>查看会话</td>\n<td>tmux ls</td>\n</tr>\n<tr>\n<td>删除会话</td>\n<td>tmux kill-session -t 会话名或id</td>\n</tr>\n<tr>\n<td>激活会话</td>\n<td>tmux attach -t 会话名或id</td>\n</tr>\n<tr>\n<td>退出当前会话</td>\n<td>ctrl+d</td>\n</tr>\n</tbody></table>\n</li>\n<li><table>\n<thead>\n<tr>\n<th></th>\n<th></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>向左打开一个窗口</td>\n<td>ctrl+b %</td>\n</tr>\n<tr>\n<td>向下打开一个窗口</td>\n<td>ctrl+b “</td>\n</tr>\n<tr>\n<td>调整窗口大小</td>\n<td>ctrl+b+方向（狂按方向）</td>\n</tr>\n</tbody></table>\n</li>\n</ul>\n<p>|                  |                         |</p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"Linux中使用tmux\"><a href=\"#Linux中使用tmux\" class=\"headerlink\" title=\"Linux中使用tmux\"></a>Linux中使用tmux</h1><ul>\n<li><p>假设你在本地windows powershell中连接了远端服务器，然后打开了服务器的tmux，然后在tmux中运行了一个python代码，代码还没跑完就将本地的windows powershell关掉了，但是你的服务器还能接着跑代码。</p>\n</li>\n<li><table>\n<thead>\n<tr>\n<th></th>\n<th></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>查看会话</td>\n<td>tmux ls</td>\n</tr>\n<tr>\n<td>删除会话</td>\n<td>tmux kill-session -t 会话名或id</td>\n</tr>\n<tr>\n<td>激活会话</td>\n<td>tmux attach -t 会话名或id</td>\n</tr>\n<tr>\n<td>退出当前会话</td>\n<td>ctrl+d</td>\n</tr>\n</tbody></table>\n</li>\n<li><table>\n<thead>\n<tr>\n<th></th>\n<th></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>向左打开一个窗口</td>\n<td>ctrl+b %</td>\n</tr>\n<tr>\n<td>向下打开一个窗口</td>\n<td>ctrl+b “</td>\n</tr>\n<tr>\n<td>调整窗口大小</td>\n<td>ctrl+b+方向（狂按方向）</td>\n</tr>\n</tbody></table>\n</li>\n</ul>\n<p>|                  |                         |</p>"},{"title":"linux中下载kaggle上的数据集","date":"2024-02-23T16:00:00.000Z","comment":true,"toc":true,"_content":"\n#\n<!--more-->\n\n- 登录kaggle，点击头像，点击设置，下划找到Create New Token，点击下载json文件\n\n![](D:\\blog\\themes\\yilia\\source\\img\\experience\\website\\1.png)\n\n![](img\\experience\\website\\1.png)\n\n- 在linux中\n\n```\npip install kaggle\n```\n\n- 将json文件放到用户根目录下的.kaggle文件夹下\n\n```\ncd ~\nmkdir .kaggle\ncd ~/.kaggle\n```\n\n- 下载数据集\n\n![](D:\\blog\\themes\\yilia\\source\\img\\experience\\website\\2.png)\n\n![](img\\experience\\website\\2.png)\n\n- 如果你使用的远程服务器无法科学上网，需要在国内找到下载地址，然后进行下载，右键正在下载的文件，复制下载地址，然后在服务器上通过`wget 网址`进行下载\n\n![](D:\\blog\\themes\\yilia\\source\\img\\experience\\website\\3.png)\n\n![](img\\experience\\website\\3.png)\n\n","source":"_posts/experience/website/kaggle.md","raw":"---\ntitle: linux中下载kaggle上的数据集\n\ndate: 2024-2-24\n\ntags: [kaggle]\n\ncategories: [经验]\n\ncomment: true\n\ntoc: true\n\n---\n\n#\n<!--more-->\n\n- 登录kaggle，点击头像，点击设置，下划找到Create New Token，点击下载json文件\n\n![](D:\\blog\\themes\\yilia\\source\\img\\experience\\website\\1.png)\n\n![](img\\experience\\website\\1.png)\n\n- 在linux中\n\n```\npip install kaggle\n```\n\n- 将json文件放到用户根目录下的.kaggle文件夹下\n\n```\ncd ~\nmkdir .kaggle\ncd ~/.kaggle\n```\n\n- 下载数据集\n\n![](D:\\blog\\themes\\yilia\\source\\img\\experience\\website\\2.png)\n\n![](img\\experience\\website\\2.png)\n\n- 如果你使用的远程服务器无法科学上网，需要在国内找到下载地址，然后进行下载，右键正在下载的文件，复制下载地址，然后在服务器上通过`wget 网址`进行下载\n\n![](D:\\blog\\themes\\yilia\\source\\img\\experience\\website\\3.png)\n\n![](img\\experience\\website\\3.png)\n\n","slug":"experience/website/kaggle","published":1,"updated":"2024-03-07T11:18:28.689Z","_id":"clvuli6ib0064zsvw0fbz1qk5","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<ul>\n<li>登录kaggle，点击头像，点击设置，下划找到Create New Token，点击下载json文件</li>\n</ul>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\experience\\website\\1.png\"></p>\n<p><img src=\"/img%5Cexperience%5Cwebsite%5C1.png\"></p>\n<ul>\n<li>在linux中</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install kaggle</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>将json文件放到用户根目录下的.kaggle文件夹下</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~</span><br><span class=\"line\">mkdir .kaggle</span><br><span class=\"line\">cd ~/.kaggle</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>下载数据集</li>\n</ul>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\experience\\website\\2.png\"></p>\n<p><img src=\"/img%5Cexperience%5Cwebsite%5C2.png\"></p>\n<ul>\n<li>如果你使用的远程服务器无法科学上网，需要在国内找到下载地址，然后进行下载，右键正在下载的文件，复制下载地址，然后在服务器上通过<code>wget 网址</code>进行下载</li>\n</ul>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\experience\\website\\3.png\"></p>\n<p><img src=\"/img%5Cexperience%5Cwebsite%5C3.png\"></p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<ul>\n<li>登录kaggle，点击头像，点击设置，下划找到Create New Token，点击下载json文件</li>\n</ul>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\experience\\website\\1.png\"></p>\n<p><img src=\"/img%5Cexperience%5Cwebsite%5C1.png\"></p>\n<ul>\n<li>在linux中</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install kaggle</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>将json文件放到用户根目录下的.kaggle文件夹下</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~</span><br><span class=\"line\">mkdir .kaggle</span><br><span class=\"line\">cd ~/.kaggle</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>下载数据集</li>\n</ul>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\experience\\website\\2.png\"></p>\n<p><img src=\"/img%5Cexperience%5Cwebsite%5C2.png\"></p>\n<ul>\n<li>如果你使用的远程服务器无法科学上网，需要在国内找到下载地址，然后进行下载，右键正在下载的文件，复制下载地址，然后在服务器上通过<code>wget 网址</code>进行下载</li>\n</ul>\n<p><img src=\"D:\\blog\\themes\\yilia\\source\\img\\experience\\website\\3.png\"></p>\n<p><img src=\"/img%5Cexperience%5Cwebsite%5C3.png\"></p>"},{"title":"使用via键盘映射","date":"2024-03-07T00:00:00.000Z","comment":true,"toc":true,"_content":"\n#\n<!--more-->\n\n# 使用via键盘映射\n\n- 键盘的F1键默认是功能键，不是原始的F1，使用F1需要Fn+F1，如何改回原来的。（请确认你的键盘支持这个功能，需要问商家要json文件）\n\n## 1. 下载via\n\n- https://github.com/WestBerryVIA/via-releases/releases\n- 根据你自己的系统下载对应的安装文件\n\n## 2. 连接键盘\n\n### 2.1 显示design\n\n![](D:/blog/themes/yilia/source/img/experience/keyboard/1.png)\n\n![](img/experience/keyboard/1.png)\n\n## 2.2 导入配置文件\n\n1. 找商家要json文件\n\n2. 将键盘调到有线模式，连线到电脑，点击load，如果无法导入json文件，可以试试开启`Use V2 definitions`选项。\n\n   ![](D:/blog/themes/yilia/source/img/experience/keyboard/2.png)\n\n   ![](img/experience/keyboard/2.png)\n\n3. 创建一个宏：Fn + F1。需要先进入全屏模式才能录制，点击圆点进行录制，然后按住Fn + F1，再点击圆点完成录制，然后点击傍边的保存按钮。\n\n   ![](D:/blog/themes/yilia/source/img/experience/keyboard/3.png)\n\n   ![](img/experience/keyboard/3.png)\n\n4. 按照如下顺序将F1（我这里因为已经换成M1:\n\n   ![](D:/blog/themes/yilia/source/img/experience/keyboard/4.png)\n\n   ![](img/experience/keyboard/4.png)\n\n## 2.3 其他功能\n\n### 2.3.1 via\n\n![](D:/blog/themes/yilia/source/img/experience/keyboard/5.png)\n\n![](img/experience/keyboard/5.png)\n\n### 2.3.2 界面\n\n![](D:/blog/themes/yilia/source/img/experience/keyboard/6.png)\n\n![](img/experience/keyboard/6.png)\n\n### 2.3.3 宏\n\n![](D:/blog/themes/yilia/source/img/experience/keyboard/7.png)\n\n![](img/experience/keyboard/7.png)\n\n### 2.3.4 层\n\n![](D:/blog/themes/yilia/source/img/experience/keyboard/8.png)\n\n![](img/experience/keyboard/8.png)\n\n### 2.3.5 any\n\n![](D:/blog/themes/yilia/source/img/experience/keyboard/9.png)\n\n![](img/experience/keyboard/9.png)","source":"_posts/experience/keyboard/keymap.md","raw":"---\ntitle: 使用via键盘映射\n\ndate: 2024-3-7 08:00:00\n\ntags: [键盘]\n\ncategories: [经验]\n\ncomment: true\n\ntoc: true\n\n---\n\n#\n<!--more-->\n\n# 使用via键盘映射\n\n- 键盘的F1键默认是功能键，不是原始的F1，使用F1需要Fn+F1，如何改回原来的。（请确认你的键盘支持这个功能，需要问商家要json文件）\n\n## 1. 下载via\n\n- https://github.com/WestBerryVIA/via-releases/releases\n- 根据你自己的系统下载对应的安装文件\n\n## 2. 连接键盘\n\n### 2.1 显示design\n\n![](D:/blog/themes/yilia/source/img/experience/keyboard/1.png)\n\n![](img/experience/keyboard/1.png)\n\n## 2.2 导入配置文件\n\n1. 找商家要json文件\n\n2. 将键盘调到有线模式，连线到电脑，点击load，如果无法导入json文件，可以试试开启`Use V2 definitions`选项。\n\n   ![](D:/blog/themes/yilia/source/img/experience/keyboard/2.png)\n\n   ![](img/experience/keyboard/2.png)\n\n3. 创建一个宏：Fn + F1。需要先进入全屏模式才能录制，点击圆点进行录制，然后按住Fn + F1，再点击圆点完成录制，然后点击傍边的保存按钮。\n\n   ![](D:/blog/themes/yilia/source/img/experience/keyboard/3.png)\n\n   ![](img/experience/keyboard/3.png)\n\n4. 按照如下顺序将F1（我这里因为已经换成M1:\n\n   ![](D:/blog/themes/yilia/source/img/experience/keyboard/4.png)\n\n   ![](img/experience/keyboard/4.png)\n\n## 2.3 其他功能\n\n### 2.3.1 via\n\n![](D:/blog/themes/yilia/source/img/experience/keyboard/5.png)\n\n![](img/experience/keyboard/5.png)\n\n### 2.3.2 界面\n\n![](D:/blog/themes/yilia/source/img/experience/keyboard/6.png)\n\n![](img/experience/keyboard/6.png)\n\n### 2.3.3 宏\n\n![](D:/blog/themes/yilia/source/img/experience/keyboard/7.png)\n\n![](img/experience/keyboard/7.png)\n\n### 2.3.4 层\n\n![](D:/blog/themes/yilia/source/img/experience/keyboard/8.png)\n\n![](img/experience/keyboard/8.png)\n\n### 2.3.5 any\n\n![](D:/blog/themes/yilia/source/img/experience/keyboard/9.png)\n\n![](img/experience/keyboard/9.png)","slug":"experience/keyboard/keymap","published":1,"updated":"2024-03-07T11:58:26.971Z","_id":"clvuli6ic0065zsvw92lk0opi","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h1 id=\"使用via键盘映射\"><a href=\"#使用via键盘映射\" class=\"headerlink\" title=\"使用via键盘映射\"></a>使用via键盘映射</h1><ul>\n<li>键盘的F1键默认是功能键，不是原始的F1，使用F1需要Fn+F1，如何改回原来的。（请确认你的键盘支持这个功能，需要问商家要json文件）</li>\n</ul>\n<h2 id=\"1-下载via\"><a href=\"#1-下载via\" class=\"headerlink\" title=\"1. 下载via\"></a>1. 下载via</h2><ul>\n<li><a href=\"https://github.com/WestBerryVIA/via-releases/releases\">https://github.com/WestBerryVIA/via-releases/releases</a></li>\n<li>根据你自己的系统下载对应的安装文件</li>\n</ul>\n<h2 id=\"2-连接键盘\"><a href=\"#2-连接键盘\" class=\"headerlink\" title=\"2. 连接键盘\"></a>2. 连接键盘</h2><h3 id=\"2-1-显示design\"><a href=\"#2-1-显示design\" class=\"headerlink\" title=\"2.1 显示design\"></a>2.1 显示design</h3><p><img src=\"D:/blog/themes/yilia/source/img/experience/keyboard/1.png\"></p>\n<p><img src=\"/img/experience/keyboard/1.png\"></p>\n<h2 id=\"2-2-导入配置文件\"><a href=\"#2-2-导入配置文件\" class=\"headerlink\" title=\"2.2 导入配置文件\"></a>2.2 导入配置文件</h2><ol>\n<li><p>找商家要json文件</p>\n</li>\n<li><p>将键盘调到有线模式，连线到电脑，点击load，如果无法导入json文件，可以试试开启<code>Use V2 definitions</code>选项。</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/experience/keyboard/2.png\"></p>\n<p><img src=\"/img/experience/keyboard/2.png\"></p>\n</li>\n<li><p>创建一个宏：Fn + F1。需要先进入全屏模式才能录制，点击圆点进行录制，然后按住Fn + F1，再点击圆点完成录制，然后点击傍边的保存按钮。</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/experience/keyboard/3.png\"></p>\n<p><img src=\"/img/experience/keyboard/3.png\"></p>\n</li>\n<li><p>按照如下顺序将F1（我这里因为已经换成M1:</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/experience/keyboard/4.png\"></p>\n<p><img src=\"/img/experience/keyboard/4.png\"></p>\n</li>\n</ol>\n<h2 id=\"2-3-其他功能\"><a href=\"#2-3-其他功能\" class=\"headerlink\" title=\"2.3 其他功能\"></a>2.3 其他功能</h2><h3 id=\"2-3-1-via\"><a href=\"#2-3-1-via\" class=\"headerlink\" title=\"2.3.1 via\"></a>2.3.1 via</h3><p><img src=\"D:/blog/themes/yilia/source/img/experience/keyboard/5.png\"></p>\n<p><img src=\"/img/experience/keyboard/5.png\"></p>\n<h3 id=\"2-3-2-界面\"><a href=\"#2-3-2-界面\" class=\"headerlink\" title=\"2.3.2 界面\"></a>2.3.2 界面</h3><p><img src=\"D:/blog/themes/yilia/source/img/experience/keyboard/6.png\"></p>\n<p><img src=\"/img/experience/keyboard/6.png\"></p>\n<h3 id=\"2-3-3-宏\"><a href=\"#2-3-3-宏\" class=\"headerlink\" title=\"2.3.3 宏\"></a>2.3.3 宏</h3><p><img src=\"D:/blog/themes/yilia/source/img/experience/keyboard/7.png\"></p>\n<p><img src=\"/img/experience/keyboard/7.png\"></p>\n<h3 id=\"2-3-4-层\"><a href=\"#2-3-4-层\" class=\"headerlink\" title=\"2.3.4 层\"></a>2.3.4 层</h3><p><img src=\"D:/blog/themes/yilia/source/img/experience/keyboard/8.png\"></p>\n<p><img src=\"/img/experience/keyboard/8.png\"></p>\n<h3 id=\"2-3-5-any\"><a href=\"#2-3-5-any\" class=\"headerlink\" title=\"2.3.5 any\"></a>2.3.5 any</h3><p><img src=\"D:/blog/themes/yilia/source/img/experience/keyboard/9.png\"></p>\n<p><img src=\"/img/experience/keyboard/9.png\"></p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"使用via键盘映射\"><a href=\"#使用via键盘映射\" class=\"headerlink\" title=\"使用via键盘映射\"></a>使用via键盘映射</h1><ul>\n<li>键盘的F1键默认是功能键，不是原始的F1，使用F1需要Fn+F1，如何改回原来的。（请确认你的键盘支持这个功能，需要问商家要json文件）</li>\n</ul>\n<h2 id=\"1-下载via\"><a href=\"#1-下载via\" class=\"headerlink\" title=\"1. 下载via\"></a>1. 下载via</h2><ul>\n<li><a href=\"https://github.com/WestBerryVIA/via-releases/releases\">https://github.com/WestBerryVIA/via-releases/releases</a></li>\n<li>根据你自己的系统下载对应的安装文件</li>\n</ul>\n<h2 id=\"2-连接键盘\"><a href=\"#2-连接键盘\" class=\"headerlink\" title=\"2. 连接键盘\"></a>2. 连接键盘</h2><h3 id=\"2-1-显示design\"><a href=\"#2-1-显示design\" class=\"headerlink\" title=\"2.1 显示design\"></a>2.1 显示design</h3><p><img src=\"D:/blog/themes/yilia/source/img/experience/keyboard/1.png\"></p>\n<p><img src=\"/img/experience/keyboard/1.png\"></p>\n<h2 id=\"2-2-导入配置文件\"><a href=\"#2-2-导入配置文件\" class=\"headerlink\" title=\"2.2 导入配置文件\"></a>2.2 导入配置文件</h2><ol>\n<li><p>找商家要json文件</p>\n</li>\n<li><p>将键盘调到有线模式，连线到电脑，点击load，如果无法导入json文件，可以试试开启<code>Use V2 definitions</code>选项。</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/experience/keyboard/2.png\"></p>\n<p><img src=\"/img/experience/keyboard/2.png\"></p>\n</li>\n<li><p>创建一个宏：Fn + F1。需要先进入全屏模式才能录制，点击圆点进行录制，然后按住Fn + F1，再点击圆点完成录制，然后点击傍边的保存按钮。</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/experience/keyboard/3.png\"></p>\n<p><img src=\"/img/experience/keyboard/3.png\"></p>\n</li>\n<li><p>按照如下顺序将F1（我这里因为已经换成M1:</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/experience/keyboard/4.png\"></p>\n<p><img src=\"/img/experience/keyboard/4.png\"></p>\n</li>\n</ol>\n<h2 id=\"2-3-其他功能\"><a href=\"#2-3-其他功能\" class=\"headerlink\" title=\"2.3 其他功能\"></a>2.3 其他功能</h2><h3 id=\"2-3-1-via\"><a href=\"#2-3-1-via\" class=\"headerlink\" title=\"2.3.1 via\"></a>2.3.1 via</h3><p><img src=\"D:/blog/themes/yilia/source/img/experience/keyboard/5.png\"></p>\n<p><img src=\"/img/experience/keyboard/5.png\"></p>\n<h3 id=\"2-3-2-界面\"><a href=\"#2-3-2-界面\" class=\"headerlink\" title=\"2.3.2 界面\"></a>2.3.2 界面</h3><p><img src=\"D:/blog/themes/yilia/source/img/experience/keyboard/6.png\"></p>\n<p><img src=\"/img/experience/keyboard/6.png\"></p>\n<h3 id=\"2-3-3-宏\"><a href=\"#2-3-3-宏\" class=\"headerlink\" title=\"2.3.3 宏\"></a>2.3.3 宏</h3><p><img src=\"D:/blog/themes/yilia/source/img/experience/keyboard/7.png\"></p>\n<p><img src=\"/img/experience/keyboard/7.png\"></p>\n<h3 id=\"2-3-4-层\"><a href=\"#2-3-4-层\" class=\"headerlink\" title=\"2.3.4 层\"></a>2.3.4 层</h3><p><img src=\"D:/blog/themes/yilia/source/img/experience/keyboard/8.png\"></p>\n<p><img src=\"/img/experience/keyboard/8.png\"></p>\n<h3 id=\"2-3-5-any\"><a href=\"#2-3-5-any\" class=\"headerlink\" title=\"2.3.5 any\"></a>2.3.5 any</h3><p><img src=\"D:/blog/themes/yilia/source/img/experience/keyboard/9.png\"></p>\n<p><img src=\"/img/experience/keyboard/9.png\"></p>"},{"title":"Linux下安装miniconda","date":"2024-04-23T00:00:00.000Z","comment":true,"toc":true,"_content":"\n#\n<!--more-->\n\n# Linux下安装miniconda\n1. 下载：\n```bash\nwget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n```\n\n2. 安装：\n```bash\nsh Miniconda3-latest-Linux-x86_64.sh\n```\n\n    然后一路回车\n\n3. 刷新终端\n```bash\nsource ~/.bashrc\n```\n\n4. 检验\n```bash\nconda -V\n```\n\n5. 若要卸载，可直接删除已安装的文件夹后，并删除相应的环境变量：\n```bash\nrm -rf /usr/local/miniconda/\nrm -rf /usr/local/anaconda/\n```\n\n6. 删除后，打开 ~/.bashrc 文件，删除以下conda的路径变量：\n```bash\nexport PATH=\" /usr/local/anaconda/bin:$PATH\" \nexport PATH=\" /usr/local/miniconda3/bin:$PATH\" \n```","source":"_posts/experience/install/miniconda.md","raw":"---\ntitle: Linux下安装miniconda\n\ndate: 2024-4-23 08:00:00\n\ntags: [安装]\n\ncategories: [安装]\n\ncomment: true\n\ntoc: true\n\n---\n\n#\n<!--more-->\n\n# Linux下安装miniconda\n1. 下载：\n```bash\nwget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n```\n\n2. 安装：\n```bash\nsh Miniconda3-latest-Linux-x86_64.sh\n```\n\n    然后一路回车\n\n3. 刷新终端\n```bash\nsource ~/.bashrc\n```\n\n4. 检验\n```bash\nconda -V\n```\n\n5. 若要卸载，可直接删除已安装的文件夹后，并删除相应的环境变量：\n```bash\nrm -rf /usr/local/miniconda/\nrm -rf /usr/local/anaconda/\n```\n\n6. 删除后，打开 ~/.bashrc 文件，删除以下conda的路径变量：\n```bash\nexport PATH=\" /usr/local/anaconda/bin:$PATH\" \nexport PATH=\" /usr/local/miniconda3/bin:$PATH\" \n```","slug":"experience/install/miniconda","published":1,"updated":"2024-04-23T09:31:00.724Z","_id":"clvuli6ig006czsvw2idth514","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h1 id=\"Linux下安装miniconda\"><a href=\"#Linux下安装miniconda\" class=\"headerlink\" title=\"Linux下安装miniconda\"></a>Linux下安装miniconda</h1><ol>\n<li><p>下载：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>安装：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sh Miniconda3-latest-Linux-x86_64.sh</span><br></pre></td></tr></table></figure>\n\n<p> 然后一路回车</p>\n</li>\n<li><p>刷新终端</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>检验</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda -V</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>若要卸载，可直接删除已安装的文件夹后，并删除相应的环境变量：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">rm</span> -rf /usr/local/miniconda/</span><br><span class=\"line\"><span class=\"built_in\">rm</span> -rf /usr/local/anaconda/</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>删除后，打开 ~&#x2F;.bashrc 文件，删除以下conda的路径变量：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">export</span> PATH=<span class=\"string\">&quot; /usr/local/anaconda/bin:<span class=\"variable\">$PATH</span>&quot;</span> </span><br><span class=\"line\"><span class=\"built_in\">export</span> PATH=<span class=\"string\">&quot; /usr/local/miniconda3/bin:<span class=\"variable\">$PATH</span>&quot;</span> </span><br></pre></td></tr></table></figure></li>\n</ol>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"Linux下安装miniconda\"><a href=\"#Linux下安装miniconda\" class=\"headerlink\" title=\"Linux下安装miniconda\"></a>Linux下安装miniconda</h1><ol>\n<li><p>下载：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>安装：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sh Miniconda3-latest-Linux-x86_64.sh</span><br></pre></td></tr></table></figure>\n\n<p> 然后一路回车</p>\n</li>\n<li><p>刷新终端</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>检验</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda -V</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>若要卸载，可直接删除已安装的文件夹后，并删除相应的环境变量：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">rm</span> -rf /usr/local/miniconda/</span><br><span class=\"line\"><span class=\"built_in\">rm</span> -rf /usr/local/anaconda/</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>删除后，打开 ~&#x2F;.bashrc 文件，删除以下conda的路径变量：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">export</span> PATH=<span class=\"string\">&quot; /usr/local/anaconda/bin:<span class=\"variable\">$PATH</span>&quot;</span> </span><br><span class=\"line\"><span class=\"built_in\">export</span> PATH=<span class=\"string\">&quot; /usr/local/miniconda3/bin:<span class=\"variable\">$PATH</span>&quot;</span> </span><br></pre></td></tr></table></figure></li>\n</ol>"},{"title":"python中PIL模块的使用","date":"2024-04-01T12:00:00.000Z","toc":true,"_content":"#  \n<!-- more -->\n\n# python中PIL模块的使用\n\n- 读取图像\n\n  ```python\n  from PIL import Image\n  img = Image.open('/')\n  #转成RGB\n  img = Image.open('/').convert('RGB')\n  ```\n\n- 缩放图像\n\n  ```python\n  from PIL import Image\n  img = Image.open('/')\n  img_ = img.resize((512,512) , Image.BICUBIC)\n  ```\n\n- 保存图像\n\n  ```python\n  from PIL import Image\n  img = Image.open()\n  img.save('/')\n  ```\n\n  ","source":"_posts/python/PIL.md","raw":"---\ntitle: python中PIL模块的使用\ndate: 2024-04-1 20:00:00\ntoc: true\ntags: [python]\ncategories: [python库]\n---\n#  \n<!-- more -->\n\n# python中PIL模块的使用\n\n- 读取图像\n\n  ```python\n  from PIL import Image\n  img = Image.open('/')\n  #转成RGB\n  img = Image.open('/').convert('RGB')\n  ```\n\n- 缩放图像\n\n  ```python\n  from PIL import Image\n  img = Image.open('/')\n  img_ = img.resize((512,512) , Image.BICUBIC)\n  ```\n\n- 保存图像\n\n  ```python\n  from PIL import Image\n  img = Image.open()\n  img.save('/')\n  ```\n\n  ","slug":"python/PIL","published":1,"updated":"2024-05-02T13:40:29.797Z","_id":"clvuli6ik006hzsvw889j45gw","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h1 id=\"python中PIL模块的使用\"><a href=\"#python中PIL模块的使用\" class=\"headerlink\" title=\"python中PIL模块的使用\"></a>python中PIL模块的使用</h1><ul>\n<li><p>读取图像</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> PIL <span class=\"keyword\">import</span> Image</span><br><span class=\"line\">img = Image.<span class=\"built_in\">open</span>(<span class=\"string\">&#x27;/&#x27;</span>)</span><br><span class=\"line\"><span class=\"comment\">#转成RGB</span></span><br><span class=\"line\">img = Image.<span class=\"built_in\">open</span>(<span class=\"string\">&#x27;/&#x27;</span>).convert(<span class=\"string\">&#x27;RGB&#x27;</span>)</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>缩放图像</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> PIL <span class=\"keyword\">import</span> Image</span><br><span class=\"line\">img = Image.<span class=\"built_in\">open</span>(<span class=\"string\">&#x27;/&#x27;</span>)</span><br><span class=\"line\">img_ = img.resize((<span class=\"number\">512</span>,<span class=\"number\">512</span>) , Image.BICUBIC)</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>保存图像</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> PIL <span class=\"keyword\">import</span> Image</span><br><span class=\"line\">img = Image.<span class=\"built_in\">open</span>()</span><br><span class=\"line\">img.save(<span class=\"string\">&#x27;/&#x27;</span>)</span><br></pre></td></tr></table></figure></li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"python中PIL模块的使用\"><a href=\"#python中PIL模块的使用\" class=\"headerlink\" title=\"python中PIL模块的使用\"></a>python中PIL模块的使用</h1><ul>\n<li><p>读取图像</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> PIL <span class=\"keyword\">import</span> Image</span><br><span class=\"line\">img = Image.<span class=\"built_in\">open</span>(<span class=\"string\">&#x27;/&#x27;</span>)</span><br><span class=\"line\"><span class=\"comment\">#转成RGB</span></span><br><span class=\"line\">img = Image.<span class=\"built_in\">open</span>(<span class=\"string\">&#x27;/&#x27;</span>).convert(<span class=\"string\">&#x27;RGB&#x27;</span>)</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>缩放图像</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> PIL <span class=\"keyword\">import</span> Image</span><br><span class=\"line\">img = Image.<span class=\"built_in\">open</span>(<span class=\"string\">&#x27;/&#x27;</span>)</span><br><span class=\"line\">img_ = img.resize((<span class=\"number\">512</span>,<span class=\"number\">512</span>) , Image.BICUBIC)</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>保存图像</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> PIL <span class=\"keyword\">import</span> Image</span><br><span class=\"line\">img = Image.<span class=\"built_in\">open</span>()</span><br><span class=\"line\">img.save(<span class=\"string\">&#x27;/&#x27;</span>)</span><br></pre></td></tr></table></figure></li>\n</ul>"},{"title":"python中os模块的使用","date":"2024-04-01T12:00:00.000Z","toc":true,"_content":"\n#  \n\n<!-- more -->\n\n# python中os模块的使用\n\n- 路径\n\n  - 表示路径：\n\n    `os.path.join()`\n\n  - 输出路径\n\n    `os.listdir()`\n\n  - 创建文件夹\n\n    ```python\n    os.makedirs('/', exits_ok=True)\n    ```\n\n    \n\n- 重命名：\n\n  `os.rename(os.path.join(), os.path.join())`","source":"_posts/python/os.md","raw":"---\ntitle: python中os模块的使用\ndate: 2024-04-1 20:00:00\ntoc: true\ntags: [python]\ncategories: [python库]\n\n\n\n---\n\n#  \n\n<!-- more -->\n\n# python中os模块的使用\n\n- 路径\n\n  - 表示路径：\n\n    `os.path.join()`\n\n  - 输出路径\n\n    `os.listdir()`\n\n  - 创建文件夹\n\n    ```python\n    os.makedirs('/', exits_ok=True)\n    ```\n\n    \n\n- 重命名：\n\n  `os.rename(os.path.join(), os.path.join())`","slug":"python/os","published":1,"updated":"2024-05-02T13:40:45.642Z","_id":"clvuli6ik006izsvwez3uhi0g","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h1 id=\"python中os模块的使用\"><a href=\"#python中os模块的使用\" class=\"headerlink\" title=\"python中os模块的使用\"></a>python中os模块的使用</h1><ul>\n<li><p>路径</p>\n<ul>\n<li><p>表示路径：</p>\n<p><code>os.path.join()</code></p>\n</li>\n<li><p>输出路径</p>\n<p><code>os.listdir()</code></p>\n</li>\n<li><p>创建文件夹</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">os.makedirs(<span class=\"string\">&#x27;/&#x27;</span>, exits_ok=<span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure></li>\n</ul>\n</li>\n<li><p>重命名：</p>\n<p><code>os.rename(os.path.join(), os.path.join())</code></p>\n</li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"python中os模块的使用\"><a href=\"#python中os模块的使用\" class=\"headerlink\" title=\"python中os模块的使用\"></a>python中os模块的使用</h1><ul>\n<li><p>路径</p>\n<ul>\n<li><p>表示路径：</p>\n<p><code>os.path.join()</code></p>\n</li>\n<li><p>输出路径</p>\n<p><code>os.listdir()</code></p>\n</li>\n<li><p>创建文件夹</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">os.makedirs(<span class=\"string\">&#x27;/&#x27;</span>, exits_ok=<span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure></li>\n</ul>\n</li>\n<li><p>重命名：</p>\n<p><code>os.rename(os.path.join(), os.path.join())</code></p>\n</li>\n</ul>"},{"title":"python中random模块的使用","date":"2024-05-02T12:00:00.000Z","toc":true,"_content":"#  \n<!-- more -->\n\n# python中random模块的使用\n\n- 随机整数\n```python\nrandom.randint(0, 9) # 左右都包含\n```\n\n- 洗牌\n```python\nrandom.shuffle(list)\n```","source":"_posts/python/ramdom.md","raw":"---\ntitle: python中random模块的使用\ndate: 2024-05-2 20:00:00\ntoc: true\ntags: [python]\ncategories: [python库]\n---\n#  \n<!-- more -->\n\n# python中random模块的使用\n\n- 随机整数\n```python\nrandom.randint(0, 9) # 左右都包含\n```\n\n- 洗牌\n```python\nrandom.shuffle(list)\n```","slug":"python/ramdom","published":1,"updated":"2024-05-02T13:42:55.552Z","_id":"clvuli6il006lzsvwdl9z084l","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h1 id=\"python中random模块的使用\"><a href=\"#python中random模块的使用\" class=\"headerlink\" title=\"python中random模块的使用\"></a>python中random模块的使用</h1><ul>\n<li><p>随机整数</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">random.randint(<span class=\"number\">0</span>, <span class=\"number\">9</span>) <span class=\"comment\"># 左右都包含</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>洗牌</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">random.shuffle(<span class=\"built_in\">list</span>)</span><br></pre></td></tr></table></figure></li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"python中random模块的使用\"><a href=\"#python中random模块的使用\" class=\"headerlink\" title=\"python中random模块的使用\"></a>python中random模块的使用</h1><ul>\n<li><p>随机整数</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">random.randint(<span class=\"number\">0</span>, <span class=\"number\">9</span>) <span class=\"comment\"># 左右都包含</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>洗牌</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">random.shuffle(<span class=\"built_in\">list</span>)</span><br></pre></td></tr></table></figure></li>\n</ul>"},{"title":"python中re模块的使用","date":"2024-04-01T12:00:00.000Z","toc":true,"_content":"\n#  \n\n<!-- more -->\n\n# python中re模块的使用\n\n- 匹配数字：`match = re.match(r'(\\d+)',string)`\n\n- 输出匹配的第一个：\n\n  `match.group(0)`","source":"_posts/python/re.md","raw":"---\ntitle: python中re模块的使用\ndate: 2024-04-1 20:00:00\ntoc: true\ntags: [python]\ncategories: [python库]\n\n\n\n---\n\n#  \n\n<!-- more -->\n\n# python中re模块的使用\n\n- 匹配数字：`match = re.match(r'(\\d+)',string)`\n\n- 输出匹配的第一个：\n\n  `match.group(0)`","slug":"python/re","published":1,"updated":"2024-05-02T13:40:52.768Z","_id":"clvuli6il006mzsvweutmfr3q","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h1 id=\"python中re模块的使用\"><a href=\"#python中re模块的使用\" class=\"headerlink\" title=\"python中re模块的使用\"></a>python中re模块的使用</h1><ul>\n<li><p>匹配数字：<code>match = re.match(r&#39;(\\d+)&#39;,string)</code></p>\n</li>\n<li><p>输出匹配的第一个：</p>\n<p><code>match.group(0)</code></p>\n</li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"python中re模块的使用\"><a href=\"#python中re模块的使用\" class=\"headerlink\" title=\"python中re模块的使用\"></a>python中re模块的使用</h1><ul>\n<li><p>匹配数字：<code>match = re.match(r&#39;(\\d+)&#39;,string)</code></p>\n</li>\n<li><p>输出匹配的第一个：</p>\n<p><code>match.group(0)</code></p>\n</li>\n</ul>"},{"title":"python tips","date":"2024-03-10T12:00:00.000Z","toc":true,"_content":"\n#  \n\n<!-- more -->\n\n\n\n- 导入当前文件夹（建了\\__init__.py）下的包：\n\n    ```python\n    from . import diffusion, unet\n    ```\n\n- 数字前自动补0：\n\n  ```python\n  name=f'{number:05d}.png'\n  ```\n\n  ","source":"_posts/python/tips.md","raw":"---\ntitle: python tips\ndate: 2024-03-10 20:00:00\ntoc: true\ntags: [python]\ncategories: [python]\n\n\n---\n\n#  \n\n<!-- more -->\n\n\n\n- 导入当前文件夹（建了\\__init__.py）下的包：\n\n    ```python\n    from . import diffusion, unet\n    ```\n\n- 数字前自动补0：\n\n  ```python\n  name=f'{number:05d}.png'\n  ```\n\n  ","slug":"python/tips","published":1,"updated":"2024-05-02T13:40:54.696Z","_id":"clvuli6im006nzsvwhzsg7dcd","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n\n\n<ul>\n<li><p>导入当前文件夹（建了_<em>init</em>_.py）下的包：</p>\n  <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> . <span class=\"keyword\">import</span> diffusion, unet</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>数字前自动补0：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">name=<span class=\"string\">f&#x27;<span class=\"subst\">&#123;number:05d&#125;</span>.png&#x27;</span></span><br></pre></td></tr></table></figure></li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<ul>\n<li><p>导入当前文件夹（建了_<em>init</em>_.py）下的包：</p>\n  <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> . <span class=\"keyword\">import</span> diffusion, unet</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>数字前自动补0：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">name=<span class=\"string\">f&#x27;<span class=\"subst\">&#123;number:05d&#125;</span>.png&#x27;</span></span><br></pre></td></tr></table></figure></li>\n</ul>"},{"title":"Windows powershell美化","date":"2024-03-11T16:00:00.000Z","comment":true,"toc":true,"_content":"\n#\n\n <!--more-->\n\n# Windows powershell美化\n\n- windows powershell编辑文件时颜色看不清（比如写python文件时，import语句看不见）\n\n## 1. 微软商店下载windows terminal\n\n![](D:/blog/themes/yilia/source/img/experience/windows/powershell/1.png)\n\n![](img/experience/windows/powershell/1.png)\n\n![](D:/blog/themes/yilia/source/img/experience/windows/powershell/2.png)\n\n![](img/experience/windows/powershell/2.png)","source":"_posts/experience/windows/powershell.md","raw":"---\ntitle: Windows powershell美化\n\ndate: 2024-3-12\n\ntags: [windows]\n\ncategories: [windows]\n\ncomment: true\n\ntoc: true\n\n---\n\n#\n\n <!--more-->\n\n# Windows powershell美化\n\n- windows powershell编辑文件时颜色看不清（比如写python文件时，import语句看不见）\n\n## 1. 微软商店下载windows terminal\n\n![](D:/blog/themes/yilia/source/img/experience/windows/powershell/1.png)\n\n![](img/experience/windows/powershell/1.png)\n\n![](D:/blog/themes/yilia/source/img/experience/windows/powershell/2.png)\n\n![](img/experience/windows/powershell/2.png)","slug":"experience/windows/powershell","published":1,"updated":"2024-03-12T01:57:25.668Z","comments":1,"layout":"post","photos":[],"_id":"clvuli6iu0073zsvwarxqbwod","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1> <span id=\"more\"></span>\n\n<h1 id=\"Windows-powershell美化\"><a href=\"#Windows-powershell美化\" class=\"headerlink\" title=\"Windows powershell美化\"></a>Windows powershell美化</h1><ul>\n<li>windows powershell编辑文件时颜色看不清（比如写python文件时，import语句看不见）</li>\n</ul>\n<h2 id=\"1-微软商店下载windows-terminal\"><a href=\"#1-微软商店下载windows-terminal\" class=\"headerlink\" title=\"1. 微软商店下载windows terminal\"></a>1. 微软商店下载windows terminal</h2><p><img src=\"D:/blog/themes/yilia/source/img/experience/windows/powershell/1.png\"></p>\n<p><img src=\"/img/experience/windows/powershell/1.png\"></p>\n<p><img src=\"D:/blog/themes/yilia/source/img/experience/windows/powershell/2.png\"></p>\n<p><img src=\"/img/experience/windows/powershell/2.png\"></p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"Windows-powershell美化\"><a href=\"#Windows-powershell美化\" class=\"headerlink\" title=\"Windows powershell美化\"></a>Windows powershell美化</h1><ul>\n<li>windows powershell编辑文件时颜色看不清（比如写python文件时，import语句看不见）</li>\n</ul>\n<h2 id=\"1-微软商店下载windows-terminal\"><a href=\"#1-微软商店下载windows-terminal\" class=\"headerlink\" title=\"1. 微软商店下载windows terminal\"></a>1. 微软商店下载windows terminal</h2><p><img src=\"D:/blog/themes/yilia/source/img/experience/windows/powershell/1.png\"></p>\n<p><img src=\"/img/experience/windows/powershell/1.png\"></p>\n<p><img src=\"D:/blog/themes/yilia/source/img/experience/windows/powershell/2.png\"></p>\n<p><img src=\"/img/experience/windows/powershell/2.png\"></p>"},{"title":"Windows卸载ssh","date":"2024-03-10T16:00:00.000Z","comment":true,"toc":true,"_content":"\n#\n\n <!--more-->\n\n# Windows使用Git的ssh\n\n- 发现Windows的ssh服务版本较低，不好更新，选择使用Git的ssh（前提是你安装过Git），每次更新Git（`git update-git-for-windows`）ssh就会自动更新。\n\n## 1. 卸载windows的ssh服务\n\n- windows搜索：`可选功能`\n- ![](D:/blog/themes/yilia/source/img/experience/windows/ssh/1.png)\n- ![](img/experience/windows/ssh/1.png)\n\n## 2. 将Git的ssh目录加入环境变量\n\n- 通常Git的ssh是在Git/usr/bin下（用\\\\)\n\n![](D:/blog/themes/yilia/source/img/experience/windows/ssh/2.png)\n![](img/experience/windows/ssh/2.png)\n\n![](D:/blog/themes/yilia/source/img/experience/windows/ssh/3.png)\n![](img/experience/windows/ssh/3.png)\n\n \n\n","source":"_posts/experience/windows/ssh.md","raw":"---\ntitle: Windows卸载ssh\n\ndate: 2024-3-11\n\ntags: [windows]\n\ncategories: [windows]\n\ncomment: true\n\ntoc: true\n---\n\n#\n\n <!--more-->\n\n# Windows使用Git的ssh\n\n- 发现Windows的ssh服务版本较低，不好更新，选择使用Git的ssh（前提是你安装过Git），每次更新Git（`git update-git-for-windows`）ssh就会自动更新。\n\n## 1. 卸载windows的ssh服务\n\n- windows搜索：`可选功能`\n- ![](D:/blog/themes/yilia/source/img/experience/windows/ssh/1.png)\n- ![](img/experience/windows/ssh/1.png)\n\n## 2. 将Git的ssh目录加入环境变量\n\n- 通常Git的ssh是在Git/usr/bin下（用\\\\)\n\n![](D:/blog/themes/yilia/source/img/experience/windows/ssh/2.png)\n![](img/experience/windows/ssh/2.png)\n\n![](D:/blog/themes/yilia/source/img/experience/windows/ssh/3.png)\n![](img/experience/windows/ssh/3.png)\n\n \n\n","slug":"experience/windows/ssh","published":1,"updated":"2024-03-12T01:24:26.668Z","comments":1,"layout":"post","photos":[],"_id":"clvuli6iv0074zsvwhefn30gp","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1> <span id=\"more\"></span>\n\n<h1 id=\"Windows使用Git的ssh\"><a href=\"#Windows使用Git的ssh\" class=\"headerlink\" title=\"Windows使用Git的ssh\"></a>Windows使用Git的ssh</h1><ul>\n<li>发现Windows的ssh服务版本较低，不好更新，选择使用Git的ssh（前提是你安装过Git），每次更新Git（<code>git update-git-for-windows</code>）ssh就会自动更新。</li>\n</ul>\n<h2 id=\"1-卸载windows的ssh服务\"><a href=\"#1-卸载windows的ssh服务\" class=\"headerlink\" title=\"1. 卸载windows的ssh服务\"></a>1. 卸载windows的ssh服务</h2><ul>\n<li>windows搜索：<code>可选功能</code></li>\n<li><img src=\"D:/blog/themes/yilia/source/img/experience/windows/ssh/1.png\"></li>\n<li><img src=\"/img/experience/windows/ssh/1.png\"></li>\n</ul>\n<h2 id=\"2-将Git的ssh目录加入环境变量\"><a href=\"#2-将Git的ssh目录加入环境变量\" class=\"headerlink\" title=\"2. 将Git的ssh目录加入环境变量\"></a>2. 将Git的ssh目录加入环境变量</h2><ul>\n<li>通常Git的ssh是在Git&#x2F;usr&#x2F;bin下（用\\)</li>\n</ul>\n<p><img src=\"D:/blog/themes/yilia/source/img/experience/windows/ssh/2.png\"><br><img src=\"/img/experience/windows/ssh/2.png\"></p>\n<p><img src=\"D:/blog/themes/yilia/source/img/experience/windows/ssh/3.png\"><br><img src=\"/img/experience/windows/ssh/3.png\"></p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"Windows使用Git的ssh\"><a href=\"#Windows使用Git的ssh\" class=\"headerlink\" title=\"Windows使用Git的ssh\"></a>Windows使用Git的ssh</h1><ul>\n<li>发现Windows的ssh服务版本较低，不好更新，选择使用Git的ssh（前提是你安装过Git），每次更新Git（<code>git update-git-for-windows</code>）ssh就会自动更新。</li>\n</ul>\n<h2 id=\"1-卸载windows的ssh服务\"><a href=\"#1-卸载windows的ssh服务\" class=\"headerlink\" title=\"1. 卸载windows的ssh服务\"></a>1. 卸载windows的ssh服务</h2><ul>\n<li>windows搜索：<code>可选功能</code></li>\n<li><img src=\"D:/blog/themes/yilia/source/img/experience/windows/ssh/1.png\"></li>\n<li><img src=\"/img/experience/windows/ssh/1.png\"></li>\n</ul>\n<h2 id=\"2-将Git的ssh目录加入环境变量\"><a href=\"#2-将Git的ssh目录加入环境变量\" class=\"headerlink\" title=\"2. 将Git的ssh目录加入环境变量\"></a>2. 将Git的ssh目录加入环境变量</h2><ul>\n<li>通常Git的ssh是在Git&#x2F;usr&#x2F;bin下（用\\)</li>\n</ul>\n<p><img src=\"D:/blog/themes/yilia/source/img/experience/windows/ssh/2.png\"><br><img src=\"/img/experience/windows/ssh/2.png\"></p>\n<p><img src=\"D:/blog/themes/yilia/source/img/experience/windows/ssh/3.png\"><br><img src=\"/img/experience/windows/ssh/3.png\"></p>"},{"title":"生产实习-3 用户模块基础代码","date":"2024-03-04T03:00:00.000Z","comment":true,"toc":true,"_content":"\n#\n<!--more-->\n\n# 3. 用户模块基础代码\n\n- spring boot运行总体流程：前端发起请求 -> 后端controller -> 后端service -> mapper -> sql\n- 我们从后往前写，先写service\n\n## 3.1 实现service\n\n- service层是一个接口，实现类的形式。\n\n  ![](D:/blog/themes/yilia/source/img/java/produce_practice/3/1.png)\n\n  ![](img/java/produce_practice/3/1.png)\n\n  1. 首先在biz/user/service下创建UserService。然后根据文档中表格信息实现接口：\n\n     ```java\n     package com.lyingedu.questionnaire.biz.user.service;\n     \n     import com.lyingedu.questionnaire.dbmap.entities.UserInfo;\n     \n     import java.util.List;\n     \n     public interface UserService {\n     \n         /**\n          * 增加用户\n          * @param userInfo\n          * @return\n          */\n         int addUserInfo(UserInfo userInfo);\n     \n         /**\n          * 修改用户\n          * @param userInfo\n          * @return\n          */\n         int modifyUserInfo(UserInfo userInfo);\n     \n     \n         /**\n          * 删除用户\n          * @param id\n          * @return\n          */\n         int deleteUserById(String id);\n     \n         /**\n          * 查询用户列表\n          * @return\n          */\n         List<UserInfo> queryUserList(UserInfo userInfo);\n     }\n     \n     ```\n\n  2. 在biz/user/service下创建UserServiceImpl。编写service实现类\n\n     - 先写public class UserServiceImpl implements UserService然后鼠标放到下划线，点击实现）\n     - 然后在类上面写注释：@Service让springboot知道这是一个操作数据库的service\n- 写入UserInfoMapper属性，属性需要在实现类里面说明。@resource注解可以让springboot自动给他注入依赖，否则UserInfoMapper找不到\n  \n     ```java\n     package com.lyingedu.questionnaire.biz.user.service;\n     \n     import com.lyingedu.questionnaire.dbmap.entities.UserInfo;\n     import com.lyingedu.questionnaire.dbmap.imapper.UserInfoMapper;\n     import jakarta.annotation.Resource;\n     import org.springframework.stereotype.Service;\n     \n     import java.util.List;\n     \n     @Service //让spring boot知道这是操作数据库的service\n     public class UserServiceImpl implements UserService{\n     \n         @Resource //resource注解可以让springboot自动给他注入依赖，否则UserInfoMapper找不到\n         private UserInfoMapper userInfoMapper; //属性需要在实现类中说明\n     \n         @Override\n         public int addUserInfo(UserInfo userInfo) {\n             //TODO\n             return 0;\n         }\n     \n         @Override\n         public int modifyUserInfo(UserInfo userInfo) {\n             return 0;\n         }\n     \n         @Override\n         public int deleteUserById(String id) {\n             return 0;\n         }\n     \n    @Override\n         public List<UserInfo> queryUserList(UserInfo userInfo) {\n             return null;\n         }\n     }\n     \n    ```\n    \n     \n\n## 3.２ 实现controller\n\n ![](D:/blog/themes/yilia/source/img/java/produce_practice/3/2.png)![](D:/blog/themes/yilia/source/img/java/produce_practice/3/3.png)\n\n![](img/java/produce_practice/3/2.png)\n\n![](img/java/produce_practice/3/3.png)\n\n1. 在biz/user/controller下创建UserController类，并注解@RestController，使用rest的接口风格；注解@RequestMapping(\"/admin\")，使用request请求\n\n2. HttpResponseEntity是一个通用的给前端返回结果的类，因此我们在questionnaire目录下创建一个beans目录存放一些通用的bean，在beans里面创建HttpResponseEntity.java\n\n   ```java\n   package com.lyingedu.questionnaire.beans;\n   \n   import lombok.Data;\n   \n   import java.io.Serializable;\n   \n   @Data //lombook的data注解可以省略get set方法\n   public class HttpResponseEntity implements Serializable {\n       private String code;//状态码\n       private Object data;//返回数据\n       private String message;//状态消息\n   \n       public HttpResponseEntity(){ //默认初始信息\n           this.code=\"0\";\n           this.message=\"操作失败\";\n       }\n   \n   \n   }\n   \n   ```\n\n   - 注意：由于代码中使用了lombok，你需要在idea中下载lombok插件并启用其注解处理（一般idea会提示你）\n\n3. 继续完善controller：\n\n   ```java\n   package com.lyingedu.questionnaire.biz.user.controller;\n   \n   import com.lyingedu.questionnaire.beans.HttpResponseEntity;\n   import com.lyingedu.questionnaire.biz.user.service.UserService;\n   import com.lyingedu.questionnaire.dbmap.entities.UserInfo;\n   import com.lyingedu.questionnaire.dbmap.imapper.UserInfoMapper;\n   import jakarta.annotation.Resource;\n   import org.springframework.web.bind.annotation.RequestMapping;\n   import org.springframework.web.bind.annotation.RestController;\n   \n   @RestController //rest的接口风格\n   @RequestMapping(\"/admin\") //使用request请求\n   public class UserController {\n   \n       @Resource\n       private UserService userService;\n   \n       /**\n        * 用户列表\n        * @param userInfo\n        * @return\n        */\n       public HttpResponseEntity queryUserList(UserInfo userInfo){\n   \n           //TODO\n           return null;\n       }\n   \n       /**\n        * 增加用户\n        * @param userInfo\n        * @return\n        */\n       public HttpResponseEntity addUserInfo(UserInfo userInfo) {\n           //TODO\n           return null;\n       }\n   \n       /**\n        * 修改用户\n        * @param userInfo\n        * @return\n        */\n       public HttpResponseEntity modifyUserInfo(UserInfo userInfo) {\n   \n           //TODO\n           return null;\n       }\n   \n       /**\n        * 删除用户\n        * @param userInfo\n        * @return\n        */\n       public HttpResponseEntity deleteUserById(UserInfo userInfo) {\n           //TODO\n           return null;\n       }\n   \n       /**\n        * 用户登录\n        * @param userInfo\n        * @return\n        */\n       public HttpResponseEntity userLogin(UserInfo userInfo) {\n   \n           //TODO\n           return null;\n       }\n   }\n   \n   ```\n\n   ","source":"_posts/java/produce_practice/3_userbase.md","raw":"---\ntitle: 生产实习-3 用户模块基础代码\n\ndate: 2024-3-4 11:00:00\n\ntags: [生产实习,java,springboot]\n\ncategories: [java]\n\ncomment: true\n\ntoc: true\n\n\n\n\n---\n\n#\n<!--more-->\n\n# 3. 用户模块基础代码\n\n- spring boot运行总体流程：前端发起请求 -> 后端controller -> 后端service -> mapper -> sql\n- 我们从后往前写，先写service\n\n## 3.1 实现service\n\n- service层是一个接口，实现类的形式。\n\n  ![](D:/blog/themes/yilia/source/img/java/produce_practice/3/1.png)\n\n  ![](img/java/produce_practice/3/1.png)\n\n  1. 首先在biz/user/service下创建UserService。然后根据文档中表格信息实现接口：\n\n     ```java\n     package com.lyingedu.questionnaire.biz.user.service;\n     \n     import com.lyingedu.questionnaire.dbmap.entities.UserInfo;\n     \n     import java.util.List;\n     \n     public interface UserService {\n     \n         /**\n          * 增加用户\n          * @param userInfo\n          * @return\n          */\n         int addUserInfo(UserInfo userInfo);\n     \n         /**\n          * 修改用户\n          * @param userInfo\n          * @return\n          */\n         int modifyUserInfo(UserInfo userInfo);\n     \n     \n         /**\n          * 删除用户\n          * @param id\n          * @return\n          */\n         int deleteUserById(String id);\n     \n         /**\n          * 查询用户列表\n          * @return\n          */\n         List<UserInfo> queryUserList(UserInfo userInfo);\n     }\n     \n     ```\n\n  2. 在biz/user/service下创建UserServiceImpl。编写service实现类\n\n     - 先写public class UserServiceImpl implements UserService然后鼠标放到下划线，点击实现）\n     - 然后在类上面写注释：@Service让springboot知道这是一个操作数据库的service\n- 写入UserInfoMapper属性，属性需要在实现类里面说明。@resource注解可以让springboot自动给他注入依赖，否则UserInfoMapper找不到\n  \n     ```java\n     package com.lyingedu.questionnaire.biz.user.service;\n     \n     import com.lyingedu.questionnaire.dbmap.entities.UserInfo;\n     import com.lyingedu.questionnaire.dbmap.imapper.UserInfoMapper;\n     import jakarta.annotation.Resource;\n     import org.springframework.stereotype.Service;\n     \n     import java.util.List;\n     \n     @Service //让spring boot知道这是操作数据库的service\n     public class UserServiceImpl implements UserService{\n     \n         @Resource //resource注解可以让springboot自动给他注入依赖，否则UserInfoMapper找不到\n         private UserInfoMapper userInfoMapper; //属性需要在实现类中说明\n     \n         @Override\n         public int addUserInfo(UserInfo userInfo) {\n             //TODO\n             return 0;\n         }\n     \n         @Override\n         public int modifyUserInfo(UserInfo userInfo) {\n             return 0;\n         }\n     \n         @Override\n         public int deleteUserById(String id) {\n             return 0;\n         }\n     \n    @Override\n         public List<UserInfo> queryUserList(UserInfo userInfo) {\n             return null;\n         }\n     }\n     \n    ```\n    \n     \n\n## 3.２ 实现controller\n\n ![](D:/blog/themes/yilia/source/img/java/produce_practice/3/2.png)![](D:/blog/themes/yilia/source/img/java/produce_practice/3/3.png)\n\n![](img/java/produce_practice/3/2.png)\n\n![](img/java/produce_practice/3/3.png)\n\n1. 在biz/user/controller下创建UserController类，并注解@RestController，使用rest的接口风格；注解@RequestMapping(\"/admin\")，使用request请求\n\n2. HttpResponseEntity是一个通用的给前端返回结果的类，因此我们在questionnaire目录下创建一个beans目录存放一些通用的bean，在beans里面创建HttpResponseEntity.java\n\n   ```java\n   package com.lyingedu.questionnaire.beans;\n   \n   import lombok.Data;\n   \n   import java.io.Serializable;\n   \n   @Data //lombook的data注解可以省略get set方法\n   public class HttpResponseEntity implements Serializable {\n       private String code;//状态码\n       private Object data;//返回数据\n       private String message;//状态消息\n   \n       public HttpResponseEntity(){ //默认初始信息\n           this.code=\"0\";\n           this.message=\"操作失败\";\n       }\n   \n   \n   }\n   \n   ```\n\n   - 注意：由于代码中使用了lombok，你需要在idea中下载lombok插件并启用其注解处理（一般idea会提示你）\n\n3. 继续完善controller：\n\n   ```java\n   package com.lyingedu.questionnaire.biz.user.controller;\n   \n   import com.lyingedu.questionnaire.beans.HttpResponseEntity;\n   import com.lyingedu.questionnaire.biz.user.service.UserService;\n   import com.lyingedu.questionnaire.dbmap.entities.UserInfo;\n   import com.lyingedu.questionnaire.dbmap.imapper.UserInfoMapper;\n   import jakarta.annotation.Resource;\n   import org.springframework.web.bind.annotation.RequestMapping;\n   import org.springframework.web.bind.annotation.RestController;\n   \n   @RestController //rest的接口风格\n   @RequestMapping(\"/admin\") //使用request请求\n   public class UserController {\n   \n       @Resource\n       private UserService userService;\n   \n       /**\n        * 用户列表\n        * @param userInfo\n        * @return\n        */\n       public HttpResponseEntity queryUserList(UserInfo userInfo){\n   \n           //TODO\n           return null;\n       }\n   \n       /**\n        * 增加用户\n        * @param userInfo\n        * @return\n        */\n       public HttpResponseEntity addUserInfo(UserInfo userInfo) {\n           //TODO\n           return null;\n       }\n   \n       /**\n        * 修改用户\n        * @param userInfo\n        * @return\n        */\n       public HttpResponseEntity modifyUserInfo(UserInfo userInfo) {\n   \n           //TODO\n           return null;\n       }\n   \n       /**\n        * 删除用户\n        * @param userInfo\n        * @return\n        */\n       public HttpResponseEntity deleteUserById(UserInfo userInfo) {\n           //TODO\n           return null;\n       }\n   \n       /**\n        * 用户登录\n        * @param userInfo\n        * @return\n        */\n       public HttpResponseEntity userLogin(UserInfo userInfo) {\n   \n           //TODO\n           return null;\n       }\n   }\n   \n   ```\n\n   ","slug":"java/produce_practice/3_userbase","published":1,"updated":"2024-03-08T05:48:35.083Z","_id":"clvuli6rs007dzsvwbgaehcc0","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h1 id=\"3-用户模块基础代码\"><a href=\"#3-用户模块基础代码\" class=\"headerlink\" title=\"3. 用户模块基础代码\"></a>3. 用户模块基础代码</h1><ul>\n<li>spring boot运行总体流程：前端发起请求 -&gt; 后端controller -&gt; 后端service -&gt; mapper -&gt; sql</li>\n<li>我们从后往前写，先写service</li>\n</ul>\n<h2 id=\"3-1-实现service\"><a href=\"#3-1-实现service\" class=\"headerlink\" title=\"3.1 实现service\"></a>3.1 实现service</h2><ul>\n<li><p>service层是一个接口，实现类的形式。</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/3/1.png\"></p>\n<p><img src=\"/img/java/produce_practice/3/1.png\"></p>\n<ol>\n<li><p>首先在biz&#x2F;user&#x2F;service下创建UserService。然后根据文档中表格信息实现接口：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> com.lyingedu.questionnaire.biz.user.service;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> com.lyingedu.questionnaire.dbmap.entities.UserInfo;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.List;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">interface</span> <span class=\"title class_\">UserService</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 增加用户</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> userInfo</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"title function_\">addUserInfo</span><span class=\"params\">(UserInfo userInfo)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 修改用户</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> userInfo</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"title function_\">modifyUserInfo</span><span class=\"params\">(UserInfo userInfo)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 删除用户</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> id</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"title function_\">deleteUserById</span><span class=\"params\">(String id)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 查询用户列表</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    List&lt;UserInfo&gt; <span class=\"title function_\">queryUserList</span><span class=\"params\">(UserInfo userInfo)</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>在biz&#x2F;user&#x2F;service下创建UserServiceImpl。编写service实现类</p>\n<ul>\n<li>先写public class UserServiceImpl implements UserService然后鼠标放到下划线，点击实现）</li>\n<li>然后在类上面写注释：@Service让springboot知道这是一个操作数据库的service</li>\n</ul>\n</li>\n</ol>\n</li>\n<li><p>写入UserInfoMapper属性，属性需要在实现类里面说明。@resource注解可以让springboot自动给他注入依赖，否则UserInfoMapper找不到</p>\n   <figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> <span class=\"keyword\">package</span> com.lyingedu.questionnaire.biz.user.service;</span><br><span class=\"line\"> </span><br><span class=\"line\"> <span class=\"keyword\">import</span> com.lyingedu.questionnaire.dbmap.entities.UserInfo;</span><br><span class=\"line\"> <span class=\"keyword\">import</span> com.lyingedu.questionnaire.dbmap.imapper.UserInfoMapper;</span><br><span class=\"line\"> <span class=\"keyword\">import</span> jakarta.annotation.Resource;</span><br><span class=\"line\"> <span class=\"keyword\">import</span> org.springframework.stereotype.Service;</span><br><span class=\"line\"> </span><br><span class=\"line\"> <span class=\"keyword\">import</span> java.util.List;</span><br><span class=\"line\"> </span><br><span class=\"line\"> <span class=\"meta\">@Service</span> <span class=\"comment\">//让spring boot知道这是操作数据库的service</span></span><br><span class=\"line\"> <span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">UserServiceImpl</span> <span class=\"keyword\">implements</span> <span class=\"title class_\">UserService</span>&#123;</span><br><span class=\"line\"> </span><br><span class=\"line\">     <span class=\"meta\">@Resource</span> <span class=\"comment\">//resource注解可以让springboot自动给他注入依赖，否则UserInfoMapper找不到</span></span><br><span class=\"line\">     <span class=\"keyword\">private</span> UserInfoMapper userInfoMapper; <span class=\"comment\">//属性需要在实现类中说明</span></span><br><span class=\"line\"> </span><br><span class=\"line\">     <span class=\"meta\">@Override</span></span><br><span class=\"line\">     <span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">addUserInfo</span><span class=\"params\">(UserInfo userInfo)</span> &#123;</span><br><span class=\"line\">         <span class=\"comment\">//TODO</span></span><br><span class=\"line\">         <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\"> </span><br><span class=\"line\">     <span class=\"meta\">@Override</span></span><br><span class=\"line\">     <span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">modifyUserInfo</span><span class=\"params\">(UserInfo userInfo)</span> &#123;</span><br><span class=\"line\">         <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\"> </span><br><span class=\"line\">     <span class=\"meta\">@Override</span></span><br><span class=\"line\">     <span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">deleteUserById</span><span class=\"params\">(String id)</span> &#123;</span><br><span class=\"line\">         <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"meta\">@Override</span></span><br><span class=\"line\">     <span class=\"keyword\">public</span> List&lt;UserInfo&gt; <span class=\"title function_\">queryUserList</span><span class=\"params\">(UserInfo userInfo)</span> &#123;</span><br><span class=\"line\">         <span class=\"keyword\">return</span> <span class=\"literal\">null</span>;</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\"> &#125;</span><br><span class=\"line\"> </span><br></pre></td></tr></table></figure></li>\n</ul>\n<h2 id=\"3-２-实现controller\"><a href=\"#3-２-实现controller\" class=\"headerlink\" title=\"3.２ 实现controller\"></a>3.２ 实现controller</h2><p> <img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/3/2.png\"><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/3/3.png\"></p>\n<p><img src=\"/img/java/produce_practice/3/2.png\"></p>\n<p><img src=\"/img/java/produce_practice/3/3.png\"></p>\n<ol>\n<li><p>在biz&#x2F;user&#x2F;controller下创建UserController类，并注解@RestController，使用rest的接口风格；注解@RequestMapping(“&#x2F;admin”)，使用request请求</p>\n</li>\n<li><p>HttpResponseEntity是一个通用的给前端返回结果的类，因此我们在questionnaire目录下创建一个beans目录存放一些通用的bean，在beans里面创建HttpResponseEntity.java</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> com.lyingedu.questionnaire.beans;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> lombok.Data;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.io.Serializable;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@Data</span> <span class=\"comment\">//lombook的data注解可以省略get set方法</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">HttpResponseEntity</span> <span class=\"keyword\">implements</span> <span class=\"title class_\">Serializable</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> String code;<span class=\"comment\">//状态码</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> Object data;<span class=\"comment\">//返回数据</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> String message;<span class=\"comment\">//状态消息</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"title function_\">HttpResponseEntity</span><span class=\"params\">()</span>&#123; <span class=\"comment\">//默认初始信息</span></span><br><span class=\"line\">        <span class=\"built_in\">this</span>.code=<span class=\"string\">&quot;0&quot;</span>;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.message=<span class=\"string\">&quot;操作失败&quot;</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>注意：由于代码中使用了lombok，你需要在idea中下载lombok插件并启用其注解处理（一般idea会提示你）</li>\n</ul>\n</li>\n<li><p>继续完善controller：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> com.lyingedu.questionnaire.biz.user.controller;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> com.lyingedu.questionnaire.beans.HttpResponseEntity;</span><br><span class=\"line\"><span class=\"keyword\">import</span> com.lyingedu.questionnaire.biz.user.service.UserService;</span><br><span class=\"line\"><span class=\"keyword\">import</span> com.lyingedu.questionnaire.dbmap.entities.UserInfo;</span><br><span class=\"line\"><span class=\"keyword\">import</span> com.lyingedu.questionnaire.dbmap.imapper.UserInfoMapper;</span><br><span class=\"line\"><span class=\"keyword\">import</span> jakarta.annotation.Resource;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.web.bind.annotation.RequestMapping;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.web.bind.annotation.RestController;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@RestController</span> <span class=\"comment\">//rest的接口风格</span></span><br><span class=\"line\"><span class=\"meta\">@RequestMapping(&quot;/admin&quot;)</span> <span class=\"comment\">//使用request请求</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">UserController</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Resource</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> UserService userService;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 用户列表</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> userInfo</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">queryUserList</span><span class=\"params\">(UserInfo userInfo)</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//TODO</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">null</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 增加用户</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> userInfo</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">addUserInfo</span><span class=\"params\">(UserInfo userInfo)</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">//TODO</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">null</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 修改用户</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> userInfo</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">modifyUserInfo</span><span class=\"params\">(UserInfo userInfo)</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//TODO</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">null</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 删除用户</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> userInfo</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">deleteUserById</span><span class=\"params\">(UserInfo userInfo)</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">//TODO</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">null</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 用户登录</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> userInfo</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">userLogin</span><span class=\"params\">(UserInfo userInfo)</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//TODO</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">null</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></li>\n</ol>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"3-用户模块基础代码\"><a href=\"#3-用户模块基础代码\" class=\"headerlink\" title=\"3. 用户模块基础代码\"></a>3. 用户模块基础代码</h1><ul>\n<li>spring boot运行总体流程：前端发起请求 -&gt; 后端controller -&gt; 后端service -&gt; mapper -&gt; sql</li>\n<li>我们从后往前写，先写service</li>\n</ul>\n<h2 id=\"3-1-实现service\"><a href=\"#3-1-实现service\" class=\"headerlink\" title=\"3.1 实现service\"></a>3.1 实现service</h2><ul>\n<li><p>service层是一个接口，实现类的形式。</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/3/1.png\"></p>\n<p><img src=\"/img/java/produce_practice/3/1.png\"></p>\n<ol>\n<li><p>首先在biz&#x2F;user&#x2F;service下创建UserService。然后根据文档中表格信息实现接口：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> com.lyingedu.questionnaire.biz.user.service;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> com.lyingedu.questionnaire.dbmap.entities.UserInfo;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.List;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">interface</span> <span class=\"title class_\">UserService</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 增加用户</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> userInfo</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"title function_\">addUserInfo</span><span class=\"params\">(UserInfo userInfo)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 修改用户</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> userInfo</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"title function_\">modifyUserInfo</span><span class=\"params\">(UserInfo userInfo)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 删除用户</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> id</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"title function_\">deleteUserById</span><span class=\"params\">(String id)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 查询用户列表</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    List&lt;UserInfo&gt; <span class=\"title function_\">queryUserList</span><span class=\"params\">(UserInfo userInfo)</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>在biz&#x2F;user&#x2F;service下创建UserServiceImpl。编写service实现类</p>\n<ul>\n<li>先写public class UserServiceImpl implements UserService然后鼠标放到下划线，点击实现）</li>\n<li>然后在类上面写注释：@Service让springboot知道这是一个操作数据库的service</li>\n</ul>\n</li>\n</ol>\n</li>\n<li><p>写入UserInfoMapper属性，属性需要在实现类里面说明。@resource注解可以让springboot自动给他注入依赖，否则UserInfoMapper找不到</p>\n   <figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> <span class=\"keyword\">package</span> com.lyingedu.questionnaire.biz.user.service;</span><br><span class=\"line\"> </span><br><span class=\"line\"> <span class=\"keyword\">import</span> com.lyingedu.questionnaire.dbmap.entities.UserInfo;</span><br><span class=\"line\"> <span class=\"keyword\">import</span> com.lyingedu.questionnaire.dbmap.imapper.UserInfoMapper;</span><br><span class=\"line\"> <span class=\"keyword\">import</span> jakarta.annotation.Resource;</span><br><span class=\"line\"> <span class=\"keyword\">import</span> org.springframework.stereotype.Service;</span><br><span class=\"line\"> </span><br><span class=\"line\"> <span class=\"keyword\">import</span> java.util.List;</span><br><span class=\"line\"> </span><br><span class=\"line\"> <span class=\"meta\">@Service</span> <span class=\"comment\">//让spring boot知道这是操作数据库的service</span></span><br><span class=\"line\"> <span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">UserServiceImpl</span> <span class=\"keyword\">implements</span> <span class=\"title class_\">UserService</span>&#123;</span><br><span class=\"line\"> </span><br><span class=\"line\">     <span class=\"meta\">@Resource</span> <span class=\"comment\">//resource注解可以让springboot自动给他注入依赖，否则UserInfoMapper找不到</span></span><br><span class=\"line\">     <span class=\"keyword\">private</span> UserInfoMapper userInfoMapper; <span class=\"comment\">//属性需要在实现类中说明</span></span><br><span class=\"line\"> </span><br><span class=\"line\">     <span class=\"meta\">@Override</span></span><br><span class=\"line\">     <span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">addUserInfo</span><span class=\"params\">(UserInfo userInfo)</span> &#123;</span><br><span class=\"line\">         <span class=\"comment\">//TODO</span></span><br><span class=\"line\">         <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\"> </span><br><span class=\"line\">     <span class=\"meta\">@Override</span></span><br><span class=\"line\">     <span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">modifyUserInfo</span><span class=\"params\">(UserInfo userInfo)</span> &#123;</span><br><span class=\"line\">         <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\"> </span><br><span class=\"line\">     <span class=\"meta\">@Override</span></span><br><span class=\"line\">     <span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">deleteUserById</span><span class=\"params\">(String id)</span> &#123;</span><br><span class=\"line\">         <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"meta\">@Override</span></span><br><span class=\"line\">     <span class=\"keyword\">public</span> List&lt;UserInfo&gt; <span class=\"title function_\">queryUserList</span><span class=\"params\">(UserInfo userInfo)</span> &#123;</span><br><span class=\"line\">         <span class=\"keyword\">return</span> <span class=\"literal\">null</span>;</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\"> &#125;</span><br><span class=\"line\"> </span><br></pre></td></tr></table></figure></li>\n</ul>\n<h2 id=\"3-２-实现controller\"><a href=\"#3-２-实现controller\" class=\"headerlink\" title=\"3.２ 实现controller\"></a>3.２ 实现controller</h2><p> <img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/3/2.png\"><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/3/3.png\"></p>\n<p><img src=\"/img/java/produce_practice/3/2.png\"></p>\n<p><img src=\"/img/java/produce_practice/3/3.png\"></p>\n<ol>\n<li><p>在biz&#x2F;user&#x2F;controller下创建UserController类，并注解@RestController，使用rest的接口风格；注解@RequestMapping(“&#x2F;admin”)，使用request请求</p>\n</li>\n<li><p>HttpResponseEntity是一个通用的给前端返回结果的类，因此我们在questionnaire目录下创建一个beans目录存放一些通用的bean，在beans里面创建HttpResponseEntity.java</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> com.lyingedu.questionnaire.beans;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> lombok.Data;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.io.Serializable;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@Data</span> <span class=\"comment\">//lombook的data注解可以省略get set方法</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">HttpResponseEntity</span> <span class=\"keyword\">implements</span> <span class=\"title class_\">Serializable</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> String code;<span class=\"comment\">//状态码</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> Object data;<span class=\"comment\">//返回数据</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> String message;<span class=\"comment\">//状态消息</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"title function_\">HttpResponseEntity</span><span class=\"params\">()</span>&#123; <span class=\"comment\">//默认初始信息</span></span><br><span class=\"line\">        <span class=\"built_in\">this</span>.code=<span class=\"string\">&quot;0&quot;</span>;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.message=<span class=\"string\">&quot;操作失败&quot;</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>注意：由于代码中使用了lombok，你需要在idea中下载lombok插件并启用其注解处理（一般idea会提示你）</li>\n</ul>\n</li>\n<li><p>继续完善controller：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> com.lyingedu.questionnaire.biz.user.controller;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> com.lyingedu.questionnaire.beans.HttpResponseEntity;</span><br><span class=\"line\"><span class=\"keyword\">import</span> com.lyingedu.questionnaire.biz.user.service.UserService;</span><br><span class=\"line\"><span class=\"keyword\">import</span> com.lyingedu.questionnaire.dbmap.entities.UserInfo;</span><br><span class=\"line\"><span class=\"keyword\">import</span> com.lyingedu.questionnaire.dbmap.imapper.UserInfoMapper;</span><br><span class=\"line\"><span class=\"keyword\">import</span> jakarta.annotation.Resource;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.web.bind.annotation.RequestMapping;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.web.bind.annotation.RestController;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@RestController</span> <span class=\"comment\">//rest的接口风格</span></span><br><span class=\"line\"><span class=\"meta\">@RequestMapping(&quot;/admin&quot;)</span> <span class=\"comment\">//使用request请求</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">UserController</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Resource</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> UserService userService;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 用户列表</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> userInfo</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">queryUserList</span><span class=\"params\">(UserInfo userInfo)</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//TODO</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">null</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 增加用户</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> userInfo</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">addUserInfo</span><span class=\"params\">(UserInfo userInfo)</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">//TODO</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">null</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 修改用户</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> userInfo</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">modifyUserInfo</span><span class=\"params\">(UserInfo userInfo)</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//TODO</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">null</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 删除用户</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> userInfo</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">deleteUserById</span><span class=\"params\">(UserInfo userInfo)</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">//TODO</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">null</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 用户登录</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> userInfo</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">userLogin</span><span class=\"params\">(UserInfo userInfo)</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//TODO</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">null</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></li>\n</ol>"},{"title":"生产实习-0 开发环境准备","date":"2024-03-01T00:00:00.000Z","comment":true,"toc":true,"_content":"\n#\n<!--more-->\n\n# 0. 开发环境准备\n\n## 0.1 安装java\n\n### 0.1.1 下载安装\n\n- 下载地址： [Archived OpenJDK GA Releases (java.net)](https://jdk.java.net/archive/) \n\n  ![](D:/blog/themes/yilia/source/img/java/produce_practice/0/1.png)\n\n  ![](img/java/produce_practice/0/1.png)\n\n- 下载后解压到你想放到的目录（比如我是D:/jdk-17\n\n### 0.1.2 配置环境变量\n\n1. windows搜索：环境变量。选择`编辑系统环境变量`\n\n2. 新建环境变量： ![](D:/blog/themes/yilia/source/img/java/produce_practice/0/2.png)\n\n   ![](img/java/produce_practice/0/2.png) \n\n3. 输入变量名：`JAVA_HOME`，变量值（根据你存放的路径）：`D:\\jdk-17`，确定。\n\n4. 双击path，选择新建，输入`%JAVA_HOME%\\bin`，确定，这样就将java的bin目录放到系统环境变量了，可以直接使用java命令了。（javac命令？？？）\n\n5. 重新启动终端（更新环境变量）输入java回车，可以看到有反应，说明ok了。\n\n\n\n## 0.2 安装docker\n\n| 方案                    | 要求               |\n| ----------------------- | ------------------ |\n| 本地下载mysql           | 本地没下载过数据库 |\n| 虚拟机 + mysql          |                    |\n| 虚拟机 + docker + mysql |                    |\n| 服务器 + docker + mysql |                    |\n\n- 本次实验采用服务器 + docker + mysql\n\n### 0.2.1 下载ubuntu镜像（使用虚拟机）\n\n-  [Get Ubuntu Server | Download | Ubuntu](https://ubuntu.com/download/server) \n\n    ![](D:/blog/themes/yilia/source/img/java/produce_practice/0/4.png)\n\n    ![](img/java/produce_practice/0/4.png) \n\n### 0.2.2 安装虚拟机\n\n- 略\n\n### 0.2.3 安装docker\n\n- 服务器中输入\n\n    ```bash\n    sudo apt install docker.io\n    ```\n\n    如果命令无效，提示用snap安装，则用snap\n\n    ，输入y是确认。安装完成收输入whereis docker可以查看docker位置\n    \n- 服务器中输入\n\n    ```bash\n    - sudo docker search mysql\n    ```\n\n### 0.2.4 安装mysql\n\n- 服务器中输入：\n\n  ```bash\n  sudo docker pull mysql:8.0.34\n  ```\n\n- 查看镜像文件：\n\n  ```bash\n  sudo docker images\n  ```\n\n  可以看到镜像文件中有一个我们刚刚拉取的镜像\n\n- 将镜像封装成容器运行：(这里将docker的3306端口映射到本地的3306，你可以改，数据库密码123456)\n\n  ```bash\n  sudo docker run -itd --name mysql8034 -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 mysql:8.0.34\n  ```\n\n- 查看容器列表\n\n  ```bash\n  sudo docker ps -a\n  ```\n\n  看到刚刚的容器就说明ok了。\n\n- 可视化工具：navicat、Mysql workbench（使用较低版本的navicat连接的时候可能会发现连接错误，需要更新navicat版本）\n\n## 0.3 安装idea\n\n- 下载地址（划到下面下载社区版）：https://www.jetbrains.com/zh-cn/idea/download\n\n- 配置国内镜像：\n\n  - 设置内搜索：maven\n\n    ![](D:/blog/themes/yilia/source/img/java/produce_practice/0/5.png)\n\n    ![](img/java/produce_practice/0/5.png) \n\n  - 创建C:\\Users\\123\\.m2\\settings.xml\n\n    ```xml\n    <settings xmlns=\"http://maven.apache.org/SETTINGS/1.0.0\"\n              xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n              xsi:schemaLocation=\"http://maven.apache.org/SETTINGS/1.0.0\n                                  http://maven.apache.org/xsd/settings-1.0.0.xsd\">\n        <mirrors>\n            <mirror>\n                <id>alimaven</id>\n                <name>aliyun maven</name>\n                <url>http://maven.aliyun.com/nexus/content/groups/public/</url>\n                <mirrorOf>central</mirrorOf>\n            </mirror>\n        </mirrors>\n    </settings>\n    ```\n\n  - 重新打开设置，将刚刚的勾勾上\n\n    ![](D:/blog/themes/yilia/source/img/java/produce_practice/0/5.png)\n\n    ![](img/java/produce_practice/0/5.png) \n\n- 检验国内镜像配置\n\n  - 进入网址（建议收藏）： [Maven Repository: Search/Browse/Explore (mvnrepository.com)](https://mvnrepository.com/) \n\n  - 搜索fastjson2：\n\n    ![](D:/blog/themes/yilia/source/img/java/produce_practice/0/6.png)\n\n    ![](img/java/produce_practice/0/6.png) \n\n    ![](D:/blog/themes/yilia/source/img/java/produce_practice/0/7.png)\n\n    ![](img/java/produce_practice/0/7.png) \n\n    ![](D:/blog/themes/yilia/source/img/java/produce_practice/0/8.png)\n\n    ![](img/java/produce_practice/0/8.png) \n\n    - 复制：\n\n    ```xml\n    <!-- https://mvnrepository.com/artifact/com.alibaba.fastjson2/fastjson2 -->\n    <dependency>\n        <groupId>com.alibaba.fastjson2</groupId>\n        <artifactId>fastjson2</artifactId>\n        <version>2.0.47</version>\n    </dependency>\n    \n    ```\n\n    - 将上面的内容粘贴到项目的pom.xml的<dependences>标签下（无则自己建），更新maven：\n\n    ![](D:/blog/themes/yilia/source/img/java/produce_practice/0/9.png)\n\n    ![](img/java/produce_practice/0/9.png) \n\n    - 此时刚刚粘贴的依赖不是红色字体了，说明下载ok，然后输入json可以看到有代码自动补全：\n\n      ![](D:/blog/themes/yilia/source/img/java/produce_practice/0/10.png)\n\n      ![](img/java/produce_practice/0/10.png) \n\n- 统一编码：（搜索encoding或编码，还有搜索控制台：编码）\n\n  ![](D:/blog/themes/yilia/source/img/java/produce_practice/0/11.png)\n\n  ![](img/java/produce_practice/0/11.png) \n\n- 避免其他插件在控制台输出中文乱码：\n\n  ![](D:/blog/themes/yilia/source/img/java/produce_practice/0/12.png)\n\n  ![](img/java/produce_practice/0/12.png) \n\n  输入：\n\n  ```\n  -Xmx4096m\n  -Dfile.encoding=UTF-8\n  ```","source":"_posts/java/produce_practice/0_config.md","raw":"---\ntitle: 生产实习-0 开发环境准备\n\ndate: 2024-3-1 08:00:00\n\ntags: [生产实习,java,springboot]\n\ncategories: [java]\n\ncomment: true\n\ntoc: true\n\n\n\n---\n\n#\n<!--more-->\n\n# 0. 开发环境准备\n\n## 0.1 安装java\n\n### 0.1.1 下载安装\n\n- 下载地址： [Archived OpenJDK GA Releases (java.net)](https://jdk.java.net/archive/) \n\n  ![](D:/blog/themes/yilia/source/img/java/produce_practice/0/1.png)\n\n  ![](img/java/produce_practice/0/1.png)\n\n- 下载后解压到你想放到的目录（比如我是D:/jdk-17\n\n### 0.1.2 配置环境变量\n\n1. windows搜索：环境变量。选择`编辑系统环境变量`\n\n2. 新建环境变量： ![](D:/blog/themes/yilia/source/img/java/produce_practice/0/2.png)\n\n   ![](img/java/produce_practice/0/2.png) \n\n3. 输入变量名：`JAVA_HOME`，变量值（根据你存放的路径）：`D:\\jdk-17`，确定。\n\n4. 双击path，选择新建，输入`%JAVA_HOME%\\bin`，确定，这样就将java的bin目录放到系统环境变量了，可以直接使用java命令了。（javac命令？？？）\n\n5. 重新启动终端（更新环境变量）输入java回车，可以看到有反应，说明ok了。\n\n\n\n## 0.2 安装docker\n\n| 方案                    | 要求               |\n| ----------------------- | ------------------ |\n| 本地下载mysql           | 本地没下载过数据库 |\n| 虚拟机 + mysql          |                    |\n| 虚拟机 + docker + mysql |                    |\n| 服务器 + docker + mysql |                    |\n\n- 本次实验采用服务器 + docker + mysql\n\n### 0.2.1 下载ubuntu镜像（使用虚拟机）\n\n-  [Get Ubuntu Server | Download | Ubuntu](https://ubuntu.com/download/server) \n\n    ![](D:/blog/themes/yilia/source/img/java/produce_practice/0/4.png)\n\n    ![](img/java/produce_practice/0/4.png) \n\n### 0.2.2 安装虚拟机\n\n- 略\n\n### 0.2.3 安装docker\n\n- 服务器中输入\n\n    ```bash\n    sudo apt install docker.io\n    ```\n\n    如果命令无效，提示用snap安装，则用snap\n\n    ，输入y是确认。安装完成收输入whereis docker可以查看docker位置\n    \n- 服务器中输入\n\n    ```bash\n    - sudo docker search mysql\n    ```\n\n### 0.2.4 安装mysql\n\n- 服务器中输入：\n\n  ```bash\n  sudo docker pull mysql:8.0.34\n  ```\n\n- 查看镜像文件：\n\n  ```bash\n  sudo docker images\n  ```\n\n  可以看到镜像文件中有一个我们刚刚拉取的镜像\n\n- 将镜像封装成容器运行：(这里将docker的3306端口映射到本地的3306，你可以改，数据库密码123456)\n\n  ```bash\n  sudo docker run -itd --name mysql8034 -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 mysql:8.0.34\n  ```\n\n- 查看容器列表\n\n  ```bash\n  sudo docker ps -a\n  ```\n\n  看到刚刚的容器就说明ok了。\n\n- 可视化工具：navicat、Mysql workbench（使用较低版本的navicat连接的时候可能会发现连接错误，需要更新navicat版本）\n\n## 0.3 安装idea\n\n- 下载地址（划到下面下载社区版）：https://www.jetbrains.com/zh-cn/idea/download\n\n- 配置国内镜像：\n\n  - 设置内搜索：maven\n\n    ![](D:/blog/themes/yilia/source/img/java/produce_practice/0/5.png)\n\n    ![](img/java/produce_practice/0/5.png) \n\n  - 创建C:\\Users\\123\\.m2\\settings.xml\n\n    ```xml\n    <settings xmlns=\"http://maven.apache.org/SETTINGS/1.0.0\"\n              xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n              xsi:schemaLocation=\"http://maven.apache.org/SETTINGS/1.0.0\n                                  http://maven.apache.org/xsd/settings-1.0.0.xsd\">\n        <mirrors>\n            <mirror>\n                <id>alimaven</id>\n                <name>aliyun maven</name>\n                <url>http://maven.aliyun.com/nexus/content/groups/public/</url>\n                <mirrorOf>central</mirrorOf>\n            </mirror>\n        </mirrors>\n    </settings>\n    ```\n\n  - 重新打开设置，将刚刚的勾勾上\n\n    ![](D:/blog/themes/yilia/source/img/java/produce_practice/0/5.png)\n\n    ![](img/java/produce_practice/0/5.png) \n\n- 检验国内镜像配置\n\n  - 进入网址（建议收藏）： [Maven Repository: Search/Browse/Explore (mvnrepository.com)](https://mvnrepository.com/) \n\n  - 搜索fastjson2：\n\n    ![](D:/blog/themes/yilia/source/img/java/produce_practice/0/6.png)\n\n    ![](img/java/produce_practice/0/6.png) \n\n    ![](D:/blog/themes/yilia/source/img/java/produce_practice/0/7.png)\n\n    ![](img/java/produce_practice/0/7.png) \n\n    ![](D:/blog/themes/yilia/source/img/java/produce_practice/0/8.png)\n\n    ![](img/java/produce_practice/0/8.png) \n\n    - 复制：\n\n    ```xml\n    <!-- https://mvnrepository.com/artifact/com.alibaba.fastjson2/fastjson2 -->\n    <dependency>\n        <groupId>com.alibaba.fastjson2</groupId>\n        <artifactId>fastjson2</artifactId>\n        <version>2.0.47</version>\n    </dependency>\n    \n    ```\n\n    - 将上面的内容粘贴到项目的pom.xml的<dependences>标签下（无则自己建），更新maven：\n\n    ![](D:/blog/themes/yilia/source/img/java/produce_practice/0/9.png)\n\n    ![](img/java/produce_practice/0/9.png) \n\n    - 此时刚刚粘贴的依赖不是红色字体了，说明下载ok，然后输入json可以看到有代码自动补全：\n\n      ![](D:/blog/themes/yilia/source/img/java/produce_practice/0/10.png)\n\n      ![](img/java/produce_practice/0/10.png) \n\n- 统一编码：（搜索encoding或编码，还有搜索控制台：编码）\n\n  ![](D:/blog/themes/yilia/source/img/java/produce_practice/0/11.png)\n\n  ![](img/java/produce_practice/0/11.png) \n\n- 避免其他插件在控制台输出中文乱码：\n\n  ![](D:/blog/themes/yilia/source/img/java/produce_practice/0/12.png)\n\n  ![](img/java/produce_practice/0/12.png) \n\n  输入：\n\n  ```\n  -Xmx4096m\n  -Dfile.encoding=UTF-8\n  ```","slug":"java/produce_practice/0_config","published":1,"updated":"2024-03-06T12:05:56.798Z","_id":"clvuli6rt007ezsvwfmmi0kz3","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h1 id=\"0-开发环境准备\"><a href=\"#0-开发环境准备\" class=\"headerlink\" title=\"0. 开发环境准备\"></a>0. 开发环境准备</h1><h2 id=\"0-1-安装java\"><a href=\"#0-1-安装java\" class=\"headerlink\" title=\"0.1 安装java\"></a>0.1 安装java</h2><h3 id=\"0-1-1-下载安装\"><a href=\"#0-1-1-下载安装\" class=\"headerlink\" title=\"0.1.1 下载安装\"></a>0.1.1 下载安装</h3><ul>\n<li><p>下载地址： <a href=\"https://jdk.java.net/archive/\">Archived OpenJDK GA Releases (java.net)</a> </p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/0/1.png\"></p>\n<p><img src=\"/img/java/produce_practice/0/1.png\"></p>\n</li>\n<li><p>下载后解压到你想放到的目录（比如我是D:&#x2F;jdk-17</p>\n</li>\n</ul>\n<h3 id=\"0-1-2-配置环境变量\"><a href=\"#0-1-2-配置环境变量\" class=\"headerlink\" title=\"0.1.2 配置环境变量\"></a>0.1.2 配置环境变量</h3><ol>\n<li><p>windows搜索：环境变量。选择<code>编辑系统环境变量</code></p>\n</li>\n<li><p>新建环境变量： <img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/0/2.png\"></p>\n<p><img src=\"/img/java/produce_practice/0/2.png\"> </p>\n</li>\n<li><p>输入变量名：<code>JAVA_HOME</code>，变量值（根据你存放的路径）：<code>D:\\jdk-17</code>，确定。</p>\n</li>\n<li><p>双击path，选择新建，输入<code>%JAVA_HOME%\\bin</code>，确定，这样就将java的bin目录放到系统环境变量了，可以直接使用java命令了。（javac命令？？？）</p>\n</li>\n<li><p>重新启动终端（更新环境变量）输入java回车，可以看到有反应，说明ok了。</p>\n</li>\n</ol>\n<h2 id=\"0-2-安装docker\"><a href=\"#0-2-安装docker\" class=\"headerlink\" title=\"0.2 安装docker\"></a>0.2 安装docker</h2><table>\n<thead>\n<tr>\n<th>方案</th>\n<th>要求</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>本地下载mysql</td>\n<td>本地没下载过数据库</td>\n</tr>\n<tr>\n<td>虚拟机 + mysql</td>\n<td></td>\n</tr>\n<tr>\n<td>虚拟机 + docker + mysql</td>\n<td></td>\n</tr>\n<tr>\n<td>服务器 + docker + mysql</td>\n<td></td>\n</tr>\n</tbody></table>\n<ul>\n<li>本次实验采用服务器 + docker + mysql</li>\n</ul>\n<h3 id=\"0-2-1-下载ubuntu镜像（使用虚拟机）\"><a href=\"#0-2-1-下载ubuntu镜像（使用虚拟机）\" class=\"headerlink\" title=\"0.2.1 下载ubuntu镜像（使用虚拟机）\"></a>0.2.1 下载ubuntu镜像（使用虚拟机）</h3><ul>\n<li><p><a href=\"https://ubuntu.com/download/server\">Get Ubuntu Server | Download | Ubuntu</a> </p>\n<p> <img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/0/4.png\"></p>\n<p> <img src=\"/img/java/produce_practice/0/4.png\"></p>\n</li>\n</ul>\n<h3 id=\"0-2-2-安装虚拟机\"><a href=\"#0-2-2-安装虚拟机\" class=\"headerlink\" title=\"0.2.2 安装虚拟机\"></a>0.2.2 安装虚拟机</h3><ul>\n<li>略</li>\n</ul>\n<h3 id=\"0-2-3-安装docker\"><a href=\"#0-2-3-安装docker\" class=\"headerlink\" title=\"0.2.3 安装docker\"></a>0.2.3 安装docker</h3><ul>\n<li><p>服务器中输入</p>\n  <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt install docker.io</span><br></pre></td></tr></table></figure>\n\n<p>  如果命令无效，提示用snap安装，则用snap</p>\n<p>  ，输入y是确认。安装完成收输入whereis docker可以查看docker位置</p>\n</li>\n<li><p>服务器中输入</p>\n  <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">- sudo docker search mysql</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h3 id=\"0-2-4-安装mysql\"><a href=\"#0-2-4-安装mysql\" class=\"headerlink\" title=\"0.2.4 安装mysql\"></a>0.2.4 安装mysql</h3><ul>\n<li><p>服务器中输入：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo docker pull mysql:8.0.34</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>查看镜像文件：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo docker images</span><br></pre></td></tr></table></figure>\n\n<p>可以看到镜像文件中有一个我们刚刚拉取的镜像</p>\n</li>\n<li><p>将镜像封装成容器运行：(这里将docker的3306端口映射到本地的3306，你可以改，数据库密码123456)</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo docker run -itd --name mysql8034 -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 mysql:8.0.34</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>查看容器列表</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo docker ps -a</span><br></pre></td></tr></table></figure>\n\n<p>看到刚刚的容器就说明ok了。</p>\n</li>\n<li><p>可视化工具：navicat、Mysql workbench（使用较低版本的navicat连接的时候可能会发现连接错误，需要更新navicat版本）</p>\n</li>\n</ul>\n<h2 id=\"0-3-安装idea\"><a href=\"#0-3-安装idea\" class=\"headerlink\" title=\"0.3 安装idea\"></a>0.3 安装idea</h2><ul>\n<li><p>下载地址（划到下面下载社区版）：<a href=\"https://www.jetbrains.com/zh-cn/idea/download\">https://www.jetbrains.com/zh-cn/idea/download</a></p>\n</li>\n<li><p>配置国内镜像：</p>\n<ul>\n<li><p>设置内搜索：maven</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/0/5.png\"></p>\n<p><img src=\"/img/java/produce_practice/0/5.png\"> </p>\n</li>\n<li><p>创建C:\\Users\\123.m2\\settings.xml</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">settings</span> <span class=\"attr\">xmlns</span>=<span class=\"string\">&quot;http://maven.apache.org/SETTINGS/1.0.0&quot;</span></span></span><br><span class=\"line\"><span class=\"tag\">          <span class=\"attr\">xmlns:xsi</span>=<span class=\"string\">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span></span><br><span class=\"line\"><span class=\"tag\">          <span class=\"attr\">xsi:schemaLocation</span>=<span class=\"string\">&quot;http://maven.apache.org/SETTINGS/1.0.0</span></span></span><br><span class=\"line\"><span class=\"string\"><span class=\"tag\">                              http://maven.apache.org/xsd/settings-1.0.0.xsd&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">mirrors</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">mirror</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">id</span>&gt;</span>alimaven<span class=\"tag\">&lt;/<span class=\"name\">id</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>aliyun maven<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">url</span>&gt;</span>http://maven.aliyun.com/nexus/content/groups/public/<span class=\"tag\">&lt;/<span class=\"name\">url</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">mirrorOf</span>&gt;</span>central<span class=\"tag\">&lt;/<span class=\"name\">mirrorOf</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">mirror</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">mirrors</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">settings</span>&gt;</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>重新打开设置，将刚刚的勾勾上</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/0/5.png\"></p>\n<p><img src=\"/img/java/produce_practice/0/5.png\"></p>\n</li>\n</ul>\n</li>\n<li><p>检验国内镜像配置</p>\n<ul>\n<li><p>进入网址（建议收藏）： <a href=\"https://mvnrepository.com/\">Maven Repository: Search&#x2F;Browse&#x2F;Explore (mvnrepository.com)</a> </p>\n</li>\n<li><p>搜索fastjson2：</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/0/6.png\"></p>\n<p><img src=\"/img/java/produce_practice/0/6.png\"> </p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/0/7.png\"></p>\n<p><img src=\"/img/java/produce_practice/0/7.png\"> </p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/0/8.png\"></p>\n<p><img src=\"/img/java/produce_practice/0/8.png\"> </p>\n<ul>\n<li>复制：</li>\n</ul>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">&lt;!-- https://mvnrepository.com/artifact/com.alibaba.fastjson2/fastjson2 --&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>com.alibaba.fastjson2<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>fastjson2<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>2.0.47<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>将上面的内容粘贴到项目的pom.xml的<dependences>标签下（无则自己建），更新maven：</li>\n</ul>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/0/9.png\"></p>\n<p><img src=\"/img/java/produce_practice/0/9.png\"> </p>\n<ul>\n<li><p>此时刚刚粘贴的依赖不是红色字体了，说明下载ok，然后输入json可以看到有代码自动补全：</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/0/10.png\"></p>\n<p><img src=\"/img/java/produce_practice/0/10.png\"></p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>统一编码：（搜索encoding或编码，还有搜索控制台：编码）</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/0/11.png\"></p>\n<p><img src=\"/img/java/produce_practice/0/11.png\"> </p>\n</li>\n<li><p>避免其他插件在控制台输出中文乱码：</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/0/12.png\"></p>\n<p><img src=\"/img/java/produce_practice/0/12.png\"> </p>\n<p>输入：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">-Xmx4096m</span><br><span class=\"line\">-Dfile.encoding=UTF-8</span><br></pre></td></tr></table></figure></li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"0-开发环境准备\"><a href=\"#0-开发环境准备\" class=\"headerlink\" title=\"0. 开发环境准备\"></a>0. 开发环境准备</h1><h2 id=\"0-1-安装java\"><a href=\"#0-1-安装java\" class=\"headerlink\" title=\"0.1 安装java\"></a>0.1 安装java</h2><h3 id=\"0-1-1-下载安装\"><a href=\"#0-1-1-下载安装\" class=\"headerlink\" title=\"0.1.1 下载安装\"></a>0.1.1 下载安装</h3><ul>\n<li><p>下载地址： <a href=\"https://jdk.java.net/archive/\">Archived OpenJDK GA Releases (java.net)</a> </p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/0/1.png\"></p>\n<p><img src=\"/img/java/produce_practice/0/1.png\"></p>\n</li>\n<li><p>下载后解压到你想放到的目录（比如我是D:&#x2F;jdk-17</p>\n</li>\n</ul>\n<h3 id=\"0-1-2-配置环境变量\"><a href=\"#0-1-2-配置环境变量\" class=\"headerlink\" title=\"0.1.2 配置环境变量\"></a>0.1.2 配置环境变量</h3><ol>\n<li><p>windows搜索：环境变量。选择<code>编辑系统环境变量</code></p>\n</li>\n<li><p>新建环境变量： <img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/0/2.png\"></p>\n<p><img src=\"/img/java/produce_practice/0/2.png\"> </p>\n</li>\n<li><p>输入变量名：<code>JAVA_HOME</code>，变量值（根据你存放的路径）：<code>D:\\jdk-17</code>，确定。</p>\n</li>\n<li><p>双击path，选择新建，输入<code>%JAVA_HOME%\\bin</code>，确定，这样就将java的bin目录放到系统环境变量了，可以直接使用java命令了。（javac命令？？？）</p>\n</li>\n<li><p>重新启动终端（更新环境变量）输入java回车，可以看到有反应，说明ok了。</p>\n</li>\n</ol>\n<h2 id=\"0-2-安装docker\"><a href=\"#0-2-安装docker\" class=\"headerlink\" title=\"0.2 安装docker\"></a>0.2 安装docker</h2><table>\n<thead>\n<tr>\n<th>方案</th>\n<th>要求</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>本地下载mysql</td>\n<td>本地没下载过数据库</td>\n</tr>\n<tr>\n<td>虚拟机 + mysql</td>\n<td></td>\n</tr>\n<tr>\n<td>虚拟机 + docker + mysql</td>\n<td></td>\n</tr>\n<tr>\n<td>服务器 + docker + mysql</td>\n<td></td>\n</tr>\n</tbody></table>\n<ul>\n<li>本次实验采用服务器 + docker + mysql</li>\n</ul>\n<h3 id=\"0-2-1-下载ubuntu镜像（使用虚拟机）\"><a href=\"#0-2-1-下载ubuntu镜像（使用虚拟机）\" class=\"headerlink\" title=\"0.2.1 下载ubuntu镜像（使用虚拟机）\"></a>0.2.1 下载ubuntu镜像（使用虚拟机）</h3><ul>\n<li><p><a href=\"https://ubuntu.com/download/server\">Get Ubuntu Server | Download | Ubuntu</a> </p>\n<p> <img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/0/4.png\"></p>\n<p> <img src=\"/img/java/produce_practice/0/4.png\"></p>\n</li>\n</ul>\n<h3 id=\"0-2-2-安装虚拟机\"><a href=\"#0-2-2-安装虚拟机\" class=\"headerlink\" title=\"0.2.2 安装虚拟机\"></a>0.2.2 安装虚拟机</h3><ul>\n<li>略</li>\n</ul>\n<h3 id=\"0-2-3-安装docker\"><a href=\"#0-2-3-安装docker\" class=\"headerlink\" title=\"0.2.3 安装docker\"></a>0.2.3 安装docker</h3><ul>\n<li><p>服务器中输入</p>\n  <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt install docker.io</span><br></pre></td></tr></table></figure>\n\n<p>  如果命令无效，提示用snap安装，则用snap</p>\n<p>  ，输入y是确认。安装完成收输入whereis docker可以查看docker位置</p>\n</li>\n<li><p>服务器中输入</p>\n  <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">- sudo docker search mysql</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h3 id=\"0-2-4-安装mysql\"><a href=\"#0-2-4-安装mysql\" class=\"headerlink\" title=\"0.2.4 安装mysql\"></a>0.2.4 安装mysql</h3><ul>\n<li><p>服务器中输入：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo docker pull mysql:8.0.34</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>查看镜像文件：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo docker images</span><br></pre></td></tr></table></figure>\n\n<p>可以看到镜像文件中有一个我们刚刚拉取的镜像</p>\n</li>\n<li><p>将镜像封装成容器运行：(这里将docker的3306端口映射到本地的3306，你可以改，数据库密码123456)</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo docker run -itd --name mysql8034 -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 mysql:8.0.34</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>查看容器列表</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo docker ps -a</span><br></pre></td></tr></table></figure>\n\n<p>看到刚刚的容器就说明ok了。</p>\n</li>\n<li><p>可视化工具：navicat、Mysql workbench（使用较低版本的navicat连接的时候可能会发现连接错误，需要更新navicat版本）</p>\n</li>\n</ul>\n<h2 id=\"0-3-安装idea\"><a href=\"#0-3-安装idea\" class=\"headerlink\" title=\"0.3 安装idea\"></a>0.3 安装idea</h2><ul>\n<li><p>下载地址（划到下面下载社区版）：<a href=\"https://www.jetbrains.com/zh-cn/idea/download\">https://www.jetbrains.com/zh-cn/idea/download</a></p>\n</li>\n<li><p>配置国内镜像：</p>\n<ul>\n<li><p>设置内搜索：maven</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/0/5.png\"></p>\n<p><img src=\"/img/java/produce_practice/0/5.png\"> </p>\n</li>\n<li><p>创建C:\\Users\\123.m2\\settings.xml</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">settings</span> <span class=\"attr\">xmlns</span>=<span class=\"string\">&quot;http://maven.apache.org/SETTINGS/1.0.0&quot;</span></span></span><br><span class=\"line\"><span class=\"tag\">          <span class=\"attr\">xmlns:xsi</span>=<span class=\"string\">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span></span><br><span class=\"line\"><span class=\"tag\">          <span class=\"attr\">xsi:schemaLocation</span>=<span class=\"string\">&quot;http://maven.apache.org/SETTINGS/1.0.0</span></span></span><br><span class=\"line\"><span class=\"string\"><span class=\"tag\">                              http://maven.apache.org/xsd/settings-1.0.0.xsd&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">mirrors</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">mirror</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">id</span>&gt;</span>alimaven<span class=\"tag\">&lt;/<span class=\"name\">id</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>aliyun maven<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">url</span>&gt;</span>http://maven.aliyun.com/nexus/content/groups/public/<span class=\"tag\">&lt;/<span class=\"name\">url</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">mirrorOf</span>&gt;</span>central<span class=\"tag\">&lt;/<span class=\"name\">mirrorOf</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">mirror</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">mirrors</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">settings</span>&gt;</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>重新打开设置，将刚刚的勾勾上</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/0/5.png\"></p>\n<p><img src=\"/img/java/produce_practice/0/5.png\"></p>\n</li>\n</ul>\n</li>\n<li><p>检验国内镜像配置</p>\n<ul>\n<li><p>进入网址（建议收藏）： <a href=\"https://mvnrepository.com/\">Maven Repository: Search&#x2F;Browse&#x2F;Explore (mvnrepository.com)</a> </p>\n</li>\n<li><p>搜索fastjson2：</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/0/6.png\"></p>\n<p><img src=\"/img/java/produce_practice/0/6.png\"> </p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/0/7.png\"></p>\n<p><img src=\"/img/java/produce_practice/0/7.png\"> </p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/0/8.png\"></p>\n<p><img src=\"/img/java/produce_practice/0/8.png\"> </p>\n<ul>\n<li>复制：</li>\n</ul>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">&lt;!-- https://mvnrepository.com/artifact/com.alibaba.fastjson2/fastjson2 --&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>com.alibaba.fastjson2<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>fastjson2<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>2.0.47<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>将上面的内容粘贴到项目的pom.xml的<dependences>标签下（无则自己建），更新maven：</li>\n</ul>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/0/9.png\"></p>\n<p><img src=\"/img/java/produce_practice/0/9.png\"> </p>\n<ul>\n<li><p>此时刚刚粘贴的依赖不是红色字体了，说明下载ok，然后输入json可以看到有代码自动补全：</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/0/10.png\"></p>\n<p><img src=\"/img/java/produce_practice/0/10.png\"></p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>统一编码：（搜索encoding或编码，还有搜索控制台：编码）</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/0/11.png\"></p>\n<p><img src=\"/img/java/produce_practice/0/11.png\"> </p>\n</li>\n<li><p>避免其他插件在控制台输出中文乱码：</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/0/12.png\"></p>\n<p><img src=\"/img/java/produce_practice/0/12.png\"> </p>\n<p>输入：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">-Xmx4096m</span><br><span class=\"line\">-Dfile.encoding=UTF-8</span><br></pre></td></tr></table></figure></li>\n</ul>"},{"title":"生产实习-2 引入静态资源","date":"2024-03-04T00:00:00.000Z","comment":true,"toc":true,"_content":"\n#\n<!--more-->\n\n# 2. 引入静态资源\n\n## 2.1 引入静态资源\n\n1. 将静态文件粘贴到static目录\n\n   ![](D:/blog/themes/yilia/source/img/java/produce_practice/2/1.png)\n\n   ![](img/java/produce_practice/2/1.png)\n\n2. 查看程序访问路径：8085端口\n\n   ![](D:/blog/themes/yilia/source/img/java/produce_practice/2/2.png)\n\n   ![](img/java/produce_practice/2/2.png)\n\n3. 修改程序配置文件的端口配置为8085\n\n   ![](D:/blog/themes/yilia/source/img/java/produce_practice/2/3.png)\n\n   ![](img/java/produce_practice/2/3.png)\n\n4. 运行并访问 ：http://127.0.0.1:8085/pages/login/index.html，能显示登录页面就ok\n\n   \n\n## 2.2 连接数据库\n\n1. 创建数据库myweb并创建两个表并插入一条数据\n\n   ```sql\n   CREATE TABLE IF NOT EXISTS `myweb`.`user_info` (\n     `id` VARCHAR(40) NOT NULL,\n     `username` VARCHAR(255) NULL,\n     `password` VARCHAR(255) NULL,\n     `start_time` DATETIME NULL,\n     `stop_time` DATETIME NULL,\n     `status` VARCHAR(255) NULL,\n     `created_by` VARCHAR(255) NULL,\n     `creation_date` DATETIME NULL,\n     `last_update_by` VARCHAR(255) NULL,\n     `last_update_date` DATETIME NULL,\n     PRIMARY KEY (`id`))\n   ENGINE = InnoDB\n   \n   INSERT INTO user_info (id, username,password)\n   VALUES ('0', 'admin', '1');\n   ```\n   \n2. 更改配置文件中数据库名：\n\n    ![](D:/blog/themes/yilia/source/img/java/produce_practice/2/4.png)\n\n   ![](img/java/produce_practice/2/4.png) \n\n3. 所有工程文件都在main/java/com/lyingedu.questionnaire下。在这里创建如下目录结构：\n\n   ![](D:/blog/themes/yilia/source/img/java/produce_practice/2/5.png)\n\n   ![](img/java/produce_practice/2/5.png)\n\n4. 使用dbgen工具将数据库导入工程：\n\n  - 用idea打开dbgen工具\n  \n  - 对myweb.xml做如下修改：\n  \n    ![](D:/blog/themes/yilia/source/img/java/produce_practice/2/6.png)\n  \n    ![](img/java/produce_practice/2/6.png)\n  \n  - 修改配置文件，运行（注意数据库地址端口用户名密码都不能错）\n  \n    ![](D:/blog/themes/yilia/source/img/java/produce_practice/2/7.png)\n  \n    ![](img/java/produce_practice/2/7.png)\n  \n  - 运行完成后会发现在主项目的questionnaire下多出了一个dbmap文件夹：\n  \n    ![](D:/blog/themes/yilia/source/img/java/produce_practice/2/8.png)\n  \n    ![](img/java/produce_practice/2/8.png)\n\n5. 数据库实体用户在生成后需要对这些对实体映射进行一些配置（不然无法在开发过程中引用）\n\n   - 指定数据库配置文件的位置：（注意不要把路径中的com/lyingedu写成了com.lyingedu ！\n\n     ![](D:/blog/themes/yilia/source/img/java/produce_practice/2/9.png)\n\n     ![](img/java/produce_practice/2/9.png)\n\n   - 在主入口映射接口文件，运行成功则配置ok\n\n      ![](D:/blog/themes/yilia/source/img/java/produce_practice/2/10.png)\n\n      ![](img/java/produce_practice/2/10.png)  ","source":"_posts/java/produce_practice/2_static.md","raw":"---\ntitle: 生产实习-2 引入静态资源\n\ndate: 2024-3-4 08:00:00\n\ntags: [生产实习,java,springboot]\n\ncategories: [java]\n\ncomment: true\n\ntoc: true\n\n\n\n---\n\n#\n<!--more-->\n\n# 2. 引入静态资源\n\n## 2.1 引入静态资源\n\n1. 将静态文件粘贴到static目录\n\n   ![](D:/blog/themes/yilia/source/img/java/produce_practice/2/1.png)\n\n   ![](img/java/produce_practice/2/1.png)\n\n2. 查看程序访问路径：8085端口\n\n   ![](D:/blog/themes/yilia/source/img/java/produce_practice/2/2.png)\n\n   ![](img/java/produce_practice/2/2.png)\n\n3. 修改程序配置文件的端口配置为8085\n\n   ![](D:/blog/themes/yilia/source/img/java/produce_practice/2/3.png)\n\n   ![](img/java/produce_practice/2/3.png)\n\n4. 运行并访问 ：http://127.0.0.1:8085/pages/login/index.html，能显示登录页面就ok\n\n   \n\n## 2.2 连接数据库\n\n1. 创建数据库myweb并创建两个表并插入一条数据\n\n   ```sql\n   CREATE TABLE IF NOT EXISTS `myweb`.`user_info` (\n     `id` VARCHAR(40) NOT NULL,\n     `username` VARCHAR(255) NULL,\n     `password` VARCHAR(255) NULL,\n     `start_time` DATETIME NULL,\n     `stop_time` DATETIME NULL,\n     `status` VARCHAR(255) NULL,\n     `created_by` VARCHAR(255) NULL,\n     `creation_date` DATETIME NULL,\n     `last_update_by` VARCHAR(255) NULL,\n     `last_update_date` DATETIME NULL,\n     PRIMARY KEY (`id`))\n   ENGINE = InnoDB\n   \n   INSERT INTO user_info (id, username,password)\n   VALUES ('0', 'admin', '1');\n   ```\n   \n2. 更改配置文件中数据库名：\n\n    ![](D:/blog/themes/yilia/source/img/java/produce_practice/2/4.png)\n\n   ![](img/java/produce_practice/2/4.png) \n\n3. 所有工程文件都在main/java/com/lyingedu.questionnaire下。在这里创建如下目录结构：\n\n   ![](D:/blog/themes/yilia/source/img/java/produce_practice/2/5.png)\n\n   ![](img/java/produce_practice/2/5.png)\n\n4. 使用dbgen工具将数据库导入工程：\n\n  - 用idea打开dbgen工具\n  \n  - 对myweb.xml做如下修改：\n  \n    ![](D:/blog/themes/yilia/source/img/java/produce_practice/2/6.png)\n  \n    ![](img/java/produce_practice/2/6.png)\n  \n  - 修改配置文件，运行（注意数据库地址端口用户名密码都不能错）\n  \n    ![](D:/blog/themes/yilia/source/img/java/produce_practice/2/7.png)\n  \n    ![](img/java/produce_practice/2/7.png)\n  \n  - 运行完成后会发现在主项目的questionnaire下多出了一个dbmap文件夹：\n  \n    ![](D:/blog/themes/yilia/source/img/java/produce_practice/2/8.png)\n  \n    ![](img/java/produce_practice/2/8.png)\n\n5. 数据库实体用户在生成后需要对这些对实体映射进行一些配置（不然无法在开发过程中引用）\n\n   - 指定数据库配置文件的位置：（注意不要把路径中的com/lyingedu写成了com.lyingedu ！\n\n     ![](D:/blog/themes/yilia/source/img/java/produce_practice/2/9.png)\n\n     ![](img/java/produce_practice/2/9.png)\n\n   - 在主入口映射接口文件，运行成功则配置ok\n\n      ![](D:/blog/themes/yilia/source/img/java/produce_practice/2/10.png)\n\n      ![](img/java/produce_practice/2/10.png)  ","slug":"java/produce_practice/2_static","published":1,"updated":"2024-03-08T02:13:24.383Z","_id":"clvuli6ru007hzsvw20nje1j2","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h1 id=\"2-引入静态资源\"><a href=\"#2-引入静态资源\" class=\"headerlink\" title=\"2. 引入静态资源\"></a>2. 引入静态资源</h1><h2 id=\"2-1-引入静态资源\"><a href=\"#2-1-引入静态资源\" class=\"headerlink\" title=\"2.1 引入静态资源\"></a>2.1 引入静态资源</h2><ol>\n<li><p>将静态文件粘贴到static目录</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/2/1.png\"></p>\n<p><img src=\"/img/java/produce_practice/2/1.png\"></p>\n</li>\n<li><p>查看程序访问路径：8085端口</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/2/2.png\"></p>\n<p><img src=\"/img/java/produce_practice/2/2.png\"></p>\n</li>\n<li><p>修改程序配置文件的端口配置为8085</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/2/3.png\"></p>\n<p><img src=\"/img/java/produce_practice/2/3.png\"></p>\n</li>\n<li><p>运行并访问 ：<a href=\"http://127.0.0.1:8085/pages/login/index.html%EF%BC%8C%E8%83%BD%E6%98%BE%E7%A4%BA%E7%99%BB%E5%BD%95%E9%A1%B5%E9%9D%A2%E5%B0%B1ok\">http://127.0.0.1:8085/pages/login/index.html，能显示登录页面就ok</a></p>\n</li>\n</ol>\n<h2 id=\"2-2-连接数据库\"><a href=\"#2-2-连接数据库\" class=\"headerlink\" title=\"2.2 连接数据库\"></a>2.2 连接数据库</h2><ol>\n<li><p>创建数据库myweb并创建两个表并插入一条数据</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> IF <span class=\"keyword\">NOT</span> <span class=\"keyword\">EXISTS</span> `myweb`.`user_info` (</span><br><span class=\"line\">  `id` <span class=\"type\">VARCHAR</span>(<span class=\"number\">40</span>) <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `username` <span class=\"type\">VARCHAR</span>(<span class=\"number\">255</span>) <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `password` <span class=\"type\">VARCHAR</span>(<span class=\"number\">255</span>) <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `start_time` DATETIME <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `stop_time` DATETIME <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `status` <span class=\"type\">VARCHAR</span>(<span class=\"number\">255</span>) <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `created_by` <span class=\"type\">VARCHAR</span>(<span class=\"number\">255</span>) <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `creation_date` DATETIME <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `last_update_by` <span class=\"type\">VARCHAR</span>(<span class=\"number\">255</span>) <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `last_update_date` DATETIME <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  <span class=\"keyword\">PRIMARY</span> KEY (`id`))</span><br><span class=\"line\">ENGINE <span class=\"operator\">=</span> InnoDB</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">INSERT</span> <span class=\"keyword\">INTO</span> user_info (id, username,password)</span><br><span class=\"line\"><span class=\"keyword\">VALUES</span> (<span class=\"string\">&#x27;0&#x27;</span>, <span class=\"string\">&#x27;admin&#x27;</span>, <span class=\"string\">&#x27;1&#x27;</span>);</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>更改配置文件中数据库名：</p>\n<p> <img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/2/4.png\"></p>\n<p><img src=\"/img/java/produce_practice/2/4.png\"> </p>\n</li>\n<li><p>所有工程文件都在main&#x2F;java&#x2F;com&#x2F;lyingedu.questionnaire下。在这里创建如下目录结构：</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/2/5.png\"></p>\n<p><img src=\"/img/java/produce_practice/2/5.png\"></p>\n</li>\n<li><p>使用dbgen工具将数据库导入工程：</p>\n</li>\n</ol>\n<ul>\n<li><p>用idea打开dbgen工具</p>\n</li>\n<li><p>对myweb.xml做如下修改：</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/2/6.png\"></p>\n<p><img src=\"/img/java/produce_practice/2/6.png\"></p>\n</li>\n<li><p>修改配置文件，运行（注意数据库地址端口用户名密码都不能错）</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/2/7.png\"></p>\n<p><img src=\"/img/java/produce_practice/2/7.png\"></p>\n</li>\n<li><p>运行完成后会发现在主项目的questionnaire下多出了一个dbmap文件夹：</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/2/8.png\"></p>\n<p><img src=\"/img/java/produce_practice/2/8.png\"></p>\n</li>\n</ul>\n<ol start=\"5\">\n<li><p>数据库实体用户在生成后需要对这些对实体映射进行一些配置（不然无法在开发过程中引用）</p>\n<ul>\n<li><p>指定数据库配置文件的位置：（注意不要把路径中的com&#x2F;lyingedu写成了com.lyingedu ！</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/2/9.png\"></p>\n<p><img src=\"/img/java/produce_practice/2/9.png\"></p>\n</li>\n<li><p>在主入口映射接口文件，运行成功则配置ok</p>\n<p> <img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/2/10.png\"></p>\n<p> <img src=\"/img/java/produce_practice/2/10.png\"></p>\n</li>\n</ul>\n</li>\n</ol>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"2-引入静态资源\"><a href=\"#2-引入静态资源\" class=\"headerlink\" title=\"2. 引入静态资源\"></a>2. 引入静态资源</h1><h2 id=\"2-1-引入静态资源\"><a href=\"#2-1-引入静态资源\" class=\"headerlink\" title=\"2.1 引入静态资源\"></a>2.1 引入静态资源</h2><ol>\n<li><p>将静态文件粘贴到static目录</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/2/1.png\"></p>\n<p><img src=\"/img/java/produce_practice/2/1.png\"></p>\n</li>\n<li><p>查看程序访问路径：8085端口</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/2/2.png\"></p>\n<p><img src=\"/img/java/produce_practice/2/2.png\"></p>\n</li>\n<li><p>修改程序配置文件的端口配置为8085</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/2/3.png\"></p>\n<p><img src=\"/img/java/produce_practice/2/3.png\"></p>\n</li>\n<li><p>运行并访问 ：<a href=\"http://127.0.0.1:8085/pages/login/index.html%EF%BC%8C%E8%83%BD%E6%98%BE%E7%A4%BA%E7%99%BB%E5%BD%95%E9%A1%B5%E9%9D%A2%E5%B0%B1ok\">http://127.0.0.1:8085/pages/login/index.html，能显示登录页面就ok</a></p>\n</li>\n</ol>\n<h2 id=\"2-2-连接数据库\"><a href=\"#2-2-连接数据库\" class=\"headerlink\" title=\"2.2 连接数据库\"></a>2.2 连接数据库</h2><ol>\n<li><p>创建数据库myweb并创建两个表并插入一条数据</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> IF <span class=\"keyword\">NOT</span> <span class=\"keyword\">EXISTS</span> `myweb`.`user_info` (</span><br><span class=\"line\">  `id` <span class=\"type\">VARCHAR</span>(<span class=\"number\">40</span>) <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `username` <span class=\"type\">VARCHAR</span>(<span class=\"number\">255</span>) <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `password` <span class=\"type\">VARCHAR</span>(<span class=\"number\">255</span>) <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `start_time` DATETIME <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `stop_time` DATETIME <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `status` <span class=\"type\">VARCHAR</span>(<span class=\"number\">255</span>) <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `created_by` <span class=\"type\">VARCHAR</span>(<span class=\"number\">255</span>) <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `creation_date` DATETIME <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `last_update_by` <span class=\"type\">VARCHAR</span>(<span class=\"number\">255</span>) <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `last_update_date` DATETIME <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  <span class=\"keyword\">PRIMARY</span> KEY (`id`))</span><br><span class=\"line\">ENGINE <span class=\"operator\">=</span> InnoDB</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">INSERT</span> <span class=\"keyword\">INTO</span> user_info (id, username,password)</span><br><span class=\"line\"><span class=\"keyword\">VALUES</span> (<span class=\"string\">&#x27;0&#x27;</span>, <span class=\"string\">&#x27;admin&#x27;</span>, <span class=\"string\">&#x27;1&#x27;</span>);</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>更改配置文件中数据库名：</p>\n<p> <img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/2/4.png\"></p>\n<p><img src=\"/img/java/produce_practice/2/4.png\"> </p>\n</li>\n<li><p>所有工程文件都在main&#x2F;java&#x2F;com&#x2F;lyingedu.questionnaire下。在这里创建如下目录结构：</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/2/5.png\"></p>\n<p><img src=\"/img/java/produce_practice/2/5.png\"></p>\n</li>\n<li><p>使用dbgen工具将数据库导入工程：</p>\n</li>\n</ol>\n<ul>\n<li><p>用idea打开dbgen工具</p>\n</li>\n<li><p>对myweb.xml做如下修改：</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/2/6.png\"></p>\n<p><img src=\"/img/java/produce_practice/2/6.png\"></p>\n</li>\n<li><p>修改配置文件，运行（注意数据库地址端口用户名密码都不能错）</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/2/7.png\"></p>\n<p><img src=\"/img/java/produce_practice/2/7.png\"></p>\n</li>\n<li><p>运行完成后会发现在主项目的questionnaire下多出了一个dbmap文件夹：</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/2/8.png\"></p>\n<p><img src=\"/img/java/produce_practice/2/8.png\"></p>\n</li>\n</ul>\n<ol start=\"5\">\n<li><p>数据库实体用户在生成后需要对这些对实体映射进行一些配置（不然无法在开发过程中引用）</p>\n<ul>\n<li><p>指定数据库配置文件的位置：（注意不要把路径中的com&#x2F;lyingedu写成了com.lyingedu ！</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/2/9.png\"></p>\n<p><img src=\"/img/java/produce_practice/2/9.png\"></p>\n</li>\n<li><p>在主入口映射接口文件，运行成功则配置ok</p>\n<p> <img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/2/10.png\"></p>\n<p> <img src=\"/img/java/produce_practice/2/10.png\"></p>\n</li>\n</ul>\n</li>\n</ol>"},{"title":"生产实习-1 系统结构搭建","date":"2024-03-02T00:00:00.000Z","comment":true,"toc":true,"_content":"\n#\n<!--more-->\n\n# 1. 系统结构搭建\n\n## 1.1 下载项目\n\n1. 查看项目规范文档中的工具版本：\n   - 使用spring boot3.1.0\n\n![](D:/blog/themes/yilia/source/img/java/produce_practice/1.png)\n\n![](img/java/produce_practice/1.png)\n\n2. [进入网址](https://start.spring.io)\n\n   - 按文档选择，其中spring boot没有这个版本，后面再改。\n\n     ![](D:/blog/themes/yilia/source/img/java/produce_practice/3.png)\n\n     ![](img/java/produce_practice/3.png)\n\n     点击`生成`,网站就会帮我们下载一个工程源文件。将该文件夹解压后就是你的项目文件夹。\n\n3. 打开里面的pom.xml文件，将里面的spring boot版本改成文档需要的3.1.0版本\n\n   ![](D:/blog/themes/yilia/source/img/java/produce_practice/4.png)\n\n   ![](img/java/produce_practice/4.png)\n\n4. 这些基础依赖里只有mybatis明确指定了版本号。我们调低了框架的主版本号，为了防止不兼容，我们需要调整mybatis版本号。\n\n   ![](D:/blog/themes/yilia/source/img/java/produce_practice/5.png)\n\n   ![](img/java/produce_practice/5.png)\n\n5. 访问之前的[第三方依赖仓库](https://mvnrepository.com)查看主框架的依赖版本号。\n\n   搜索spring boot的依赖版本\n\n   - 搜索：\n\n   ![](D:/blog/themes/yilia/source/img/java/produce_practice/6.png)\n\n   ![](img/java/produce_practice/6.png)\n\n   - 点击3.1.0版本：\n\n   ![](D:/blog/themes/yilia/source/img/java/produce_practice/7.png)\n\n   ![](img/java/produce_practice/7.png)\n\n   - 发现依赖的spring web版本为6.0.9：\n\n   ![](D:/blog/themes/yilia/source/img/java/produce_practice/8.png)\n\n   ![](img/java/produce_practice/8.png)\n\n   \n\n   搜索mybatis starter查询他的依赖版本\n\n    ![](D:/blog/themes/yilia/source/img/java/produce_practice/9.png)\n\n    ![](img/java/produce_practice/9.png)\n\n   - 搜索\n\n      ![](D:/blog/themes/yilia/source/img/java/produce_practice/10.png)\n\n      ![](img/java/produce_practice/10.png)\n\n   - 点击上面看到的版本：3.0.3\n\n      ![](D:/blog/themes/yilia/source/img/java/produce_practice/11.png)\n\n      ![](img/java/produce_practice/11.png)\n\n   - 点击对应版本：\n\n      ![](D:/blog/themes/yilia/source/img/java/produce_practice/12.png)\n\n      ![](img/java/produce_practice/12.png)\n\n     发现对应的spring context版本是6.1.0，与之前主框架的6.0.9不一致。\n\n      ![](D:/blog/themes/yilia/source/img/java/produce_practice/13.png)\n\n      ![](img/java/produce_practice/13.png)\n\n   - 需要将mybatis starter版本降低，经查询3.0.2版本依赖的spring context也是6.0.9，因此可以将pom.xml中对应版本改为3.0.2\n\n## 1.2 运行项目\n\n- 用idea打开刚刚下载的项目会自动下载依赖。可以看到下面有很多依赖项：\n\n ![](D:/blog/themes/yilia/source/img/java/produce_practice/14.png)\n\n![](img/java/produce_practice/14.png)\n\n- 现在暂时不连接数据库，需要注释掉mybatis和mysql连接器，否则会出错。\n\n![](D:/blog/themes/yilia/source/img/java/produce_practice/15.png)\n\n![](img/java/produce_practice/15.png)\n\n- 刷新maven后依赖更新：\n\n![](D:/blog/themes/yilia/source/img/java/produce_practice/16.png)\n\n![](img/java/produce_practice/16.png)\n\n- 找到主程序，运行，记住程序端口：\n\n![](D:/blog/themes/yilia/source/img/java/produce_practice/17.png)\n\n![](img/java/produce_practice/17.png)\n\n- 在static下新建一个index.html测试一下：\n\n![](D:/blog/themes/yilia/source/img/java/produce_practice/18.png)\n\n![](img/java/produce_practice/18.png)\n\n- 重新运行，输入网址，测试成功：\n\n ![](D:/blog/themes/yilia/source/img/java/produce_practice/19.png)\n\n ![](img/java/produce_practice/19.png)  \n\n## 1.3 连接数据库\n\n- 使用阿里云服务器的mysql，第二天发现被攻击了，数据库丢失，所以要做好备份，或者即使关闭端口。可以参考[数据库恢复部分](##1.4)不一定有用。\n\n- 解除之前的注释，刷新maven\n\n   ![](D:/blog/themes/yilia/source/img/java/produce_practice/21.png)\n\n   ![](img/java/produce_practice/21.png)\n\n- 将resources/application.properties重命名：application.yml，配置数据库信息：\n\n   ![](D:/blog/themes/yilia/source/img/java/produce_practice/20.png)\n\n   ![](img/java/produce_practice/20.png)\n\n- 注意：如果你将数据库配置那一段删掉，那么运行的时候会出错，但是就算你数据库配置写错了，运行也不会报错，你需要通过一个测试单元来尝试连接一下数据库才能发现是否能够连接成功。\n\n- 在数据库中创建test表，test1列，写入数据：`hello`，在src/test/java/下面有个测试文件，写入如下代码后运行：\n\n  ```java\n  package com.lyingedu.questionnaire;\n  \n  import org.junit.jupiter.api.Test;\n  import org.springframework.beans.factory.annotation.Autowired;\n  import org.springframework.boot.test.context.SpringBootTest;\n  import org.springframework.jdbc.core.JdbcTemplate;\n  import org.springframework.jdbc.core.RowMapper;\n  \n  import java.sql.ResultSet;\n  import java.sql.SQLException;\n  \n  @SpringBootTest\n  class QuestionnaireApplicationTests {\n  \n  \t@Autowired\n  \tprivate JdbcTemplate jdbcTemplate;\n  \n  \t@Test\n  \tvoid contextLoads() {\n  \t\t// 定义一个RowMapper来处理查询结果\n  \t\tRowMapper<String> rowMapper = new RowMapper<String>() {\n  \t\t\t@Override\n  \t\t\tpublic String mapRow(ResultSet rs, int rowNum) throws SQLException {\n  \t\t\t\t// 获取test1列的值\n  \t\t\t\treturn rs.getString(\"test1\");\n  \t\t\t}\n  \t\t};\n  \n  \t\t// 执行查询并获取结果\n  \t\tString result = jdbcTemplate.queryForObject(\"SELECT test1 FROM test LIMIT 1\", rowMapper);\n  \n  \t\t// 打印结果\n  \t\tSystem.out.println(\"查询结果: \" + result);\n  \t}\n  }\n  \n  ```\n\n  如果能够输出结果，说明连接成功。\n\n   ![](D:/blog/themes/yilia/source/img/java/produce_practice/23.png)\n\n   ![](img/java/produce_practice/23.png)\n\n  \n\n## 1.4 数据库修复\n\n- 如果你的远程数据库被攻击了或者被删了，你可以参考以下操作（不一定有用）\n\n  - 查找mysql的binlog文件：连接数据库后执行sql语句\n\n    ```sql\n    SHOW BINARY LOGS;\n    ```\n\n    ![](D:/blog/themes/yilia/source/img/java/produce_practice/35.png)\n\n    ![](img/java/produce_practice/35.png)\n\n    发现有多个日志文件\n\n  - 使用sql语句查看日志信息\n\n    ```sql\n    show binlog events in 'binlog.000002';\n    ```\n\n  - 根据日志文件名查找所在目录：在服务器中运行（root用户）\n\n    ```bash\n    find / -name \"binlog.000001\"\n    ```\n\n    然后cd打开目录\n\n  - 将日志文件变成sql文件：\n\n    ```bash\n    mysqlbinlog binlog.000001 > binlog1.sql\n    ```\n\n    如果提示没有mysqlbinlog，你需要根据提示下载：\n\n    ```\n    apt install mysql-server-core-8.0\n    ```\n\n    显示的带*的就是已经有的功能，选择那个前面是空的带log的功能\n\n  - 将转换好的sql文件复制到docker中\n\n    ```bash\n    docker cp binlog1.sql 容器名:/路径\n    ```\n\n    容器名通过以下命令查看\n\n    ```bash\n    docker ps -a\n    ```\n\n  - 进入docker：\n\n    ```\n    docker exec -it 容器名 bash\n    mysql -u 数据库用户名 -p < sql文件路径\n    ```\n\n  - [参考](https://developer.aliyun.com/article/515402)\n\n    ","source":"_posts/java/produce_practice/1_setup.md","raw":"---\ntitle: 生产实习-1 系统结构搭建\n\ndate: 2024-3-2 08:00:00\n\ntags: [生产实习,java,springboot]\n\ncategories: [java]\n\ncomment: true\n\ntoc: true\n\n\n---\n\n#\n<!--more-->\n\n# 1. 系统结构搭建\n\n## 1.1 下载项目\n\n1. 查看项目规范文档中的工具版本：\n   - 使用spring boot3.1.0\n\n![](D:/blog/themes/yilia/source/img/java/produce_practice/1.png)\n\n![](img/java/produce_practice/1.png)\n\n2. [进入网址](https://start.spring.io)\n\n   - 按文档选择，其中spring boot没有这个版本，后面再改。\n\n     ![](D:/blog/themes/yilia/source/img/java/produce_practice/3.png)\n\n     ![](img/java/produce_practice/3.png)\n\n     点击`生成`,网站就会帮我们下载一个工程源文件。将该文件夹解压后就是你的项目文件夹。\n\n3. 打开里面的pom.xml文件，将里面的spring boot版本改成文档需要的3.1.0版本\n\n   ![](D:/blog/themes/yilia/source/img/java/produce_practice/4.png)\n\n   ![](img/java/produce_practice/4.png)\n\n4. 这些基础依赖里只有mybatis明确指定了版本号。我们调低了框架的主版本号，为了防止不兼容，我们需要调整mybatis版本号。\n\n   ![](D:/blog/themes/yilia/source/img/java/produce_practice/5.png)\n\n   ![](img/java/produce_practice/5.png)\n\n5. 访问之前的[第三方依赖仓库](https://mvnrepository.com)查看主框架的依赖版本号。\n\n   搜索spring boot的依赖版本\n\n   - 搜索：\n\n   ![](D:/blog/themes/yilia/source/img/java/produce_practice/6.png)\n\n   ![](img/java/produce_practice/6.png)\n\n   - 点击3.1.0版本：\n\n   ![](D:/blog/themes/yilia/source/img/java/produce_practice/7.png)\n\n   ![](img/java/produce_practice/7.png)\n\n   - 发现依赖的spring web版本为6.0.9：\n\n   ![](D:/blog/themes/yilia/source/img/java/produce_practice/8.png)\n\n   ![](img/java/produce_practice/8.png)\n\n   \n\n   搜索mybatis starter查询他的依赖版本\n\n    ![](D:/blog/themes/yilia/source/img/java/produce_practice/9.png)\n\n    ![](img/java/produce_practice/9.png)\n\n   - 搜索\n\n      ![](D:/blog/themes/yilia/source/img/java/produce_practice/10.png)\n\n      ![](img/java/produce_practice/10.png)\n\n   - 点击上面看到的版本：3.0.3\n\n      ![](D:/blog/themes/yilia/source/img/java/produce_practice/11.png)\n\n      ![](img/java/produce_practice/11.png)\n\n   - 点击对应版本：\n\n      ![](D:/blog/themes/yilia/source/img/java/produce_practice/12.png)\n\n      ![](img/java/produce_practice/12.png)\n\n     发现对应的spring context版本是6.1.0，与之前主框架的6.0.9不一致。\n\n      ![](D:/blog/themes/yilia/source/img/java/produce_practice/13.png)\n\n      ![](img/java/produce_practice/13.png)\n\n   - 需要将mybatis starter版本降低，经查询3.0.2版本依赖的spring context也是6.0.9，因此可以将pom.xml中对应版本改为3.0.2\n\n## 1.2 运行项目\n\n- 用idea打开刚刚下载的项目会自动下载依赖。可以看到下面有很多依赖项：\n\n ![](D:/blog/themes/yilia/source/img/java/produce_practice/14.png)\n\n![](img/java/produce_practice/14.png)\n\n- 现在暂时不连接数据库，需要注释掉mybatis和mysql连接器，否则会出错。\n\n![](D:/blog/themes/yilia/source/img/java/produce_practice/15.png)\n\n![](img/java/produce_practice/15.png)\n\n- 刷新maven后依赖更新：\n\n![](D:/blog/themes/yilia/source/img/java/produce_practice/16.png)\n\n![](img/java/produce_practice/16.png)\n\n- 找到主程序，运行，记住程序端口：\n\n![](D:/blog/themes/yilia/source/img/java/produce_practice/17.png)\n\n![](img/java/produce_practice/17.png)\n\n- 在static下新建一个index.html测试一下：\n\n![](D:/blog/themes/yilia/source/img/java/produce_practice/18.png)\n\n![](img/java/produce_practice/18.png)\n\n- 重新运行，输入网址，测试成功：\n\n ![](D:/blog/themes/yilia/source/img/java/produce_practice/19.png)\n\n ![](img/java/produce_practice/19.png)  \n\n## 1.3 连接数据库\n\n- 使用阿里云服务器的mysql，第二天发现被攻击了，数据库丢失，所以要做好备份，或者即使关闭端口。可以参考[数据库恢复部分](##1.4)不一定有用。\n\n- 解除之前的注释，刷新maven\n\n   ![](D:/blog/themes/yilia/source/img/java/produce_practice/21.png)\n\n   ![](img/java/produce_practice/21.png)\n\n- 将resources/application.properties重命名：application.yml，配置数据库信息：\n\n   ![](D:/blog/themes/yilia/source/img/java/produce_practice/20.png)\n\n   ![](img/java/produce_practice/20.png)\n\n- 注意：如果你将数据库配置那一段删掉，那么运行的时候会出错，但是就算你数据库配置写错了，运行也不会报错，你需要通过一个测试单元来尝试连接一下数据库才能发现是否能够连接成功。\n\n- 在数据库中创建test表，test1列，写入数据：`hello`，在src/test/java/下面有个测试文件，写入如下代码后运行：\n\n  ```java\n  package com.lyingedu.questionnaire;\n  \n  import org.junit.jupiter.api.Test;\n  import org.springframework.beans.factory.annotation.Autowired;\n  import org.springframework.boot.test.context.SpringBootTest;\n  import org.springframework.jdbc.core.JdbcTemplate;\n  import org.springframework.jdbc.core.RowMapper;\n  \n  import java.sql.ResultSet;\n  import java.sql.SQLException;\n  \n  @SpringBootTest\n  class QuestionnaireApplicationTests {\n  \n  \t@Autowired\n  \tprivate JdbcTemplate jdbcTemplate;\n  \n  \t@Test\n  \tvoid contextLoads() {\n  \t\t// 定义一个RowMapper来处理查询结果\n  \t\tRowMapper<String> rowMapper = new RowMapper<String>() {\n  \t\t\t@Override\n  \t\t\tpublic String mapRow(ResultSet rs, int rowNum) throws SQLException {\n  \t\t\t\t// 获取test1列的值\n  \t\t\t\treturn rs.getString(\"test1\");\n  \t\t\t}\n  \t\t};\n  \n  \t\t// 执行查询并获取结果\n  \t\tString result = jdbcTemplate.queryForObject(\"SELECT test1 FROM test LIMIT 1\", rowMapper);\n  \n  \t\t// 打印结果\n  \t\tSystem.out.println(\"查询结果: \" + result);\n  \t}\n  }\n  \n  ```\n\n  如果能够输出结果，说明连接成功。\n\n   ![](D:/blog/themes/yilia/source/img/java/produce_practice/23.png)\n\n   ![](img/java/produce_practice/23.png)\n\n  \n\n## 1.4 数据库修复\n\n- 如果你的远程数据库被攻击了或者被删了，你可以参考以下操作（不一定有用）\n\n  - 查找mysql的binlog文件：连接数据库后执行sql语句\n\n    ```sql\n    SHOW BINARY LOGS;\n    ```\n\n    ![](D:/blog/themes/yilia/source/img/java/produce_practice/35.png)\n\n    ![](img/java/produce_practice/35.png)\n\n    发现有多个日志文件\n\n  - 使用sql语句查看日志信息\n\n    ```sql\n    show binlog events in 'binlog.000002';\n    ```\n\n  - 根据日志文件名查找所在目录：在服务器中运行（root用户）\n\n    ```bash\n    find / -name \"binlog.000001\"\n    ```\n\n    然后cd打开目录\n\n  - 将日志文件变成sql文件：\n\n    ```bash\n    mysqlbinlog binlog.000001 > binlog1.sql\n    ```\n\n    如果提示没有mysqlbinlog，你需要根据提示下载：\n\n    ```\n    apt install mysql-server-core-8.0\n    ```\n\n    显示的带*的就是已经有的功能，选择那个前面是空的带log的功能\n\n  - 将转换好的sql文件复制到docker中\n\n    ```bash\n    docker cp binlog1.sql 容器名:/路径\n    ```\n\n    容器名通过以下命令查看\n\n    ```bash\n    docker ps -a\n    ```\n\n  - 进入docker：\n\n    ```\n    docker exec -it 容器名 bash\n    mysql -u 数据库用户名 -p < sql文件路径\n    ```\n\n  - [参考](https://developer.aliyun.com/article/515402)\n\n    ","slug":"java/produce_practice/1_setup","published":1,"updated":"2024-03-06T10:35:54.176Z","_id":"clvuli6ru007izsvw6q5o4b9a","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h1 id=\"1-系统结构搭建\"><a href=\"#1-系统结构搭建\" class=\"headerlink\" title=\"1. 系统结构搭建\"></a>1. 系统结构搭建</h1><h2 id=\"1-1-下载项目\"><a href=\"#1-1-下载项目\" class=\"headerlink\" title=\"1.1 下载项目\"></a>1.1 下载项目</h2><ol>\n<li>查看项目规范文档中的工具版本：<ul>\n<li>使用spring boot3.1.0</li>\n</ul>\n</li>\n</ol>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/1.png\"></p>\n<p><img src=\"/img/java/produce_practice/1.png\"></p>\n<ol start=\"2\">\n<li><p><a href=\"https://start.spring.io/\">进入网址</a></p>\n<ul>\n<li><p>按文档选择，其中spring boot没有这个版本，后面再改。</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/3.png\"></p>\n<p><img src=\"/img/java/produce_practice/3.png\"></p>\n<p>点击<code>生成</code>,网站就会帮我们下载一个工程源文件。将该文件夹解压后就是你的项目文件夹。</p>\n</li>\n</ul>\n</li>\n<li><p>打开里面的pom.xml文件，将里面的spring boot版本改成文档需要的3.1.0版本</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/4.png\"></p>\n<p><img src=\"/img/java/produce_practice/4.png\"></p>\n</li>\n<li><p>这些基础依赖里只有mybatis明确指定了版本号。我们调低了框架的主版本号，为了防止不兼容，我们需要调整mybatis版本号。</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/5.png\"></p>\n<p><img src=\"/img/java/produce_practice/5.png\"></p>\n</li>\n<li><p>访问之前的<a href=\"https://mvnrepository.com/\">第三方依赖仓库</a>查看主框架的依赖版本号。</p>\n<p>搜索spring boot的依赖版本</p>\n<ul>\n<li>搜索：</li>\n</ul>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/6.png\"></p>\n<p><img src=\"/img/java/produce_practice/6.png\"></p>\n<ul>\n<li>点击3.1.0版本：</li>\n</ul>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/7.png\"></p>\n<p><img src=\"/img/java/produce_practice/7.png\"></p>\n<ul>\n<li>发现依赖的spring web版本为6.0.9：</li>\n</ul>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/8.png\"></p>\n<p><img src=\"/img/java/produce_practice/8.png\"></p>\n<p>搜索mybatis starter查询他的依赖版本</p>\n<p> <img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/9.png\"></p>\n<p> <img src=\"/img/java/produce_practice/9.png\"></p>\n<ul>\n<li><p>搜索</p>\n<p> <img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/10.png\"></p>\n<p> <img src=\"/img/java/produce_practice/10.png\"></p>\n</li>\n<li><p>点击上面看到的版本：3.0.3</p>\n<p> <img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/11.png\"></p>\n<p> <img src=\"/img/java/produce_practice/11.png\"></p>\n</li>\n<li><p>点击对应版本：</p>\n<p> <img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/12.png\"></p>\n<p> <img src=\"/img/java/produce_practice/12.png\"></p>\n<p>发现对应的spring context版本是6.1.0，与之前主框架的6.0.9不一致。</p>\n<p> <img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/13.png\"></p>\n<p> <img src=\"/img/java/produce_practice/13.png\"></p>\n</li>\n<li><p>需要将mybatis starter版本降低，经查询3.0.2版本依赖的spring context也是6.0.9，因此可以将pom.xml中对应版本改为3.0.2</p>\n</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"1-2-运行项目\"><a href=\"#1-2-运行项目\" class=\"headerlink\" title=\"1.2 运行项目\"></a>1.2 运行项目</h2><ul>\n<li>用idea打开刚刚下载的项目会自动下载依赖。可以看到下面有很多依赖项：</li>\n</ul>\n<p> <img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/14.png\"></p>\n<p><img src=\"/img/java/produce_practice/14.png\"></p>\n<ul>\n<li>现在暂时不连接数据库，需要注释掉mybatis和mysql连接器，否则会出错。</li>\n</ul>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/15.png\"></p>\n<p><img src=\"/img/java/produce_practice/15.png\"></p>\n<ul>\n<li>刷新maven后依赖更新：</li>\n</ul>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/16.png\"></p>\n<p><img src=\"/img/java/produce_practice/16.png\"></p>\n<ul>\n<li>找到主程序，运行，记住程序端口：</li>\n</ul>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/17.png\"></p>\n<p><img src=\"/img/java/produce_practice/17.png\"></p>\n<ul>\n<li>在static下新建一个index.html测试一下：</li>\n</ul>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/18.png\"></p>\n<p><img src=\"/img/java/produce_practice/18.png\"></p>\n<ul>\n<li>重新运行，输入网址，测试成功：</li>\n</ul>\n<p> <img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/19.png\"></p>\n<p> <img src=\"/img/java/produce_practice/19.png\">  </p>\n<h2 id=\"1-3-连接数据库\"><a href=\"#1-3-连接数据库\" class=\"headerlink\" title=\"1.3 连接数据库\"></a>1.3 连接数据库</h2><ul>\n<li><p>使用阿里云服务器的mysql，第二天发现被攻击了，数据库丢失，所以要做好备份，或者即使关闭端口。可以参考<a href=\"##1.4\">数据库恢复部分</a>不一定有用。</p>\n</li>\n<li><p>解除之前的注释，刷新maven</p>\n<p> <img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/21.png\"></p>\n<p> <img src=\"/img/java/produce_practice/21.png\"></p>\n</li>\n<li><p>将resources&#x2F;application.properties重命名：application.yml，配置数据库信息：</p>\n<p> <img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/20.png\"></p>\n<p> <img src=\"/img/java/produce_practice/20.png\"></p>\n</li>\n<li><p>注意：如果你将数据库配置那一段删掉，那么运行的时候会出错，但是就算你数据库配置写错了，运行也不会报错，你需要通过一个测试单元来尝试连接一下数据库才能发现是否能够连接成功。</p>\n</li>\n<li><p>在数据库中创建test表，test1列，写入数据：<code>hello</code>，在src&#x2F;test&#x2F;java&#x2F;下面有个测试文件，写入如下代码后运行：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> com.lyingedu.questionnaire;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.junit.jupiter.api.Test;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.beans.factory.annotation.Autowired;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.boot.test.context.SpringBootTest;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.jdbc.core.JdbcTemplate;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.jdbc.core.RowMapper;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.sql.ResultSet;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.sql.SQLException;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@SpringBootTest</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">QuestionnaireApplicationTests</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"meta\">@Autowired</span></span><br><span class=\"line\">\t<span class=\"keyword\">private</span> JdbcTemplate jdbcTemplate;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"meta\">@Test</span></span><br><span class=\"line\">\t<span class=\"keyword\">void</span> <span class=\"title function_\">contextLoads</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">\t\t<span class=\"comment\">// 定义一个RowMapper来处理查询结果</span></span><br><span class=\"line\">\t\tRowMapper&lt;String&gt; rowMapper = <span class=\"keyword\">new</span> <span class=\"title class_\">RowMapper</span>&lt;String&gt;() &#123;</span><br><span class=\"line\">\t\t\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">public</span> String <span class=\"title function_\">mapRow</span><span class=\"params\">(ResultSet rs, <span class=\"type\">int</span> rowNum)</span> <span class=\"keyword\">throws</span> SQLException &#123;</span><br><span class=\"line\">\t\t\t\t<span class=\"comment\">// 获取test1列的值</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">return</span> rs.getString(<span class=\"string\">&quot;test1&quot;</span>);</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 执行查询并获取结果</span></span><br><span class=\"line\">\t\t<span class=\"type\">String</span> <span class=\"variable\">result</span> <span class=\"operator\">=</span> jdbcTemplate.queryForObject(<span class=\"string\">&quot;SELECT test1 FROM test LIMIT 1&quot;</span>, rowMapper);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 打印结果</span></span><br><span class=\"line\">\t\tSystem.out.println(<span class=\"string\">&quot;查询结果: &quot;</span> + result);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>如果能够输出结果，说明连接成功。</p>\n<p> <img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/23.png\"></p>\n<p> <img src=\"/img/java/produce_practice/23.png\"></p>\n</li>\n</ul>\n<h2 id=\"1-4-数据库修复\"><a href=\"#1-4-数据库修复\" class=\"headerlink\" title=\"1.4 数据库修复\"></a>1.4 数据库修复</h2><ul>\n<li><p>如果你的远程数据库被攻击了或者被删了，你可以参考以下操作（不一定有用）</p>\n<ul>\n<li><p>查找mysql的binlog文件：连接数据库后执行sql语句</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SHOW</span> <span class=\"type\">BINARY</span> LOGS;</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/35.png\"></p>\n<p><img src=\"/img/java/produce_practice/35.png\"></p>\n<p>发现有多个日志文件</p>\n</li>\n<li><p>使用sql语句查看日志信息</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">show</span> binlog events <span class=\"keyword\">in</span> <span class=\"string\">&#x27;binlog.000002&#x27;</span>;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>根据日志文件名查找所在目录：在服务器中运行（root用户）</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">find / -name <span class=\"string\">&quot;binlog.000001&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>然后cd打开目录</p>\n</li>\n<li><p>将日志文件变成sql文件：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysqlbinlog binlog.000001 &gt; binlog1.sql</span><br></pre></td></tr></table></figure>\n\n<p>如果提示没有mysqlbinlog，你需要根据提示下载：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apt install mysql-server-core-8.0</span><br></pre></td></tr></table></figure>\n\n<p>显示的带*的就是已经有的功能，选择那个前面是空的带log的功能</p>\n</li>\n<li><p>将转换好的sql文件复制到docker中</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker <span class=\"built_in\">cp</span> binlog1.sql 容器名:/路径</span><br></pre></td></tr></table></figure>\n\n<p>容器名通过以下命令查看</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker ps -a</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>进入docker：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker exec -it 容器名 bash</span><br><span class=\"line\">mysql -u 数据库用户名 -p &lt; sql文件路径</span><br></pre></td></tr></table></figure>\n</li>\n<li><p><a href=\"https://developer.aliyun.com/article/515402\">参考</a></p>\n</li>\n</ul>\n</li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"1-系统结构搭建\"><a href=\"#1-系统结构搭建\" class=\"headerlink\" title=\"1. 系统结构搭建\"></a>1. 系统结构搭建</h1><h2 id=\"1-1-下载项目\"><a href=\"#1-1-下载项目\" class=\"headerlink\" title=\"1.1 下载项目\"></a>1.1 下载项目</h2><ol>\n<li>查看项目规范文档中的工具版本：<ul>\n<li>使用spring boot3.1.0</li>\n</ul>\n</li>\n</ol>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/1.png\"></p>\n<p><img src=\"/img/java/produce_practice/1.png\"></p>\n<ol start=\"2\">\n<li><p><a href=\"https://start.spring.io/\">进入网址</a></p>\n<ul>\n<li><p>按文档选择，其中spring boot没有这个版本，后面再改。</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/3.png\"></p>\n<p><img src=\"/img/java/produce_practice/3.png\"></p>\n<p>点击<code>生成</code>,网站就会帮我们下载一个工程源文件。将该文件夹解压后就是你的项目文件夹。</p>\n</li>\n</ul>\n</li>\n<li><p>打开里面的pom.xml文件，将里面的spring boot版本改成文档需要的3.1.0版本</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/4.png\"></p>\n<p><img src=\"/img/java/produce_practice/4.png\"></p>\n</li>\n<li><p>这些基础依赖里只有mybatis明确指定了版本号。我们调低了框架的主版本号，为了防止不兼容，我们需要调整mybatis版本号。</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/5.png\"></p>\n<p><img src=\"/img/java/produce_practice/5.png\"></p>\n</li>\n<li><p>访问之前的<a href=\"https://mvnrepository.com/\">第三方依赖仓库</a>查看主框架的依赖版本号。</p>\n<p>搜索spring boot的依赖版本</p>\n<ul>\n<li>搜索：</li>\n</ul>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/6.png\"></p>\n<p><img src=\"/img/java/produce_practice/6.png\"></p>\n<ul>\n<li>点击3.1.0版本：</li>\n</ul>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/7.png\"></p>\n<p><img src=\"/img/java/produce_practice/7.png\"></p>\n<ul>\n<li>发现依赖的spring web版本为6.0.9：</li>\n</ul>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/8.png\"></p>\n<p><img src=\"/img/java/produce_practice/8.png\"></p>\n<p>搜索mybatis starter查询他的依赖版本</p>\n<p> <img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/9.png\"></p>\n<p> <img src=\"/img/java/produce_practice/9.png\"></p>\n<ul>\n<li><p>搜索</p>\n<p> <img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/10.png\"></p>\n<p> <img src=\"/img/java/produce_practice/10.png\"></p>\n</li>\n<li><p>点击上面看到的版本：3.0.3</p>\n<p> <img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/11.png\"></p>\n<p> <img src=\"/img/java/produce_practice/11.png\"></p>\n</li>\n<li><p>点击对应版本：</p>\n<p> <img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/12.png\"></p>\n<p> <img src=\"/img/java/produce_practice/12.png\"></p>\n<p>发现对应的spring context版本是6.1.0，与之前主框架的6.0.9不一致。</p>\n<p> <img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/13.png\"></p>\n<p> <img src=\"/img/java/produce_practice/13.png\"></p>\n</li>\n<li><p>需要将mybatis starter版本降低，经查询3.0.2版本依赖的spring context也是6.0.9，因此可以将pom.xml中对应版本改为3.0.2</p>\n</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"1-2-运行项目\"><a href=\"#1-2-运行项目\" class=\"headerlink\" title=\"1.2 运行项目\"></a>1.2 运行项目</h2><ul>\n<li>用idea打开刚刚下载的项目会自动下载依赖。可以看到下面有很多依赖项：</li>\n</ul>\n<p> <img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/14.png\"></p>\n<p><img src=\"/img/java/produce_practice/14.png\"></p>\n<ul>\n<li>现在暂时不连接数据库，需要注释掉mybatis和mysql连接器，否则会出错。</li>\n</ul>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/15.png\"></p>\n<p><img src=\"/img/java/produce_practice/15.png\"></p>\n<ul>\n<li>刷新maven后依赖更新：</li>\n</ul>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/16.png\"></p>\n<p><img src=\"/img/java/produce_practice/16.png\"></p>\n<ul>\n<li>找到主程序，运行，记住程序端口：</li>\n</ul>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/17.png\"></p>\n<p><img src=\"/img/java/produce_practice/17.png\"></p>\n<ul>\n<li>在static下新建一个index.html测试一下：</li>\n</ul>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/18.png\"></p>\n<p><img src=\"/img/java/produce_practice/18.png\"></p>\n<ul>\n<li>重新运行，输入网址，测试成功：</li>\n</ul>\n<p> <img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/19.png\"></p>\n<p> <img src=\"/img/java/produce_practice/19.png\">  </p>\n<h2 id=\"1-3-连接数据库\"><a href=\"#1-3-连接数据库\" class=\"headerlink\" title=\"1.3 连接数据库\"></a>1.3 连接数据库</h2><ul>\n<li><p>使用阿里云服务器的mysql，第二天发现被攻击了，数据库丢失，所以要做好备份，或者即使关闭端口。可以参考<a href=\"##1.4\">数据库恢复部分</a>不一定有用。</p>\n</li>\n<li><p>解除之前的注释，刷新maven</p>\n<p> <img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/21.png\"></p>\n<p> <img src=\"/img/java/produce_practice/21.png\"></p>\n</li>\n<li><p>将resources&#x2F;application.properties重命名：application.yml，配置数据库信息：</p>\n<p> <img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/20.png\"></p>\n<p> <img src=\"/img/java/produce_practice/20.png\"></p>\n</li>\n<li><p>注意：如果你将数据库配置那一段删掉，那么运行的时候会出错，但是就算你数据库配置写错了，运行也不会报错，你需要通过一个测试单元来尝试连接一下数据库才能发现是否能够连接成功。</p>\n</li>\n<li><p>在数据库中创建test表，test1列，写入数据：<code>hello</code>，在src&#x2F;test&#x2F;java&#x2F;下面有个测试文件，写入如下代码后运行：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> com.lyingedu.questionnaire;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.junit.jupiter.api.Test;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.beans.factory.annotation.Autowired;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.boot.test.context.SpringBootTest;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.jdbc.core.JdbcTemplate;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.jdbc.core.RowMapper;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.sql.ResultSet;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.sql.SQLException;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@SpringBootTest</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">QuestionnaireApplicationTests</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"meta\">@Autowired</span></span><br><span class=\"line\">\t<span class=\"keyword\">private</span> JdbcTemplate jdbcTemplate;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"meta\">@Test</span></span><br><span class=\"line\">\t<span class=\"keyword\">void</span> <span class=\"title function_\">contextLoads</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">\t\t<span class=\"comment\">// 定义一个RowMapper来处理查询结果</span></span><br><span class=\"line\">\t\tRowMapper&lt;String&gt; rowMapper = <span class=\"keyword\">new</span> <span class=\"title class_\">RowMapper</span>&lt;String&gt;() &#123;</span><br><span class=\"line\">\t\t\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">public</span> String <span class=\"title function_\">mapRow</span><span class=\"params\">(ResultSet rs, <span class=\"type\">int</span> rowNum)</span> <span class=\"keyword\">throws</span> SQLException &#123;</span><br><span class=\"line\">\t\t\t\t<span class=\"comment\">// 获取test1列的值</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">return</span> rs.getString(<span class=\"string\">&quot;test1&quot;</span>);</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 执行查询并获取结果</span></span><br><span class=\"line\">\t\t<span class=\"type\">String</span> <span class=\"variable\">result</span> <span class=\"operator\">=</span> jdbcTemplate.queryForObject(<span class=\"string\">&quot;SELECT test1 FROM test LIMIT 1&quot;</span>, rowMapper);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 打印结果</span></span><br><span class=\"line\">\t\tSystem.out.println(<span class=\"string\">&quot;查询结果: &quot;</span> + result);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>如果能够输出结果，说明连接成功。</p>\n<p> <img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/23.png\"></p>\n<p> <img src=\"/img/java/produce_practice/23.png\"></p>\n</li>\n</ul>\n<h2 id=\"1-4-数据库修复\"><a href=\"#1-4-数据库修复\" class=\"headerlink\" title=\"1.4 数据库修复\"></a>1.4 数据库修复</h2><ul>\n<li><p>如果你的远程数据库被攻击了或者被删了，你可以参考以下操作（不一定有用）</p>\n<ul>\n<li><p>查找mysql的binlog文件：连接数据库后执行sql语句</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SHOW</span> <span class=\"type\">BINARY</span> LOGS;</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/35.png\"></p>\n<p><img src=\"/img/java/produce_practice/35.png\"></p>\n<p>发现有多个日志文件</p>\n</li>\n<li><p>使用sql语句查看日志信息</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">show</span> binlog events <span class=\"keyword\">in</span> <span class=\"string\">&#x27;binlog.000002&#x27;</span>;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>根据日志文件名查找所在目录：在服务器中运行（root用户）</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">find / -name <span class=\"string\">&quot;binlog.000001&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>然后cd打开目录</p>\n</li>\n<li><p>将日志文件变成sql文件：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysqlbinlog binlog.000001 &gt; binlog1.sql</span><br></pre></td></tr></table></figure>\n\n<p>如果提示没有mysqlbinlog，你需要根据提示下载：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apt install mysql-server-core-8.0</span><br></pre></td></tr></table></figure>\n\n<p>显示的带*的就是已经有的功能，选择那个前面是空的带log的功能</p>\n</li>\n<li><p>将转换好的sql文件复制到docker中</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker <span class=\"built_in\">cp</span> binlog1.sql 容器名:/路径</span><br></pre></td></tr></table></figure>\n\n<p>容器名通过以下命令查看</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker ps -a</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>进入docker：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker exec -it 容器名 bash</span><br><span class=\"line\">mysql -u 数据库用户名 -p &lt; sql文件路径</span><br></pre></td></tr></table></figure>\n</li>\n<li><p><a href=\"https://developer.aliyun.com/article/515402\">参考</a></p>\n</li>\n</ul>\n</li>\n</ul>"},{"title":"生产实习-4 登录及创建用户功能开发","date":"2024-03-05T01:00:00.000Z","comment":true,"toc":true,"_content":"\n#\n<!--more-->\n\n\n\n# 4. 登录及创建用户功能开发\n\n## 4.1 登录功能\n\n### 4.1.1 查看前端代码\n\n- 首先在resources/static/pages/login/index.html查看登录按钮的动作：发现是调用了onLogin函数。\n\n- 进入./index.js查看该函数：\n\n  ![](D:/blog/themes/yilia/source/img/java/produce_practice/4/1.png)\n\n  ![](img/java/produce_practice/4/1.png)\n\n### 4.1.2 编写UserController.userLogin方法（control层）\n\n- 然后我们到biz/user/controller/UserController，其中有\n\n  ```java\n  @RequestMapping(\"/admin\") //使用request请求\n  ```\n\n  对应/admin路径。然后我们找到其中的userLogin方法，进行编写：\n\n  ```java\n      @PostMapping(\"/userLogin\")\n      public HttpResponseEntity userLogin(@RequestBody UserInfo userInfo) { //requestbody注解，将请求的json数据转换为对象\n  \n          HttpResponseEntity httpResponseEntity = new HttpResponseEntity();\n          //TODO\n          List<UserInfo> userInfos = userService.queryUserList(userInfo); //使用service层的方法查询用户信息\n  \n          if(userInfos.size() != 0) {//如果查询到用户信息则状态码为666，否则为默认值\n              httpResponseEntity.setCode(\"666\");\n              httpResponseEntity.setData(userInfos.get(0));\n          }\n          return httpResponseEntity;//传给前端\n      }\n  ```\n\n- 然后进行调试，可以先在该方法内打个断点，然后调试，进入 http://127.0.0.1:8085/pages/login/index.html，输入用户名密码，确认后可以看到代码运行到了断点处，可以查看一下userInfo变量是否能获得。\n\n  - 注意：如果你输入了localhost而非127.0.0.1你可能会发现代码没有运行到断点处，无法查看变量。在浏览器打开检查页面。\n\n    ![](D:/blog/themes/yilia/source/img/java/produce_practice/4/2.png)\n\n    ![](img/java/produce_practice/4/2.png)\n\n### 4.1.3 编写userserviceImpl.queryUserList方法（service层）\n\n- 具体查询是在userserviceImpl.queryUserList中实现的：\n\n  ```java\n      /**\n       * 查询用户列表\n       * @param userInfo\n       * @return\n       */\n      @Override\n      public List<UserInfo> queryUserList(UserInfo userInfo) {//1. 用userInof查询数据库\n  \n          UserInfoExample userInfoExample = new UserInfoExample(); //3 在dbmap/entities里面是有userinfoexample的\n          UserInfoExample.Criteria userInfoCriteria = userInfoExample.createCriteria(); //4 然后用这个example创建一个条件\n          userInfoCriteria.andUsernameEqualTo(userInfo.getUsername()) //根据这两个条件进行查询\n                          .andPasswordEqualTo(userInfo.getPassword());\n  \n  \n          return userInfoMapper.selectByExample(userInfoExample);//2 使用userInfoMapper里面自带一些方法，使用里面的selectbyexample查询\n      }\n  }\n  ```\n\n### 4.1.4 调试结果\n\n- 然后继续在usercontroller的返回那一行打个断点，继续调试，发现确认用户名密码后发生错误，调试终止。这是因为一些xml文件放在了java包下面，无法编译到工程中，需要在项目根目录的pom.xml的<build>中加入以下代码：\n\n  ```xml\n  \t<resources>\n  \t\t\t<resource>\n  \t\t\t\t<directory>src/main/java</directory>\n  \t\t\t\t<includes>\n  \t\t\t\t\t<include>**/*.xml</include>\n  \t\t\t\t</includes>\n  \t\t\t\t<filtering>false</filtering>\n  \t\t\t</resource>\n  \t\t\t<resource>\n  \t\t\t\t<directory>src/main/resources</directory>\n  \t\t\t</resource>\n  \t\t</resources>\n  ```\n\n  ![](D:/blog/themes/yilia/source/img/java/produce_practice/4/3.png)\n\n  ![](img/java/produce_practice/4/3.png)\n\n  然后刷新maven即可。\n\n- 此时发现userInfos的id是0，说明查询数据库成功了（id值来自数据库）\n\n  ![](D:/blog/themes/yilia/source/img/java/produce_practice/4/4.png)\n\n  ![](img/java/produce_practice/4/4.png)\n\n  - 然后点击调试控制台的恢复程序按钮\n\n     ![](D:/blog/themes/yilia/source/img/java/produce_practice/4/5.png)\n\n    ![](img/java/produce_practice/4/5.png) \n\n    发现前端页面跳转：\n\n    ![](D:/blog/themes/yilia/source/img/java/produce_practice/4/6.png)\n\n    ![](img/java/produce_practice/4/6.png)\n\n## 4.2 创建用户功能\n\n### 4.2.1 查看前端代码\n\n- 发现路径是admin/addUserInfo，参照4.1的经验，我们接下来要找control层的addUserInfo方法\n\n  ![](D:/blog/themes/yilia/source/img/java/produce_practice/4/7.png)\n\n  ![](img/java/produce_practice/4/7.png) \n\n\n\n### 4.2.2 编写UserController.userLogin方法（control层）\n\n```java\n    public HttpResponseEntity addUserInfo(@RequestBody UserInfo userInfo) {\n\n        HttpResponseEntity httpResponseEntity = new HttpResponseEntity();\n        int count = userService.addUserInfo(userInfo); //返回受影响的行数\n        if(count > 0) { //插入成功则状态码为666，否则为默认错误值\n            httpResponseEntity.setCode(\"666\");\n        }\n        return httpResponseEntity;\n    }\n```\n\n### 4.2.3 编写userserviceImpl.addUserInfo方法（service层）\n\n- 由于数据库设计问题，每次插入数据都需要一个userid，userid不是自动编号的，需要自己写入，而前端没有写入，因此我们还需要一个在后端生成userid的方法在后端。为此我们创建src/main/java/com/lyingedu/questionnaire/common/utils/UUIDUtil.java:\n\n  ```java\n  package com.lyingedu.questionnaire.common.utils;\n  \n  \n  import java.util.UUID;\n  \n  public class UUIDUtil {\n  \n      /**\n       *获取一个UUID\n       */\n      public static String getOneUUID(){\n          //获取UUID\n          String s = UUID.randomUUID().toString();\n          //去掉“-”符号\n          return s.substring(0,8)+s.substring(9,13)+s.substring(14,18)+s.substring(19,23)+s.substring(24);\n      }\n      /**\n       * 获得指定数目的UUID\n       * @param number int 需要获得的UUID数量\n       * @return String[] UUID数组\n       */\n      public static String[] getUUID(int number){\n          if(number < 1){\n              return null;\n          }\n          String[] ss = new String[number];\n          for(int i=0;i<number;i++){\n              ss[i] = getOneUUID();\n          }\n          return ss;\n      }\n  }\n  \n  ```\n\n- 然后我们再来编写userserviceImpl.addUserInfo方法：\n\n  ```java\n      public int addUserInfo(UserInfo userInfo) {\n  \n          userInfo.setId(UUIDUtil.getOneUUID());\n          userInfo.setStatus(\"1\");//1有效，0无效\n          \n          return userInfoMapper.insert(userInfo);\n      }\n  ```\n\n- 最后还需要将index.js里面的两句判断状态码从1改成自己的666：\n\n  ```js\n  if (res.code === \"666\") {\n  ```\n\n  ","source":"_posts/java/produce_practice/4_login.md","raw":"---\ntitle: 生产实习-4 登录及创建用户功能开发\n\ndate: 2024-3-5 09:00:00\n\ntags: [生产实习,java,springboot]\n\ncategories: [java]\n\ncomment: true\n\ntoc: true\n\n---\n\n#\n<!--more-->\n\n\n\n# 4. 登录及创建用户功能开发\n\n## 4.1 登录功能\n\n### 4.1.1 查看前端代码\n\n- 首先在resources/static/pages/login/index.html查看登录按钮的动作：发现是调用了onLogin函数。\n\n- 进入./index.js查看该函数：\n\n  ![](D:/blog/themes/yilia/source/img/java/produce_practice/4/1.png)\n\n  ![](img/java/produce_practice/4/1.png)\n\n### 4.1.2 编写UserController.userLogin方法（control层）\n\n- 然后我们到biz/user/controller/UserController，其中有\n\n  ```java\n  @RequestMapping(\"/admin\") //使用request请求\n  ```\n\n  对应/admin路径。然后我们找到其中的userLogin方法，进行编写：\n\n  ```java\n      @PostMapping(\"/userLogin\")\n      public HttpResponseEntity userLogin(@RequestBody UserInfo userInfo) { //requestbody注解，将请求的json数据转换为对象\n  \n          HttpResponseEntity httpResponseEntity = new HttpResponseEntity();\n          //TODO\n          List<UserInfo> userInfos = userService.queryUserList(userInfo); //使用service层的方法查询用户信息\n  \n          if(userInfos.size() != 0) {//如果查询到用户信息则状态码为666，否则为默认值\n              httpResponseEntity.setCode(\"666\");\n              httpResponseEntity.setData(userInfos.get(0));\n          }\n          return httpResponseEntity;//传给前端\n      }\n  ```\n\n- 然后进行调试，可以先在该方法内打个断点，然后调试，进入 http://127.0.0.1:8085/pages/login/index.html，输入用户名密码，确认后可以看到代码运行到了断点处，可以查看一下userInfo变量是否能获得。\n\n  - 注意：如果你输入了localhost而非127.0.0.1你可能会发现代码没有运行到断点处，无法查看变量。在浏览器打开检查页面。\n\n    ![](D:/blog/themes/yilia/source/img/java/produce_practice/4/2.png)\n\n    ![](img/java/produce_practice/4/2.png)\n\n### 4.1.3 编写userserviceImpl.queryUserList方法（service层）\n\n- 具体查询是在userserviceImpl.queryUserList中实现的：\n\n  ```java\n      /**\n       * 查询用户列表\n       * @param userInfo\n       * @return\n       */\n      @Override\n      public List<UserInfo> queryUserList(UserInfo userInfo) {//1. 用userInof查询数据库\n  \n          UserInfoExample userInfoExample = new UserInfoExample(); //3 在dbmap/entities里面是有userinfoexample的\n          UserInfoExample.Criteria userInfoCriteria = userInfoExample.createCriteria(); //4 然后用这个example创建一个条件\n          userInfoCriteria.andUsernameEqualTo(userInfo.getUsername()) //根据这两个条件进行查询\n                          .andPasswordEqualTo(userInfo.getPassword());\n  \n  \n          return userInfoMapper.selectByExample(userInfoExample);//2 使用userInfoMapper里面自带一些方法，使用里面的selectbyexample查询\n      }\n  }\n  ```\n\n### 4.1.4 调试结果\n\n- 然后继续在usercontroller的返回那一行打个断点，继续调试，发现确认用户名密码后发生错误，调试终止。这是因为一些xml文件放在了java包下面，无法编译到工程中，需要在项目根目录的pom.xml的<build>中加入以下代码：\n\n  ```xml\n  \t<resources>\n  \t\t\t<resource>\n  \t\t\t\t<directory>src/main/java</directory>\n  \t\t\t\t<includes>\n  \t\t\t\t\t<include>**/*.xml</include>\n  \t\t\t\t</includes>\n  \t\t\t\t<filtering>false</filtering>\n  \t\t\t</resource>\n  \t\t\t<resource>\n  \t\t\t\t<directory>src/main/resources</directory>\n  \t\t\t</resource>\n  \t\t</resources>\n  ```\n\n  ![](D:/blog/themes/yilia/source/img/java/produce_practice/4/3.png)\n\n  ![](img/java/produce_practice/4/3.png)\n\n  然后刷新maven即可。\n\n- 此时发现userInfos的id是0，说明查询数据库成功了（id值来自数据库）\n\n  ![](D:/blog/themes/yilia/source/img/java/produce_practice/4/4.png)\n\n  ![](img/java/produce_practice/4/4.png)\n\n  - 然后点击调试控制台的恢复程序按钮\n\n     ![](D:/blog/themes/yilia/source/img/java/produce_practice/4/5.png)\n\n    ![](img/java/produce_practice/4/5.png) \n\n    发现前端页面跳转：\n\n    ![](D:/blog/themes/yilia/source/img/java/produce_practice/4/6.png)\n\n    ![](img/java/produce_practice/4/6.png)\n\n## 4.2 创建用户功能\n\n### 4.2.1 查看前端代码\n\n- 发现路径是admin/addUserInfo，参照4.1的经验，我们接下来要找control层的addUserInfo方法\n\n  ![](D:/blog/themes/yilia/source/img/java/produce_practice/4/7.png)\n\n  ![](img/java/produce_practice/4/7.png) \n\n\n\n### 4.2.2 编写UserController.userLogin方法（control层）\n\n```java\n    public HttpResponseEntity addUserInfo(@RequestBody UserInfo userInfo) {\n\n        HttpResponseEntity httpResponseEntity = new HttpResponseEntity();\n        int count = userService.addUserInfo(userInfo); //返回受影响的行数\n        if(count > 0) { //插入成功则状态码为666，否则为默认错误值\n            httpResponseEntity.setCode(\"666\");\n        }\n        return httpResponseEntity;\n    }\n```\n\n### 4.2.3 编写userserviceImpl.addUserInfo方法（service层）\n\n- 由于数据库设计问题，每次插入数据都需要一个userid，userid不是自动编号的，需要自己写入，而前端没有写入，因此我们还需要一个在后端生成userid的方法在后端。为此我们创建src/main/java/com/lyingedu/questionnaire/common/utils/UUIDUtil.java:\n\n  ```java\n  package com.lyingedu.questionnaire.common.utils;\n  \n  \n  import java.util.UUID;\n  \n  public class UUIDUtil {\n  \n      /**\n       *获取一个UUID\n       */\n      public static String getOneUUID(){\n          //获取UUID\n          String s = UUID.randomUUID().toString();\n          //去掉“-”符号\n          return s.substring(0,8)+s.substring(9,13)+s.substring(14,18)+s.substring(19,23)+s.substring(24);\n      }\n      /**\n       * 获得指定数目的UUID\n       * @param number int 需要获得的UUID数量\n       * @return String[] UUID数组\n       */\n      public static String[] getUUID(int number){\n          if(number < 1){\n              return null;\n          }\n          String[] ss = new String[number];\n          for(int i=0;i<number;i++){\n              ss[i] = getOneUUID();\n          }\n          return ss;\n      }\n  }\n  \n  ```\n\n- 然后我们再来编写userserviceImpl.addUserInfo方法：\n\n  ```java\n      public int addUserInfo(UserInfo userInfo) {\n  \n          userInfo.setId(UUIDUtil.getOneUUID());\n          userInfo.setStatus(\"1\");//1有效，0无效\n          \n          return userInfoMapper.insert(userInfo);\n      }\n  ```\n\n- 最后还需要将index.js里面的两句判断状态码从1改成自己的666：\n\n  ```js\n  if (res.code === \"666\") {\n  ```\n\n  ","slug":"java/produce_practice/4_login","published":1,"updated":"2024-03-05T11:39:22.574Z","_id":"clvuli6ru007jzsvwb5sz6mmf","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n\n\n<h1 id=\"4-登录及创建用户功能开发\"><a href=\"#4-登录及创建用户功能开发\" class=\"headerlink\" title=\"4. 登录及创建用户功能开发\"></a>4. 登录及创建用户功能开发</h1><h2 id=\"4-1-登录功能\"><a href=\"#4-1-登录功能\" class=\"headerlink\" title=\"4.1 登录功能\"></a>4.1 登录功能</h2><h3 id=\"4-1-1-查看前端代码\"><a href=\"#4-1-1-查看前端代码\" class=\"headerlink\" title=\"4.1.1 查看前端代码\"></a>4.1.1 查看前端代码</h3><ul>\n<li><p>首先在resources&#x2F;static&#x2F;pages&#x2F;login&#x2F;index.html查看登录按钮的动作：发现是调用了onLogin函数。</p>\n</li>\n<li><p>进入.&#x2F;index.js查看该函数：</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/4/1.png\"></p>\n<p><img src=\"/img/java/produce_practice/4/1.png\"></p>\n</li>\n</ul>\n<h3 id=\"4-1-2-编写UserController-userLogin方法（control层）\"><a href=\"#4-1-2-编写UserController-userLogin方法（control层）\" class=\"headerlink\" title=\"4.1.2 编写UserController.userLogin方法（control层）\"></a>4.1.2 编写UserController.userLogin方法（control层）</h3><ul>\n<li><p>然后我们到biz&#x2F;user&#x2F;controller&#x2F;UserController，其中有</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@RequestMapping(&quot;/admin&quot;)</span> <span class=\"comment\">//使用request请求</span></span><br></pre></td></tr></table></figure>\n\n<p>对应&#x2F;admin路径。然后我们找到其中的userLogin方法，进行编写：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@PostMapping(&quot;/userLogin&quot;)</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">userLogin</span><span class=\"params\">(<span class=\"meta\">@RequestBody</span> UserInfo userInfo)</span> &#123; <span class=\"comment\">//requestbody注解，将请求的json数据转换为对象</span></span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"type\">HttpResponseEntity</span> <span class=\"variable\">httpResponseEntity</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">HttpResponseEntity</span>();</span><br><span class=\"line\">    <span class=\"comment\">//TODO</span></span><br><span class=\"line\">    List&lt;UserInfo&gt; userInfos = userService.queryUserList(userInfo); <span class=\"comment\">//使用service层的方法查询用户信息</span></span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"keyword\">if</span>(userInfos.size() != <span class=\"number\">0</span>) &#123;<span class=\"comment\">//如果查询到用户信息则状态码为666，否则为默认值</span></span><br><span class=\"line\">        httpResponseEntity.setCode(<span class=\"string\">&quot;666&quot;</span>);</span><br><span class=\"line\">        httpResponseEntity.setData(userInfos.get(<span class=\"number\">0</span>));</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> httpResponseEntity;<span class=\"comment\">//传给前端</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>然后进行调试，可以先在该方法内打个断点，然后调试，进入 <a href=\"http://127.0.0.1:8085/pages/login/index.html%EF%BC%8C%E8%BE%93%E5%85%A5%E7%94%A8%E6%88%B7%E5%90%8D%E5%AF%86%E7%A0%81%EF%BC%8C%E7%A1%AE%E8%AE%A4%E5%90%8E%E5%8F%AF%E4%BB%A5%E7%9C%8B%E5%88%B0%E4%BB%A3%E7%A0%81%E8%BF%90%E8%A1%8C%E5%88%B0%E4%BA%86%E6%96%AD%E7%82%B9%E5%A4%84%EF%BC%8C%E5%8F%AF%E4%BB%A5%E6%9F%A5%E7%9C%8B%E4%B8%80%E4%B8%8BuserInfo%E5%8F%98%E9%87%8F%E6%98%AF%E5%90%A6%E8%83%BD%E8%8E%B7%E5%BE%97%E3%80%82\">http://127.0.0.1:8085/pages/login/index.html，输入用户名密码，确认后可以看到代码运行到了断点处，可以查看一下userInfo变量是否能获得。</a></p>\n<ul>\n<li><p>注意：如果你输入了localhost而非127.0.0.1你可能会发现代码没有运行到断点处，无法查看变量。在浏览器打开检查页面。</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/4/2.png\"></p>\n<p><img src=\"/img/java/produce_practice/4/2.png\"></p>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"4-1-3-编写userserviceImpl-queryUserList方法（service层）\"><a href=\"#4-1-3-编写userserviceImpl-queryUserList方法（service层）\" class=\"headerlink\" title=\"4.1.3 编写userserviceImpl.queryUserList方法（service层）\"></a>4.1.3 编写userserviceImpl.queryUserList方法（service层）</h3><ul>\n<li><p>具体查询是在userserviceImpl.queryUserList中实现的：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 查询用户列表</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> userInfo</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> List&lt;UserInfo&gt; <span class=\"title function_\">queryUserList</span><span class=\"params\">(UserInfo userInfo)</span> &#123;<span class=\"comment\">//1. 用userInof查询数据库</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">UserInfoExample</span> <span class=\"variable\">userInfoExample</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">UserInfoExample</span>(); <span class=\"comment\">//3 在dbmap/entities里面是有userinfoexample的</span></span><br><span class=\"line\">        UserInfoExample.<span class=\"type\">Criteria</span> <span class=\"variable\">userInfoCriteria</span> <span class=\"operator\">=</span> userInfoExample.createCriteria(); <span class=\"comment\">//4 然后用这个example创建一个条件</span></span><br><span class=\"line\">        userInfoCriteria.andUsernameEqualTo(userInfo.getUsername()) <span class=\"comment\">//根据这两个条件进行查询</span></span><br><span class=\"line\">                        .andPasswordEqualTo(userInfo.getPassword());</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> userInfoMapper.selectByExample(userInfoExample);<span class=\"comment\">//2 使用userInfoMapper里面自带一些方法，使用里面的selectbyexample查询</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h3 id=\"4-1-4-调试结果\"><a href=\"#4-1-4-调试结果\" class=\"headerlink\" title=\"4.1.4 调试结果\"></a>4.1.4 调试结果</h3><ul>\n<li><p>然后继续在usercontroller的返回那一行打个断点，继续调试，发现确认用户名密码后发生错误，调试终止。这是因为一些xml文件放在了java包下面，无法编译到工程中，需要在项目根目录的pom.xml的<build>中加入以下代码：</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">resources</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;<span class=\"name\">resource</span>&gt;</span></span><br><span class=\"line\">\t\t\t<span class=\"tag\">&lt;<span class=\"name\">directory</span>&gt;</span>src/main/java<span class=\"tag\">&lt;/<span class=\"name\">directory</span>&gt;</span></span><br><span class=\"line\">\t\t\t<span class=\"tag\">&lt;<span class=\"name\">includes</span>&gt;</span></span><br><span class=\"line\">\t\t\t\t<span class=\"tag\">&lt;<span class=\"name\">include</span>&gt;</span>**/*.xml<span class=\"tag\">&lt;/<span class=\"name\">include</span>&gt;</span></span><br><span class=\"line\">\t\t\t<span class=\"tag\">&lt;/<span class=\"name\">includes</span>&gt;</span></span><br><span class=\"line\">\t\t\t<span class=\"tag\">&lt;<span class=\"name\">filtering</span>&gt;</span>false<span class=\"tag\">&lt;/<span class=\"name\">filtering</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;/<span class=\"name\">resource</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;<span class=\"name\">resource</span>&gt;</span></span><br><span class=\"line\">\t\t\t<span class=\"tag\">&lt;<span class=\"name\">directory</span>&gt;</span>src/main/resources<span class=\"tag\">&lt;/<span class=\"name\">directory</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;/<span class=\"name\">resource</span>&gt;</span></span><br><span class=\"line\">\t<span class=\"tag\">&lt;/<span class=\"name\">resources</span>&gt;</span></span><br></pre></td></tr></table></figure>\n\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/4/3.png\"></p>\n<p><img src=\"/img/java/produce_practice/4/3.png\"></p>\n<p>然后刷新maven即可。</p>\n</li>\n<li><p>此时发现userInfos的id是0，说明查询数据库成功了（id值来自数据库）</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/4/4.png\"></p>\n<p><img src=\"/img/java/produce_practice/4/4.png\"></p>\n<ul>\n<li><p>然后点击调试控制台的恢复程序按钮</p>\n<p> <img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/4/5.png\"></p>\n<p><img src=\"/img/java/produce_practice/4/5.png\"> </p>\n<p>发现前端页面跳转：</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/4/6.png\"></p>\n<p><img src=\"/img/java/produce_practice/4/6.png\"></p>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"4-2-创建用户功能\"><a href=\"#4-2-创建用户功能\" class=\"headerlink\" title=\"4.2 创建用户功能\"></a>4.2 创建用户功能</h2><h3 id=\"4-2-1-查看前端代码\"><a href=\"#4-2-1-查看前端代码\" class=\"headerlink\" title=\"4.2.1 查看前端代码\"></a>4.2.1 查看前端代码</h3><ul>\n<li><p>发现路径是admin&#x2F;addUserInfo，参照4.1的经验，我们接下来要找control层的addUserInfo方法</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/4/7.png\"></p>\n<p><img src=\"/img/java/produce_practice/4/7.png\"></p>\n</li>\n</ul>\n<h3 id=\"4-2-2-编写UserController-userLogin方法（control层）\"><a href=\"#4-2-2-编写UserController-userLogin方法（control层）\" class=\"headerlink\" title=\"4.2.2 编写UserController.userLogin方法（control层）\"></a>4.2.2 编写UserController.userLogin方法（control层）</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">addUserInfo</span><span class=\"params\">(<span class=\"meta\">@RequestBody</span> UserInfo userInfo)</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">HttpResponseEntity</span> <span class=\"variable\">httpResponseEntity</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">HttpResponseEntity</span>();</span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"variable\">count</span> <span class=\"operator\">=</span> userService.addUserInfo(userInfo); <span class=\"comment\">//返回受影响的行数</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span>(count &gt; <span class=\"number\">0</span>) &#123; <span class=\"comment\">//插入成功则状态码为666，否则为默认错误值</span></span><br><span class=\"line\">        httpResponseEntity.setCode(<span class=\"string\">&quot;666&quot;</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> httpResponseEntity;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"4-2-3-编写userserviceImpl-addUserInfo方法（service层）\"><a href=\"#4-2-3-编写userserviceImpl-addUserInfo方法（service层）\" class=\"headerlink\" title=\"4.2.3 编写userserviceImpl.addUserInfo方法（service层）\"></a>4.2.3 编写userserviceImpl.addUserInfo方法（service层）</h3><ul>\n<li><p>由于数据库设计问题，每次插入数据都需要一个userid，userid不是自动编号的，需要自己写入，而前端没有写入，因此我们还需要一个在后端生成userid的方法在后端。为此我们创建src&#x2F;main&#x2F;java&#x2F;com&#x2F;lyingedu&#x2F;questionnaire&#x2F;common&#x2F;utils&#x2F;UUIDUtil.java:</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> com.lyingedu.questionnaire.common.utils;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.UUID;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">UUIDUtil</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     *获取一个UUID</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> String <span class=\"title function_\">getOneUUID</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">//获取UUID</span></span><br><span class=\"line\">        <span class=\"type\">String</span> <span class=\"variable\">s</span> <span class=\"operator\">=</span> UUID.randomUUID().toString();</span><br><span class=\"line\">        <span class=\"comment\">//去掉“-”符号</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> s.substring(<span class=\"number\">0</span>,<span class=\"number\">8</span>)+s.substring(<span class=\"number\">9</span>,<span class=\"number\">13</span>)+s.substring(<span class=\"number\">14</span>,<span class=\"number\">18</span>)+s.substring(<span class=\"number\">19</span>,<span class=\"number\">23</span>)+s.substring(<span class=\"number\">24</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 获得指定数目的UUID</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> number int 需要获得的UUID数量</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span> String[] UUID数组</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> String[] getUUID(<span class=\"type\">int</span> number)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(number &lt; <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"literal\">null</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        String[] ss = <span class=\"keyword\">new</span> <span class=\"title class_\">String</span>[number];</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">0</span>;i&lt;number;i++)&#123;</span><br><span class=\"line\">            ss[i] = getOneUUID();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ss;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>然后我们再来编写userserviceImpl.addUserInfo方法：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">addUserInfo</span><span class=\"params\">(UserInfo userInfo)</span> &#123;</span><br><span class=\"line\">  </span><br><span class=\"line\">    userInfo.setId(UUIDUtil.getOneUUID());</span><br><span class=\"line\">    userInfo.setStatus(<span class=\"string\">&quot;1&quot;</span>);<span class=\"comment\">//1有效，0无效</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> userInfoMapper.insert(userInfo);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>最后还需要将index.js里面的两句判断状态码从1改成自己的666：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> (res.<span class=\"property\">code</span> === <span class=\"string\">&quot;666&quot;</span>) &#123;</span><br></pre></td></tr></table></figure></li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"4-登录及创建用户功能开发\"><a href=\"#4-登录及创建用户功能开发\" class=\"headerlink\" title=\"4. 登录及创建用户功能开发\"></a>4. 登录及创建用户功能开发</h1><h2 id=\"4-1-登录功能\"><a href=\"#4-1-登录功能\" class=\"headerlink\" title=\"4.1 登录功能\"></a>4.1 登录功能</h2><h3 id=\"4-1-1-查看前端代码\"><a href=\"#4-1-1-查看前端代码\" class=\"headerlink\" title=\"4.1.1 查看前端代码\"></a>4.1.1 查看前端代码</h3><ul>\n<li><p>首先在resources&#x2F;static&#x2F;pages&#x2F;login&#x2F;index.html查看登录按钮的动作：发现是调用了onLogin函数。</p>\n</li>\n<li><p>进入.&#x2F;index.js查看该函数：</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/4/1.png\"></p>\n<p><img src=\"/img/java/produce_practice/4/1.png\"></p>\n</li>\n</ul>\n<h3 id=\"4-1-2-编写UserController-userLogin方法（control层）\"><a href=\"#4-1-2-编写UserController-userLogin方法（control层）\" class=\"headerlink\" title=\"4.1.2 编写UserController.userLogin方法（control层）\"></a>4.1.2 编写UserController.userLogin方法（control层）</h3><ul>\n<li><p>然后我们到biz&#x2F;user&#x2F;controller&#x2F;UserController，其中有</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@RequestMapping(&quot;/admin&quot;)</span> <span class=\"comment\">//使用request请求</span></span><br></pre></td></tr></table></figure>\n\n<p>对应&#x2F;admin路径。然后我们找到其中的userLogin方法，进行编写：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@PostMapping(&quot;/userLogin&quot;)</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">userLogin</span><span class=\"params\">(<span class=\"meta\">@RequestBody</span> UserInfo userInfo)</span> &#123; <span class=\"comment\">//requestbody注解，将请求的json数据转换为对象</span></span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"type\">HttpResponseEntity</span> <span class=\"variable\">httpResponseEntity</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">HttpResponseEntity</span>();</span><br><span class=\"line\">    <span class=\"comment\">//TODO</span></span><br><span class=\"line\">    List&lt;UserInfo&gt; userInfos = userService.queryUserList(userInfo); <span class=\"comment\">//使用service层的方法查询用户信息</span></span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"keyword\">if</span>(userInfos.size() != <span class=\"number\">0</span>) &#123;<span class=\"comment\">//如果查询到用户信息则状态码为666，否则为默认值</span></span><br><span class=\"line\">        httpResponseEntity.setCode(<span class=\"string\">&quot;666&quot;</span>);</span><br><span class=\"line\">        httpResponseEntity.setData(userInfos.get(<span class=\"number\">0</span>));</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> httpResponseEntity;<span class=\"comment\">//传给前端</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>然后进行调试，可以先在该方法内打个断点，然后调试，进入 <a href=\"http://127.0.0.1:8085/pages/login/index.html%EF%BC%8C%E8%BE%93%E5%85%A5%E7%94%A8%E6%88%B7%E5%90%8D%E5%AF%86%E7%A0%81%EF%BC%8C%E7%A1%AE%E8%AE%A4%E5%90%8E%E5%8F%AF%E4%BB%A5%E7%9C%8B%E5%88%B0%E4%BB%A3%E7%A0%81%E8%BF%90%E8%A1%8C%E5%88%B0%E4%BA%86%E6%96%AD%E7%82%B9%E5%A4%84%EF%BC%8C%E5%8F%AF%E4%BB%A5%E6%9F%A5%E7%9C%8B%E4%B8%80%E4%B8%8BuserInfo%E5%8F%98%E9%87%8F%E6%98%AF%E5%90%A6%E8%83%BD%E8%8E%B7%E5%BE%97%E3%80%82\">http://127.0.0.1:8085/pages/login/index.html，输入用户名密码，确认后可以看到代码运行到了断点处，可以查看一下userInfo变量是否能获得。</a></p>\n<ul>\n<li><p>注意：如果你输入了localhost而非127.0.0.1你可能会发现代码没有运行到断点处，无法查看变量。在浏览器打开检查页面。</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/4/2.png\"></p>\n<p><img src=\"/img/java/produce_practice/4/2.png\"></p>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"4-1-3-编写userserviceImpl-queryUserList方法（service层）\"><a href=\"#4-1-3-编写userserviceImpl-queryUserList方法（service层）\" class=\"headerlink\" title=\"4.1.3 编写userserviceImpl.queryUserList方法（service层）\"></a>4.1.3 编写userserviceImpl.queryUserList方法（service层）</h3><ul>\n<li><p>具体查询是在userserviceImpl.queryUserList中实现的：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 查询用户列表</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> userInfo</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> List&lt;UserInfo&gt; <span class=\"title function_\">queryUserList</span><span class=\"params\">(UserInfo userInfo)</span> &#123;<span class=\"comment\">//1. 用userInof查询数据库</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">UserInfoExample</span> <span class=\"variable\">userInfoExample</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">UserInfoExample</span>(); <span class=\"comment\">//3 在dbmap/entities里面是有userinfoexample的</span></span><br><span class=\"line\">        UserInfoExample.<span class=\"type\">Criteria</span> <span class=\"variable\">userInfoCriteria</span> <span class=\"operator\">=</span> userInfoExample.createCriteria(); <span class=\"comment\">//4 然后用这个example创建一个条件</span></span><br><span class=\"line\">        userInfoCriteria.andUsernameEqualTo(userInfo.getUsername()) <span class=\"comment\">//根据这两个条件进行查询</span></span><br><span class=\"line\">                        .andPasswordEqualTo(userInfo.getPassword());</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> userInfoMapper.selectByExample(userInfoExample);<span class=\"comment\">//2 使用userInfoMapper里面自带一些方法，使用里面的selectbyexample查询</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h3 id=\"4-1-4-调试结果\"><a href=\"#4-1-4-调试结果\" class=\"headerlink\" title=\"4.1.4 调试结果\"></a>4.1.4 调试结果</h3><ul>\n<li><p>然后继续在usercontroller的返回那一行打个断点，继续调试，发现确认用户名密码后发生错误，调试终止。这是因为一些xml文件放在了java包下面，无法编译到工程中，需要在项目根目录的pom.xml的<build>中加入以下代码：</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">resources</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;<span class=\"name\">resource</span>&gt;</span></span><br><span class=\"line\">\t\t\t<span class=\"tag\">&lt;<span class=\"name\">directory</span>&gt;</span>src/main/java<span class=\"tag\">&lt;/<span class=\"name\">directory</span>&gt;</span></span><br><span class=\"line\">\t\t\t<span class=\"tag\">&lt;<span class=\"name\">includes</span>&gt;</span></span><br><span class=\"line\">\t\t\t\t<span class=\"tag\">&lt;<span class=\"name\">include</span>&gt;</span>**/*.xml<span class=\"tag\">&lt;/<span class=\"name\">include</span>&gt;</span></span><br><span class=\"line\">\t\t\t<span class=\"tag\">&lt;/<span class=\"name\">includes</span>&gt;</span></span><br><span class=\"line\">\t\t\t<span class=\"tag\">&lt;<span class=\"name\">filtering</span>&gt;</span>false<span class=\"tag\">&lt;/<span class=\"name\">filtering</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;/<span class=\"name\">resource</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;<span class=\"name\">resource</span>&gt;</span></span><br><span class=\"line\">\t\t\t<span class=\"tag\">&lt;<span class=\"name\">directory</span>&gt;</span>src/main/resources<span class=\"tag\">&lt;/<span class=\"name\">directory</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;/<span class=\"name\">resource</span>&gt;</span></span><br><span class=\"line\">\t<span class=\"tag\">&lt;/<span class=\"name\">resources</span>&gt;</span></span><br></pre></td></tr></table></figure>\n\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/4/3.png\"></p>\n<p><img src=\"/img/java/produce_practice/4/3.png\"></p>\n<p>然后刷新maven即可。</p>\n</li>\n<li><p>此时发现userInfos的id是0，说明查询数据库成功了（id值来自数据库）</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/4/4.png\"></p>\n<p><img src=\"/img/java/produce_practice/4/4.png\"></p>\n<ul>\n<li><p>然后点击调试控制台的恢复程序按钮</p>\n<p> <img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/4/5.png\"></p>\n<p><img src=\"/img/java/produce_practice/4/5.png\"> </p>\n<p>发现前端页面跳转：</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/4/6.png\"></p>\n<p><img src=\"/img/java/produce_practice/4/6.png\"></p>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"4-2-创建用户功能\"><a href=\"#4-2-创建用户功能\" class=\"headerlink\" title=\"4.2 创建用户功能\"></a>4.2 创建用户功能</h2><h3 id=\"4-2-1-查看前端代码\"><a href=\"#4-2-1-查看前端代码\" class=\"headerlink\" title=\"4.2.1 查看前端代码\"></a>4.2.1 查看前端代码</h3><ul>\n<li><p>发现路径是admin&#x2F;addUserInfo，参照4.1的经验，我们接下来要找control层的addUserInfo方法</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/4/7.png\"></p>\n<p><img src=\"/img/java/produce_practice/4/7.png\"></p>\n</li>\n</ul>\n<h3 id=\"4-2-2-编写UserController-userLogin方法（control层）\"><a href=\"#4-2-2-编写UserController-userLogin方法（control层）\" class=\"headerlink\" title=\"4.2.2 编写UserController.userLogin方法（control层）\"></a>4.2.2 编写UserController.userLogin方法（control层）</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">addUserInfo</span><span class=\"params\">(<span class=\"meta\">@RequestBody</span> UserInfo userInfo)</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">HttpResponseEntity</span> <span class=\"variable\">httpResponseEntity</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">HttpResponseEntity</span>();</span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"variable\">count</span> <span class=\"operator\">=</span> userService.addUserInfo(userInfo); <span class=\"comment\">//返回受影响的行数</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span>(count &gt; <span class=\"number\">0</span>) &#123; <span class=\"comment\">//插入成功则状态码为666，否则为默认错误值</span></span><br><span class=\"line\">        httpResponseEntity.setCode(<span class=\"string\">&quot;666&quot;</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> httpResponseEntity;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"4-2-3-编写userserviceImpl-addUserInfo方法（service层）\"><a href=\"#4-2-3-编写userserviceImpl-addUserInfo方法（service层）\" class=\"headerlink\" title=\"4.2.3 编写userserviceImpl.addUserInfo方法（service层）\"></a>4.2.3 编写userserviceImpl.addUserInfo方法（service层）</h3><ul>\n<li><p>由于数据库设计问题，每次插入数据都需要一个userid，userid不是自动编号的，需要自己写入，而前端没有写入，因此我们还需要一个在后端生成userid的方法在后端。为此我们创建src&#x2F;main&#x2F;java&#x2F;com&#x2F;lyingedu&#x2F;questionnaire&#x2F;common&#x2F;utils&#x2F;UUIDUtil.java:</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> com.lyingedu.questionnaire.common.utils;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.UUID;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">UUIDUtil</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     *获取一个UUID</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> String <span class=\"title function_\">getOneUUID</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">//获取UUID</span></span><br><span class=\"line\">        <span class=\"type\">String</span> <span class=\"variable\">s</span> <span class=\"operator\">=</span> UUID.randomUUID().toString();</span><br><span class=\"line\">        <span class=\"comment\">//去掉“-”符号</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> s.substring(<span class=\"number\">0</span>,<span class=\"number\">8</span>)+s.substring(<span class=\"number\">9</span>,<span class=\"number\">13</span>)+s.substring(<span class=\"number\">14</span>,<span class=\"number\">18</span>)+s.substring(<span class=\"number\">19</span>,<span class=\"number\">23</span>)+s.substring(<span class=\"number\">24</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 获得指定数目的UUID</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> number int 需要获得的UUID数量</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span> String[] UUID数组</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> String[] getUUID(<span class=\"type\">int</span> number)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(number &lt; <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"literal\">null</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        String[] ss = <span class=\"keyword\">new</span> <span class=\"title class_\">String</span>[number];</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">0</span>;i&lt;number;i++)&#123;</span><br><span class=\"line\">            ss[i] = getOneUUID();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ss;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>然后我们再来编写userserviceImpl.addUserInfo方法：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">addUserInfo</span><span class=\"params\">(UserInfo userInfo)</span> &#123;</span><br><span class=\"line\">  </span><br><span class=\"line\">    userInfo.setId(UUIDUtil.getOneUUID());</span><br><span class=\"line\">    userInfo.setStatus(<span class=\"string\">&quot;1&quot;</span>);<span class=\"comment\">//1有效，0无效</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> userInfoMapper.insert(userInfo);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>最后还需要将index.js里面的两句判断状态码从1改成自己的666：</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> (res.<span class=\"property\">code</span> === <span class=\"string\">&quot;666&quot;</span>) &#123;</span><br></pre></td></tr></table></figure></li>\n</ul>"},{"title":"生产实习-7 项目功能开发","date":"2024-03-08T04:00:00.000Z","comment":true,"toc":true,"_content":"\n#\n<!--more-->\n\n# 7 项目功能开发\n![](D:/blog/themes/yilia/source/img/java/produce_practice/6/0.png)\n\n![](img/java/produce_practice/6/0.png)\n\n![](D:/blog/themes/yilia/source/img/java/produce_practice/6/1.png)\n\n![](img/java/produce_practice/6/1.png)\n\n## 7.1 项目实体创建\n\n- 创建ProjectInfo（之前用dbgan程序生成过），还需要在数据库创建表：\n\n  ```sql\n  CREATE TABLE IF NOT EXISTS `myweb`.`project_info` (\n    `id` VARCHAR(45) NOT NULL,\n    `user_id` VARCHAR(45) NULL,\n    `project_name` VARCHAR(45) NULL,\n    `project_content` VARCHAR(45) NULL,\n    `created_by` VARCHAR(45) NULL,\n    `creation_date` DATETIME NULL,\n    `last_updated_by` VARCHAR(45) NULL,\n    `last_update_date` DATETIME NULL,\n    PRIMARY KEY (`id`))\n  ENGINE = InnoDB\n  ```\n\n## 7.2 数据库映射创建\n\n- 创建ProjectInfoMapper接口（之前dbgan程序生成过xml文件）\n\n## 7.3 创建功能接口\n\n- 先写service层的接口：ProjectService：\n\n  ![](D:/blog/themes/yilia/source/img/java/produce_practice/6/3.png)\n\n  ![](img/java/produce_practice/6/3.png)\n\n  ```java\n  package com.lyingedu.questionnaire.biz.user.service;\n  \n  import com.lyingedu.questionnaire.dbmap.entities.ProjectInfo;\n  \n  import java.util.List;\n  \n  public interface ProjectService {\n      /**\n       * 增加项目\n       * @param projectInfo\n       * @return\n       */\n      int addProjectInfo(ProjectInfo projectInfo, String UserId);\n  \n      /**\n       * 修改项目\n       * @param projectInfo\n       * @return\n       */\n      int modifyProjectInfo(ProjectInfo projectInfo);\n  \n      /**\n       * 删除项目\n       * @param id\n       * @return\n       */\n      int deleteProjectById(String id);\n  \n      /**\n       * 查询项目列表\n       * @return\n       */\n      List<ProjectInfo> queryProjectList(ProjectInfo projectInfo);\n  }\n  \n  ```\n\n- 再写service实现类：ProjectServiceImpl:\n\n  ```java\n  package com.lyingedu.questionnaire.biz.user.service;\n  \n  import com.lyingedu.questionnaire.dbmap.entities.ProjectInfo;\n  import com.lyingedu.questionnaire.dbmap.imapper.ProjectInfoMapper;\n  import jakarta.annotation.Resource;\n  import org.springframework.stereotype.Service;\n  \n  import java.util.List;\n  \n  @Service\n  public class ProjectServiceImpl implements ProjectService {\n      \n      @Resource\n      private ProjectInfoMapper projectInfoMapper;\n  \n      @Override\n      public int addProjectInfo(ProjectInfo projectInfo, String UserId) {\n          return 0;\n      }\n  \n      @Override\n      public int modifyProjectInfo(ProjectInfo projectInfo) {\n          return 0;\n      }\n  \n      @Override\n      public int deleteProjectById(String id) {\n          return 0;\n      }\n  \n      @Override\n      public List<ProjectInfo> queryProjectList(ProjectInfo projectInfo) {\n          return null;\n      }\n  \n  \n  }\n  \n  ```\n\n- 最后写control层：\n\n  ![](D:/blog/themes/yilia/source/img/java/produce_practice/6/2.png)\n\n  ![](img/java/produce_practice/6/2.png)\n\n  ```java\n  package com.lyingedu.questionnaire.biz.user.controller;\n  \n  import com.lyingedu.questionnaire.beans.HttpResponseEntity;\n  import com.lyingedu.questionnaire.biz.user.service.ProjectService;\n  import com.lyingedu.questionnaire.dbmap.entities.ProjectInfo;\n  import jakarta.annotation.Resource;\n  import lombok.extern.slf4j.Slf4j;\n  import org.springframework.web.bind.annotation.RequestBody;\n  import org.springframework.web.bind.annotation.RestController;\n  \n  \n  @RestController //rest的接口风格\n  @Slf4j\n  public class ProjectController {\n      @Resource\n      private ProjectService projectService;\n  \n      public HttpResponseEntity queryProjectList(@RequestBody ProjectInfo projectInfo){\n          return null;\n      }\n  \n      public HttpResponseEntity addProjectInfo(@RequestBody ProjectInfo projectInfo){\n          return null;\n      }\n  \n      public HttpResponseEntity modifyProjectInfo(@RequestBody ProjectInfo projectInfo){\n          return null;\n      }\n  \n      public HttpResponseEntity deleteProjectById(@RequestBody String id){\n          return null;\n      }\n  \n  }\n  \n  ```\n\n  \n\n## 7.3 项目列表功能\n\n- 先查看对应的前端代码得到路径\n\n### 7.3.1 control层\n\n- ProjectController.queryProjectList：\n\n```java\n    @PostMapping(\"/queryProjectList\")\n    public HttpResponseEntity queryProjectList(@RequestBody ProjectInfo projectInfo){\n\n        HttpResponseEntity httpResponseEntity = new HttpResponseEntity();\n\n        List<ProjectInfo> projectInfos = projectService.queryProjectList(projectInfo);\n        if(projectInfos.size() == 1) {\n            httpResponseEntity.setCode(\"666\");\n        }\n        httpResponseEntity.setData(projectInfos);\n\n        return httpResponseEntity;\n    }\n```\n\n\n\n### 7.3.2 service层\n\n- ProjectServiceImpl.queryProjectList：\n\n```java\n    public List<ProjectInfo> queryProjectList(ProjectInfo projectInfo) {\n\n        ProjectInfoExample projectInfoExample = new ProjectInfoExample();\n        ProjectInfoExample.Criteria projectInfoCriteria = projectInfoExample.createCriteria();\n        projectInfoCriteria.andCreatedByEqualTo(projectInfo.getCreatedBy());\n        return projectInfoMapper.selectByExample(projectInfoExample);\n    }\n```\n\n### 7.3.3 数据库映射\n\n- 实现selectByEanmple：\n\n- imapper函数接口（已经生成过了）：\n\n  ```java\n  package com.lyingedu.questionnaire.dbmap.imapper;\n  \n  import com.lyingedu.questionnaire.dbmap.entities.ProjectInfo;\n  import com.lyingedu.questionnaire.dbmap.entities.ProjectInfoExample;\n  import java.util.List;\n  import org.apache.ibatis.annotations.Param;\n  \n  public interface ProjectInfoMapper {\n      /**\n       * This method was generated by MyBatis Generator.\n       * This method corresponds to the database table project_info\n       *\n       * @mbg.generated Tue Mar 05 14:17:07 CST 2024\n       */\n      long countByExample(ProjectInfoExample example);\n  \n      /**\n       * This method was generated by MyBatis Generator.\n       * This method corresponds to the database table project_info\n       *\n       * @mbg.generated Tue Mar 05 14:17:07 CST 2024\n       */\n      int deleteByExample(ProjectInfoExample example);\n  \n      /**\n       * This method was generated by MyBatis Generator.\n       * This method corresponds to the database table project_info\n       *\n       * @mbg.generated Tue Mar 05 14:17:07 CST 2024\n       */\n      int deleteByPrimaryKey(String id);\n  \n      /**\n       * This method was generated by MyBatis Generator.\n       * This method corresponds to the database table project_info\n       *\n       * @mbg.generated Tue Mar 05 14:17:07 CST 2024\n       */\n      int insert(ProjectInfo row);\n  \n      /**\n       * This method was generated by MyBatis Generator.\n       * This method corresponds to the database table project_info\n       *\n       * @mbg.generated Tue Mar 05 14:17:07 CST 2024\n       */\n      int insertSelective(ProjectInfo row);\n  \n      /**\n       * This method was generated by MyBatis Generator.\n       * This method corresponds to the database table project_info\n       *\n       * @mbg.generated Tue Mar 05 14:17:07 CST 2024\n       */\n      List<ProjectInfo> selectByExample(ProjectInfoExample example);\n  \n      /**\n       * This method was generated by MyBatis Generator.\n       * This method corresponds to the database table project_info\n       *\n       * @mbg.generated Tue Mar 05 14:17:07 CST 2024\n       */\n      ProjectInfo selectByPrimaryKey(String id);\n  \n      /**\n       * This method was generated by MyBatis Generator.\n       * This method corresponds to the database table project_info\n       *\n       * @mbg.generated Tue Mar 05 14:17:07 CST 2024\n       */\n      int updateByExampleSelective(@Param(\"row\") ProjectInfo row, @Param(\"example\") ProjectInfoExample example);\n  \n      /**\n       * This method was generated by MyBatis Generator.\n       * This method corresponds to the database table project_info\n       *\n       * @mbg.generated Tue Mar 05 14:17:07 CST 2024\n       */\n      int updateByExample(@Param(\"row\") ProjectInfo row, @Param(\"example\") ProjectInfoExample example);\n  \n      /**\n       * This method was generated by MyBatis Generator.\n       * This method corresponds to the database table project_info\n       *\n       * @mbg.generated Tue Mar 05 14:17:07 CST 2024\n       */\n      int updateByPrimaryKeySelective(ProjectInfo row);\n  \n      /**\n       * This method was generated by MyBatis Generator.\n       * This method corresponds to the database table project_info\n       *\n       * @mbg.generated Tue Mar 05 14:17:07 CST 2024\n       */\n      int updateByPrimaryKey(ProjectInfo row);\n  }\n  ```\n\n  \n\n- xmapper执行sql（已经生成过了）：\n\n  ```java\n    <select id=\"selectByExample\" parameterType=\"com.lyingedu.questionnaire.dbmap.entities.ProjectInfoExample\" resultMap=\"BaseResultMap\">\n      <!--\n        WARNING - @mbg.generated\n        This element is automatically generated by MyBatis Generator, do not modify.\n        This element was generated on Tue Mar 05 14:17:07 CST 2024.\n      -->\n      select\n      <if test=\"distinct\">\n        distinct\n      </if>\n      <include refid=\"Base_Column_List\" />\n      from project_info\n      <if test=\"_parameter != null\">\n        <include refid=\"Example_Where_Clause\" />\n      </if>\n      <if test=\"orderByClause != null\">\n        order by ${orderByClause}\n      </if>\n    </select>\n  ```\n\n  \n\n### 7.3.3 测试\n\n- 登陆后会显示当前用户创建的问卷\n\n## 7.4 新建项目功能\n\n- 查看前端代码得到路径\n\n### 7.4.1 control层\n\n```java\n    @PostMapping(\"/addProjectInfo\")\n    public HttpResponseEntity addProjectInfo(@RequestBody ProjectInfo projectInfo) {\n\n        HttpResponseEntity httpResponseEntity = new HttpResponseEntity();\n        int count = projectService.addProjectInfo(projectInfo, projectInfo.getCreatedBy()); //返回受影响的行数 //用户名要唯一\n        if(count == 1) { //插入成功则状态码为666，否则为默认错误值\n            httpResponseEntity.setCode(\"666\");\n        }\n        return httpResponseEntity;\n    }\n```\n\n### 7.4.2 service层\n\n```java\n    public int addProjectInfo(ProjectInfo projectInfo, String userName) {\n\n        projectInfo.setId(UUIDUtil.getOneUUID());\n        projectInfo.setCreatedBy(userName);\n        return projectInfoMapper.insert(projectInfo);\n    }\n```\n\n\n\n## 7.5 项目修改\n\n### 7.5.1 control层\n\n```java\n    @PostMapping(\"/modifyProjectInfo\")\n    public HttpResponseEntity modifyProjectInfo(@RequestBody ProjectInfo projectInfo) {\n\n        HttpResponseEntity httpResponseEntity = new HttpResponseEntity();\n        projectService.modifyProjectInfo(projectInfo);\n        httpResponseEntity.setCode(\"666\");\n        httpResponseEntity.setMessage(\"修改成功\");\n        return httpResponseEntity;\n    }\n```\n\n### 7.5.2 service层\n\n```java\n    public int modifyProjectInfo(ProjectInfo projectInfo) {\n        return projectInfoMapper.updateByPrimaryKeySelective(projectInfo)\n    }\n```\n\n\n\n## 7.6 项目删除\n\n### 7.6.1 control层\n\n```java\n    @PostMapping(\"/deleteProjectById\")\n    public HttpResponseEntity deleteProjectById(@RequestBody ProjectInfo projectInfo) {\n\n        HttpResponseEntity httpResponseEntity = new HttpResponseEntity();\n        projectService.deleteProjectById(projectInfo.getId());\n        httpResponseEntity.setCode(\"666\");\n        httpResponseEntity.setMessage(\"删除成功\");\n        return httpResponseEntity;\n    }\n```\n\n### 7.6.2 service层\n\n```java\n    public int deleteProjectById(String id) {\n        return projectInfoMapper.deleteByPrimaryKey(id);\n    }\n```\n\n","source":"_posts/java/produce_practice/7_project.md","raw":"---\ntitle: 生产实习-7 项目功能开发\n\ndate: 2024-3-8 12:00:00\n\ntags: [生产实习,java,springboot]\n\ncategories: [java]\n\ncomment: true\n\ntoc: true\n---\n\n#\n<!--more-->\n\n# 7 项目功能开发\n![](D:/blog/themes/yilia/source/img/java/produce_practice/6/0.png)\n\n![](img/java/produce_practice/6/0.png)\n\n![](D:/blog/themes/yilia/source/img/java/produce_practice/6/1.png)\n\n![](img/java/produce_practice/6/1.png)\n\n## 7.1 项目实体创建\n\n- 创建ProjectInfo（之前用dbgan程序生成过），还需要在数据库创建表：\n\n  ```sql\n  CREATE TABLE IF NOT EXISTS `myweb`.`project_info` (\n    `id` VARCHAR(45) NOT NULL,\n    `user_id` VARCHAR(45) NULL,\n    `project_name` VARCHAR(45) NULL,\n    `project_content` VARCHAR(45) NULL,\n    `created_by` VARCHAR(45) NULL,\n    `creation_date` DATETIME NULL,\n    `last_updated_by` VARCHAR(45) NULL,\n    `last_update_date` DATETIME NULL,\n    PRIMARY KEY (`id`))\n  ENGINE = InnoDB\n  ```\n\n## 7.2 数据库映射创建\n\n- 创建ProjectInfoMapper接口（之前dbgan程序生成过xml文件）\n\n## 7.3 创建功能接口\n\n- 先写service层的接口：ProjectService：\n\n  ![](D:/blog/themes/yilia/source/img/java/produce_practice/6/3.png)\n\n  ![](img/java/produce_practice/6/3.png)\n\n  ```java\n  package com.lyingedu.questionnaire.biz.user.service;\n  \n  import com.lyingedu.questionnaire.dbmap.entities.ProjectInfo;\n  \n  import java.util.List;\n  \n  public interface ProjectService {\n      /**\n       * 增加项目\n       * @param projectInfo\n       * @return\n       */\n      int addProjectInfo(ProjectInfo projectInfo, String UserId);\n  \n      /**\n       * 修改项目\n       * @param projectInfo\n       * @return\n       */\n      int modifyProjectInfo(ProjectInfo projectInfo);\n  \n      /**\n       * 删除项目\n       * @param id\n       * @return\n       */\n      int deleteProjectById(String id);\n  \n      /**\n       * 查询项目列表\n       * @return\n       */\n      List<ProjectInfo> queryProjectList(ProjectInfo projectInfo);\n  }\n  \n  ```\n\n- 再写service实现类：ProjectServiceImpl:\n\n  ```java\n  package com.lyingedu.questionnaire.biz.user.service;\n  \n  import com.lyingedu.questionnaire.dbmap.entities.ProjectInfo;\n  import com.lyingedu.questionnaire.dbmap.imapper.ProjectInfoMapper;\n  import jakarta.annotation.Resource;\n  import org.springframework.stereotype.Service;\n  \n  import java.util.List;\n  \n  @Service\n  public class ProjectServiceImpl implements ProjectService {\n      \n      @Resource\n      private ProjectInfoMapper projectInfoMapper;\n  \n      @Override\n      public int addProjectInfo(ProjectInfo projectInfo, String UserId) {\n          return 0;\n      }\n  \n      @Override\n      public int modifyProjectInfo(ProjectInfo projectInfo) {\n          return 0;\n      }\n  \n      @Override\n      public int deleteProjectById(String id) {\n          return 0;\n      }\n  \n      @Override\n      public List<ProjectInfo> queryProjectList(ProjectInfo projectInfo) {\n          return null;\n      }\n  \n  \n  }\n  \n  ```\n\n- 最后写control层：\n\n  ![](D:/blog/themes/yilia/source/img/java/produce_practice/6/2.png)\n\n  ![](img/java/produce_practice/6/2.png)\n\n  ```java\n  package com.lyingedu.questionnaire.biz.user.controller;\n  \n  import com.lyingedu.questionnaire.beans.HttpResponseEntity;\n  import com.lyingedu.questionnaire.biz.user.service.ProjectService;\n  import com.lyingedu.questionnaire.dbmap.entities.ProjectInfo;\n  import jakarta.annotation.Resource;\n  import lombok.extern.slf4j.Slf4j;\n  import org.springframework.web.bind.annotation.RequestBody;\n  import org.springframework.web.bind.annotation.RestController;\n  \n  \n  @RestController //rest的接口风格\n  @Slf4j\n  public class ProjectController {\n      @Resource\n      private ProjectService projectService;\n  \n      public HttpResponseEntity queryProjectList(@RequestBody ProjectInfo projectInfo){\n          return null;\n      }\n  \n      public HttpResponseEntity addProjectInfo(@RequestBody ProjectInfo projectInfo){\n          return null;\n      }\n  \n      public HttpResponseEntity modifyProjectInfo(@RequestBody ProjectInfo projectInfo){\n          return null;\n      }\n  \n      public HttpResponseEntity deleteProjectById(@RequestBody String id){\n          return null;\n      }\n  \n  }\n  \n  ```\n\n  \n\n## 7.3 项目列表功能\n\n- 先查看对应的前端代码得到路径\n\n### 7.3.1 control层\n\n- ProjectController.queryProjectList：\n\n```java\n    @PostMapping(\"/queryProjectList\")\n    public HttpResponseEntity queryProjectList(@RequestBody ProjectInfo projectInfo){\n\n        HttpResponseEntity httpResponseEntity = new HttpResponseEntity();\n\n        List<ProjectInfo> projectInfos = projectService.queryProjectList(projectInfo);\n        if(projectInfos.size() == 1) {\n            httpResponseEntity.setCode(\"666\");\n        }\n        httpResponseEntity.setData(projectInfos);\n\n        return httpResponseEntity;\n    }\n```\n\n\n\n### 7.3.2 service层\n\n- ProjectServiceImpl.queryProjectList：\n\n```java\n    public List<ProjectInfo> queryProjectList(ProjectInfo projectInfo) {\n\n        ProjectInfoExample projectInfoExample = new ProjectInfoExample();\n        ProjectInfoExample.Criteria projectInfoCriteria = projectInfoExample.createCriteria();\n        projectInfoCriteria.andCreatedByEqualTo(projectInfo.getCreatedBy());\n        return projectInfoMapper.selectByExample(projectInfoExample);\n    }\n```\n\n### 7.3.3 数据库映射\n\n- 实现selectByEanmple：\n\n- imapper函数接口（已经生成过了）：\n\n  ```java\n  package com.lyingedu.questionnaire.dbmap.imapper;\n  \n  import com.lyingedu.questionnaire.dbmap.entities.ProjectInfo;\n  import com.lyingedu.questionnaire.dbmap.entities.ProjectInfoExample;\n  import java.util.List;\n  import org.apache.ibatis.annotations.Param;\n  \n  public interface ProjectInfoMapper {\n      /**\n       * This method was generated by MyBatis Generator.\n       * This method corresponds to the database table project_info\n       *\n       * @mbg.generated Tue Mar 05 14:17:07 CST 2024\n       */\n      long countByExample(ProjectInfoExample example);\n  \n      /**\n       * This method was generated by MyBatis Generator.\n       * This method corresponds to the database table project_info\n       *\n       * @mbg.generated Tue Mar 05 14:17:07 CST 2024\n       */\n      int deleteByExample(ProjectInfoExample example);\n  \n      /**\n       * This method was generated by MyBatis Generator.\n       * This method corresponds to the database table project_info\n       *\n       * @mbg.generated Tue Mar 05 14:17:07 CST 2024\n       */\n      int deleteByPrimaryKey(String id);\n  \n      /**\n       * This method was generated by MyBatis Generator.\n       * This method corresponds to the database table project_info\n       *\n       * @mbg.generated Tue Mar 05 14:17:07 CST 2024\n       */\n      int insert(ProjectInfo row);\n  \n      /**\n       * This method was generated by MyBatis Generator.\n       * This method corresponds to the database table project_info\n       *\n       * @mbg.generated Tue Mar 05 14:17:07 CST 2024\n       */\n      int insertSelective(ProjectInfo row);\n  \n      /**\n       * This method was generated by MyBatis Generator.\n       * This method corresponds to the database table project_info\n       *\n       * @mbg.generated Tue Mar 05 14:17:07 CST 2024\n       */\n      List<ProjectInfo> selectByExample(ProjectInfoExample example);\n  \n      /**\n       * This method was generated by MyBatis Generator.\n       * This method corresponds to the database table project_info\n       *\n       * @mbg.generated Tue Mar 05 14:17:07 CST 2024\n       */\n      ProjectInfo selectByPrimaryKey(String id);\n  \n      /**\n       * This method was generated by MyBatis Generator.\n       * This method corresponds to the database table project_info\n       *\n       * @mbg.generated Tue Mar 05 14:17:07 CST 2024\n       */\n      int updateByExampleSelective(@Param(\"row\") ProjectInfo row, @Param(\"example\") ProjectInfoExample example);\n  \n      /**\n       * This method was generated by MyBatis Generator.\n       * This method corresponds to the database table project_info\n       *\n       * @mbg.generated Tue Mar 05 14:17:07 CST 2024\n       */\n      int updateByExample(@Param(\"row\") ProjectInfo row, @Param(\"example\") ProjectInfoExample example);\n  \n      /**\n       * This method was generated by MyBatis Generator.\n       * This method corresponds to the database table project_info\n       *\n       * @mbg.generated Tue Mar 05 14:17:07 CST 2024\n       */\n      int updateByPrimaryKeySelective(ProjectInfo row);\n  \n      /**\n       * This method was generated by MyBatis Generator.\n       * This method corresponds to the database table project_info\n       *\n       * @mbg.generated Tue Mar 05 14:17:07 CST 2024\n       */\n      int updateByPrimaryKey(ProjectInfo row);\n  }\n  ```\n\n  \n\n- xmapper执行sql（已经生成过了）：\n\n  ```java\n    <select id=\"selectByExample\" parameterType=\"com.lyingedu.questionnaire.dbmap.entities.ProjectInfoExample\" resultMap=\"BaseResultMap\">\n      <!--\n        WARNING - @mbg.generated\n        This element is automatically generated by MyBatis Generator, do not modify.\n        This element was generated on Tue Mar 05 14:17:07 CST 2024.\n      -->\n      select\n      <if test=\"distinct\">\n        distinct\n      </if>\n      <include refid=\"Base_Column_List\" />\n      from project_info\n      <if test=\"_parameter != null\">\n        <include refid=\"Example_Where_Clause\" />\n      </if>\n      <if test=\"orderByClause != null\">\n        order by ${orderByClause}\n      </if>\n    </select>\n  ```\n\n  \n\n### 7.3.3 测试\n\n- 登陆后会显示当前用户创建的问卷\n\n## 7.4 新建项目功能\n\n- 查看前端代码得到路径\n\n### 7.4.1 control层\n\n```java\n    @PostMapping(\"/addProjectInfo\")\n    public HttpResponseEntity addProjectInfo(@RequestBody ProjectInfo projectInfo) {\n\n        HttpResponseEntity httpResponseEntity = new HttpResponseEntity();\n        int count = projectService.addProjectInfo(projectInfo, projectInfo.getCreatedBy()); //返回受影响的行数 //用户名要唯一\n        if(count == 1) { //插入成功则状态码为666，否则为默认错误值\n            httpResponseEntity.setCode(\"666\");\n        }\n        return httpResponseEntity;\n    }\n```\n\n### 7.4.2 service层\n\n```java\n    public int addProjectInfo(ProjectInfo projectInfo, String userName) {\n\n        projectInfo.setId(UUIDUtil.getOneUUID());\n        projectInfo.setCreatedBy(userName);\n        return projectInfoMapper.insert(projectInfo);\n    }\n```\n\n\n\n## 7.5 项目修改\n\n### 7.5.1 control层\n\n```java\n    @PostMapping(\"/modifyProjectInfo\")\n    public HttpResponseEntity modifyProjectInfo(@RequestBody ProjectInfo projectInfo) {\n\n        HttpResponseEntity httpResponseEntity = new HttpResponseEntity();\n        projectService.modifyProjectInfo(projectInfo);\n        httpResponseEntity.setCode(\"666\");\n        httpResponseEntity.setMessage(\"修改成功\");\n        return httpResponseEntity;\n    }\n```\n\n### 7.5.2 service层\n\n```java\n    public int modifyProjectInfo(ProjectInfo projectInfo) {\n        return projectInfoMapper.updateByPrimaryKeySelective(projectInfo)\n    }\n```\n\n\n\n## 7.6 项目删除\n\n### 7.6.1 control层\n\n```java\n    @PostMapping(\"/deleteProjectById\")\n    public HttpResponseEntity deleteProjectById(@RequestBody ProjectInfo projectInfo) {\n\n        HttpResponseEntity httpResponseEntity = new HttpResponseEntity();\n        projectService.deleteProjectById(projectInfo.getId());\n        httpResponseEntity.setCode(\"666\");\n        httpResponseEntity.setMessage(\"删除成功\");\n        return httpResponseEntity;\n    }\n```\n\n### 7.6.2 service层\n\n```java\n    public int deleteProjectById(String id) {\n        return projectInfoMapper.deleteByPrimaryKey(id);\n    }\n```\n\n","slug":"java/produce_practice/7_project","published":1,"updated":"2024-03-08T09:09:05.970Z","_id":"clvuli6rv007mzsvw3u0sbtnt","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h1 id=\"7-项目功能开发\"><a href=\"#7-项目功能开发\" class=\"headerlink\" title=\"7 项目功能开发\"></a>7 项目功能开发</h1><p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/6/0.png\"></p>\n<p><img src=\"/img/java/produce_practice/6/0.png\"></p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/6/1.png\"></p>\n<p><img src=\"/img/java/produce_practice/6/1.png\"></p>\n<h2 id=\"7-1-项目实体创建\"><a href=\"#7-1-项目实体创建\" class=\"headerlink\" title=\"7.1 项目实体创建\"></a>7.1 项目实体创建</h2><ul>\n<li><p>创建ProjectInfo（之前用dbgan程序生成过），还需要在数据库创建表：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> IF <span class=\"keyword\">NOT</span> <span class=\"keyword\">EXISTS</span> `myweb`.`project_info` (</span><br><span class=\"line\">  `id` <span class=\"type\">VARCHAR</span>(<span class=\"number\">45</span>) <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `user_id` <span class=\"type\">VARCHAR</span>(<span class=\"number\">45</span>) <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `project_name` <span class=\"type\">VARCHAR</span>(<span class=\"number\">45</span>) <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `project_content` <span class=\"type\">VARCHAR</span>(<span class=\"number\">45</span>) <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `created_by` <span class=\"type\">VARCHAR</span>(<span class=\"number\">45</span>) <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `creation_date` DATETIME <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `last_updated_by` <span class=\"type\">VARCHAR</span>(<span class=\"number\">45</span>) <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `last_update_date` DATETIME <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  <span class=\"keyword\">PRIMARY</span> KEY (`id`))</span><br><span class=\"line\">ENGINE <span class=\"operator\">=</span> InnoDB</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h2 id=\"7-2-数据库映射创建\"><a href=\"#7-2-数据库映射创建\" class=\"headerlink\" title=\"7.2 数据库映射创建\"></a>7.2 数据库映射创建</h2><ul>\n<li>创建ProjectInfoMapper接口（之前dbgan程序生成过xml文件）</li>\n</ul>\n<h2 id=\"7-3-创建功能接口\"><a href=\"#7-3-创建功能接口\" class=\"headerlink\" title=\"7.3 创建功能接口\"></a>7.3 创建功能接口</h2><ul>\n<li><p>先写service层的接口：ProjectService：</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/6/3.png\"></p>\n<p><img src=\"/img/java/produce_practice/6/3.png\"></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> com.lyingedu.questionnaire.biz.user.service;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> com.lyingedu.questionnaire.dbmap.entities.ProjectInfo;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.List;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">interface</span> <span class=\"title class_\">ProjectService</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 增加项目</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> projectInfo</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"title function_\">addProjectInfo</span><span class=\"params\">(ProjectInfo projectInfo, String UserId)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 修改项目</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> projectInfo</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"title function_\">modifyProjectInfo</span><span class=\"params\">(ProjectInfo projectInfo)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 删除项目</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> id</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"title function_\">deleteProjectById</span><span class=\"params\">(String id)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 查询项目列表</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    List&lt;ProjectInfo&gt; <span class=\"title function_\">queryProjectList</span><span class=\"params\">(ProjectInfo projectInfo)</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>再写service实现类：ProjectServiceImpl:</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> com.lyingedu.questionnaire.biz.user.service;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> com.lyingedu.questionnaire.dbmap.entities.ProjectInfo;</span><br><span class=\"line\"><span class=\"keyword\">import</span> com.lyingedu.questionnaire.dbmap.imapper.ProjectInfoMapper;</span><br><span class=\"line\"><span class=\"keyword\">import</span> jakarta.annotation.Resource;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.stereotype.Service;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.List;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@Service</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">ProjectServiceImpl</span> <span class=\"keyword\">implements</span> <span class=\"title class_\">ProjectService</span> &#123;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"meta\">@Resource</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> ProjectInfoMapper projectInfoMapper;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">addProjectInfo</span><span class=\"params\">(ProjectInfo projectInfo, String UserId)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">modifyProjectInfo</span><span class=\"params\">(ProjectInfo projectInfo)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">deleteProjectById</span><span class=\"params\">(String id)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> List&lt;ProjectInfo&gt; <span class=\"title function_\">queryProjectList</span><span class=\"params\">(ProjectInfo projectInfo)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">null</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>最后写control层：</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/6/2.png\"></p>\n<p><img src=\"/img/java/produce_practice/6/2.png\"></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> com.lyingedu.questionnaire.biz.user.controller;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> com.lyingedu.questionnaire.beans.HttpResponseEntity;</span><br><span class=\"line\"><span class=\"keyword\">import</span> com.lyingedu.questionnaire.biz.user.service.ProjectService;</span><br><span class=\"line\"><span class=\"keyword\">import</span> com.lyingedu.questionnaire.dbmap.entities.ProjectInfo;</span><br><span class=\"line\"><span class=\"keyword\">import</span> jakarta.annotation.Resource;</span><br><span class=\"line\"><span class=\"keyword\">import</span> lombok.extern.slf4j.Slf4j;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.web.bind.annotation.RequestBody;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.web.bind.annotation.RestController;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@RestController</span> <span class=\"comment\">//rest的接口风格</span></span><br><span class=\"line\"><span class=\"meta\">@Slf4j</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">ProjectController</span> &#123;</span><br><span class=\"line\">    <span class=\"meta\">@Resource</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> ProjectService projectService;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">queryProjectList</span><span class=\"params\">(<span class=\"meta\">@RequestBody</span> ProjectInfo projectInfo)</span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">null</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">addProjectInfo</span><span class=\"params\">(<span class=\"meta\">@RequestBody</span> ProjectInfo projectInfo)</span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">null</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">modifyProjectInfo</span><span class=\"params\">(<span class=\"meta\">@RequestBody</span> ProjectInfo projectInfo)</span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">null</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">deleteProjectById</span><span class=\"params\">(<span class=\"meta\">@RequestBody</span> String id)</span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">null</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></li>\n</ul>\n<h2 id=\"7-3-项目列表功能\"><a href=\"#7-3-项目列表功能\" class=\"headerlink\" title=\"7.3 项目列表功能\"></a>7.3 项目列表功能</h2><ul>\n<li>先查看对应的前端代码得到路径</li>\n</ul>\n<h3 id=\"7-3-1-control层\"><a href=\"#7-3-1-control层\" class=\"headerlink\" title=\"7.3.1 control层\"></a>7.3.1 control层</h3><ul>\n<li>ProjectController.queryProjectList：</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@PostMapping(&quot;/queryProjectList&quot;)</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">queryProjectList</span><span class=\"params\">(<span class=\"meta\">@RequestBody</span> ProjectInfo projectInfo)</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">HttpResponseEntity</span> <span class=\"variable\">httpResponseEntity</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">HttpResponseEntity</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">    List&lt;ProjectInfo&gt; projectInfos = projectService.queryProjectList(projectInfo);</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(projectInfos.size() == <span class=\"number\">1</span>) &#123;</span><br><span class=\"line\">        httpResponseEntity.setCode(<span class=\"string\">&quot;666&quot;</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    httpResponseEntity.setData(projectInfos);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> httpResponseEntity;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"7-3-2-service层\"><a href=\"#7-3-2-service层\" class=\"headerlink\" title=\"7.3.2 service层\"></a>7.3.2 service层</h3><ul>\n<li>ProjectServiceImpl.queryProjectList：</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> List&lt;ProjectInfo&gt; <span class=\"title function_\">queryProjectList</span><span class=\"params\">(ProjectInfo projectInfo)</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">ProjectInfoExample</span> <span class=\"variable\">projectInfoExample</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">ProjectInfoExample</span>();</span><br><span class=\"line\">    ProjectInfoExample.<span class=\"type\">Criteria</span> <span class=\"variable\">projectInfoCriteria</span> <span class=\"operator\">=</span> projectInfoExample.createCriteria();</span><br><span class=\"line\">    projectInfoCriteria.andCreatedByEqualTo(projectInfo.getCreatedBy());</span><br><span class=\"line\">    <span class=\"keyword\">return</span> projectInfoMapper.selectByExample(projectInfoExample);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"7-3-3-数据库映射\"><a href=\"#7-3-3-数据库映射\" class=\"headerlink\" title=\"7.3.3 数据库映射\"></a>7.3.3 数据库映射</h3><ul>\n<li><p>实现selectByEanmple：</p>\n</li>\n<li><p>imapper函数接口（已经生成过了）：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> com.lyingedu.questionnaire.dbmap.imapper;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> com.lyingedu.questionnaire.dbmap.entities.ProjectInfo;</span><br><span class=\"line\"><span class=\"keyword\">import</span> com.lyingedu.questionnaire.dbmap.entities.ProjectInfoExample;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.List;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.ibatis.annotations.Param;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">interface</span> <span class=\"title class_\">ProjectInfoMapper</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * This method was generated by MyBatis Generator.</span></span><br><span class=\"line\"><span class=\"comment\">     * This method corresponds to the database table project_info</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@mbg</span>.generated Tue Mar 05 14:17:07 CST 2024</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"type\">long</span> <span class=\"title function_\">countByExample</span><span class=\"params\">(ProjectInfoExample example)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * This method was generated by MyBatis Generator.</span></span><br><span class=\"line\"><span class=\"comment\">     * This method corresponds to the database table project_info</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@mbg</span>.generated Tue Mar 05 14:17:07 CST 2024</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"title function_\">deleteByExample</span><span class=\"params\">(ProjectInfoExample example)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * This method was generated by MyBatis Generator.</span></span><br><span class=\"line\"><span class=\"comment\">     * This method corresponds to the database table project_info</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@mbg</span>.generated Tue Mar 05 14:17:07 CST 2024</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"title function_\">deleteByPrimaryKey</span><span class=\"params\">(String id)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * This method was generated by MyBatis Generator.</span></span><br><span class=\"line\"><span class=\"comment\">     * This method corresponds to the database table project_info</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@mbg</span>.generated Tue Mar 05 14:17:07 CST 2024</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"title function_\">insert</span><span class=\"params\">(ProjectInfo row)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * This method was generated by MyBatis Generator.</span></span><br><span class=\"line\"><span class=\"comment\">     * This method corresponds to the database table project_info</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@mbg</span>.generated Tue Mar 05 14:17:07 CST 2024</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"title function_\">insertSelective</span><span class=\"params\">(ProjectInfo row)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * This method was generated by MyBatis Generator.</span></span><br><span class=\"line\"><span class=\"comment\">     * This method corresponds to the database table project_info</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@mbg</span>.generated Tue Mar 05 14:17:07 CST 2024</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    List&lt;ProjectInfo&gt; <span class=\"title function_\">selectByExample</span><span class=\"params\">(ProjectInfoExample example)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * This method was generated by MyBatis Generator.</span></span><br><span class=\"line\"><span class=\"comment\">     * This method corresponds to the database table project_info</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@mbg</span>.generated Tue Mar 05 14:17:07 CST 2024</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    ProjectInfo <span class=\"title function_\">selectByPrimaryKey</span><span class=\"params\">(String id)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * This method was generated by MyBatis Generator.</span></span><br><span class=\"line\"><span class=\"comment\">     * This method corresponds to the database table project_info</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@mbg</span>.generated Tue Mar 05 14:17:07 CST 2024</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"title function_\">updateByExampleSelective</span><span class=\"params\">(<span class=\"meta\">@Param(&quot;row&quot;)</span> ProjectInfo row, <span class=\"meta\">@Param(&quot;example&quot;)</span> ProjectInfoExample example)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * This method was generated by MyBatis Generator.</span></span><br><span class=\"line\"><span class=\"comment\">     * This method corresponds to the database table project_info</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@mbg</span>.generated Tue Mar 05 14:17:07 CST 2024</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"title function_\">updateByExample</span><span class=\"params\">(<span class=\"meta\">@Param(&quot;row&quot;)</span> ProjectInfo row, <span class=\"meta\">@Param(&quot;example&quot;)</span> ProjectInfoExample example)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * This method was generated by MyBatis Generator.</span></span><br><span class=\"line\"><span class=\"comment\">     * This method corresponds to the database table project_info</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@mbg</span>.generated Tue Mar 05 14:17:07 CST 2024</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"title function_\">updateByPrimaryKeySelective</span><span class=\"params\">(ProjectInfo row)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * This method was generated by MyBatis Generator.</span></span><br><span class=\"line\"><span class=\"comment\">     * This method corresponds to the database table project_info</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@mbg</span>.generated Tue Mar 05 14:17:07 CST 2024</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"title function_\">updateByPrimaryKey</span><span class=\"params\">(ProjectInfo row)</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n</li>\n<li><p>xmapper执行sql（已经生成过了）：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;select id=<span class=\"string\">&quot;selectByExample&quot;</span> parameterType=<span class=\"string\">&quot;com.lyingedu.questionnaire.dbmap.entities.ProjectInfoExample&quot;</span> resultMap=<span class=\"string\">&quot;BaseResultMap&quot;</span>&gt;</span><br><span class=\"line\">  &lt;!--</span><br><span class=\"line\">    WARNING - <span class=\"meta\">@mbg</span>.generated</span><br><span class=\"line\">    This element is automatically generated by MyBatis Generator, <span class=\"keyword\">do</span> not modify.</span><br><span class=\"line\">    This element was generated on Tue Mar <span class=\"number\">05</span> <span class=\"number\">14</span>:<span class=\"number\">17</span>:<span class=\"number\">07</span> CST <span class=\"number\">2024.</span></span><br><span class=\"line\">  --&gt;</span><br><span class=\"line\">  select</span><br><span class=\"line\">  &lt;<span class=\"keyword\">if</span> test=<span class=\"string\">&quot;distinct&quot;</span>&gt;</span><br><span class=\"line\">    distinct</span><br><span class=\"line\">  &lt;/<span class=\"keyword\">if</span>&gt;</span><br><span class=\"line\">  &lt;include refid=<span class=\"string\">&quot;Base_Column_List&quot;</span> /&gt;</span><br><span class=\"line\">  from project_info</span><br><span class=\"line\">  &lt;<span class=\"keyword\">if</span> test=<span class=\"string\">&quot;_parameter != null&quot;</span>&gt;</span><br><span class=\"line\">    &lt;include refid=<span class=\"string\">&quot;Example_Where_Clause&quot;</span> /&gt;</span><br><span class=\"line\">  &lt;/<span class=\"keyword\">if</span>&gt;</span><br><span class=\"line\">  &lt;<span class=\"keyword\">if</span> test=<span class=\"string\">&quot;orderByClause != null&quot;</span>&gt;</span><br><span class=\"line\">    order by $&#123;orderByClause&#125;</span><br><span class=\"line\">  &lt;/<span class=\"keyword\">if</span>&gt;</span><br><span class=\"line\">&lt;/select&gt;</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h3 id=\"7-3-3-测试\"><a href=\"#7-3-3-测试\" class=\"headerlink\" title=\"7.3.3 测试\"></a>7.3.3 测试</h3><ul>\n<li>登陆后会显示当前用户创建的问卷</li>\n</ul>\n<h2 id=\"7-4-新建项目功能\"><a href=\"#7-4-新建项目功能\" class=\"headerlink\" title=\"7.4 新建项目功能\"></a>7.4 新建项目功能</h2><ul>\n<li>查看前端代码得到路径</li>\n</ul>\n<h3 id=\"7-4-1-control层\"><a href=\"#7-4-1-control层\" class=\"headerlink\" title=\"7.4.1 control层\"></a>7.4.1 control层</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@PostMapping(&quot;/addProjectInfo&quot;)</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">addProjectInfo</span><span class=\"params\">(<span class=\"meta\">@RequestBody</span> ProjectInfo projectInfo)</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">HttpResponseEntity</span> <span class=\"variable\">httpResponseEntity</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">HttpResponseEntity</span>();</span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"variable\">count</span> <span class=\"operator\">=</span> projectService.addProjectInfo(projectInfo, projectInfo.getCreatedBy()); <span class=\"comment\">//返回受影响的行数 //用户名要唯一</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span>(count == <span class=\"number\">1</span>) &#123; <span class=\"comment\">//插入成功则状态码为666，否则为默认错误值</span></span><br><span class=\"line\">        httpResponseEntity.setCode(<span class=\"string\">&quot;666&quot;</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> httpResponseEntity;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"7-4-2-service层\"><a href=\"#7-4-2-service层\" class=\"headerlink\" title=\"7.4.2 service层\"></a>7.4.2 service层</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">addProjectInfo</span><span class=\"params\">(ProjectInfo projectInfo, String userName)</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    projectInfo.setId(UUIDUtil.getOneUUID());</span><br><span class=\"line\">    projectInfo.setCreatedBy(userName);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> projectInfoMapper.insert(projectInfo);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<h2 id=\"7-5-项目修改\"><a href=\"#7-5-项目修改\" class=\"headerlink\" title=\"7.5 项目修改\"></a>7.5 项目修改</h2><h3 id=\"7-5-1-control层\"><a href=\"#7-5-1-control层\" class=\"headerlink\" title=\"7.5.1 control层\"></a>7.5.1 control层</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@PostMapping(&quot;/modifyProjectInfo&quot;)</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">modifyProjectInfo</span><span class=\"params\">(<span class=\"meta\">@RequestBody</span> ProjectInfo projectInfo)</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">HttpResponseEntity</span> <span class=\"variable\">httpResponseEntity</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">HttpResponseEntity</span>();</span><br><span class=\"line\">    projectService.modifyProjectInfo(projectInfo);</span><br><span class=\"line\">    httpResponseEntity.setCode(<span class=\"string\">&quot;666&quot;</span>);</span><br><span class=\"line\">    httpResponseEntity.setMessage(<span class=\"string\">&quot;修改成功&quot;</span>);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> httpResponseEntity;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"7-5-2-service层\"><a href=\"#7-5-2-service层\" class=\"headerlink\" title=\"7.5.2 service层\"></a>7.5.2 service层</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">modifyProjectInfo</span><span class=\"params\">(ProjectInfo projectInfo)</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> projectInfoMapper.updateByPrimaryKeySelective(projectInfo)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<h2 id=\"7-6-项目删除\"><a href=\"#7-6-项目删除\" class=\"headerlink\" title=\"7.6 项目删除\"></a>7.6 项目删除</h2><h3 id=\"7-6-1-control层\"><a href=\"#7-6-1-control层\" class=\"headerlink\" title=\"7.6.1 control层\"></a>7.6.1 control层</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@PostMapping(&quot;/deleteProjectById&quot;)</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">deleteProjectById</span><span class=\"params\">(<span class=\"meta\">@RequestBody</span> ProjectInfo projectInfo)</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">HttpResponseEntity</span> <span class=\"variable\">httpResponseEntity</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">HttpResponseEntity</span>();</span><br><span class=\"line\">    projectService.deleteProjectById(projectInfo.getId());</span><br><span class=\"line\">    httpResponseEntity.setCode(<span class=\"string\">&quot;666&quot;</span>);</span><br><span class=\"line\">    httpResponseEntity.setMessage(<span class=\"string\">&quot;删除成功&quot;</span>);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> httpResponseEntity;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"7-6-2-service层\"><a href=\"#7-6-2-service层\" class=\"headerlink\" title=\"7.6.2 service层\"></a>7.6.2 service层</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">deleteProjectById</span><span class=\"params\">(String id)</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> projectInfoMapper.deleteByPrimaryKey(id);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"7-项目功能开发\"><a href=\"#7-项目功能开发\" class=\"headerlink\" title=\"7 项目功能开发\"></a>7 项目功能开发</h1><p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/6/0.png\"></p>\n<p><img src=\"/img/java/produce_practice/6/0.png\"></p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/6/1.png\"></p>\n<p><img src=\"/img/java/produce_practice/6/1.png\"></p>\n<h2 id=\"7-1-项目实体创建\"><a href=\"#7-1-项目实体创建\" class=\"headerlink\" title=\"7.1 项目实体创建\"></a>7.1 项目实体创建</h2><ul>\n<li><p>创建ProjectInfo（之前用dbgan程序生成过），还需要在数据库创建表：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> IF <span class=\"keyword\">NOT</span> <span class=\"keyword\">EXISTS</span> `myweb`.`project_info` (</span><br><span class=\"line\">  `id` <span class=\"type\">VARCHAR</span>(<span class=\"number\">45</span>) <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `user_id` <span class=\"type\">VARCHAR</span>(<span class=\"number\">45</span>) <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `project_name` <span class=\"type\">VARCHAR</span>(<span class=\"number\">45</span>) <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `project_content` <span class=\"type\">VARCHAR</span>(<span class=\"number\">45</span>) <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `created_by` <span class=\"type\">VARCHAR</span>(<span class=\"number\">45</span>) <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `creation_date` DATETIME <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `last_updated_by` <span class=\"type\">VARCHAR</span>(<span class=\"number\">45</span>) <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `last_update_date` DATETIME <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  <span class=\"keyword\">PRIMARY</span> KEY (`id`))</span><br><span class=\"line\">ENGINE <span class=\"operator\">=</span> InnoDB</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h2 id=\"7-2-数据库映射创建\"><a href=\"#7-2-数据库映射创建\" class=\"headerlink\" title=\"7.2 数据库映射创建\"></a>7.2 数据库映射创建</h2><ul>\n<li>创建ProjectInfoMapper接口（之前dbgan程序生成过xml文件）</li>\n</ul>\n<h2 id=\"7-3-创建功能接口\"><a href=\"#7-3-创建功能接口\" class=\"headerlink\" title=\"7.3 创建功能接口\"></a>7.3 创建功能接口</h2><ul>\n<li><p>先写service层的接口：ProjectService：</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/6/3.png\"></p>\n<p><img src=\"/img/java/produce_practice/6/3.png\"></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> com.lyingedu.questionnaire.biz.user.service;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> com.lyingedu.questionnaire.dbmap.entities.ProjectInfo;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.List;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">interface</span> <span class=\"title class_\">ProjectService</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 增加项目</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> projectInfo</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"title function_\">addProjectInfo</span><span class=\"params\">(ProjectInfo projectInfo, String UserId)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 修改项目</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> projectInfo</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"title function_\">modifyProjectInfo</span><span class=\"params\">(ProjectInfo projectInfo)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 删除项目</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> id</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"title function_\">deleteProjectById</span><span class=\"params\">(String id)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 查询项目列表</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    List&lt;ProjectInfo&gt; <span class=\"title function_\">queryProjectList</span><span class=\"params\">(ProjectInfo projectInfo)</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>再写service实现类：ProjectServiceImpl:</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> com.lyingedu.questionnaire.biz.user.service;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> com.lyingedu.questionnaire.dbmap.entities.ProjectInfo;</span><br><span class=\"line\"><span class=\"keyword\">import</span> com.lyingedu.questionnaire.dbmap.imapper.ProjectInfoMapper;</span><br><span class=\"line\"><span class=\"keyword\">import</span> jakarta.annotation.Resource;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.stereotype.Service;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.List;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@Service</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">ProjectServiceImpl</span> <span class=\"keyword\">implements</span> <span class=\"title class_\">ProjectService</span> &#123;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"meta\">@Resource</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> ProjectInfoMapper projectInfoMapper;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">addProjectInfo</span><span class=\"params\">(ProjectInfo projectInfo, String UserId)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">modifyProjectInfo</span><span class=\"params\">(ProjectInfo projectInfo)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">deleteProjectById</span><span class=\"params\">(String id)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> List&lt;ProjectInfo&gt; <span class=\"title function_\">queryProjectList</span><span class=\"params\">(ProjectInfo projectInfo)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">null</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>最后写control层：</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/6/2.png\"></p>\n<p><img src=\"/img/java/produce_practice/6/2.png\"></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> com.lyingedu.questionnaire.biz.user.controller;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> com.lyingedu.questionnaire.beans.HttpResponseEntity;</span><br><span class=\"line\"><span class=\"keyword\">import</span> com.lyingedu.questionnaire.biz.user.service.ProjectService;</span><br><span class=\"line\"><span class=\"keyword\">import</span> com.lyingedu.questionnaire.dbmap.entities.ProjectInfo;</span><br><span class=\"line\"><span class=\"keyword\">import</span> jakarta.annotation.Resource;</span><br><span class=\"line\"><span class=\"keyword\">import</span> lombok.extern.slf4j.Slf4j;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.web.bind.annotation.RequestBody;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.web.bind.annotation.RestController;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@RestController</span> <span class=\"comment\">//rest的接口风格</span></span><br><span class=\"line\"><span class=\"meta\">@Slf4j</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">ProjectController</span> &#123;</span><br><span class=\"line\">    <span class=\"meta\">@Resource</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> ProjectService projectService;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">queryProjectList</span><span class=\"params\">(<span class=\"meta\">@RequestBody</span> ProjectInfo projectInfo)</span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">null</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">addProjectInfo</span><span class=\"params\">(<span class=\"meta\">@RequestBody</span> ProjectInfo projectInfo)</span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">null</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">modifyProjectInfo</span><span class=\"params\">(<span class=\"meta\">@RequestBody</span> ProjectInfo projectInfo)</span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">null</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">deleteProjectById</span><span class=\"params\">(<span class=\"meta\">@RequestBody</span> String id)</span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">null</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></li>\n</ul>\n<h2 id=\"7-3-项目列表功能\"><a href=\"#7-3-项目列表功能\" class=\"headerlink\" title=\"7.3 项目列表功能\"></a>7.3 项目列表功能</h2><ul>\n<li>先查看对应的前端代码得到路径</li>\n</ul>\n<h3 id=\"7-3-1-control层\"><a href=\"#7-3-1-control层\" class=\"headerlink\" title=\"7.3.1 control层\"></a>7.3.1 control层</h3><ul>\n<li>ProjectController.queryProjectList：</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@PostMapping(&quot;/queryProjectList&quot;)</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">queryProjectList</span><span class=\"params\">(<span class=\"meta\">@RequestBody</span> ProjectInfo projectInfo)</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">HttpResponseEntity</span> <span class=\"variable\">httpResponseEntity</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">HttpResponseEntity</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">    List&lt;ProjectInfo&gt; projectInfos = projectService.queryProjectList(projectInfo);</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(projectInfos.size() == <span class=\"number\">1</span>) &#123;</span><br><span class=\"line\">        httpResponseEntity.setCode(<span class=\"string\">&quot;666&quot;</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    httpResponseEntity.setData(projectInfos);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> httpResponseEntity;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"7-3-2-service层\"><a href=\"#7-3-2-service层\" class=\"headerlink\" title=\"7.3.2 service层\"></a>7.3.2 service层</h3><ul>\n<li>ProjectServiceImpl.queryProjectList：</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> List&lt;ProjectInfo&gt; <span class=\"title function_\">queryProjectList</span><span class=\"params\">(ProjectInfo projectInfo)</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">ProjectInfoExample</span> <span class=\"variable\">projectInfoExample</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">ProjectInfoExample</span>();</span><br><span class=\"line\">    ProjectInfoExample.<span class=\"type\">Criteria</span> <span class=\"variable\">projectInfoCriteria</span> <span class=\"operator\">=</span> projectInfoExample.createCriteria();</span><br><span class=\"line\">    projectInfoCriteria.andCreatedByEqualTo(projectInfo.getCreatedBy());</span><br><span class=\"line\">    <span class=\"keyword\">return</span> projectInfoMapper.selectByExample(projectInfoExample);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"7-3-3-数据库映射\"><a href=\"#7-3-3-数据库映射\" class=\"headerlink\" title=\"7.3.3 数据库映射\"></a>7.3.3 数据库映射</h3><ul>\n<li><p>实现selectByEanmple：</p>\n</li>\n<li><p>imapper函数接口（已经生成过了）：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> com.lyingedu.questionnaire.dbmap.imapper;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> com.lyingedu.questionnaire.dbmap.entities.ProjectInfo;</span><br><span class=\"line\"><span class=\"keyword\">import</span> com.lyingedu.questionnaire.dbmap.entities.ProjectInfoExample;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.List;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.ibatis.annotations.Param;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">interface</span> <span class=\"title class_\">ProjectInfoMapper</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * This method was generated by MyBatis Generator.</span></span><br><span class=\"line\"><span class=\"comment\">     * This method corresponds to the database table project_info</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@mbg</span>.generated Tue Mar 05 14:17:07 CST 2024</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"type\">long</span> <span class=\"title function_\">countByExample</span><span class=\"params\">(ProjectInfoExample example)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * This method was generated by MyBatis Generator.</span></span><br><span class=\"line\"><span class=\"comment\">     * This method corresponds to the database table project_info</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@mbg</span>.generated Tue Mar 05 14:17:07 CST 2024</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"title function_\">deleteByExample</span><span class=\"params\">(ProjectInfoExample example)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * This method was generated by MyBatis Generator.</span></span><br><span class=\"line\"><span class=\"comment\">     * This method corresponds to the database table project_info</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@mbg</span>.generated Tue Mar 05 14:17:07 CST 2024</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"title function_\">deleteByPrimaryKey</span><span class=\"params\">(String id)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * This method was generated by MyBatis Generator.</span></span><br><span class=\"line\"><span class=\"comment\">     * This method corresponds to the database table project_info</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@mbg</span>.generated Tue Mar 05 14:17:07 CST 2024</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"title function_\">insert</span><span class=\"params\">(ProjectInfo row)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * This method was generated by MyBatis Generator.</span></span><br><span class=\"line\"><span class=\"comment\">     * This method corresponds to the database table project_info</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@mbg</span>.generated Tue Mar 05 14:17:07 CST 2024</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"title function_\">insertSelective</span><span class=\"params\">(ProjectInfo row)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * This method was generated by MyBatis Generator.</span></span><br><span class=\"line\"><span class=\"comment\">     * This method corresponds to the database table project_info</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@mbg</span>.generated Tue Mar 05 14:17:07 CST 2024</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    List&lt;ProjectInfo&gt; <span class=\"title function_\">selectByExample</span><span class=\"params\">(ProjectInfoExample example)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * This method was generated by MyBatis Generator.</span></span><br><span class=\"line\"><span class=\"comment\">     * This method corresponds to the database table project_info</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@mbg</span>.generated Tue Mar 05 14:17:07 CST 2024</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    ProjectInfo <span class=\"title function_\">selectByPrimaryKey</span><span class=\"params\">(String id)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * This method was generated by MyBatis Generator.</span></span><br><span class=\"line\"><span class=\"comment\">     * This method corresponds to the database table project_info</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@mbg</span>.generated Tue Mar 05 14:17:07 CST 2024</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"title function_\">updateByExampleSelective</span><span class=\"params\">(<span class=\"meta\">@Param(&quot;row&quot;)</span> ProjectInfo row, <span class=\"meta\">@Param(&quot;example&quot;)</span> ProjectInfoExample example)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * This method was generated by MyBatis Generator.</span></span><br><span class=\"line\"><span class=\"comment\">     * This method corresponds to the database table project_info</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@mbg</span>.generated Tue Mar 05 14:17:07 CST 2024</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"title function_\">updateByExample</span><span class=\"params\">(<span class=\"meta\">@Param(&quot;row&quot;)</span> ProjectInfo row, <span class=\"meta\">@Param(&quot;example&quot;)</span> ProjectInfoExample example)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * This method was generated by MyBatis Generator.</span></span><br><span class=\"line\"><span class=\"comment\">     * This method corresponds to the database table project_info</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@mbg</span>.generated Tue Mar 05 14:17:07 CST 2024</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"title function_\">updateByPrimaryKeySelective</span><span class=\"params\">(ProjectInfo row)</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * This method was generated by MyBatis Generator.</span></span><br><span class=\"line\"><span class=\"comment\">     * This method corresponds to the database table project_info</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@mbg</span>.generated Tue Mar 05 14:17:07 CST 2024</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"title function_\">updateByPrimaryKey</span><span class=\"params\">(ProjectInfo row)</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n</li>\n<li><p>xmapper执行sql（已经生成过了）：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;select id=<span class=\"string\">&quot;selectByExample&quot;</span> parameterType=<span class=\"string\">&quot;com.lyingedu.questionnaire.dbmap.entities.ProjectInfoExample&quot;</span> resultMap=<span class=\"string\">&quot;BaseResultMap&quot;</span>&gt;</span><br><span class=\"line\">  &lt;!--</span><br><span class=\"line\">    WARNING - <span class=\"meta\">@mbg</span>.generated</span><br><span class=\"line\">    This element is automatically generated by MyBatis Generator, <span class=\"keyword\">do</span> not modify.</span><br><span class=\"line\">    This element was generated on Tue Mar <span class=\"number\">05</span> <span class=\"number\">14</span>:<span class=\"number\">17</span>:<span class=\"number\">07</span> CST <span class=\"number\">2024.</span></span><br><span class=\"line\">  --&gt;</span><br><span class=\"line\">  select</span><br><span class=\"line\">  &lt;<span class=\"keyword\">if</span> test=<span class=\"string\">&quot;distinct&quot;</span>&gt;</span><br><span class=\"line\">    distinct</span><br><span class=\"line\">  &lt;/<span class=\"keyword\">if</span>&gt;</span><br><span class=\"line\">  &lt;include refid=<span class=\"string\">&quot;Base_Column_List&quot;</span> /&gt;</span><br><span class=\"line\">  from project_info</span><br><span class=\"line\">  &lt;<span class=\"keyword\">if</span> test=<span class=\"string\">&quot;_parameter != null&quot;</span>&gt;</span><br><span class=\"line\">    &lt;include refid=<span class=\"string\">&quot;Example_Where_Clause&quot;</span> /&gt;</span><br><span class=\"line\">  &lt;/<span class=\"keyword\">if</span>&gt;</span><br><span class=\"line\">  &lt;<span class=\"keyword\">if</span> test=<span class=\"string\">&quot;orderByClause != null&quot;</span>&gt;</span><br><span class=\"line\">    order by $&#123;orderByClause&#125;</span><br><span class=\"line\">  &lt;/<span class=\"keyword\">if</span>&gt;</span><br><span class=\"line\">&lt;/select&gt;</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h3 id=\"7-3-3-测试\"><a href=\"#7-3-3-测试\" class=\"headerlink\" title=\"7.3.3 测试\"></a>7.3.3 测试</h3><ul>\n<li>登陆后会显示当前用户创建的问卷</li>\n</ul>\n<h2 id=\"7-4-新建项目功能\"><a href=\"#7-4-新建项目功能\" class=\"headerlink\" title=\"7.4 新建项目功能\"></a>7.4 新建项目功能</h2><ul>\n<li>查看前端代码得到路径</li>\n</ul>\n<h3 id=\"7-4-1-control层\"><a href=\"#7-4-1-control层\" class=\"headerlink\" title=\"7.4.1 control层\"></a>7.4.1 control层</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@PostMapping(&quot;/addProjectInfo&quot;)</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">addProjectInfo</span><span class=\"params\">(<span class=\"meta\">@RequestBody</span> ProjectInfo projectInfo)</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">HttpResponseEntity</span> <span class=\"variable\">httpResponseEntity</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">HttpResponseEntity</span>();</span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"variable\">count</span> <span class=\"operator\">=</span> projectService.addProjectInfo(projectInfo, projectInfo.getCreatedBy()); <span class=\"comment\">//返回受影响的行数 //用户名要唯一</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span>(count == <span class=\"number\">1</span>) &#123; <span class=\"comment\">//插入成功则状态码为666，否则为默认错误值</span></span><br><span class=\"line\">        httpResponseEntity.setCode(<span class=\"string\">&quot;666&quot;</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> httpResponseEntity;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"7-4-2-service层\"><a href=\"#7-4-2-service层\" class=\"headerlink\" title=\"7.4.2 service层\"></a>7.4.2 service层</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">addProjectInfo</span><span class=\"params\">(ProjectInfo projectInfo, String userName)</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    projectInfo.setId(UUIDUtil.getOneUUID());</span><br><span class=\"line\">    projectInfo.setCreatedBy(userName);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> projectInfoMapper.insert(projectInfo);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<h2 id=\"7-5-项目修改\"><a href=\"#7-5-项目修改\" class=\"headerlink\" title=\"7.5 项目修改\"></a>7.5 项目修改</h2><h3 id=\"7-5-1-control层\"><a href=\"#7-5-1-control层\" class=\"headerlink\" title=\"7.5.1 control层\"></a>7.5.1 control层</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@PostMapping(&quot;/modifyProjectInfo&quot;)</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">modifyProjectInfo</span><span class=\"params\">(<span class=\"meta\">@RequestBody</span> ProjectInfo projectInfo)</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">HttpResponseEntity</span> <span class=\"variable\">httpResponseEntity</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">HttpResponseEntity</span>();</span><br><span class=\"line\">    projectService.modifyProjectInfo(projectInfo);</span><br><span class=\"line\">    httpResponseEntity.setCode(<span class=\"string\">&quot;666&quot;</span>);</span><br><span class=\"line\">    httpResponseEntity.setMessage(<span class=\"string\">&quot;修改成功&quot;</span>);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> httpResponseEntity;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"7-5-2-service层\"><a href=\"#7-5-2-service层\" class=\"headerlink\" title=\"7.5.2 service层\"></a>7.5.2 service层</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">modifyProjectInfo</span><span class=\"params\">(ProjectInfo projectInfo)</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> projectInfoMapper.updateByPrimaryKeySelective(projectInfo)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<h2 id=\"7-6-项目删除\"><a href=\"#7-6-项目删除\" class=\"headerlink\" title=\"7.6 项目删除\"></a>7.6 项目删除</h2><h3 id=\"7-6-1-control层\"><a href=\"#7-6-1-control层\" class=\"headerlink\" title=\"7.6.1 control层\"></a>7.6.1 control层</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@PostMapping(&quot;/deleteProjectById&quot;)</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">deleteProjectById</span><span class=\"params\">(<span class=\"meta\">@RequestBody</span> ProjectInfo projectInfo)</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">HttpResponseEntity</span> <span class=\"variable\">httpResponseEntity</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">HttpResponseEntity</span>();</span><br><span class=\"line\">    projectService.deleteProjectById(projectInfo.getId());</span><br><span class=\"line\">    httpResponseEntity.setCode(<span class=\"string\">&quot;666&quot;</span>);</span><br><span class=\"line\">    httpResponseEntity.setMessage(<span class=\"string\">&quot;删除成功&quot;</span>);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> httpResponseEntity;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"7-6-2-service层\"><a href=\"#7-6-2-service层\" class=\"headerlink\" title=\"7.6.2 service层\"></a>7.6.2 service层</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">deleteProjectById</span><span class=\"params\">(String id)</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> projectInfoMapper.deleteByPrimaryKey(id);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>"},{"title":"生产实习-5 用户管理功能","date":"2024-03-05T03:00:00.000Z","comment":true,"toc":true,"_content":"\n#\n<!--more-->\n\n\n\n# 5. 用户管理功能\n\n## 5.1 用户列表功能\n\n- 查看pages/user/index.js可以看到需要完成的函数是：queryUserList\n\n### 5.1.1 control层queryUserList\n\n```java\n    @PostMapping(\"/queryUserList\")\n    public HttpResponseEntity queryUserList(@RequestBody UserInfo userInfo){\n\n        HttpResponseEntity httpResponseEntity = new HttpResponseEntity();\n\n        List<UserInfo> userInfos = userService.queryUserList(userInfo);\n        httpResponseEntity.setCode(\"666\");\n        httpResponseEntity.setData(userInfos);\n\n        return httpResponseEntity;\n    }\n```\n\n### 5.1.2 server层queryUserList\n\n```java\n    public List<UserInfo> queryUserList(UserInfo userInfo) {//1. 用userInof查询数据库\n\n        UserInfoExample userInfoExample = new UserInfoExample(); //3 在dbmap/entities里面是有userinfoexample的\n\n        if(userInfo.getUsername()!=null){//登录\n            UserInfoExample.Criteria userInfoCriteria = userInfoExample.createCriteria(); //4 然后用这个example创建一个条件\n            userInfoCriteria.andUsernameEqualTo(userInfo.getUsername()) //根据这两个条件进行查询\n                    .andPasswordEqualTo(userInfo.getPassword());\n        }\n        //没有条件的就是全查\n\n        return userInfoMapper.selectByExample(userInfoExample);//2 使用userInfoMapper里面自带一些方法，使用里面的selectbyexample查询\n    }\n```\n\n## 5.2 用户编辑\n\n- 首先在浏览器检查源码，查看编辑用户按钮调用的是哪个函数（handleEdit)：\n\n  ![](D:/blog/themes/yilia/source/img/java/produce_practice/5/1.png)\n\n  ![](img/java/produce_practice/5/1.png)\n\n  - 在pages/user/index.js里面也可以看到编辑按钮对应的是handleEdit，然后在handleEdit里面发现又跳转到创建用户页。\n  - 然后去createUser/index.html里面看到创建用户按钮对应index.js下的handleCreateUser函数，在handleCreateUser函数里面判断，如果传入了user.id说明是要修改，没传入则是要新建，在修改部分找到路径：/admin/modifyUserInfo。\n\n### 5.2.1 control层modifyUserInfo\n\n```java\n    @PostMapping(\"/modifyUserInfo\")\n    public HttpResponseEntity modifyUserInfo(@RequestBody UserInfo userInfo) {\n\n        HttpResponseEntity httpResponseEntity = new HttpResponseEntity();\n        userService.modifyUserInfo(userInfo);\n        httpResponseEntity.setCode(\"666\");\n        return httpResponseEntity;\n    }\n```\n\n### 5.2.2 service层modifyUserInfo\n\n```java\n    public int modifyUserInfo(UserInfo userInfo) {\n        //根据主键修改，只修改提供的部分，不提供的部分不修改，不带selective的方法对于不提供的部分默认改成null\n        return userInfoMapper.updateByPrimaryKeySelective(userInfo);\n    }\n```\n\n## 5.3 删除用户\n\n- 同理，在user/index.js里面找到删除按钮，对应deleteUser函数，在deleteUser函数中找到路径admin/deleteUserinfo，所以要写deleteUserinfo函数\n\n### 5.3.1 control层deleteUserinfo\n\n```java\n    @PostMapping(\"/deleteUserinfo\")\n    public HttpResponseEntity deleteUserById(@RequestBody UserInfo userInfo) {\n        \n        userService.deleteUserById(userInfo.getId());\n        HttpResponseEntity httpResponseEntity = new HttpResponseEntity();\n        httpResponseEntity.setCode(\"666\");\n        \n        return httpResponseEntity;\n    }\n```\n\n\n\n### 5.3.2 server层deleteUserinfo\n\n```java\n    public int deleteUserById(String id) {\n        return userInfoMapper.deleteByPrimaryKey(id);\n        \n    }\n```","source":"_posts/java/produce_practice/5_usermanage.md","raw":"---\ntitle: 生产实习-5 用户管理功能\n\ndate: 2024-3-5 11:00:00\n\ntags: [生产实习,java,springboot]\n\ncategories: [java]\n\ncomment: true\n\ntoc: true\n\n\n---\n\n#\n<!--more-->\n\n\n\n# 5. 用户管理功能\n\n## 5.1 用户列表功能\n\n- 查看pages/user/index.js可以看到需要完成的函数是：queryUserList\n\n### 5.1.1 control层queryUserList\n\n```java\n    @PostMapping(\"/queryUserList\")\n    public HttpResponseEntity queryUserList(@RequestBody UserInfo userInfo){\n\n        HttpResponseEntity httpResponseEntity = new HttpResponseEntity();\n\n        List<UserInfo> userInfos = userService.queryUserList(userInfo);\n        httpResponseEntity.setCode(\"666\");\n        httpResponseEntity.setData(userInfos);\n\n        return httpResponseEntity;\n    }\n```\n\n### 5.1.2 server层queryUserList\n\n```java\n    public List<UserInfo> queryUserList(UserInfo userInfo) {//1. 用userInof查询数据库\n\n        UserInfoExample userInfoExample = new UserInfoExample(); //3 在dbmap/entities里面是有userinfoexample的\n\n        if(userInfo.getUsername()!=null){//登录\n            UserInfoExample.Criteria userInfoCriteria = userInfoExample.createCriteria(); //4 然后用这个example创建一个条件\n            userInfoCriteria.andUsernameEqualTo(userInfo.getUsername()) //根据这两个条件进行查询\n                    .andPasswordEqualTo(userInfo.getPassword());\n        }\n        //没有条件的就是全查\n\n        return userInfoMapper.selectByExample(userInfoExample);//2 使用userInfoMapper里面自带一些方法，使用里面的selectbyexample查询\n    }\n```\n\n## 5.2 用户编辑\n\n- 首先在浏览器检查源码，查看编辑用户按钮调用的是哪个函数（handleEdit)：\n\n  ![](D:/blog/themes/yilia/source/img/java/produce_practice/5/1.png)\n\n  ![](img/java/produce_practice/5/1.png)\n\n  - 在pages/user/index.js里面也可以看到编辑按钮对应的是handleEdit，然后在handleEdit里面发现又跳转到创建用户页。\n  - 然后去createUser/index.html里面看到创建用户按钮对应index.js下的handleCreateUser函数，在handleCreateUser函数里面判断，如果传入了user.id说明是要修改，没传入则是要新建，在修改部分找到路径：/admin/modifyUserInfo。\n\n### 5.2.1 control层modifyUserInfo\n\n```java\n    @PostMapping(\"/modifyUserInfo\")\n    public HttpResponseEntity modifyUserInfo(@RequestBody UserInfo userInfo) {\n\n        HttpResponseEntity httpResponseEntity = new HttpResponseEntity();\n        userService.modifyUserInfo(userInfo);\n        httpResponseEntity.setCode(\"666\");\n        return httpResponseEntity;\n    }\n```\n\n### 5.2.2 service层modifyUserInfo\n\n```java\n    public int modifyUserInfo(UserInfo userInfo) {\n        //根据主键修改，只修改提供的部分，不提供的部分不修改，不带selective的方法对于不提供的部分默认改成null\n        return userInfoMapper.updateByPrimaryKeySelective(userInfo);\n    }\n```\n\n## 5.3 删除用户\n\n- 同理，在user/index.js里面找到删除按钮，对应deleteUser函数，在deleteUser函数中找到路径admin/deleteUserinfo，所以要写deleteUserinfo函数\n\n### 5.3.1 control层deleteUserinfo\n\n```java\n    @PostMapping(\"/deleteUserinfo\")\n    public HttpResponseEntity deleteUserById(@RequestBody UserInfo userInfo) {\n        \n        userService.deleteUserById(userInfo.getId());\n        HttpResponseEntity httpResponseEntity = new HttpResponseEntity();\n        httpResponseEntity.setCode(\"666\");\n        \n        return httpResponseEntity;\n    }\n```\n\n\n\n### 5.3.2 server层deleteUserinfo\n\n```java\n    public int deleteUserById(String id) {\n        return userInfoMapper.deleteByPrimaryKey(id);\n        \n    }\n```","slug":"java/produce_practice/5_usermanage","published":1,"updated":"2024-03-05T12:16:12.394Z","_id":"clvuli6rw007nzsvwc330fg0q","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n\n\n<h1 id=\"5-用户管理功能\"><a href=\"#5-用户管理功能\" class=\"headerlink\" title=\"5. 用户管理功能\"></a>5. 用户管理功能</h1><h2 id=\"5-1-用户列表功能\"><a href=\"#5-1-用户列表功能\" class=\"headerlink\" title=\"5.1 用户列表功能\"></a>5.1 用户列表功能</h2><ul>\n<li>查看pages&#x2F;user&#x2F;index.js可以看到需要完成的函数是：queryUserList</li>\n</ul>\n<h3 id=\"5-1-1-control层queryUserList\"><a href=\"#5-1-1-control层queryUserList\" class=\"headerlink\" title=\"5.1.1 control层queryUserList\"></a>5.1.1 control层queryUserList</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@PostMapping(&quot;/queryUserList&quot;)</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">queryUserList</span><span class=\"params\">(<span class=\"meta\">@RequestBody</span> UserInfo userInfo)</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">HttpResponseEntity</span> <span class=\"variable\">httpResponseEntity</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">HttpResponseEntity</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">    List&lt;UserInfo&gt; userInfos = userService.queryUserList(userInfo);</span><br><span class=\"line\">    httpResponseEntity.setCode(<span class=\"string\">&quot;666&quot;</span>);</span><br><span class=\"line\">    httpResponseEntity.setData(userInfos);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> httpResponseEntity;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"5-1-2-server层queryUserList\"><a href=\"#5-1-2-server层queryUserList\" class=\"headerlink\" title=\"5.1.2 server层queryUserList\"></a>5.1.2 server层queryUserList</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> List&lt;UserInfo&gt; <span class=\"title function_\">queryUserList</span><span class=\"params\">(UserInfo userInfo)</span> &#123;<span class=\"comment\">//1. 用userInof查询数据库</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">UserInfoExample</span> <span class=\"variable\">userInfoExample</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">UserInfoExample</span>(); <span class=\"comment\">//3 在dbmap/entities里面是有userinfoexample的</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span>(userInfo.getUsername()!=<span class=\"literal\">null</span>)&#123;<span class=\"comment\">//登录</span></span><br><span class=\"line\">        UserInfoExample.<span class=\"type\">Criteria</span> <span class=\"variable\">userInfoCriteria</span> <span class=\"operator\">=</span> userInfoExample.createCriteria(); <span class=\"comment\">//4 然后用这个example创建一个条件</span></span><br><span class=\"line\">        userInfoCriteria.andUsernameEqualTo(userInfo.getUsername()) <span class=\"comment\">//根据这两个条件进行查询</span></span><br><span class=\"line\">                .andPasswordEqualTo(userInfo.getPassword());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">//没有条件的就是全查</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> userInfoMapper.selectByExample(userInfoExample);<span class=\"comment\">//2 使用userInfoMapper里面自带一些方法，使用里面的selectbyexample查询</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"5-2-用户编辑\"><a href=\"#5-2-用户编辑\" class=\"headerlink\" title=\"5.2 用户编辑\"></a>5.2 用户编辑</h2><ul>\n<li><p>首先在浏览器检查源码，查看编辑用户按钮调用的是哪个函数（handleEdit)：</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/5/1.png\"></p>\n<p><img src=\"/img/java/produce_practice/5/1.png\"></p>\n<ul>\n<li>在pages&#x2F;user&#x2F;index.js里面也可以看到编辑按钮对应的是handleEdit，然后在handleEdit里面发现又跳转到创建用户页。</li>\n<li>然后去createUser&#x2F;index.html里面看到创建用户按钮对应index.js下的handleCreateUser函数，在handleCreateUser函数里面判断，如果传入了user.id说明是要修改，没传入则是要新建，在修改部分找到路径：&#x2F;admin&#x2F;modifyUserInfo。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"5-2-1-control层modifyUserInfo\"><a href=\"#5-2-1-control层modifyUserInfo\" class=\"headerlink\" title=\"5.2.1 control层modifyUserInfo\"></a>5.2.1 control层modifyUserInfo</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@PostMapping(&quot;/modifyUserInfo&quot;)</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">modifyUserInfo</span><span class=\"params\">(<span class=\"meta\">@RequestBody</span> UserInfo userInfo)</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">HttpResponseEntity</span> <span class=\"variable\">httpResponseEntity</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">HttpResponseEntity</span>();</span><br><span class=\"line\">    userService.modifyUserInfo(userInfo);</span><br><span class=\"line\">    httpResponseEntity.setCode(<span class=\"string\">&quot;666&quot;</span>);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> httpResponseEntity;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"5-2-2-service层modifyUserInfo\"><a href=\"#5-2-2-service层modifyUserInfo\" class=\"headerlink\" title=\"5.2.2 service层modifyUserInfo\"></a>5.2.2 service层modifyUserInfo</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">modifyUserInfo</span><span class=\"params\">(UserInfo userInfo)</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">//根据主键修改，只修改提供的部分，不提供的部分不修改，不带selective的方法对于不提供的部分默认改成null</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> userInfoMapper.updateByPrimaryKeySelective(userInfo);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"5-3-删除用户\"><a href=\"#5-3-删除用户\" class=\"headerlink\" title=\"5.3 删除用户\"></a>5.3 删除用户</h2><ul>\n<li>同理，在user&#x2F;index.js里面找到删除按钮，对应deleteUser函数，在deleteUser函数中找到路径admin&#x2F;deleteUserinfo，所以要写deleteUserinfo函数</li>\n</ul>\n<h3 id=\"5-3-1-control层deleteUserinfo\"><a href=\"#5-3-1-control层deleteUserinfo\" class=\"headerlink\" title=\"5.3.1 control层deleteUserinfo\"></a>5.3.1 control层deleteUserinfo</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@PostMapping(&quot;/deleteUserinfo&quot;)</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">deleteUserById</span><span class=\"params\">(<span class=\"meta\">@RequestBody</span> UserInfo userInfo)</span> &#123;</span><br><span class=\"line\">    </span><br><span class=\"line\">    userService.deleteUserById(userInfo.getId());</span><br><span class=\"line\">    <span class=\"type\">HttpResponseEntity</span> <span class=\"variable\">httpResponseEntity</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">HttpResponseEntity</span>();</span><br><span class=\"line\">    httpResponseEntity.setCode(<span class=\"string\">&quot;666&quot;</span>);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> httpResponseEntity;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"5-3-2-server层deleteUserinfo\"><a href=\"#5-3-2-server层deleteUserinfo\" class=\"headerlink\" title=\"5.3.2 server层deleteUserinfo\"></a>5.3.2 server层deleteUserinfo</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">deleteUserById</span><span class=\"params\">(String id)</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> userInfoMapper.deleteByPrimaryKey(id);</span><br><span class=\"line\">    </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"5-用户管理功能\"><a href=\"#5-用户管理功能\" class=\"headerlink\" title=\"5. 用户管理功能\"></a>5. 用户管理功能</h1><h2 id=\"5-1-用户列表功能\"><a href=\"#5-1-用户列表功能\" class=\"headerlink\" title=\"5.1 用户列表功能\"></a>5.1 用户列表功能</h2><ul>\n<li>查看pages&#x2F;user&#x2F;index.js可以看到需要完成的函数是：queryUserList</li>\n</ul>\n<h3 id=\"5-1-1-control层queryUserList\"><a href=\"#5-1-1-control层queryUserList\" class=\"headerlink\" title=\"5.1.1 control层queryUserList\"></a>5.1.1 control层queryUserList</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@PostMapping(&quot;/queryUserList&quot;)</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">queryUserList</span><span class=\"params\">(<span class=\"meta\">@RequestBody</span> UserInfo userInfo)</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">HttpResponseEntity</span> <span class=\"variable\">httpResponseEntity</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">HttpResponseEntity</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">    List&lt;UserInfo&gt; userInfos = userService.queryUserList(userInfo);</span><br><span class=\"line\">    httpResponseEntity.setCode(<span class=\"string\">&quot;666&quot;</span>);</span><br><span class=\"line\">    httpResponseEntity.setData(userInfos);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> httpResponseEntity;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"5-1-2-server层queryUserList\"><a href=\"#5-1-2-server层queryUserList\" class=\"headerlink\" title=\"5.1.2 server层queryUserList\"></a>5.1.2 server层queryUserList</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> List&lt;UserInfo&gt; <span class=\"title function_\">queryUserList</span><span class=\"params\">(UserInfo userInfo)</span> &#123;<span class=\"comment\">//1. 用userInof查询数据库</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">UserInfoExample</span> <span class=\"variable\">userInfoExample</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">UserInfoExample</span>(); <span class=\"comment\">//3 在dbmap/entities里面是有userinfoexample的</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span>(userInfo.getUsername()!=<span class=\"literal\">null</span>)&#123;<span class=\"comment\">//登录</span></span><br><span class=\"line\">        UserInfoExample.<span class=\"type\">Criteria</span> <span class=\"variable\">userInfoCriteria</span> <span class=\"operator\">=</span> userInfoExample.createCriteria(); <span class=\"comment\">//4 然后用这个example创建一个条件</span></span><br><span class=\"line\">        userInfoCriteria.andUsernameEqualTo(userInfo.getUsername()) <span class=\"comment\">//根据这两个条件进行查询</span></span><br><span class=\"line\">                .andPasswordEqualTo(userInfo.getPassword());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">//没有条件的就是全查</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> userInfoMapper.selectByExample(userInfoExample);<span class=\"comment\">//2 使用userInfoMapper里面自带一些方法，使用里面的selectbyexample查询</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"5-2-用户编辑\"><a href=\"#5-2-用户编辑\" class=\"headerlink\" title=\"5.2 用户编辑\"></a>5.2 用户编辑</h2><ul>\n<li><p>首先在浏览器检查源码，查看编辑用户按钮调用的是哪个函数（handleEdit)：</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/5/1.png\"></p>\n<p><img src=\"/img/java/produce_practice/5/1.png\"></p>\n<ul>\n<li>在pages&#x2F;user&#x2F;index.js里面也可以看到编辑按钮对应的是handleEdit，然后在handleEdit里面发现又跳转到创建用户页。</li>\n<li>然后去createUser&#x2F;index.html里面看到创建用户按钮对应index.js下的handleCreateUser函数，在handleCreateUser函数里面判断，如果传入了user.id说明是要修改，没传入则是要新建，在修改部分找到路径：&#x2F;admin&#x2F;modifyUserInfo。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"5-2-1-control层modifyUserInfo\"><a href=\"#5-2-1-control层modifyUserInfo\" class=\"headerlink\" title=\"5.2.1 control层modifyUserInfo\"></a>5.2.1 control层modifyUserInfo</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@PostMapping(&quot;/modifyUserInfo&quot;)</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">modifyUserInfo</span><span class=\"params\">(<span class=\"meta\">@RequestBody</span> UserInfo userInfo)</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">HttpResponseEntity</span> <span class=\"variable\">httpResponseEntity</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">HttpResponseEntity</span>();</span><br><span class=\"line\">    userService.modifyUserInfo(userInfo);</span><br><span class=\"line\">    httpResponseEntity.setCode(<span class=\"string\">&quot;666&quot;</span>);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> httpResponseEntity;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"5-2-2-service层modifyUserInfo\"><a href=\"#5-2-2-service层modifyUserInfo\" class=\"headerlink\" title=\"5.2.2 service层modifyUserInfo\"></a>5.2.2 service层modifyUserInfo</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">modifyUserInfo</span><span class=\"params\">(UserInfo userInfo)</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">//根据主键修改，只修改提供的部分，不提供的部分不修改，不带selective的方法对于不提供的部分默认改成null</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> userInfoMapper.updateByPrimaryKeySelective(userInfo);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"5-3-删除用户\"><a href=\"#5-3-删除用户\" class=\"headerlink\" title=\"5.3 删除用户\"></a>5.3 删除用户</h2><ul>\n<li>同理，在user&#x2F;index.js里面找到删除按钮，对应deleteUser函数，在deleteUser函数中找到路径admin&#x2F;deleteUserinfo，所以要写deleteUserinfo函数</li>\n</ul>\n<h3 id=\"5-3-1-control层deleteUserinfo\"><a href=\"#5-3-1-control层deleteUserinfo\" class=\"headerlink\" title=\"5.3.1 control层deleteUserinfo\"></a>5.3.1 control层deleteUserinfo</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@PostMapping(&quot;/deleteUserinfo&quot;)</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">deleteUserById</span><span class=\"params\">(<span class=\"meta\">@RequestBody</span> UserInfo userInfo)</span> &#123;</span><br><span class=\"line\">    </span><br><span class=\"line\">    userService.deleteUserById(userInfo.getId());</span><br><span class=\"line\">    <span class=\"type\">HttpResponseEntity</span> <span class=\"variable\">httpResponseEntity</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">HttpResponseEntity</span>();</span><br><span class=\"line\">    httpResponseEntity.setCode(<span class=\"string\">&quot;666&quot;</span>);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> httpResponseEntity;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"5-3-2-server层deleteUserinfo\"><a href=\"#5-3-2-server层deleteUserinfo\" class=\"headerlink\" title=\"5.3.2 server层deleteUserinfo\"></a>5.3.2 server层deleteUserinfo</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"type\">int</span> <span class=\"title function_\">deleteUserById</span><span class=\"params\">(String id)</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> userInfoMapper.deleteByPrimaryKey(id);</span><br><span class=\"line\">    </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>"},{"title":"生产实习-6 单元测试与log日志输出","date":"2024-03-05T04:00:00.000Z","comment":true,"toc":true,"_content":"\n#\n<!--more-->\n\n\n\n# 6. 单元测试与log日志输出\n\n\n\n## 6.1 配置日志文件\n\n- 将logback-spring.xml放到resources下，做如下修改：\n\n  ![](D:/blog/themes/yilia/source/img/java/produce_practice/5/2.png)\n\n  ![](img/java/produce_practice/5/2.png)\n\n- 假设你需要在每次登录的时候都记录日志，那在controller中找到登录方法所在的类，然后在类上面加上@Slf4j的注释，可以发现结构中多了一个log：\n\n    ![](D:/blog/themes/yilia/source/img/java/produce_practice/5/3.png)\n\n    ![](img/java/produce_practice/5/3.png)\n\n- 在登录方法中加入日志记录：\n\n  ```java\n      public HttpResponseEntity userLogin(@RequestBody UserInfo userInfo) { //requestbody注解，将请求的json数据转换为对象\n  \n          log.info(\"用户\" + userInfo.getUsername() + \"登录\");\n  ```\n\n  然后运行项目，登录后可以看到控制台有输出登录信息，并且log目录下也会有日志记录。\n\n## 6.2 单元测试\n\n- 在src/test下可以找到测试类\n\n- 在pom.xml的dependences标签下加入依赖：（记得刷新maven）这样就可以使用@RunWith注解，它可以使用springboot的依赖注入\n\n  ```xml\n  \t\t<dependency>\n  \t\t\t<groupId>junit</groupId>\n  \t\t\t<artifactId>junit</artifactId>\n  \t\t\t<version>4.12</version>\n  \t\t</dependency>\n  ```\n\n- 测试代码：\n\n  ```java\n  package com.lyingedu.questionnaire;\n  \n  import com.lyingedu.questionnaire.beans.HttpResponseEntity;\n  import com.lyingedu.questionnaire.biz.user.controller.UserController;\n  import com.lyingedu.questionnaire.common.utils.UUIDUtil;\n  import com.lyingedu.questionnaire.dbmap.entities.UserInfo;\n  import jakarta.annotation.Resource;\n  import lombok.extern.slf4j.Slf4j;\n  import org.junit.Test;\n  import org.junit.runner.RunWith;\n  import org.springframework.boot.test.context.SpringBootTest;\n  import org.springframework.test.context.junit4.SpringRunner;\n  \n  import java.util.Date;\n  import java.util.List;\n  \n  @Slf4j\n  @RunWith(SpringRunner.class) //这样可以用springboot的依赖注入\n  @SpringBootTest\n  public class QuestionnaireApplicationTests {\n      @Resource\n      private UserController userController;\n  \n      /*\n      * 测试查询功能\n       */\n      @Test //注意使用junit，而不是junit.api\n      public void testQueryUserInfoList(){\n          UserInfo userInfo = new UserInfo();\n          HttpResponseEntity httpResponseEntity = userController.queryUserList(userInfo);\n          if (\"666\".equals(httpResponseEntity.getCode())) {\n              log.info(\">>queryUserList用户列表查询测试成功\");\n          }\n      }\n  \n      /*\n      * 测试用户登录\n       */\n      @Test\n      public void testUserLogin(){\n          UserInfo userInfo = new UserInfo();\n          userInfo.setUsername(\"admin\");\n          userInfo.setPassword(\"12\");\n          HttpResponseEntity httpResponseEntity = userController.queryUserList(userInfo);\n          if (\"666\".equals(httpResponseEntity.getCode())) {\n              log.info(\">>selectUserInfo用户\"+userInfo.getUsername()+\"登录测试成功\");\n          }else {\n              log.info(\">>selectUserInfo用户登录测试失败\");\n          }\n      }\n  \n      /*\n      * 测试创建用户\n       */\n      @Test\n      public void testCreateUser(){\n          UserInfo userInfo = new UserInfo();\n          userInfo.setId(UUIDUtil.getOneUUID());\n          userInfo.setUsername(\"test005\");\n          userInfo.setPassword(\"123\");\n          userInfo.setStatus(\"1\");\n          userInfo.setCreatedBy(\"anyone\");\n          userInfo.setCreationDate(new Date());\n          userInfo.setStartTime(new Date());\n          userInfo.setStopTime(new Date());\n          userInfo.setLastUpdateBy(\"anyone\");\n          userInfo.setLastUpdateDate(new Date());\n  \n          HttpResponseEntity httpResponseEntity = userController.addUserInfo(userInfo);\n          if(\"666\".equals(httpResponseEntity.getCode())){\n              log.info(\">>adduserinfo插入用户测试成功\");\n          }\n  \n      }\n  \n      /*\n      * 测试修改用户\n       */\n      @Test\n      public void testUpdUser(){\n          UserInfo userInfo = new UserInfo();\n          userInfo.setUsername(\"test005\");\n          userInfo.setPassword(\"123\");\n  \n          HttpResponseEntity httpResponseEntity = userController.queryUserList(userInfo);\n  \n          if(\"666\".equals(httpResponseEntity.getCode())){//查询成功\n              List list=(List) httpResponseEntity.getData();\n              if(list.size() !=0){\n                  UserInfo data = (UserInfo) list.get(0);\n                  data.setUsername(\"test006\");\n                  HttpResponseEntity httpResponseEntity1 = userController.modifyUserInfo(data);\n                  if(\"666\".equals(httpResponseEntity1.getCode())){\n                      log.info(\">>update修改用户更新测试成功\");\n                  }\n              }\n  \n          }\n      }\n  \n      /*\n      * 测试删除用户\n       */\n      @Test\n      public void testDeleteUser(){\n          UserInfo userInfo = new UserInfo();\n          userInfo.setUsername(\"test006\");\n          userInfo.setPassword(\"123\");\n  \n          HttpResponseEntity httpResponseEntity = userController.queryUserList(userInfo);\n  \n          if(\"666\".equals(httpResponseEntity.getCode())){//查询成功\n              List list=(List) httpResponseEntity.getData();\n              if(list.size() !=0){\n                  UserInfo data = (UserInfo) list.get(0);\n  \n                  HttpResponseEntity httpResponseEntity1 = userController.deleteUserById(data);\n                  if(\"666\".equals(httpResponseEntity1.getCode())){\n                      log.info(\">>delete用户删除测试成功\");\n                  }\n              }\n  \n          }\n      }\n  }\n  \n  ```\n\n  ","source":"_posts/java/produce_practice/6_testlog.md","raw":"---\ntitle: 生产实习-6 单元测试与log日志输出\n\ndate: 2024-3-5 12:00:00\n\ntags: [生产实习,java,springboot]\n\ncategories: [java]\n\ncomment: true\n\ntoc: true\n\n\n\n---\n\n#\n<!--more-->\n\n\n\n# 6. 单元测试与log日志输出\n\n\n\n## 6.1 配置日志文件\n\n- 将logback-spring.xml放到resources下，做如下修改：\n\n  ![](D:/blog/themes/yilia/source/img/java/produce_practice/5/2.png)\n\n  ![](img/java/produce_practice/5/2.png)\n\n- 假设你需要在每次登录的时候都记录日志，那在controller中找到登录方法所在的类，然后在类上面加上@Slf4j的注释，可以发现结构中多了一个log：\n\n    ![](D:/blog/themes/yilia/source/img/java/produce_practice/5/3.png)\n\n    ![](img/java/produce_practice/5/3.png)\n\n- 在登录方法中加入日志记录：\n\n  ```java\n      public HttpResponseEntity userLogin(@RequestBody UserInfo userInfo) { //requestbody注解，将请求的json数据转换为对象\n  \n          log.info(\"用户\" + userInfo.getUsername() + \"登录\");\n  ```\n\n  然后运行项目，登录后可以看到控制台有输出登录信息，并且log目录下也会有日志记录。\n\n## 6.2 单元测试\n\n- 在src/test下可以找到测试类\n\n- 在pom.xml的dependences标签下加入依赖：（记得刷新maven）这样就可以使用@RunWith注解，它可以使用springboot的依赖注入\n\n  ```xml\n  \t\t<dependency>\n  \t\t\t<groupId>junit</groupId>\n  \t\t\t<artifactId>junit</artifactId>\n  \t\t\t<version>4.12</version>\n  \t\t</dependency>\n  ```\n\n- 测试代码：\n\n  ```java\n  package com.lyingedu.questionnaire;\n  \n  import com.lyingedu.questionnaire.beans.HttpResponseEntity;\n  import com.lyingedu.questionnaire.biz.user.controller.UserController;\n  import com.lyingedu.questionnaire.common.utils.UUIDUtil;\n  import com.lyingedu.questionnaire.dbmap.entities.UserInfo;\n  import jakarta.annotation.Resource;\n  import lombok.extern.slf4j.Slf4j;\n  import org.junit.Test;\n  import org.junit.runner.RunWith;\n  import org.springframework.boot.test.context.SpringBootTest;\n  import org.springframework.test.context.junit4.SpringRunner;\n  \n  import java.util.Date;\n  import java.util.List;\n  \n  @Slf4j\n  @RunWith(SpringRunner.class) //这样可以用springboot的依赖注入\n  @SpringBootTest\n  public class QuestionnaireApplicationTests {\n      @Resource\n      private UserController userController;\n  \n      /*\n      * 测试查询功能\n       */\n      @Test //注意使用junit，而不是junit.api\n      public void testQueryUserInfoList(){\n          UserInfo userInfo = new UserInfo();\n          HttpResponseEntity httpResponseEntity = userController.queryUserList(userInfo);\n          if (\"666\".equals(httpResponseEntity.getCode())) {\n              log.info(\">>queryUserList用户列表查询测试成功\");\n          }\n      }\n  \n      /*\n      * 测试用户登录\n       */\n      @Test\n      public void testUserLogin(){\n          UserInfo userInfo = new UserInfo();\n          userInfo.setUsername(\"admin\");\n          userInfo.setPassword(\"12\");\n          HttpResponseEntity httpResponseEntity = userController.queryUserList(userInfo);\n          if (\"666\".equals(httpResponseEntity.getCode())) {\n              log.info(\">>selectUserInfo用户\"+userInfo.getUsername()+\"登录测试成功\");\n          }else {\n              log.info(\">>selectUserInfo用户登录测试失败\");\n          }\n      }\n  \n      /*\n      * 测试创建用户\n       */\n      @Test\n      public void testCreateUser(){\n          UserInfo userInfo = new UserInfo();\n          userInfo.setId(UUIDUtil.getOneUUID());\n          userInfo.setUsername(\"test005\");\n          userInfo.setPassword(\"123\");\n          userInfo.setStatus(\"1\");\n          userInfo.setCreatedBy(\"anyone\");\n          userInfo.setCreationDate(new Date());\n          userInfo.setStartTime(new Date());\n          userInfo.setStopTime(new Date());\n          userInfo.setLastUpdateBy(\"anyone\");\n          userInfo.setLastUpdateDate(new Date());\n  \n          HttpResponseEntity httpResponseEntity = userController.addUserInfo(userInfo);\n          if(\"666\".equals(httpResponseEntity.getCode())){\n              log.info(\">>adduserinfo插入用户测试成功\");\n          }\n  \n      }\n  \n      /*\n      * 测试修改用户\n       */\n      @Test\n      public void testUpdUser(){\n          UserInfo userInfo = new UserInfo();\n          userInfo.setUsername(\"test005\");\n          userInfo.setPassword(\"123\");\n  \n          HttpResponseEntity httpResponseEntity = userController.queryUserList(userInfo);\n  \n          if(\"666\".equals(httpResponseEntity.getCode())){//查询成功\n              List list=(List) httpResponseEntity.getData();\n              if(list.size() !=0){\n                  UserInfo data = (UserInfo) list.get(0);\n                  data.setUsername(\"test006\");\n                  HttpResponseEntity httpResponseEntity1 = userController.modifyUserInfo(data);\n                  if(\"666\".equals(httpResponseEntity1.getCode())){\n                      log.info(\">>update修改用户更新测试成功\");\n                  }\n              }\n  \n          }\n      }\n  \n      /*\n      * 测试删除用户\n       */\n      @Test\n      public void testDeleteUser(){\n          UserInfo userInfo = new UserInfo();\n          userInfo.setUsername(\"test006\");\n          userInfo.setPassword(\"123\");\n  \n          HttpResponseEntity httpResponseEntity = userController.queryUserList(userInfo);\n  \n          if(\"666\".equals(httpResponseEntity.getCode())){//查询成功\n              List list=(List) httpResponseEntity.getData();\n              if(list.size() !=0){\n                  UserInfo data = (UserInfo) list.get(0);\n  \n                  HttpResponseEntity httpResponseEntity1 = userController.deleteUserById(data);\n                  if(\"666\".equals(httpResponseEntity1.getCode())){\n                      log.info(\">>delete用户删除测试成功\");\n                  }\n              }\n  \n          }\n      }\n  }\n  \n  ```\n\n  ","slug":"java/produce_practice/6_testlog","published":1,"updated":"2024-03-05T14:41:09.469Z","comments":1,"layout":"post","photos":[],"_id":"clvuli6s4008uzsvw6l2y84hk","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n\n\n<h1 id=\"6-单元测试与log日志输出\"><a href=\"#6-单元测试与log日志输出\" class=\"headerlink\" title=\"6. 单元测试与log日志输出\"></a>6. 单元测试与log日志输出</h1><h2 id=\"6-1-配置日志文件\"><a href=\"#6-1-配置日志文件\" class=\"headerlink\" title=\"6.1 配置日志文件\"></a>6.1 配置日志文件</h2><ul>\n<li><p>将logback-spring.xml放到resources下，做如下修改：</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/5/2.png\"></p>\n<p><img src=\"/img/java/produce_practice/5/2.png\"></p>\n</li>\n<li><p>假设你需要在每次登录的时候都记录日志，那在controller中找到登录方法所在的类，然后在类上面加上@Slf4j的注释，可以发现结构中多了一个log：</p>\n<p>  <img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/5/3.png\"></p>\n<p>  <img src=\"/img/java/produce_practice/5/3.png\"></p>\n</li>\n<li><p>在登录方法中加入日志记录：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">userLogin</span><span class=\"params\">(<span class=\"meta\">@RequestBody</span> UserInfo userInfo)</span> &#123; <span class=\"comment\">//requestbody注解，将请求的json数据转换为对象</span></span><br><span class=\"line\">  </span><br><span class=\"line\">    log.info(<span class=\"string\">&quot;用户&quot;</span> + userInfo.getUsername() + <span class=\"string\">&quot;登录&quot;</span>);</span><br></pre></td></tr></table></figure>\n\n<p>然后运行项目，登录后可以看到控制台有输出登录信息，并且log目录下也会有日志记录。</p>\n</li>\n</ul>\n<h2 id=\"6-2-单元测试\"><a href=\"#6-2-单元测试\" class=\"headerlink\" title=\"6.2 单元测试\"></a>6.2 单元测试</h2><ul>\n<li><p>在src&#x2F;test下可以找到测试类</p>\n</li>\n<li><p>在pom.xml的dependences标签下加入依赖：（记得刷新maven）这样就可以使用@RunWith注解，它可以使用springboot的依赖注入</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">\t<span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>junit<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">\t<span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>junit<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">\t<span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>4.12<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>测试代码：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> com.lyingedu.questionnaire;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> com.lyingedu.questionnaire.beans.HttpResponseEntity;</span><br><span class=\"line\"><span class=\"keyword\">import</span> com.lyingedu.questionnaire.biz.user.controller.UserController;</span><br><span class=\"line\"><span class=\"keyword\">import</span> com.lyingedu.questionnaire.common.utils.UUIDUtil;</span><br><span class=\"line\"><span class=\"keyword\">import</span> com.lyingedu.questionnaire.dbmap.entities.UserInfo;</span><br><span class=\"line\"><span class=\"keyword\">import</span> jakarta.annotation.Resource;</span><br><span class=\"line\"><span class=\"keyword\">import</span> lombok.extern.slf4j.Slf4j;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.junit.Test;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.junit.runner.RunWith;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.boot.test.context.SpringBootTest;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.test.context.junit4.SpringRunner;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.Date;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.List;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@Slf4j</span></span><br><span class=\"line\"><span class=\"meta\">@RunWith(SpringRunner.class)</span> <span class=\"comment\">//这样可以用springboot的依赖注入</span></span><br><span class=\"line\"><span class=\"meta\">@SpringBootTest</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">QuestionnaireApplicationTests</span> &#123;</span><br><span class=\"line\">    <span class=\"meta\">@Resource</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> UserController userController;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">    * 测试查询功能</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"meta\">@Test</span> <span class=\"comment\">//注意使用junit，而不是junit.api</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">testQueryUserInfoList</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">        <span class=\"type\">UserInfo</span> <span class=\"variable\">userInfo</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">UserInfo</span>();</span><br><span class=\"line\">        <span class=\"type\">HttpResponseEntity</span> <span class=\"variable\">httpResponseEntity</span> <span class=\"operator\">=</span> userController.queryUserList(userInfo);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (<span class=\"string\">&quot;666&quot;</span>.equals(httpResponseEntity.getCode())) &#123;</span><br><span class=\"line\">            log.info(<span class=\"string\">&quot;&gt;&gt;queryUserList用户列表查询测试成功&quot;</span>);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">    * 测试用户登录</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"meta\">@Test</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">testUserLogin</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">        <span class=\"type\">UserInfo</span> <span class=\"variable\">userInfo</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">UserInfo</span>();</span><br><span class=\"line\">        userInfo.setUsername(<span class=\"string\">&quot;admin&quot;</span>);</span><br><span class=\"line\">        userInfo.setPassword(<span class=\"string\">&quot;12&quot;</span>);</span><br><span class=\"line\">        <span class=\"type\">HttpResponseEntity</span> <span class=\"variable\">httpResponseEntity</span> <span class=\"operator\">=</span> userController.queryUserList(userInfo);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (<span class=\"string\">&quot;666&quot;</span>.equals(httpResponseEntity.getCode())) &#123;</span><br><span class=\"line\">            log.info(<span class=\"string\">&quot;&gt;&gt;selectUserInfo用户&quot;</span>+userInfo.getUsername()+<span class=\"string\">&quot;登录测试成功&quot;</span>);</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">            log.info(<span class=\"string\">&quot;&gt;&gt;selectUserInfo用户登录测试失败&quot;</span>);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">    * 测试创建用户</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"meta\">@Test</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">testCreateUser</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">        <span class=\"type\">UserInfo</span> <span class=\"variable\">userInfo</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">UserInfo</span>();</span><br><span class=\"line\">        userInfo.setId(UUIDUtil.getOneUUID());</span><br><span class=\"line\">        userInfo.setUsername(<span class=\"string\">&quot;test005&quot;</span>);</span><br><span class=\"line\">        userInfo.setPassword(<span class=\"string\">&quot;123&quot;</span>);</span><br><span class=\"line\">        userInfo.setStatus(<span class=\"string\">&quot;1&quot;</span>);</span><br><span class=\"line\">        userInfo.setCreatedBy(<span class=\"string\">&quot;anyone&quot;</span>);</span><br><span class=\"line\">        userInfo.setCreationDate(<span class=\"keyword\">new</span> <span class=\"title class_\">Date</span>());</span><br><span class=\"line\">        userInfo.setStartTime(<span class=\"keyword\">new</span> <span class=\"title class_\">Date</span>());</span><br><span class=\"line\">        userInfo.setStopTime(<span class=\"keyword\">new</span> <span class=\"title class_\">Date</span>());</span><br><span class=\"line\">        userInfo.setLastUpdateBy(<span class=\"string\">&quot;anyone&quot;</span>);</span><br><span class=\"line\">        userInfo.setLastUpdateDate(<span class=\"keyword\">new</span> <span class=\"title class_\">Date</span>());</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">HttpResponseEntity</span> <span class=\"variable\">httpResponseEntity</span> <span class=\"operator\">=</span> userController.addUserInfo(userInfo);</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(<span class=\"string\">&quot;666&quot;</span>.equals(httpResponseEntity.getCode()))&#123;</span><br><span class=\"line\">            log.info(<span class=\"string\">&quot;&gt;&gt;adduserinfo插入用户测试成功&quot;</span>);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">    * 测试修改用户</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"meta\">@Test</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">testUpdUser</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">        <span class=\"type\">UserInfo</span> <span class=\"variable\">userInfo</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">UserInfo</span>();</span><br><span class=\"line\">        userInfo.setUsername(<span class=\"string\">&quot;test005&quot;</span>);</span><br><span class=\"line\">        userInfo.setPassword(<span class=\"string\">&quot;123&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">HttpResponseEntity</span> <span class=\"variable\">httpResponseEntity</span> <span class=\"operator\">=</span> userController.queryUserList(userInfo);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span>(<span class=\"string\">&quot;666&quot;</span>.equals(httpResponseEntity.getCode()))&#123;<span class=\"comment\">//查询成功</span></span><br><span class=\"line\">            List list=(List) httpResponseEntity.getData();</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(list.size() !=<span class=\"number\">0</span>)&#123;</span><br><span class=\"line\">                <span class=\"type\">UserInfo</span> <span class=\"variable\">data</span> <span class=\"operator\">=</span> (UserInfo) list.get(<span class=\"number\">0</span>);</span><br><span class=\"line\">                data.setUsername(<span class=\"string\">&quot;test006&quot;</span>);</span><br><span class=\"line\">                <span class=\"type\">HttpResponseEntity</span> <span class=\"variable\">httpResponseEntity1</span> <span class=\"operator\">=</span> userController.modifyUserInfo(data);</span><br><span class=\"line\">                <span class=\"keyword\">if</span>(<span class=\"string\">&quot;666&quot;</span>.equals(httpResponseEntity1.getCode()))&#123;</span><br><span class=\"line\">                    log.info(<span class=\"string\">&quot;&gt;&gt;update修改用户更新测试成功&quot;</span>);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">    * 测试删除用户</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"meta\">@Test</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">testDeleteUser</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">        <span class=\"type\">UserInfo</span> <span class=\"variable\">userInfo</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">UserInfo</span>();</span><br><span class=\"line\">        userInfo.setUsername(<span class=\"string\">&quot;test006&quot;</span>);</span><br><span class=\"line\">        userInfo.setPassword(<span class=\"string\">&quot;123&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">HttpResponseEntity</span> <span class=\"variable\">httpResponseEntity</span> <span class=\"operator\">=</span> userController.queryUserList(userInfo);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span>(<span class=\"string\">&quot;666&quot;</span>.equals(httpResponseEntity.getCode()))&#123;<span class=\"comment\">//查询成功</span></span><br><span class=\"line\">            List list=(List) httpResponseEntity.getData();</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(list.size() !=<span class=\"number\">0</span>)&#123;</span><br><span class=\"line\">                <span class=\"type\">UserInfo</span> <span class=\"variable\">data</span> <span class=\"operator\">=</span> (UserInfo) list.get(<span class=\"number\">0</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"type\">HttpResponseEntity</span> <span class=\"variable\">httpResponseEntity1</span> <span class=\"operator\">=</span> userController.deleteUserById(data);</span><br><span class=\"line\">                <span class=\"keyword\">if</span>(<span class=\"string\">&quot;666&quot;</span>.equals(httpResponseEntity1.getCode()))&#123;</span><br><span class=\"line\">                    log.info(<span class=\"string\">&quot;&gt;&gt;delete用户删除测试成功&quot;</span>);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"6-单元测试与log日志输出\"><a href=\"#6-单元测试与log日志输出\" class=\"headerlink\" title=\"6. 单元测试与log日志输出\"></a>6. 单元测试与log日志输出</h1><h2 id=\"6-1-配置日志文件\"><a href=\"#6-1-配置日志文件\" class=\"headerlink\" title=\"6.1 配置日志文件\"></a>6.1 配置日志文件</h2><ul>\n<li><p>将logback-spring.xml放到resources下，做如下修改：</p>\n<p><img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/5/2.png\"></p>\n<p><img src=\"/img/java/produce_practice/5/2.png\"></p>\n</li>\n<li><p>假设你需要在每次登录的时候都记录日志，那在controller中找到登录方法所在的类，然后在类上面加上@Slf4j的注释，可以发现结构中多了一个log：</p>\n<p>  <img src=\"D:/blog/themes/yilia/source/img/java/produce_practice/5/3.png\"></p>\n<p>  <img src=\"/img/java/produce_practice/5/3.png\"></p>\n</li>\n<li><p>在登录方法中加入日志记录：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> HttpResponseEntity <span class=\"title function_\">userLogin</span><span class=\"params\">(<span class=\"meta\">@RequestBody</span> UserInfo userInfo)</span> &#123; <span class=\"comment\">//requestbody注解，将请求的json数据转换为对象</span></span><br><span class=\"line\">  </span><br><span class=\"line\">    log.info(<span class=\"string\">&quot;用户&quot;</span> + userInfo.getUsername() + <span class=\"string\">&quot;登录&quot;</span>);</span><br></pre></td></tr></table></figure>\n\n<p>然后运行项目，登录后可以看到控制台有输出登录信息，并且log目录下也会有日志记录。</p>\n</li>\n</ul>\n<h2 id=\"6-2-单元测试\"><a href=\"#6-2-单元测试\" class=\"headerlink\" title=\"6.2 单元测试\"></a>6.2 单元测试</h2><ul>\n<li><p>在src&#x2F;test下可以找到测试类</p>\n</li>\n<li><p>在pom.xml的dependences标签下加入依赖：（记得刷新maven）这样就可以使用@RunWith注解，它可以使用springboot的依赖注入</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\">\t<span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>junit<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">\t<span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>junit<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">\t<span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>4.12<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>测试代码：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> com.lyingedu.questionnaire;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> com.lyingedu.questionnaire.beans.HttpResponseEntity;</span><br><span class=\"line\"><span class=\"keyword\">import</span> com.lyingedu.questionnaire.biz.user.controller.UserController;</span><br><span class=\"line\"><span class=\"keyword\">import</span> com.lyingedu.questionnaire.common.utils.UUIDUtil;</span><br><span class=\"line\"><span class=\"keyword\">import</span> com.lyingedu.questionnaire.dbmap.entities.UserInfo;</span><br><span class=\"line\"><span class=\"keyword\">import</span> jakarta.annotation.Resource;</span><br><span class=\"line\"><span class=\"keyword\">import</span> lombok.extern.slf4j.Slf4j;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.junit.Test;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.junit.runner.RunWith;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.boot.test.context.SpringBootTest;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.springframework.test.context.junit4.SpringRunner;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.Date;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.util.List;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@Slf4j</span></span><br><span class=\"line\"><span class=\"meta\">@RunWith(SpringRunner.class)</span> <span class=\"comment\">//这样可以用springboot的依赖注入</span></span><br><span class=\"line\"><span class=\"meta\">@SpringBootTest</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">QuestionnaireApplicationTests</span> &#123;</span><br><span class=\"line\">    <span class=\"meta\">@Resource</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> UserController userController;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">    * 测试查询功能</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"meta\">@Test</span> <span class=\"comment\">//注意使用junit，而不是junit.api</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">testQueryUserInfoList</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">        <span class=\"type\">UserInfo</span> <span class=\"variable\">userInfo</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">UserInfo</span>();</span><br><span class=\"line\">        <span class=\"type\">HttpResponseEntity</span> <span class=\"variable\">httpResponseEntity</span> <span class=\"operator\">=</span> userController.queryUserList(userInfo);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (<span class=\"string\">&quot;666&quot;</span>.equals(httpResponseEntity.getCode())) &#123;</span><br><span class=\"line\">            log.info(<span class=\"string\">&quot;&gt;&gt;queryUserList用户列表查询测试成功&quot;</span>);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">    * 测试用户登录</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"meta\">@Test</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">testUserLogin</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">        <span class=\"type\">UserInfo</span> <span class=\"variable\">userInfo</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">UserInfo</span>();</span><br><span class=\"line\">        userInfo.setUsername(<span class=\"string\">&quot;admin&quot;</span>);</span><br><span class=\"line\">        userInfo.setPassword(<span class=\"string\">&quot;12&quot;</span>);</span><br><span class=\"line\">        <span class=\"type\">HttpResponseEntity</span> <span class=\"variable\">httpResponseEntity</span> <span class=\"operator\">=</span> userController.queryUserList(userInfo);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (<span class=\"string\">&quot;666&quot;</span>.equals(httpResponseEntity.getCode())) &#123;</span><br><span class=\"line\">            log.info(<span class=\"string\">&quot;&gt;&gt;selectUserInfo用户&quot;</span>+userInfo.getUsername()+<span class=\"string\">&quot;登录测试成功&quot;</span>);</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">            log.info(<span class=\"string\">&quot;&gt;&gt;selectUserInfo用户登录测试失败&quot;</span>);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">    * 测试创建用户</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"meta\">@Test</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">testCreateUser</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">        <span class=\"type\">UserInfo</span> <span class=\"variable\">userInfo</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">UserInfo</span>();</span><br><span class=\"line\">        userInfo.setId(UUIDUtil.getOneUUID());</span><br><span class=\"line\">        userInfo.setUsername(<span class=\"string\">&quot;test005&quot;</span>);</span><br><span class=\"line\">        userInfo.setPassword(<span class=\"string\">&quot;123&quot;</span>);</span><br><span class=\"line\">        userInfo.setStatus(<span class=\"string\">&quot;1&quot;</span>);</span><br><span class=\"line\">        userInfo.setCreatedBy(<span class=\"string\">&quot;anyone&quot;</span>);</span><br><span class=\"line\">        userInfo.setCreationDate(<span class=\"keyword\">new</span> <span class=\"title class_\">Date</span>());</span><br><span class=\"line\">        userInfo.setStartTime(<span class=\"keyword\">new</span> <span class=\"title class_\">Date</span>());</span><br><span class=\"line\">        userInfo.setStopTime(<span class=\"keyword\">new</span> <span class=\"title class_\">Date</span>());</span><br><span class=\"line\">        userInfo.setLastUpdateBy(<span class=\"string\">&quot;anyone&quot;</span>);</span><br><span class=\"line\">        userInfo.setLastUpdateDate(<span class=\"keyword\">new</span> <span class=\"title class_\">Date</span>());</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">HttpResponseEntity</span> <span class=\"variable\">httpResponseEntity</span> <span class=\"operator\">=</span> userController.addUserInfo(userInfo);</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(<span class=\"string\">&quot;666&quot;</span>.equals(httpResponseEntity.getCode()))&#123;</span><br><span class=\"line\">            log.info(<span class=\"string\">&quot;&gt;&gt;adduserinfo插入用户测试成功&quot;</span>);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">    * 测试修改用户</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"meta\">@Test</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">testUpdUser</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">        <span class=\"type\">UserInfo</span> <span class=\"variable\">userInfo</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">UserInfo</span>();</span><br><span class=\"line\">        userInfo.setUsername(<span class=\"string\">&quot;test005&quot;</span>);</span><br><span class=\"line\">        userInfo.setPassword(<span class=\"string\">&quot;123&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">HttpResponseEntity</span> <span class=\"variable\">httpResponseEntity</span> <span class=\"operator\">=</span> userController.queryUserList(userInfo);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span>(<span class=\"string\">&quot;666&quot;</span>.equals(httpResponseEntity.getCode()))&#123;<span class=\"comment\">//查询成功</span></span><br><span class=\"line\">            List list=(List) httpResponseEntity.getData();</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(list.size() !=<span class=\"number\">0</span>)&#123;</span><br><span class=\"line\">                <span class=\"type\">UserInfo</span> <span class=\"variable\">data</span> <span class=\"operator\">=</span> (UserInfo) list.get(<span class=\"number\">0</span>);</span><br><span class=\"line\">                data.setUsername(<span class=\"string\">&quot;test006&quot;</span>);</span><br><span class=\"line\">                <span class=\"type\">HttpResponseEntity</span> <span class=\"variable\">httpResponseEntity1</span> <span class=\"operator\">=</span> userController.modifyUserInfo(data);</span><br><span class=\"line\">                <span class=\"keyword\">if</span>(<span class=\"string\">&quot;666&quot;</span>.equals(httpResponseEntity1.getCode()))&#123;</span><br><span class=\"line\">                    log.info(<span class=\"string\">&quot;&gt;&gt;update修改用户更新测试成功&quot;</span>);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">    * 测试删除用户</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"meta\">@Test</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">testDeleteUser</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">        <span class=\"type\">UserInfo</span> <span class=\"variable\">userInfo</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">UserInfo</span>();</span><br><span class=\"line\">        userInfo.setUsername(<span class=\"string\">&quot;test006&quot;</span>);</span><br><span class=\"line\">        userInfo.setPassword(<span class=\"string\">&quot;123&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">HttpResponseEntity</span> <span class=\"variable\">httpResponseEntity</span> <span class=\"operator\">=</span> userController.queryUserList(userInfo);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span>(<span class=\"string\">&quot;666&quot;</span>.equals(httpResponseEntity.getCode()))&#123;<span class=\"comment\">//查询成功</span></span><br><span class=\"line\">            List list=(List) httpResponseEntity.getData();</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(list.size() !=<span class=\"number\">0</span>)&#123;</span><br><span class=\"line\">                <span class=\"type\">UserInfo</span> <span class=\"variable\">data</span> <span class=\"operator\">=</span> (UserInfo) list.get(<span class=\"number\">0</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"type\">HttpResponseEntity</span> <span class=\"variable\">httpResponseEntity1</span> <span class=\"operator\">=</span> userController.deleteUserById(data);</span><br><span class=\"line\">                <span class=\"keyword\">if</span>(<span class=\"string\">&quot;666&quot;</span>.equals(httpResponseEntity1.getCode()))&#123;</span><br><span class=\"line\">                    log.info(<span class=\"string\">&quot;&gt;&gt;delete用户删除测试成功&quot;</span>);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></li>\n</ul>"},{"title":"7.1 序列模型","date":"2024-04-30T06:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n- 到目前为止我们默认数据都来自于某种分布，并且所有样本都是独立同分布的。然而，大多数的数据并非如此。例如，文章中的单词是按顺序写的，如果顺序被随机地重排，就很难理解文章原始的意思。\n- 我们不仅仅可以接收一个序列作为输入，而是还可能期望继续猜测这个序列的后续。\n- 如果说卷积神经网络可以有效地处理空间信息，那么循环神经网络（recurrent neural network，RNN）则可以更好地处理序列信息。循环神经网络通过引入状态变量存储过去的信息和当前的输入，从而可以确定当前的输出。\n# 1 序列模型\n## 1.1 统计工具\n- 以股票价格（富时100指数）为例。\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/7_rnn/1_seqmodel/1.png)\n![](img/deeplearning/code/pytorch/7_rnn/1_seqmodel/1.png)\n    - 在时间步t观察到的价格为$x_ t$。t对于本文中的序列通常是离散的，并在整数或其子集上变化。假设一个交易员想在t日的股市中表现良好，于是通过以下途径预测xt：\n    $$x_ t ∼ P(x_ t | x_ {t−1}, . . . , x_ 1)$$\n### 1.1.1 自回归模型\n- 输入数据的数量将会随着我们遇到的数据量的增加而增加，因此需要一个近似方法来使这个计算变得容易处理。如何有效估计P(xt | xt−1, . . . , x1),归结为以下两种策略。\n\n    1. 假设在现实情况下相当长的序列 xt−1, . . . , x1可能是不必要的，因此我们只需要满足某个长度为$\\tau$的时间跨度。这种模型被称为自回归模型（autoregressive models），因为它们是对自己执行回归。\n\n    2. 如下图是保留一些对过去观测的总结$h_ t$，并且同时更新预测$\\hat x_ t$的模型和总结$h_ t$。即：基于$\\hat x_ t = P(x_ t | h_ t)$估计$x_ t$， 并使用$h_ t = g(x_ {t−1}, h_ {t−1})$来更新模型。由于$h _t$从未被观测到，这类模型也被称为 隐变量自回归模型（latent autoregressive models）。\n\n    ![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/7_rnn/1_seqmodel/2.png)\n    ![](img/deeplearning/code/pytorch/7_rnn/1_seqmodel/2.png)\n\n- 这两种情况都有一个显而易见的问题：如何生成训练数据？可以使用历史观测来预测下一个未来观测。整个序列的估计值都将通过以下的方式获得：\n$$P(x_ 1, . . . , x_ T) = \\prod _{t=1} ^T P(x_ t | x_ {t−1}, . . . , x_ 1)$$\n### 1.1.2 马尔可夫模型\n- 在自回归模型的近似法中，我们使用$x_ {t-1},\\cdots,x_ {t - \\tau}$而不是$x_ {t-1},\\cdots,x_ 1$来估计$x_ t$。我们称这个模型为马尔可夫模型。如果$\\tau = 1$，那么这个模型就是一阶马尔可夫模型：\n    $$P(x_ 1, . . . , x_ T) = \\prod _{t=1} ^T P(x_ t | x_ {t−1})$$\n\n    $$P(x_ 1 | x_ 0) = P(x_ 1)$$\n\n- 假设$x_ t$仅是离散值，使用动态规划可以沿着马尔可夫链精确地计算结果。例如，我们可以高效地计算$P(x_ {t+1} | X_ {t-1})$:\n$$\\begin{aligned} P(x_ { t + 1 } | x_ { t - 1 }) &= \\frac { \\sum _{ x_ t } P( x_ { t + 1 }, x_ t, x_ { t - 1 }) } { \\sum _{ x_ t, x_ { t - 1 } } P( x_ { t + 1 }, x_ t, x_ { t - 1 }) } \\\\ &= \\frac { \\sum _{ x_ t } P( x_ { t + 1 } | x_ t, x_ { t - 1 }) P( x_ t | x_ { t - 1 }) P( x_ { t - 1 }) } { \\sum _{ x_ t } P( x_ { t + 1 } | x_ t, x_ { t - 1 }) P( x_ t | x_ { t - 1 }) P( x_ { t - 1 }) } \\\\ &= \\sum _{ x_ t } P( x_ { t + 1 } | x_ t ) P( x_ t | x_ { t - 1 }) \\end{aligned}$$\n- 对$x_ t$求和结果为1，相当于求和符号与$x_ t$可以消掉。\n\n### 1.1.3 因果关系\n- 通常，可以将$P(x_ 1, \\cdots, x_ T)$倒序展开：\n$$P(x_ 1, \\cdots, x_ T) = \\prod _{t=T} ^1 P(x_ t | x_ {t+1}, \\cdots, x_ T)$$\n- 如果基于一个马尔可夫模型，我们还可以得到一个反向的条件概率分布。但通常数据存在一个自然的方向，即在时间上是前进的。\n\n## 1.2 训练\n- 使用正弦函数和一些可加性噪声来生成序列数据，时间步为1, 2, . . . , 1000。\n\n\n```python\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK']='True'\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\nT = 1000 #产生1000个点\ntime = torch.arange(1, T+1, dtype=torch.float32)\nx = torch.sin(0.01 * time) + torch.normal(0, 0.2, (T,)) #加入噪声\nd2l.plot(time, [x], 'time', 'x', xlim=[1, 1000], figsize=(6, 3))\n```\n\n\n    \n![svg](1_seqmodel_files/1_seqmodel_1_0.svg)\n![](img/deeplearning/code/pytorch/7_rnn/1_seqmodel_files/1_seqmodel_1_0.svg)\n    \n\n\n- 将这个序列转换为模型的特征－标签（feature‐label）对。将数据映射为数据对$y_ t = x_ t$和 \\mathbf{ x }_ t = [x_ { t - \\tau }, \\cdots, x_ { t - 1 }]$。这比我们提供的数据样本少了$\\tau$个，在这里，我们仅使用前600个“特征－标签”对进行训练。\n\n\n```python\ntau = 4\nfeatures = torch.zeros((T - tau, tau)) #[996, 4]\nfor i in range(tau):\n    features[:, i] = x[i:T - tau+i] #[996]\nlabels = x[tau:].reshape((-1, 1)) #[996, 1]\n'''\n[x0, x1, x2, x3] -> [x4]\n[x1, x2, x3, x4] -> [x5]\n...\n'''\n\nbatch_size, n_train = 16, 600\n# 只有前n_train个样本用于训练\ntrain_iter = d2l.load_array((features[:n_train], labels[:n_train]), batch_size, is_train=True)\n```\n\n- 构建一个简单的模型：\n    - 两个全连接层的多层感知机，ReLU激活函数和平方损失。\n\n\n```python\n# 初始化权重\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        nn.init.xavier_uniform_(m.weight)\n\n# mlp\ndef get_net():\n    net = nn.Sequential(nn.Linear(4,10),\n                        nn.ReLU(),\n                        nn.Linear(10, 1))\n    net.apply(init_weights)\n    return net\n\n# 平方损失。MSELoss计算平方误差时不带系数1/2\nloss = nn.MSELoss(reduction='none')\n```\n\n- 训练\n\n\n```python\ndef train(net, train_iter, loss, epochs, lr):\n    trainer = torch.optim.Adam(net.parameters(), lr)\n    for epoch in range(epochs):\n        for X, y in train_iter:\n            trainer.zero_grad()\n            l = loss(net(X), y)\n            l.sum().backward()\n            trainer.step()\n        print(f'epoch {epoch+1},'\n              f'loss:{d2l.evaluate_loss(net, train_iter, loss):f}')\n\nnet = get_net()\ntrain(net, train_iter, loss, 5, 0.01)\n```\n\n    epoch 1,loss:0.093809\n    epoch 2,loss:0.058604\n    epoch 3,loss:0.052532\n    epoch 4,loss:0.058827\n    epoch 5,loss:0.049325\n    \n\n## 1.3 预测\n- 可以看到训练损失很小，因此我们期望模型能有很好的工作效果。单步预测：\n\n\n```python\nonestep_preds = net(features)\nd2l.plot(\n    [time, time[tau:]],\n    [x.detach().numpy(), onestep_preds.detach().numpy()], 'time',\n    'x', legend=['data', '1-step preds'], xlim=[1, 1000],\n                 figsize=(6, 3)\n        )\n```\n\n\n    \n![svg](1_seqmodel_files/1_seqmodel_9_0.svg)\n![](img/deeplearning/code/pytorch/7_rnn/1_seqmodel_files/1_seqmodel_9_0.svg)\n\n\n- 对于直到$x_ t$的观测序列，其在时间步t + k处的预测输出$\\hat x_ {t+k}$称为k步预测。\n- 由于我们的观察已经到了x604，我们必须使用我们自己的预测（而不是原始数据）来进行多步预测。\n\n\n```python\nmultistep_ppreds = torch.zeros(T)\nmultistep_ppreds[:n_train+ tau] = x[:n_train+ tau]\nfor i in range(n_train+ tau, T):\n    multistep_ppreds[i] = net(multistep_ppreds[i- tau:i].reshape((1, -1)))\n\nd2l.plot([time, time[tau:], time[n_train + tau:]],\n         [x.detach().numpy(), onestep_preds.detach().numpy(), multistep_ppreds[n_train + tau:].detach().numpy()], 'time',\n         'x', legend=['data', '1-step pred', 'multistep preds'],\n         xlim=[1, 1000], figsize=(6,3))\n```\n\n\n    \n![svg](1_seqmodel_files/1_seqmodel_11_0.svg)\n![](img/deeplearning/code/pytorch/7_rnn/1_seqmodel_files/1_seqmodel_11_0.svg)\n\n\n- 绿线的预测显然并不理想。经过几个预测步骤之后，预测的结果很快就会衰减到一个常数。为什么这个算法效果这么差呢？事实是由于错误的累积\n- 基于k = 1, 4, 16, 64，通过对整个序列预测的计算，让我们更仔细地看一下k步预测的困难。\n\n\n\n```python\nmax_steps = 64\n\nfeatures = torch.zeros((T - tau - max_steps + 1, tau+max_steps)) #\n# 列i（i<tau）是来自x的观测，其时间步从（i）到（i+T-tau-max_steps+1）\\\n'''\nx0 x1 x2 x3\nx1 x2 x3 x4\n'''\nfor i in range(tau):\n    features[:, i] = x[i: i+T-tau-max_steps+1]\n# 列i（i>=tau）是来自（i-tau+1）步的预测，其时间步从（i）到（i+T-tau-max_steps+1）\n'''\nx4 x5 x6 ... x67\nx5 x6 x7 ... x68\n'''\nfor i in range(tau, tau+max_steps):\n    features[:, i] = net(features[:, i-tau:i]).reshape(-1)\n\nsteps = (1, 4, 16, 64)\nd2l.plot([time[tau + i - 1: T - max_steps + i] for i in steps],\n        [features[:, (tau + i - 1)].detach().numpy() for i in steps], 'time', 'x',\n        legend=[f'{i}-step preds' for i in steps], xlim=[5, 1000],\n        figsize=(6, 3))\n```\n\n\n    \n![svg](1_seqmodel_files/1_seqmodel_13_0.svg)\n![](img/deeplearning/code/pytorch/7_rnn/1_seqmodel_files/1_seqmodel_13_0.svg)\n\n\n","source":"_posts/deeplearning/code/pytorch/7_rnn/1_seqmodel.md","raw":"---\ntitle: 7.1 序列模型\ndate: 2024-4-30 14:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n- 到目前为止我们默认数据都来自于某种分布，并且所有样本都是独立同分布的。然而，大多数的数据并非如此。例如，文章中的单词是按顺序写的，如果顺序被随机地重排，就很难理解文章原始的意思。\n- 我们不仅仅可以接收一个序列作为输入，而是还可能期望继续猜测这个序列的后续。\n- 如果说卷积神经网络可以有效地处理空间信息，那么循环神经网络（recurrent neural network，RNN）则可以更好地处理序列信息。循环神经网络通过引入状态变量存储过去的信息和当前的输入，从而可以确定当前的输出。\n# 1 序列模型\n## 1.1 统计工具\n- 以股票价格（富时100指数）为例。\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/7_rnn/1_seqmodel/1.png)\n![](img/deeplearning/code/pytorch/7_rnn/1_seqmodel/1.png)\n    - 在时间步t观察到的价格为$x_ t$。t对于本文中的序列通常是离散的，并在整数或其子集上变化。假设一个交易员想在t日的股市中表现良好，于是通过以下途径预测xt：\n    $$x_ t ∼ P(x_ t | x_ {t−1}, . . . , x_ 1)$$\n### 1.1.1 自回归模型\n- 输入数据的数量将会随着我们遇到的数据量的增加而增加，因此需要一个近似方法来使这个计算变得容易处理。如何有效估计P(xt | xt−1, . . . , x1),归结为以下两种策略。\n\n    1. 假设在现实情况下相当长的序列 xt−1, . . . , x1可能是不必要的，因此我们只需要满足某个长度为$\\tau$的时间跨度。这种模型被称为自回归模型（autoregressive models），因为它们是对自己执行回归。\n\n    2. 如下图是保留一些对过去观测的总结$h_ t$，并且同时更新预测$\\hat x_ t$的模型和总结$h_ t$。即：基于$\\hat x_ t = P(x_ t | h_ t)$估计$x_ t$， 并使用$h_ t = g(x_ {t−1}, h_ {t−1})$来更新模型。由于$h _t$从未被观测到，这类模型也被称为 隐变量自回归模型（latent autoregressive models）。\n\n    ![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/7_rnn/1_seqmodel/2.png)\n    ![](img/deeplearning/code/pytorch/7_rnn/1_seqmodel/2.png)\n\n- 这两种情况都有一个显而易见的问题：如何生成训练数据？可以使用历史观测来预测下一个未来观测。整个序列的估计值都将通过以下的方式获得：\n$$P(x_ 1, . . . , x_ T) = \\prod _{t=1} ^T P(x_ t | x_ {t−1}, . . . , x_ 1)$$\n### 1.1.2 马尔可夫模型\n- 在自回归模型的近似法中，我们使用$x_ {t-1},\\cdots,x_ {t - \\tau}$而不是$x_ {t-1},\\cdots,x_ 1$来估计$x_ t$。我们称这个模型为马尔可夫模型。如果$\\tau = 1$，那么这个模型就是一阶马尔可夫模型：\n    $$P(x_ 1, . . . , x_ T) = \\prod _{t=1} ^T P(x_ t | x_ {t−1})$$\n\n    $$P(x_ 1 | x_ 0) = P(x_ 1)$$\n\n- 假设$x_ t$仅是离散值，使用动态规划可以沿着马尔可夫链精确地计算结果。例如，我们可以高效地计算$P(x_ {t+1} | X_ {t-1})$:\n$$\\begin{aligned} P(x_ { t + 1 } | x_ { t - 1 }) &= \\frac { \\sum _{ x_ t } P( x_ { t + 1 }, x_ t, x_ { t - 1 }) } { \\sum _{ x_ t, x_ { t - 1 } } P( x_ { t + 1 }, x_ t, x_ { t - 1 }) } \\\\ &= \\frac { \\sum _{ x_ t } P( x_ { t + 1 } | x_ t, x_ { t - 1 }) P( x_ t | x_ { t - 1 }) P( x_ { t - 1 }) } { \\sum _{ x_ t } P( x_ { t + 1 } | x_ t, x_ { t - 1 }) P( x_ t | x_ { t - 1 }) P( x_ { t - 1 }) } \\\\ &= \\sum _{ x_ t } P( x_ { t + 1 } | x_ t ) P( x_ t | x_ { t - 1 }) \\end{aligned}$$\n- 对$x_ t$求和结果为1，相当于求和符号与$x_ t$可以消掉。\n\n### 1.1.3 因果关系\n- 通常，可以将$P(x_ 1, \\cdots, x_ T)$倒序展开：\n$$P(x_ 1, \\cdots, x_ T) = \\prod _{t=T} ^1 P(x_ t | x_ {t+1}, \\cdots, x_ T)$$\n- 如果基于一个马尔可夫模型，我们还可以得到一个反向的条件概率分布。但通常数据存在一个自然的方向，即在时间上是前进的。\n\n## 1.2 训练\n- 使用正弦函数和一些可加性噪声来生成序列数据，时间步为1, 2, . . . , 1000。\n\n\n```python\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK']='True'\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\nT = 1000 #产生1000个点\ntime = torch.arange(1, T+1, dtype=torch.float32)\nx = torch.sin(0.01 * time) + torch.normal(0, 0.2, (T,)) #加入噪声\nd2l.plot(time, [x], 'time', 'x', xlim=[1, 1000], figsize=(6, 3))\n```\n\n\n    \n![svg](1_seqmodel_files/1_seqmodel_1_0.svg)\n![](img/deeplearning/code/pytorch/7_rnn/1_seqmodel_files/1_seqmodel_1_0.svg)\n    \n\n\n- 将这个序列转换为模型的特征－标签（feature‐label）对。将数据映射为数据对$y_ t = x_ t$和 \\mathbf{ x }_ t = [x_ { t - \\tau }, \\cdots, x_ { t - 1 }]$。这比我们提供的数据样本少了$\\tau$个，在这里，我们仅使用前600个“特征－标签”对进行训练。\n\n\n```python\ntau = 4\nfeatures = torch.zeros((T - tau, tau)) #[996, 4]\nfor i in range(tau):\n    features[:, i] = x[i:T - tau+i] #[996]\nlabels = x[tau:].reshape((-1, 1)) #[996, 1]\n'''\n[x0, x1, x2, x3] -> [x4]\n[x1, x2, x3, x4] -> [x5]\n...\n'''\n\nbatch_size, n_train = 16, 600\n# 只有前n_train个样本用于训练\ntrain_iter = d2l.load_array((features[:n_train], labels[:n_train]), batch_size, is_train=True)\n```\n\n- 构建一个简单的模型：\n    - 两个全连接层的多层感知机，ReLU激活函数和平方损失。\n\n\n```python\n# 初始化权重\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        nn.init.xavier_uniform_(m.weight)\n\n# mlp\ndef get_net():\n    net = nn.Sequential(nn.Linear(4,10),\n                        nn.ReLU(),\n                        nn.Linear(10, 1))\n    net.apply(init_weights)\n    return net\n\n# 平方损失。MSELoss计算平方误差时不带系数1/2\nloss = nn.MSELoss(reduction='none')\n```\n\n- 训练\n\n\n```python\ndef train(net, train_iter, loss, epochs, lr):\n    trainer = torch.optim.Adam(net.parameters(), lr)\n    for epoch in range(epochs):\n        for X, y in train_iter:\n            trainer.zero_grad()\n            l = loss(net(X), y)\n            l.sum().backward()\n            trainer.step()\n        print(f'epoch {epoch+1},'\n              f'loss:{d2l.evaluate_loss(net, train_iter, loss):f}')\n\nnet = get_net()\ntrain(net, train_iter, loss, 5, 0.01)\n```\n\n    epoch 1,loss:0.093809\n    epoch 2,loss:0.058604\n    epoch 3,loss:0.052532\n    epoch 4,loss:0.058827\n    epoch 5,loss:0.049325\n    \n\n## 1.3 预测\n- 可以看到训练损失很小，因此我们期望模型能有很好的工作效果。单步预测：\n\n\n```python\nonestep_preds = net(features)\nd2l.plot(\n    [time, time[tau:]],\n    [x.detach().numpy(), onestep_preds.detach().numpy()], 'time',\n    'x', legend=['data', '1-step preds'], xlim=[1, 1000],\n                 figsize=(6, 3)\n        )\n```\n\n\n    \n![svg](1_seqmodel_files/1_seqmodel_9_0.svg)\n![](img/deeplearning/code/pytorch/7_rnn/1_seqmodel_files/1_seqmodel_9_0.svg)\n\n\n- 对于直到$x_ t$的观测序列，其在时间步t + k处的预测输出$\\hat x_ {t+k}$称为k步预测。\n- 由于我们的观察已经到了x604，我们必须使用我们自己的预测（而不是原始数据）来进行多步预测。\n\n\n```python\nmultistep_ppreds = torch.zeros(T)\nmultistep_ppreds[:n_train+ tau] = x[:n_train+ tau]\nfor i in range(n_train+ tau, T):\n    multistep_ppreds[i] = net(multistep_ppreds[i- tau:i].reshape((1, -1)))\n\nd2l.plot([time, time[tau:], time[n_train + tau:]],\n         [x.detach().numpy(), onestep_preds.detach().numpy(), multistep_ppreds[n_train + tau:].detach().numpy()], 'time',\n         'x', legend=['data', '1-step pred', 'multistep preds'],\n         xlim=[1, 1000], figsize=(6,3))\n```\n\n\n    \n![svg](1_seqmodel_files/1_seqmodel_11_0.svg)\n![](img/deeplearning/code/pytorch/7_rnn/1_seqmodel_files/1_seqmodel_11_0.svg)\n\n\n- 绿线的预测显然并不理想。经过几个预测步骤之后，预测的结果很快就会衰减到一个常数。为什么这个算法效果这么差呢？事实是由于错误的累积\n- 基于k = 1, 4, 16, 64，通过对整个序列预测的计算，让我们更仔细地看一下k步预测的困难。\n\n\n\n```python\nmax_steps = 64\n\nfeatures = torch.zeros((T - tau - max_steps + 1, tau+max_steps)) #\n# 列i（i<tau）是来自x的观测，其时间步从（i）到（i+T-tau-max_steps+1）\\\n'''\nx0 x1 x2 x3\nx1 x2 x3 x4\n'''\nfor i in range(tau):\n    features[:, i] = x[i: i+T-tau-max_steps+1]\n# 列i（i>=tau）是来自（i-tau+1）步的预测，其时间步从（i）到（i+T-tau-max_steps+1）\n'''\nx4 x5 x6 ... x67\nx5 x6 x7 ... x68\n'''\nfor i in range(tau, tau+max_steps):\n    features[:, i] = net(features[:, i-tau:i]).reshape(-1)\n\nsteps = (1, 4, 16, 64)\nd2l.plot([time[tau + i - 1: T - max_steps + i] for i in steps],\n        [features[:, (tau + i - 1)].detach().numpy() for i in steps], 'time', 'x',\n        legend=[f'{i}-step preds' for i in steps], xlim=[5, 1000],\n        figsize=(6, 3))\n```\n\n\n    \n![svg](1_seqmodel_files/1_seqmodel_13_0.svg)\n![](img/deeplearning/code/pytorch/7_rnn/1_seqmodel_files/1_seqmodel_13_0.svg)\n\n\n","slug":"deeplearning/code/pytorch/7_rnn/1_seqmodel","published":1,"updated":"2024-05-05T14:13:39.848Z","comments":1,"layout":"post","photos":[],"_id":"clvuli6ux008zzsvw6ky64vxy","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<ul>\n<li>到目前为止我们默认数据都来自于某种分布，并且所有样本都是独立同分布的。然而，大多数的数据并非如此。例如，文章中的单词是按顺序写的，如果顺序被随机地重排，就很难理解文章原始的意思。</li>\n<li>我们不仅仅可以接收一个序列作为输入，而是还可能期望继续猜测这个序列的后续。</li>\n<li>如果说卷积神经网络可以有效地处理空间信息，那么循环神经网络（recurrent neural network，RNN）则可以更好地处理序列信息。循环神经网络通过引入状态变量存储过去的信息和当前的输入，从而可以确定当前的输出。</li>\n</ul>\n<h1 id=\"1-序列模型\"><a href=\"#1-序列模型\" class=\"headerlink\" title=\"1 序列模型\"></a>1 序列模型</h1><h2 id=\"1-1-统计工具\"><a href=\"#1-1-统计工具\" class=\"headerlink\" title=\"1.1 统计工具\"></a>1.1 统计工具</h2><ul>\n<li>以股票价格（富时100指数）为例。<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/7_rnn/1_seqmodel/1.png\"><br><img src=\"/img/deeplearning/code/pytorch/7_rnn/1_seqmodel/1.png\"><ul>\n<li>在时间步t观察到的价格为$x_ t$。t对于本文中的序列通常是离散的，并在整数或其子集上变化。假设一个交易员想在t日的股市中表现良好，于是通过以下途径预测xt：<br>  $$x_ t ∼ P(x_ t | x_ {t−1}, . . . , x_ 1)$$</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"1-1-1-自回归模型\"><a href=\"#1-1-1-自回归模型\" class=\"headerlink\" title=\"1.1.1 自回归模型\"></a>1.1.1 自回归模型</h3><ul>\n<li><p>输入数据的数量将会随着我们遇到的数据量的增加而增加，因此需要一个近似方法来使这个计算变得容易处理。如何有效估计P(xt | xt−1, . . . , x1),归结为以下两种策略。</p>\n<ol>\n<li><p>假设在现实情况下相当长的序列 xt−1, . . . , x1可能是不必要的，因此我们只需要满足某个长度为$\\tau$的时间跨度。这种模型被称为自回归模型（autoregressive models），因为它们是对自己执行回归。</p>\n</li>\n<li><p>如下图是保留一些对过去观测的总结$h_ t$，并且同时更新预测$\\hat x_ t$的模型和总结$h_ t$。即：基于$\\hat x_ t &#x3D; P(x_ t | h_ t)$估计$x_ t$， 并使用$h_ t &#x3D; g(x_ {t−1}, h_ {t−1})$来更新模型。由于$h _t$从未被观测到，这类模型也被称为 隐变量自回归模型（latent autoregressive models）。</p>\n</li>\n</ol>\n<p>  <img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/7_rnn/1_seqmodel/2.png\"><br>  <img src=\"/img/deeplearning/code/pytorch/7_rnn/1_seqmodel/2.png\"></p>\n</li>\n<li><p>这两种情况都有一个显而易见的问题：如何生成训练数据？可以使用历史观测来预测下一个未来观测。整个序列的估计值都将通过以下的方式获得：<br>$$P(x_ 1, . . . , x_ T) &#x3D; \\prod <em>{t&#x3D;1} ^T P(x</em> t | x_ {t−1}, . . . , x_ 1)$$</p>\n</li>\n</ul>\n<h3 id=\"1-1-2-马尔可夫模型\"><a href=\"#1-1-2-马尔可夫模型\" class=\"headerlink\" title=\"1.1.2 马尔可夫模型\"></a>1.1.2 马尔可夫模型</h3><ul>\n<li><p>在自回归模型的近似法中，我们使用$x_ {t-1},\\cdots,x_ {t - \\tau}$而不是$x_ {t-1},\\cdots,x_ 1$来估计$x_ t$。我们称这个模型为马尔可夫模型。如果$\\tau &#x3D; 1$，那么这个模型就是一阶马尔可夫模型：<br>  $$P(x_ 1, . . . , x_ T) &#x3D; \\prod <em>{t&#x3D;1} ^T P(x</em> t | x_ {t−1})$$</p>\n<p>  $$P(x_ 1 | x_ 0) &#x3D; P(x_ 1)$$</p>\n</li>\n<li><p>假设$x_ t$仅是离散值，使用动态规划可以沿着马尔可夫链精确地计算结果。例如，我们可以高效地计算$P(x_ {t+1} | X_ {t-1})$:<br>$$\\begin{aligned} P(x_ { t + 1 } | x_ { t - 1 }) &amp;&#x3D; \\frac { \\sum <em>{ x</em> t } P( x_ { t + 1 }, x_ t, x_ { t - 1 }) } { \\sum <em>{ x</em> t, x_ { t - 1 } } P( x_ { t + 1 }, x_ t, x_ { t - 1 }) } \\ &amp;&#x3D; \\frac { \\sum <em>{ x</em> t } P( x_ { t + 1 } | x_ t, x_ { t - 1 }) P( x_ t | x_ { t - 1 }) P( x_ { t - 1 }) } { \\sum <em>{ x</em> t } P( x_ { t + 1 } | x_ t, x_ { t - 1 }) P( x_ t | x_ { t - 1 }) P( x_ { t - 1 }) } \\ &amp;&#x3D; \\sum <em>{ x</em> t } P( x_ { t + 1 } | x_ t ) P( x_ t | x_ { t - 1 }) \\end{aligned}$$</p>\n</li>\n<li><p>对$x_ t$求和结果为1，相当于求和符号与$x_ t$可以消掉。</p>\n</li>\n</ul>\n<h3 id=\"1-1-3-因果关系\"><a href=\"#1-1-3-因果关系\" class=\"headerlink\" title=\"1.1.3 因果关系\"></a>1.1.3 因果关系</h3><ul>\n<li>通常，可以将$P(x_ 1, \\cdots, x_ T)$倒序展开：<br>$$P(x_ 1, \\cdots, x_ T) &#x3D; \\prod <em>{t&#x3D;T} ^1 P(x</em> t | x_ {t+1}, \\cdots, x_ T)$$</li>\n<li>如果基于一个马尔可夫模型，我们还可以得到一个反向的条件概率分布。但通常数据存在一个自然的方向，即在时间上是前进的。</li>\n</ul>\n<h2 id=\"1-2-训练\"><a href=\"#1-2-训练\" class=\"headerlink\" title=\"1.2 训练\"></a>1.2 训练</h2><ul>\n<li>使用正弦函数和一些可加性噪声来生成序列数据，时间步为1, 2, . . . , 1000。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>]=<span class=\"string\">&#x27;True&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">T = <span class=\"number\">1000</span> <span class=\"comment\">#产生1000个点</span></span><br><span class=\"line\">time = torch.arange(<span class=\"number\">1</span>, T+<span class=\"number\">1</span>, dtype=torch.float32)</span><br><span class=\"line\">x = torch.sin(<span class=\"number\">0.01</span> * time) + torch.normal(<span class=\"number\">0</span>, <span class=\"number\">0.2</span>, (T,)) <span class=\"comment\">#加入噪声</span></span><br><span class=\"line\">d2l.plot(time, [x], <span class=\"string\">&#x27;time&#x27;</span>, <span class=\"string\">&#x27;x&#x27;</span>, xlim=[<span class=\"number\">1</span>, <span class=\"number\">1000</span>], figsize=(<span class=\"number\">6</span>, <span class=\"number\">3</span>))</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/1_seqmodel_files/1_seqmodel_1_0.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/7_rnn/1_seqmodel_files/1_seqmodel_1_0.svg\"></p>\n<ul>\n<li>将这个序列转换为模型的特征－标签（feature‐label）对。将数据映射为数据对$y_ t &#x3D; x_ t$和 \\mathbf{ x }_ t &#x3D; [x_ { t - \\tau }, \\cdots, x_ { t - 1 }]$。这比我们提供的数据样本少了$\\tau$个，在这里，我们仅使用前600个“特征－标签”对进行训练。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tau = <span class=\"number\">4</span></span><br><span class=\"line\">features = torch.zeros((T - tau, tau)) <span class=\"comment\">#[996, 4]</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(tau):</span><br><span class=\"line\">    features[:, i] = x[i:T - tau+i] <span class=\"comment\">#[996]</span></span><br><span class=\"line\">labels = x[tau:].reshape((-<span class=\"number\">1</span>, <span class=\"number\">1</span>)) <span class=\"comment\">#[996, 1]</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">[x0, x1, x2, x3] -&gt; [x4]</span></span><br><span class=\"line\"><span class=\"string\">[x1, x2, x3, x4] -&gt; [x5]</span></span><br><span class=\"line\"><span class=\"string\">...</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\">batch_size, n_train = <span class=\"number\">16</span>, <span class=\"number\">600</span></span><br><span class=\"line\"><span class=\"comment\"># 只有前n_train个样本用于训练</span></span><br><span class=\"line\">train_iter = d2l.load_array((features[:n_train], labels[:n_train]), batch_size, is_train=<span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>构建一个简单的模型：<ul>\n<li>两个全连接层的多层感知机，ReLU激活函数和平方损失。</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 初始化权重</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_weights</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear:</span><br><span class=\"line\">        nn.init.xavier_uniform_(m.weight)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># mlp</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">get_net</span>():</span><br><span class=\"line\">    net = nn.Sequential(nn.Linear(<span class=\"number\">4</span>,<span class=\"number\">10</span>),</span><br><span class=\"line\">                        nn.ReLU(),</span><br><span class=\"line\">                        nn.Linear(<span class=\"number\">10</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">    net.apply(init_weights)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> net</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 平方损失。MSELoss计算平方误差时不带系数1/2</span></span><br><span class=\"line\">loss = nn.MSELoss(reduction=<span class=\"string\">&#x27;none&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>训练</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train</span>(<span class=\"params\">net, train_iter, loss, epochs, lr</span>):</span><br><span class=\"line\">    trainer = torch.optim.Adam(net.parameters(), lr)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(epochs):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> train_iter:</span><br><span class=\"line\">            trainer.zero_grad()</span><br><span class=\"line\">            l = loss(net(X), y)</span><br><span class=\"line\">            l.<span class=\"built_in\">sum</span>().backward()</span><br><span class=\"line\">            trainer.step()</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;epoch <span class=\"subst\">&#123;epoch+<span class=\"number\">1</span>&#125;</span>,&#x27;</span></span><br><span class=\"line\">              <span class=\"string\">f&#x27;loss:<span class=\"subst\">&#123;d2l.evaluate_loss(net, train_iter, loss):f&#125;</span>&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">net = get_net()</span><br><span class=\"line\">train(net, train_iter, loss, <span class=\"number\">5</span>, <span class=\"number\">0.01</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>epoch 1,loss:0.093809\nepoch 2,loss:0.058604\nepoch 3,loss:0.052532\nepoch 4,loss:0.058827\nepoch 5,loss:0.049325\n</code></pre>\n<h2 id=\"1-3-预测\"><a href=\"#1-3-预测\" class=\"headerlink\" title=\"1.3 预测\"></a>1.3 预测</h2><ul>\n<li>可以看到训练损失很小，因此我们期望模型能有很好的工作效果。单步预测：</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">onestep_preds = net(features)</span><br><span class=\"line\">d2l.plot(</span><br><span class=\"line\">    [time, time[tau:]],</span><br><span class=\"line\">    [x.detach().numpy(), onestep_preds.detach().numpy()], <span class=\"string\">&#x27;time&#x27;</span>,</span><br><span class=\"line\">    <span class=\"string\">&#x27;x&#x27;</span>, legend=[<span class=\"string\">&#x27;data&#x27;</span>, <span class=\"string\">&#x27;1-step preds&#x27;</span>], xlim=[<span class=\"number\">1</span>, <span class=\"number\">1000</span>],</span><br><span class=\"line\">                 figsize=(<span class=\"number\">6</span>, <span class=\"number\">3</span>)</span><br><span class=\"line\">        )</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/1_seqmodel_files/1_seqmodel_9_0.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/7_rnn/1_seqmodel_files/1_seqmodel_9_0.svg\"></p>\n<ul>\n<li>对于直到$x_ t$的观测序列，其在时间步t + k处的预测输出$\\hat x_ {t+k}$称为k步预测。</li>\n<li>由于我们的观察已经到了x604，我们必须使用我们自己的预测（而不是原始数据）来进行多步预测。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">multistep_ppreds = torch.zeros(T)</span><br><span class=\"line\">multistep_ppreds[:n_train+ tau] = x[:n_train+ tau]</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(n_train+ tau, T):</span><br><span class=\"line\">    multistep_ppreds[i] = net(multistep_ppreds[i- tau:i].reshape((<span class=\"number\">1</span>, -<span class=\"number\">1</span>)))</span><br><span class=\"line\"></span><br><span class=\"line\">d2l.plot([time, time[tau:], time[n_train + tau:]],</span><br><span class=\"line\">         [x.detach().numpy(), onestep_preds.detach().numpy(), multistep_ppreds[n_train + tau:].detach().numpy()], <span class=\"string\">&#x27;time&#x27;</span>,</span><br><span class=\"line\">         <span class=\"string\">&#x27;x&#x27;</span>, legend=[<span class=\"string\">&#x27;data&#x27;</span>, <span class=\"string\">&#x27;1-step pred&#x27;</span>, <span class=\"string\">&#x27;multistep preds&#x27;</span>],</span><br><span class=\"line\">         xlim=[<span class=\"number\">1</span>, <span class=\"number\">1000</span>], figsize=(<span class=\"number\">6</span>,<span class=\"number\">3</span>))</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/1_seqmodel_files/1_seqmodel_11_0.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/7_rnn/1_seqmodel_files/1_seqmodel_11_0.svg\"></p>\n<ul>\n<li>绿线的预测显然并不理想。经过几个预测步骤之后，预测的结果很快就会衰减到一个常数。为什么这个算法效果这么差呢？事实是由于错误的累积</li>\n<li>基于k &#x3D; 1, 4, 16, 64，通过对整个序列预测的计算，让我们更仔细地看一下k步预测的困难。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">max_steps = <span class=\"number\">64</span></span><br><span class=\"line\"></span><br><span class=\"line\">features = torch.zeros((T - tau - max_steps + <span class=\"number\">1</span>, tau+max_steps)) <span class=\"comment\">#</span></span><br><span class=\"line\"><span class=\"comment\"># 列i（i&lt;tau）是来自x的观测，其时间步从（i）到（i+T-tau-max_steps+1）\\</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">x0 x1 x2 x3</span></span><br><span class=\"line\"><span class=\"string\">x1 x2 x3 x4</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(tau):</span><br><span class=\"line\">    features[:, i] = x[i: i+T-tau-max_steps+<span class=\"number\">1</span>]</span><br><span class=\"line\"><span class=\"comment\"># 列i（i&gt;=tau）是来自（i-tau+1）步的预测，其时间步从（i）到（i+T-tau-max_steps+1）</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">x4 x5 x6 ... x67</span></span><br><span class=\"line\"><span class=\"string\">x5 x6 x7 ... x68</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(tau, tau+max_steps):</span><br><span class=\"line\">    features[:, i] = net(features[:, i-tau:i]).reshape(-<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">steps = (<span class=\"number\">1</span>, <span class=\"number\">4</span>, <span class=\"number\">16</span>, <span class=\"number\">64</span>)</span><br><span class=\"line\">d2l.plot([time[tau + i - <span class=\"number\">1</span>: T - max_steps + i] <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> steps],</span><br><span class=\"line\">        [features[:, (tau + i - <span class=\"number\">1</span>)].detach().numpy() <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> steps], <span class=\"string\">&#x27;time&#x27;</span>, <span class=\"string\">&#x27;x&#x27;</span>,</span><br><span class=\"line\">        legend=[<span class=\"string\">f&#x27;<span class=\"subst\">&#123;i&#125;</span>-step preds&#x27;</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> steps], xlim=[<span class=\"number\">5</span>, <span class=\"number\">1000</span>],</span><br><span class=\"line\">        figsize=(<span class=\"number\">6</span>, <span class=\"number\">3</span>))</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/1_seqmodel_files/1_seqmodel_13_0.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/7_rnn/1_seqmodel_files/1_seqmodel_13_0.svg\"></p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<ul>\n<li>到目前为止我们默认数据都来自于某种分布，并且所有样本都是独立同分布的。然而，大多数的数据并非如此。例如，文章中的单词是按顺序写的，如果顺序被随机地重排，就很难理解文章原始的意思。</li>\n<li>我们不仅仅可以接收一个序列作为输入，而是还可能期望继续猜测这个序列的后续。</li>\n<li>如果说卷积神经网络可以有效地处理空间信息，那么循环神经网络（recurrent neural network，RNN）则可以更好地处理序列信息。循环神经网络通过引入状态变量存储过去的信息和当前的输入，从而可以确定当前的输出。</li>\n</ul>\n<h1 id=\"1-序列模型\"><a href=\"#1-序列模型\" class=\"headerlink\" title=\"1 序列模型\"></a>1 序列模型</h1><h2 id=\"1-1-统计工具\"><a href=\"#1-1-统计工具\" class=\"headerlink\" title=\"1.1 统计工具\"></a>1.1 统计工具</h2><ul>\n<li>以股票价格（富时100指数）为例。<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/7_rnn/1_seqmodel/1.png\"><br><img src=\"/img/deeplearning/code/pytorch/7_rnn/1_seqmodel/1.png\"><ul>\n<li>在时间步t观察到的价格为$x_ t$。t对于本文中的序列通常是离散的，并在整数或其子集上变化。假设一个交易员想在t日的股市中表现良好，于是通过以下途径预测xt：<br>  $$x_ t ∼ P(x_ t | x_ {t−1}, . . . , x_ 1)$$</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"1-1-1-自回归模型\"><a href=\"#1-1-1-自回归模型\" class=\"headerlink\" title=\"1.1.1 自回归模型\"></a>1.1.1 自回归模型</h3><ul>\n<li><p>输入数据的数量将会随着我们遇到的数据量的增加而增加，因此需要一个近似方法来使这个计算变得容易处理。如何有效估计P(xt | xt−1, . . . , x1),归结为以下两种策略。</p>\n<ol>\n<li><p>假设在现实情况下相当长的序列 xt−1, . . . , x1可能是不必要的，因此我们只需要满足某个长度为$\\tau$的时间跨度。这种模型被称为自回归模型（autoregressive models），因为它们是对自己执行回归。</p>\n</li>\n<li><p>如下图是保留一些对过去观测的总结$h_ t$，并且同时更新预测$\\hat x_ t$的模型和总结$h_ t$。即：基于$\\hat x_ t &#x3D; P(x_ t | h_ t)$估计$x_ t$， 并使用$h_ t &#x3D; g(x_ {t−1}, h_ {t−1})$来更新模型。由于$h _t$从未被观测到，这类模型也被称为 隐变量自回归模型（latent autoregressive models）。</p>\n</li>\n</ol>\n<p>  <img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/7_rnn/1_seqmodel/2.png\"><br>  <img src=\"/img/deeplearning/code/pytorch/7_rnn/1_seqmodel/2.png\"></p>\n</li>\n<li><p>这两种情况都有一个显而易见的问题：如何生成训练数据？可以使用历史观测来预测下一个未来观测。整个序列的估计值都将通过以下的方式获得：<br>$$P(x_ 1, . . . , x_ T) &#x3D; \\prod <em>{t&#x3D;1} ^T P(x</em> t | x_ {t−1}, . . . , x_ 1)$$</p>\n</li>\n</ul>\n<h3 id=\"1-1-2-马尔可夫模型\"><a href=\"#1-1-2-马尔可夫模型\" class=\"headerlink\" title=\"1.1.2 马尔可夫模型\"></a>1.1.2 马尔可夫模型</h3><ul>\n<li><p>在自回归模型的近似法中，我们使用$x_ {t-1},\\cdots,x_ {t - \\tau}$而不是$x_ {t-1},\\cdots,x_ 1$来估计$x_ t$。我们称这个模型为马尔可夫模型。如果$\\tau &#x3D; 1$，那么这个模型就是一阶马尔可夫模型：<br>  $$P(x_ 1, . . . , x_ T) &#x3D; \\prod <em>{t&#x3D;1} ^T P(x</em> t | x_ {t−1})$$</p>\n<p>  $$P(x_ 1 | x_ 0) &#x3D; P(x_ 1)$$</p>\n</li>\n<li><p>假设$x_ t$仅是离散值，使用动态规划可以沿着马尔可夫链精确地计算结果。例如，我们可以高效地计算$P(x_ {t+1} | X_ {t-1})$:<br>$$\\begin{aligned} P(x_ { t + 1 } | x_ { t - 1 }) &amp;&#x3D; \\frac { \\sum <em>{ x</em> t } P( x_ { t + 1 }, x_ t, x_ { t - 1 }) } { \\sum <em>{ x</em> t, x_ { t - 1 } } P( x_ { t + 1 }, x_ t, x_ { t - 1 }) } \\ &amp;&#x3D; \\frac { \\sum <em>{ x</em> t } P( x_ { t + 1 } | x_ t, x_ { t - 1 }) P( x_ t | x_ { t - 1 }) P( x_ { t - 1 }) } { \\sum <em>{ x</em> t } P( x_ { t + 1 } | x_ t, x_ { t - 1 }) P( x_ t | x_ { t - 1 }) P( x_ { t - 1 }) } \\ &amp;&#x3D; \\sum <em>{ x</em> t } P( x_ { t + 1 } | x_ t ) P( x_ t | x_ { t - 1 }) \\end{aligned}$$</p>\n</li>\n<li><p>对$x_ t$求和结果为1，相当于求和符号与$x_ t$可以消掉。</p>\n</li>\n</ul>\n<h3 id=\"1-1-3-因果关系\"><a href=\"#1-1-3-因果关系\" class=\"headerlink\" title=\"1.1.3 因果关系\"></a>1.1.3 因果关系</h3><ul>\n<li>通常，可以将$P(x_ 1, \\cdots, x_ T)$倒序展开：<br>$$P(x_ 1, \\cdots, x_ T) &#x3D; \\prod <em>{t&#x3D;T} ^1 P(x</em> t | x_ {t+1}, \\cdots, x_ T)$$</li>\n<li>如果基于一个马尔可夫模型，我们还可以得到一个反向的条件概率分布。但通常数据存在一个自然的方向，即在时间上是前进的。</li>\n</ul>\n<h2 id=\"1-2-训练\"><a href=\"#1-2-训练\" class=\"headerlink\" title=\"1.2 训练\"></a>1.2 训练</h2><ul>\n<li>使用正弦函数和一些可加性噪声来生成序列数据，时间步为1, 2, . . . , 1000。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>]=<span class=\"string\">&#x27;True&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">T = <span class=\"number\">1000</span> <span class=\"comment\">#产生1000个点</span></span><br><span class=\"line\">time = torch.arange(<span class=\"number\">1</span>, T+<span class=\"number\">1</span>, dtype=torch.float32)</span><br><span class=\"line\">x = torch.sin(<span class=\"number\">0.01</span> * time) + torch.normal(<span class=\"number\">0</span>, <span class=\"number\">0.2</span>, (T,)) <span class=\"comment\">#加入噪声</span></span><br><span class=\"line\">d2l.plot(time, [x], <span class=\"string\">&#x27;time&#x27;</span>, <span class=\"string\">&#x27;x&#x27;</span>, xlim=[<span class=\"number\">1</span>, <span class=\"number\">1000</span>], figsize=(<span class=\"number\">6</span>, <span class=\"number\">3</span>))</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/1_seqmodel_files/1_seqmodel_1_0.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/7_rnn/1_seqmodel_files/1_seqmodel_1_0.svg\"></p>\n<ul>\n<li>将这个序列转换为模型的特征－标签（feature‐label）对。将数据映射为数据对$y_ t &#x3D; x_ t$和 \\mathbf{ x }_ t &#x3D; [x_ { t - \\tau }, \\cdots, x_ { t - 1 }]$。这比我们提供的数据样本少了$\\tau$个，在这里，我们仅使用前600个“特征－标签”对进行训练。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tau = <span class=\"number\">4</span></span><br><span class=\"line\">features = torch.zeros((T - tau, tau)) <span class=\"comment\">#[996, 4]</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(tau):</span><br><span class=\"line\">    features[:, i] = x[i:T - tau+i] <span class=\"comment\">#[996]</span></span><br><span class=\"line\">labels = x[tau:].reshape((-<span class=\"number\">1</span>, <span class=\"number\">1</span>)) <span class=\"comment\">#[996, 1]</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">[x0, x1, x2, x3] -&gt; [x4]</span></span><br><span class=\"line\"><span class=\"string\">[x1, x2, x3, x4] -&gt; [x5]</span></span><br><span class=\"line\"><span class=\"string\">...</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\">batch_size, n_train = <span class=\"number\">16</span>, <span class=\"number\">600</span></span><br><span class=\"line\"><span class=\"comment\"># 只有前n_train个样本用于训练</span></span><br><span class=\"line\">train_iter = d2l.load_array((features[:n_train], labels[:n_train]), batch_size, is_train=<span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>构建一个简单的模型：<ul>\n<li>两个全连接层的多层感知机，ReLU激活函数和平方损失。</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 初始化权重</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_weights</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear:</span><br><span class=\"line\">        nn.init.xavier_uniform_(m.weight)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># mlp</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">get_net</span>():</span><br><span class=\"line\">    net = nn.Sequential(nn.Linear(<span class=\"number\">4</span>,<span class=\"number\">10</span>),</span><br><span class=\"line\">                        nn.ReLU(),</span><br><span class=\"line\">                        nn.Linear(<span class=\"number\">10</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">    net.apply(init_weights)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> net</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 平方损失。MSELoss计算平方误差时不带系数1/2</span></span><br><span class=\"line\">loss = nn.MSELoss(reduction=<span class=\"string\">&#x27;none&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>训练</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train</span>(<span class=\"params\">net, train_iter, loss, epochs, lr</span>):</span><br><span class=\"line\">    trainer = torch.optim.Adam(net.parameters(), lr)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(epochs):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> train_iter:</span><br><span class=\"line\">            trainer.zero_grad()</span><br><span class=\"line\">            l = loss(net(X), y)</span><br><span class=\"line\">            l.<span class=\"built_in\">sum</span>().backward()</span><br><span class=\"line\">            trainer.step()</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;epoch <span class=\"subst\">&#123;epoch+<span class=\"number\">1</span>&#125;</span>,&#x27;</span></span><br><span class=\"line\">              <span class=\"string\">f&#x27;loss:<span class=\"subst\">&#123;d2l.evaluate_loss(net, train_iter, loss):f&#125;</span>&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">net = get_net()</span><br><span class=\"line\">train(net, train_iter, loss, <span class=\"number\">5</span>, <span class=\"number\">0.01</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>epoch 1,loss:0.093809\nepoch 2,loss:0.058604\nepoch 3,loss:0.052532\nepoch 4,loss:0.058827\nepoch 5,loss:0.049325\n</code></pre>\n<h2 id=\"1-3-预测\"><a href=\"#1-3-预测\" class=\"headerlink\" title=\"1.3 预测\"></a>1.3 预测</h2><ul>\n<li>可以看到训练损失很小，因此我们期望模型能有很好的工作效果。单步预测：</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">onestep_preds = net(features)</span><br><span class=\"line\">d2l.plot(</span><br><span class=\"line\">    [time, time[tau:]],</span><br><span class=\"line\">    [x.detach().numpy(), onestep_preds.detach().numpy()], <span class=\"string\">&#x27;time&#x27;</span>,</span><br><span class=\"line\">    <span class=\"string\">&#x27;x&#x27;</span>, legend=[<span class=\"string\">&#x27;data&#x27;</span>, <span class=\"string\">&#x27;1-step preds&#x27;</span>], xlim=[<span class=\"number\">1</span>, <span class=\"number\">1000</span>],</span><br><span class=\"line\">                 figsize=(<span class=\"number\">6</span>, <span class=\"number\">3</span>)</span><br><span class=\"line\">        )</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/1_seqmodel_files/1_seqmodel_9_0.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/7_rnn/1_seqmodel_files/1_seqmodel_9_0.svg\"></p>\n<ul>\n<li>对于直到$x_ t$的观测序列，其在时间步t + k处的预测输出$\\hat x_ {t+k}$称为k步预测。</li>\n<li>由于我们的观察已经到了x604，我们必须使用我们自己的预测（而不是原始数据）来进行多步预测。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">multistep_ppreds = torch.zeros(T)</span><br><span class=\"line\">multistep_ppreds[:n_train+ tau] = x[:n_train+ tau]</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(n_train+ tau, T):</span><br><span class=\"line\">    multistep_ppreds[i] = net(multistep_ppreds[i- tau:i].reshape((<span class=\"number\">1</span>, -<span class=\"number\">1</span>)))</span><br><span class=\"line\"></span><br><span class=\"line\">d2l.plot([time, time[tau:], time[n_train + tau:]],</span><br><span class=\"line\">         [x.detach().numpy(), onestep_preds.detach().numpy(), multistep_ppreds[n_train + tau:].detach().numpy()], <span class=\"string\">&#x27;time&#x27;</span>,</span><br><span class=\"line\">         <span class=\"string\">&#x27;x&#x27;</span>, legend=[<span class=\"string\">&#x27;data&#x27;</span>, <span class=\"string\">&#x27;1-step pred&#x27;</span>, <span class=\"string\">&#x27;multistep preds&#x27;</span>],</span><br><span class=\"line\">         xlim=[<span class=\"number\">1</span>, <span class=\"number\">1000</span>], figsize=(<span class=\"number\">6</span>,<span class=\"number\">3</span>))</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/1_seqmodel_files/1_seqmodel_11_0.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/7_rnn/1_seqmodel_files/1_seqmodel_11_0.svg\"></p>\n<ul>\n<li>绿线的预测显然并不理想。经过几个预测步骤之后，预测的结果很快就会衰减到一个常数。为什么这个算法效果这么差呢？事实是由于错误的累积</li>\n<li>基于k &#x3D; 1, 4, 16, 64，通过对整个序列预测的计算，让我们更仔细地看一下k步预测的困难。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">max_steps = <span class=\"number\">64</span></span><br><span class=\"line\"></span><br><span class=\"line\">features = torch.zeros((T - tau - max_steps + <span class=\"number\">1</span>, tau+max_steps)) <span class=\"comment\">#</span></span><br><span class=\"line\"><span class=\"comment\"># 列i（i&lt;tau）是来自x的观测，其时间步从（i）到（i+T-tau-max_steps+1）\\</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">x0 x1 x2 x3</span></span><br><span class=\"line\"><span class=\"string\">x1 x2 x3 x4</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(tau):</span><br><span class=\"line\">    features[:, i] = x[i: i+T-tau-max_steps+<span class=\"number\">1</span>]</span><br><span class=\"line\"><span class=\"comment\"># 列i（i&gt;=tau）是来自（i-tau+1）步的预测，其时间步从（i）到（i+T-tau-max_steps+1）</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">x4 x5 x6 ... x67</span></span><br><span class=\"line\"><span class=\"string\">x5 x6 x7 ... x68</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(tau, tau+max_steps):</span><br><span class=\"line\">    features[:, i] = net(features[:, i-tau:i]).reshape(-<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">steps = (<span class=\"number\">1</span>, <span class=\"number\">4</span>, <span class=\"number\">16</span>, <span class=\"number\">64</span>)</span><br><span class=\"line\">d2l.plot([time[tau + i - <span class=\"number\">1</span>: T - max_steps + i] <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> steps],</span><br><span class=\"line\">        [features[:, (tau + i - <span class=\"number\">1</span>)].detach().numpy() <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> steps], <span class=\"string\">&#x27;time&#x27;</span>, <span class=\"string\">&#x27;x&#x27;</span>,</span><br><span class=\"line\">        legend=[<span class=\"string\">f&#x27;<span class=\"subst\">&#123;i&#125;</span>-step preds&#x27;</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> steps], xlim=[<span class=\"number\">5</span>, <span class=\"number\">1000</span>],</span><br><span class=\"line\">        figsize=(<span class=\"number\">6</span>, <span class=\"number\">3</span>))</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/1_seqmodel_files/1_seqmodel_13_0.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/7_rnn/1_seqmodel_files/1_seqmodel_13_0.svg\"></p>"},{"title":"7.3 语言模型和数据集","date":"2024-05-02T03:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 3 语言模型和数据集\n- 假设长度为T的文本序列中的词元依次为$x_ 1$,$x_ 2$,...,$x_ T$。$x_ t$可以被认为是文本序列在时间步t处的观测或标签。语言模型（language model）的目标是估计序列的联合概率：\n$$P(x_ 1,x_ 2,...,x_ T)$$\n- 只需要一次抽取一个词元$x_t ~ P(x_t|x_ {t-1},...,x_ 1)$，一个理想的语言模型就能够基于模型本身生成自然文本。只需要基于前面的对话片断中的文本，就足以生成一个有意义的对话。\n## 3.1 学习语言模型\n- 我们面对的问题是如何对一个文档，甚至是一个词元序列进行建模。\n$$P(x_ 1,x_ 2,...,x_ T) = \\prod_ { t = 1 }^ T P(x_ t|x_ 1,x_ 2,...,x_ {t-1})$$\n\n- 例如，包含了四个单词的一个文本序列的概率是：\n$$P(deep, learning, is, fun) = P(deep)P(learning|deep)P(is|deep, learning)P(fun|deep, learning, is)$$\n\n\n- 为了训练语言模型，我们需要计算单词的概率，以及给定前面几个单词后出现某个单词的条件概率。这些概率本质上就是语言模型的参数。\n- 假设训练数据集是一个大型的文本语料库。训练数据集中词的概率可以根据给定词的相对词频来计算。例如，可以将估计值$\\hat P(deep)$计算为任何以单词“deep”开头的句子的概率，或“deep”在数据集中的出现次数。接下来，我们可以尝试估计：\n$$\\hat P(learning|deep) = \\frac{n(deep, learning)}{n(deep)}$$\n\n- 其中$n(deep,learning)$是' deep learning'连续出现的次数。由于连续单词对“deep learning”的出现频率要低得多，所以估计这类单词正确的概率要困难得多。许多合理的三个单词组合可能是存在的，但是在数据集中却找不到。\n- 一种常见的策略是执行某种形式的拉普拉斯平滑（Laplace smoothing）：在所有计数中添加一个小常量。用n表示训练集中的单词总数，用m表示唯一单词的数量。此解决方案有助于处理单元素问题：\n$$\\begin{aligned} \\hat P(x) & = \\frac{n(x) + \\frac{ \\epsilon_1 }{m}}{n + \\epsilon_1} \\\\ \\hat P(x'|x) & = \\frac{ n(x,x' ) + \\epsilon_2 \\hat P(x') }{n(x) + \\epsilon_2} \\\\ \\hat P(x''|x,x') & = \\frac{ n(x,x',x'') + \\epsilon_3 \\hat P(x'') }{n(x,x') + \\epsilon_3} \\end{aligned}$$\n\n- 其中$\\epsilon$是一个超参数，当$\\epsilon = 0$时，不应用平滑；当$\\epsilon$接近正无穷时，$\\hat P(x)$接近均匀分布 $\\frac{1}{m}$。\n- 这样的模型很容易变得无效，首先，我们需要存储所有的计数；其次，这完全忽略了单词的意思。例如，“猫”（cat）和“猫科动物”（feline）可能出现在相关的上下文中，但是想根据上下文调整这类模型其实是相当困难的。最后，长单词序列大部分是没出现过的，因此一个模型如果只是简单地统计先前“看到”的单词序列频率，那么模型面对这种问题肯定是表现不佳的。\n\n## 3.2 马尔可夫模型与n元语法\n- 如果$P(x_ {t+1} | X_ t, \\cdots, x_ 1) = P(x_ {t+1} | x_ t)$,则序列上的分布满足一阶马尔可夫性质。阶数越高，对应的依赖关系就越长。这种性质推导出了许多可以应用于序列建模的近似公式：\n$$\\begin{aligned} P(x_ 1,x_ 2,x_ 3,x_ 4) & = P(x_ 1)P(x_ 2)P(x_ 3)P(x_ 4|x_ 3) \\\\ P(x_ 1,x_ 2,x_ 3,x_ 4) & = P(x_ 1)P(x_ 2|x_ 1)P(x_ 3|x_ 2)P(x_ 4|x_ 3) \\\\ P(x_ 1,x_ 2,x_ 3,x_ 4) & = P(x_ 1)P(x_ 2|x_ 1)P(x_ 3|x_ 1,x_ 2)P(x_ 4|x_ 2,x_ 3) \\end{aligned}$$\n\n- 涉及一个、两个和三个变量的概率公式分别被称为 一元语法（unigram）、二元语法（bigram）和三元语法（trigram）模型。\n\n## 3.3 自然语言统计\n- 在真实数据上如果进行自然语言统计。根据数据集构建词表，并打印前10个最常用的（频率最高的）单词。\n\n\n```python\nimport random\nimport torch\nfrom d2l import  torch as d2l\n\ntokens = d2l.tokenize(d2l.read_time_machine())\n# 因为每个文本行不一定是一个句子或一个段落，因此我们把所有文本行拼接到一起\ncorpus = [token for line in tokens for token in line]\nvocab = d2l.Vocab(corpus)\nvocab.token_freqs[:10]\n```\n\n\n\n\n    [('the', 2261),\n     ('i', 1267),\n     ('and', 1245),\n     ('of', 1155),\n     ('a', 816),\n     ('to', 695),\n     ('was', 552),\n     ('in', 541),\n     ('that', 443),\n     ('my', 440)]\n\n\n\n- 最流行的词看起来很无聊，这些词通常被称为停用词（stop words），因此可以被过滤掉。还有个明显的问题是词频衰减的速度相当地快。例如，最常用单词的词频对比，第10个还不到第1个的1/5。\n- 词频图：\n\n\n```python\nimport os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\nfreqs = [freq for token, freq in vocab.token_freqs]\nd2l.plot(freqs, xlabel='token: x', ylabel='frequency: n(x)', xscale='log', yscale='log')\n```\n\n\n    \n![svg](3_lmdataset_files/3_lmdataset_3_0.svg)\n![](img/deeplearning/code/pytorch/7_rnn/3_lmdataset_files/3_lmdataset_3_0.svg)\n\n\n- 将前几个单词作为例外消除后，剩余的所有单词大致遵循双对数坐标图上的一条直线。这意味着单词的频率满足齐普夫定律即第i个最常用单词的频率ni为：\n$$ n_ i \\propto \\frac{1}{i^ \\alpha}$$\n\n- 等价于：\n$$ \\log(n_ i) = - \\alpha \\log(i) + c$$\n\n- 这告诉我们想要通过计数统计和平滑来建模单词是不可行的，因为这样建模的结果会大大高估尾部单词的频率，也就是所谓的不常用单词。\n- 我们来看看二元语法的频率是否与一元语法的频率表现出相同的行为方式。\n\n\n```python\nbigram_tokens = [pair for pair in zip(corpus[:-1], corpus[1:])] #[0,1,2] + [1,2,3] = [(0,1),(1,2),(2,3)]\nbigram_vocab = d2l.Vocab(bigram_tokens)\nbigram_vocab.token_freqs[:10]\n```\n\n\n\n\n    [(('of', 'the'), 309),\n     (('in', 'the'), 169),\n     (('i', 'had'), 130),\n     (('i', 'was'), 112),\n     (('and', 'the'), 109),\n     (('the', 'time'), 102),\n     (('it', 'was'), 99),\n     (('to', 'the'), 85),\n     (('as', 'i'), 78),\n     (('of', 'a'), 73)]\n\n\n\n- 发现在十个最频繁的词对中，有九个是由两个停用词组成的，只有一个与“the time”有关。\n- 三元语法：\n\n\n```python\ntrigram_tokens = [triple for triple in zip(corpus[:-2], corpus[1:-1], corpus[2:])]\ntrigram_vocab = d2l.Vocab(trigram_tokens)\ntrigram_vocab.token_freqs[:10]\n```\n\n\n\n\n    [(('the', 'time', 'traveller'), 59),\n     (('the', 'time', 'machine'), 30),\n     (('the', 'medical', 'man'), 24),\n     (('it', 'seemed', 'to'), 16),\n     (('it', 'was', 'a'), 15),\n     (('here', 'and', 'there'), 15),\n     (('seemed', 'to', 'me'), 14),\n     (('i', 'did', 'not'), 14),\n     (('i', 'saw', 'the'), 13),\n     (('i', 'began', 'to'), 13)]\n\n\n\n- 直观地对比三种模型中的词元频率：一元语法、二元语法和三元语法。\n\n\n```python\nbigram_freqs = [freq for token, freq in bigram_vocab.token_freqs]\ntrigram_freqs = [freq for token, freq in trigram_vocab.token_freqs]\nd2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel='token: x', \n         ylabel='frequency: n(x)', xscale='log', yscale='log',\n         legend=['unigram', 'bigram', 'trigram'])\n```\n\n\n    \n![svg](3_lmdataset_files/3_lmdataset_9_0.svg)\n![](img/deeplearning/code/pytorch/7_rnn/3_lmdataset_files/3_lmdataset_9_0.svg)\n\n\n- 我们发现：\n    - 除了一元语法词，单词序列似乎也遵循齐普夫定律\n    - 词表中n元组的数量并没有那么大，这说明语言中存在相当多的结构，这些结构给了我们应用模型的希望\n    - 很多n元组很少出现，这使得拉普拉斯平滑非常不适合语言建模。作为代替，我们将使用基于深度学习的模型。\n\n## 3.4 读取长序列数据\n- 由于序列数据本质上是连续的，因此我们在处理数据时需要解决这个问题。一种方法：当序列变得太长而不能被模型一次性全部处理时，我们可能希望拆分这样的序列方便模型读取。\n- 总体策略：\n    - 假设我们将使用神经网络来训练语言模型，模型中的网络一次处理具有预定义长度（例如n个时间步）的一个小批量序列。现在的问题是如何随机生成一个小批量数据的特征和标签以供读取。\n    - 由于文本序列可以是任意长的，任意长的序列可以被我们划分为具有相同时间步数的子序列。训练时，这样的小批量子序列将被输入到模型中。如下图n=5，每个时间步的词元对应于一个字符，可以选择任意偏移量来指示初始位置\n    ![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/7_rnn/3_lmdataset/1.png)\n    ![](img/deeplearning/code/pytorch/7_rnn/3_lmdataset/1.png)\n    - 选择图中的哪一个呢？都差不多。如果我们只选择一个偏移量，那么用于训练网络的、所有可能的子序列的覆盖范围将是有限的。因此，我们可以从随机偏移量开始划分序列，以同时获得覆盖性（coverage）和随机性（randomness）。如何实现随机采样（random sampling）和顺序分区（sequential partitioning）策略。\n\n### 3.4.1 随机采样\n- 对于语言建模，目标是基于到目前为止我们看到的词元来预测下一个词元，因此标签是移位了一个词元的原始序列。\n- 每次从数据中随机生成一个小批量。batch_size指定了每个小批量中子序列样本的数目，num_steps是每个子序列中预定义的时间步数(n)。\n\n\n\n```python\ndef seq_data_iter_random(corpus, batch_size, num_steps): #@save\n    '''使用随机抽样生成一个小批量子序列'''\n    # 从随机偏移量开始对序列进行分区，随机范围包括num_steps -1\n    corpus = corpus[random.randint(0, num_steps -1):]\n    # 减去1，是因为我们需要考虑标签\n    num_subseqs = (len(corpus) -1) // num_steps # 多少个子序列\n    # 长度为num_steps的子序列的起始索引\n    initial_indices = list(range(0, num_subseqs*num_steps, num_steps))\n    # 在随机抽样的迭代过程中，来自两个相邻的、随机的、小批量的子序列不一定在原始序列上相邻\n    random.shuffle(initial_indices)\n\n    def data(pos):\n        # 返回从pos开始的长度为num_steps的序列\n        return corpus[pos: pos + num_steps]\n    \n    num_batches = num_subseqs // batch_size # 将整个数据集分成多少个batch\n    for i in range(0, batch_size*num_batches, batch_size):\n        # initial_indices包含子序列的随机起始索引\n        initial_indices_per_batch = initial_indices[i: i + batch_size] # 每个batch的起始索引\n        X = [data(j) for j in initial_indices_per_batch] # 获得batch_size个子序列\n        Y = [data(j + 1) for j in initial_indices_per_batch]\n        yield torch.tensor(X), torch.tensor(Y)\n```\n\n- 生成一个从0到34的序列。batch_size=2，num_steps=5。可以生成(35-1)/5=6个子序列对，6/2=3个小批量。\n\n\n```python\nmy_seq = list(range(35))\nfor X,Y in seq_data_iter_random(my_seq, batch_size=2, num_steps=5):\n    print('X: ', X, '\\nY: ', Y)\n```\n\n    X:  tensor([[ 7,  8,  9, 10, 11],\n            [27, 28, 29, 30, 31]]) \n    Y:  tensor([[ 8,  9, 10, 11, 12],\n            [28, 29, 30, 31, 32]])\n    X:  tensor([[12, 13, 14, 15, 16],\n            [ 2,  3,  4,  5,  6]]) \n    Y:  tensor([[13, 14, 15, 16, 17],\n            [ 3,  4,  5,  6,  7]])\n    X:  tensor([[22, 23, 24, 25, 26],\n            [17, 18, 19, 20, 21]]) \n    Y:  tensor([[23, 24, 25, 26, 27],\n            [18, 19, 20, 21, 22]])\n    \n\n### 3.4.2 顺序分区\n- 迭代过程中，除了对原始序列可以随机抽样外，我们还可以保证两个相邻的小批量中的子序列在原始序列上也是相邻的。\n\n\n```python\ndef seq_data_iter_sequential(corpus, batch_size, num_steps): #@save\n    '''使用顺序分区生成一个小批量子序列'''\n    # 从随机偏移量开始划分序列\n    offset = random.randint(0, num_steps)\n    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size # 可用于训练的tokens数量\n    Xs = torch.tensor(corpus[offset: offset + num_tokens])\n    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])\n    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\n    print('Xs：',Xs)\n    num_batches = Xs.shape[1] // num_steps # batch_size是子序列个数，不是token个数\n    for i in range(0, num_steps*num_batches, num_steps):\n        X = Xs[:, i: i+num_steps] #一个batch里面包含token数：num_steps*batch_size\n        Y = Ys[:, i: i+num_steps]\n        yield X, Y\n\nfor X,Y in seq_data_iter_sequential(my_seq, batch_size=2, num_steps=5):\n    print('X: ', X, '\\nY: ', Y)\n```\n\n    Xs： tensor([[ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17],\n            [18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]])\n    X:  tensor([[ 2,  3,  4,  5,  6],\n            [18, 19, 20, 21, 22]]) \n    Y:  tensor([[ 3,  4,  5,  6,  7],\n            [19, 20, 21, 22, 23]])\n    X:  tensor([[ 7,  8,  9, 10, 11],\n            [23, 24, 25, 26, 27]]) \n    Y:  tensor([[ 8,  9, 10, 11, 12],\n            [24, 25, 26, 27, 28]])\n    X:  tensor([[12, 13, 14, 15, 16],\n            [28, 29, 30, 31, 32]]) \n    Y:  tensor([[13, 14, 15, 16, 17],\n            [29, 30, 31, 32, 33]])\n    \n\n- 将两个采样函数包装到一个类中，以便稍后可以将其用作数据迭代器。\n\n\n```python\nclass SeqDataLoader: #@save\n    '''加载序列数据的迭代器'''\n    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):\n        if use_random_iter:\n            self.data_iter_fn = d2l.seq_data_iter_random\n        else:\n            self.data_iter_fn = d2l.seq_data_iter_sequential\n        self.corpus, self.vocab = d2l.load_corpus_time_machine(max_tokens)\n        self.batch_size, self.num_steps = batch_size, num_steps\n    \n    def __iter__(self):\n        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)\n    \n# 定义函数，同时返回数据迭代器和词表，因此可以与其他带有load_data前缀的函数（如d2l.load_data_fashion_mnist）类似地使用。\ndef load_data_time_machine(batch_size, num_steps, #@save\n                           use_random_iter=False, max_tokens=10000):\n    '''返回时光机数据集的迭代器和词表'''\n    data_iter = SeqDataLoader(batch_size, num_steps, use_random_iter, max_tokens)\n```\n","source":"_posts/deeplearning/code/pytorch/7_rnn/3_lmdataset.md","raw":"---\ntitle: 7.3 语言模型和数据集\ndate: 2024-5-2 11:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 3 语言模型和数据集\n- 假设长度为T的文本序列中的词元依次为$x_ 1$,$x_ 2$,...,$x_ T$。$x_ t$可以被认为是文本序列在时间步t处的观测或标签。语言模型（language model）的目标是估计序列的联合概率：\n$$P(x_ 1,x_ 2,...,x_ T)$$\n- 只需要一次抽取一个词元$x_t ~ P(x_t|x_ {t-1},...,x_ 1)$，一个理想的语言模型就能够基于模型本身生成自然文本。只需要基于前面的对话片断中的文本，就足以生成一个有意义的对话。\n## 3.1 学习语言模型\n- 我们面对的问题是如何对一个文档，甚至是一个词元序列进行建模。\n$$P(x_ 1,x_ 2,...,x_ T) = \\prod_ { t = 1 }^ T P(x_ t|x_ 1,x_ 2,...,x_ {t-1})$$\n\n- 例如，包含了四个单词的一个文本序列的概率是：\n$$P(deep, learning, is, fun) = P(deep)P(learning|deep)P(is|deep, learning)P(fun|deep, learning, is)$$\n\n\n- 为了训练语言模型，我们需要计算单词的概率，以及给定前面几个单词后出现某个单词的条件概率。这些概率本质上就是语言模型的参数。\n- 假设训练数据集是一个大型的文本语料库。训练数据集中词的概率可以根据给定词的相对词频来计算。例如，可以将估计值$\\hat P(deep)$计算为任何以单词“deep”开头的句子的概率，或“deep”在数据集中的出现次数。接下来，我们可以尝试估计：\n$$\\hat P(learning|deep) = \\frac{n(deep, learning)}{n(deep)}$$\n\n- 其中$n(deep,learning)$是' deep learning'连续出现的次数。由于连续单词对“deep learning”的出现频率要低得多，所以估计这类单词正确的概率要困难得多。许多合理的三个单词组合可能是存在的，但是在数据集中却找不到。\n- 一种常见的策略是执行某种形式的拉普拉斯平滑（Laplace smoothing）：在所有计数中添加一个小常量。用n表示训练集中的单词总数，用m表示唯一单词的数量。此解决方案有助于处理单元素问题：\n$$\\begin{aligned} \\hat P(x) & = \\frac{n(x) + \\frac{ \\epsilon_1 }{m}}{n + \\epsilon_1} \\\\ \\hat P(x'|x) & = \\frac{ n(x,x' ) + \\epsilon_2 \\hat P(x') }{n(x) + \\epsilon_2} \\\\ \\hat P(x''|x,x') & = \\frac{ n(x,x',x'') + \\epsilon_3 \\hat P(x'') }{n(x,x') + \\epsilon_3} \\end{aligned}$$\n\n- 其中$\\epsilon$是一个超参数，当$\\epsilon = 0$时，不应用平滑；当$\\epsilon$接近正无穷时，$\\hat P(x)$接近均匀分布 $\\frac{1}{m}$。\n- 这样的模型很容易变得无效，首先，我们需要存储所有的计数；其次，这完全忽略了单词的意思。例如，“猫”（cat）和“猫科动物”（feline）可能出现在相关的上下文中，但是想根据上下文调整这类模型其实是相当困难的。最后，长单词序列大部分是没出现过的，因此一个模型如果只是简单地统计先前“看到”的单词序列频率，那么模型面对这种问题肯定是表现不佳的。\n\n## 3.2 马尔可夫模型与n元语法\n- 如果$P(x_ {t+1} | X_ t, \\cdots, x_ 1) = P(x_ {t+1} | x_ t)$,则序列上的分布满足一阶马尔可夫性质。阶数越高，对应的依赖关系就越长。这种性质推导出了许多可以应用于序列建模的近似公式：\n$$\\begin{aligned} P(x_ 1,x_ 2,x_ 3,x_ 4) & = P(x_ 1)P(x_ 2)P(x_ 3)P(x_ 4|x_ 3) \\\\ P(x_ 1,x_ 2,x_ 3,x_ 4) & = P(x_ 1)P(x_ 2|x_ 1)P(x_ 3|x_ 2)P(x_ 4|x_ 3) \\\\ P(x_ 1,x_ 2,x_ 3,x_ 4) & = P(x_ 1)P(x_ 2|x_ 1)P(x_ 3|x_ 1,x_ 2)P(x_ 4|x_ 2,x_ 3) \\end{aligned}$$\n\n- 涉及一个、两个和三个变量的概率公式分别被称为 一元语法（unigram）、二元语法（bigram）和三元语法（trigram）模型。\n\n## 3.3 自然语言统计\n- 在真实数据上如果进行自然语言统计。根据数据集构建词表，并打印前10个最常用的（频率最高的）单词。\n\n\n```python\nimport random\nimport torch\nfrom d2l import  torch as d2l\n\ntokens = d2l.tokenize(d2l.read_time_machine())\n# 因为每个文本行不一定是一个句子或一个段落，因此我们把所有文本行拼接到一起\ncorpus = [token for line in tokens for token in line]\nvocab = d2l.Vocab(corpus)\nvocab.token_freqs[:10]\n```\n\n\n\n\n    [('the', 2261),\n     ('i', 1267),\n     ('and', 1245),\n     ('of', 1155),\n     ('a', 816),\n     ('to', 695),\n     ('was', 552),\n     ('in', 541),\n     ('that', 443),\n     ('my', 440)]\n\n\n\n- 最流行的词看起来很无聊，这些词通常被称为停用词（stop words），因此可以被过滤掉。还有个明显的问题是词频衰减的速度相当地快。例如，最常用单词的词频对比，第10个还不到第1个的1/5。\n- 词频图：\n\n\n```python\nimport os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\nfreqs = [freq for token, freq in vocab.token_freqs]\nd2l.plot(freqs, xlabel='token: x', ylabel='frequency: n(x)', xscale='log', yscale='log')\n```\n\n\n    \n![svg](3_lmdataset_files/3_lmdataset_3_0.svg)\n![](img/deeplearning/code/pytorch/7_rnn/3_lmdataset_files/3_lmdataset_3_0.svg)\n\n\n- 将前几个单词作为例外消除后，剩余的所有单词大致遵循双对数坐标图上的一条直线。这意味着单词的频率满足齐普夫定律即第i个最常用单词的频率ni为：\n$$ n_ i \\propto \\frac{1}{i^ \\alpha}$$\n\n- 等价于：\n$$ \\log(n_ i) = - \\alpha \\log(i) + c$$\n\n- 这告诉我们想要通过计数统计和平滑来建模单词是不可行的，因为这样建模的结果会大大高估尾部单词的频率，也就是所谓的不常用单词。\n- 我们来看看二元语法的频率是否与一元语法的频率表现出相同的行为方式。\n\n\n```python\nbigram_tokens = [pair for pair in zip(corpus[:-1], corpus[1:])] #[0,1,2] + [1,2,3] = [(0,1),(1,2),(2,3)]\nbigram_vocab = d2l.Vocab(bigram_tokens)\nbigram_vocab.token_freqs[:10]\n```\n\n\n\n\n    [(('of', 'the'), 309),\n     (('in', 'the'), 169),\n     (('i', 'had'), 130),\n     (('i', 'was'), 112),\n     (('and', 'the'), 109),\n     (('the', 'time'), 102),\n     (('it', 'was'), 99),\n     (('to', 'the'), 85),\n     (('as', 'i'), 78),\n     (('of', 'a'), 73)]\n\n\n\n- 发现在十个最频繁的词对中，有九个是由两个停用词组成的，只有一个与“the time”有关。\n- 三元语法：\n\n\n```python\ntrigram_tokens = [triple for triple in zip(corpus[:-2], corpus[1:-1], corpus[2:])]\ntrigram_vocab = d2l.Vocab(trigram_tokens)\ntrigram_vocab.token_freqs[:10]\n```\n\n\n\n\n    [(('the', 'time', 'traveller'), 59),\n     (('the', 'time', 'machine'), 30),\n     (('the', 'medical', 'man'), 24),\n     (('it', 'seemed', 'to'), 16),\n     (('it', 'was', 'a'), 15),\n     (('here', 'and', 'there'), 15),\n     (('seemed', 'to', 'me'), 14),\n     (('i', 'did', 'not'), 14),\n     (('i', 'saw', 'the'), 13),\n     (('i', 'began', 'to'), 13)]\n\n\n\n- 直观地对比三种模型中的词元频率：一元语法、二元语法和三元语法。\n\n\n```python\nbigram_freqs = [freq for token, freq in bigram_vocab.token_freqs]\ntrigram_freqs = [freq for token, freq in trigram_vocab.token_freqs]\nd2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel='token: x', \n         ylabel='frequency: n(x)', xscale='log', yscale='log',\n         legend=['unigram', 'bigram', 'trigram'])\n```\n\n\n    \n![svg](3_lmdataset_files/3_lmdataset_9_0.svg)\n![](img/deeplearning/code/pytorch/7_rnn/3_lmdataset_files/3_lmdataset_9_0.svg)\n\n\n- 我们发现：\n    - 除了一元语法词，单词序列似乎也遵循齐普夫定律\n    - 词表中n元组的数量并没有那么大，这说明语言中存在相当多的结构，这些结构给了我们应用模型的希望\n    - 很多n元组很少出现，这使得拉普拉斯平滑非常不适合语言建模。作为代替，我们将使用基于深度学习的模型。\n\n## 3.4 读取长序列数据\n- 由于序列数据本质上是连续的，因此我们在处理数据时需要解决这个问题。一种方法：当序列变得太长而不能被模型一次性全部处理时，我们可能希望拆分这样的序列方便模型读取。\n- 总体策略：\n    - 假设我们将使用神经网络来训练语言模型，模型中的网络一次处理具有预定义长度（例如n个时间步）的一个小批量序列。现在的问题是如何随机生成一个小批量数据的特征和标签以供读取。\n    - 由于文本序列可以是任意长的，任意长的序列可以被我们划分为具有相同时间步数的子序列。训练时，这样的小批量子序列将被输入到模型中。如下图n=5，每个时间步的词元对应于一个字符，可以选择任意偏移量来指示初始位置\n    ![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/7_rnn/3_lmdataset/1.png)\n    ![](img/deeplearning/code/pytorch/7_rnn/3_lmdataset/1.png)\n    - 选择图中的哪一个呢？都差不多。如果我们只选择一个偏移量，那么用于训练网络的、所有可能的子序列的覆盖范围将是有限的。因此，我们可以从随机偏移量开始划分序列，以同时获得覆盖性（coverage）和随机性（randomness）。如何实现随机采样（random sampling）和顺序分区（sequential partitioning）策略。\n\n### 3.4.1 随机采样\n- 对于语言建模，目标是基于到目前为止我们看到的词元来预测下一个词元，因此标签是移位了一个词元的原始序列。\n- 每次从数据中随机生成一个小批量。batch_size指定了每个小批量中子序列样本的数目，num_steps是每个子序列中预定义的时间步数(n)。\n\n\n\n```python\ndef seq_data_iter_random(corpus, batch_size, num_steps): #@save\n    '''使用随机抽样生成一个小批量子序列'''\n    # 从随机偏移量开始对序列进行分区，随机范围包括num_steps -1\n    corpus = corpus[random.randint(0, num_steps -1):]\n    # 减去1，是因为我们需要考虑标签\n    num_subseqs = (len(corpus) -1) // num_steps # 多少个子序列\n    # 长度为num_steps的子序列的起始索引\n    initial_indices = list(range(0, num_subseqs*num_steps, num_steps))\n    # 在随机抽样的迭代过程中，来自两个相邻的、随机的、小批量的子序列不一定在原始序列上相邻\n    random.shuffle(initial_indices)\n\n    def data(pos):\n        # 返回从pos开始的长度为num_steps的序列\n        return corpus[pos: pos + num_steps]\n    \n    num_batches = num_subseqs // batch_size # 将整个数据集分成多少个batch\n    for i in range(0, batch_size*num_batches, batch_size):\n        # initial_indices包含子序列的随机起始索引\n        initial_indices_per_batch = initial_indices[i: i + batch_size] # 每个batch的起始索引\n        X = [data(j) for j in initial_indices_per_batch] # 获得batch_size个子序列\n        Y = [data(j + 1) for j in initial_indices_per_batch]\n        yield torch.tensor(X), torch.tensor(Y)\n```\n\n- 生成一个从0到34的序列。batch_size=2，num_steps=5。可以生成(35-1)/5=6个子序列对，6/2=3个小批量。\n\n\n```python\nmy_seq = list(range(35))\nfor X,Y in seq_data_iter_random(my_seq, batch_size=2, num_steps=5):\n    print('X: ', X, '\\nY: ', Y)\n```\n\n    X:  tensor([[ 7,  8,  9, 10, 11],\n            [27, 28, 29, 30, 31]]) \n    Y:  tensor([[ 8,  9, 10, 11, 12],\n            [28, 29, 30, 31, 32]])\n    X:  tensor([[12, 13, 14, 15, 16],\n            [ 2,  3,  4,  5,  6]]) \n    Y:  tensor([[13, 14, 15, 16, 17],\n            [ 3,  4,  5,  6,  7]])\n    X:  tensor([[22, 23, 24, 25, 26],\n            [17, 18, 19, 20, 21]]) \n    Y:  tensor([[23, 24, 25, 26, 27],\n            [18, 19, 20, 21, 22]])\n    \n\n### 3.4.2 顺序分区\n- 迭代过程中，除了对原始序列可以随机抽样外，我们还可以保证两个相邻的小批量中的子序列在原始序列上也是相邻的。\n\n\n```python\ndef seq_data_iter_sequential(corpus, batch_size, num_steps): #@save\n    '''使用顺序分区生成一个小批量子序列'''\n    # 从随机偏移量开始划分序列\n    offset = random.randint(0, num_steps)\n    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size # 可用于训练的tokens数量\n    Xs = torch.tensor(corpus[offset: offset + num_tokens])\n    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])\n    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\n    print('Xs：',Xs)\n    num_batches = Xs.shape[1] // num_steps # batch_size是子序列个数，不是token个数\n    for i in range(0, num_steps*num_batches, num_steps):\n        X = Xs[:, i: i+num_steps] #一个batch里面包含token数：num_steps*batch_size\n        Y = Ys[:, i: i+num_steps]\n        yield X, Y\n\nfor X,Y in seq_data_iter_sequential(my_seq, batch_size=2, num_steps=5):\n    print('X: ', X, '\\nY: ', Y)\n```\n\n    Xs： tensor([[ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17],\n            [18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]])\n    X:  tensor([[ 2,  3,  4,  5,  6],\n            [18, 19, 20, 21, 22]]) \n    Y:  tensor([[ 3,  4,  5,  6,  7],\n            [19, 20, 21, 22, 23]])\n    X:  tensor([[ 7,  8,  9, 10, 11],\n            [23, 24, 25, 26, 27]]) \n    Y:  tensor([[ 8,  9, 10, 11, 12],\n            [24, 25, 26, 27, 28]])\n    X:  tensor([[12, 13, 14, 15, 16],\n            [28, 29, 30, 31, 32]]) \n    Y:  tensor([[13, 14, 15, 16, 17],\n            [29, 30, 31, 32, 33]])\n    \n\n- 将两个采样函数包装到一个类中，以便稍后可以将其用作数据迭代器。\n\n\n```python\nclass SeqDataLoader: #@save\n    '''加载序列数据的迭代器'''\n    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):\n        if use_random_iter:\n            self.data_iter_fn = d2l.seq_data_iter_random\n        else:\n            self.data_iter_fn = d2l.seq_data_iter_sequential\n        self.corpus, self.vocab = d2l.load_corpus_time_machine(max_tokens)\n        self.batch_size, self.num_steps = batch_size, num_steps\n    \n    def __iter__(self):\n        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)\n    \n# 定义函数，同时返回数据迭代器和词表，因此可以与其他带有load_data前缀的函数（如d2l.load_data_fashion_mnist）类似地使用。\ndef load_data_time_machine(batch_size, num_steps, #@save\n                           use_random_iter=False, max_tokens=10000):\n    '''返回时光机数据集的迭代器和词表'''\n    data_iter = SeqDataLoader(batch_size, num_steps, use_random_iter, max_tokens)\n```\n","slug":"deeplearning/code/pytorch/7_rnn/3_lmdataset","published":1,"updated":"2024-05-05T14:16:32.833Z","comments":1,"layout":"post","photos":[],"_id":"clvuli6uy0090zsvw8g1j37jp","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"3-语言模型和数据集\"><a href=\"#3-语言模型和数据集\" class=\"headerlink\" title=\"3 语言模型和数据集\"></a>3 语言模型和数据集</h1><ul>\n<li>假设长度为T的文本序列中的词元依次为$x_ 1$,$x_ 2$,…,$x_ T$。$x_ t$可以被认为是文本序列在时间步t处的观测或标签。语言模型（language model）的目标是估计序列的联合概率：<br>$$P(x_ 1,x_ 2,…,x_ T)$$</li>\n<li>只需要一次抽取一个词元$x_t ~ P(x_t|x_ {t-1},…,x_ 1)$，一个理想的语言模型就能够基于模型本身生成自然文本。只需要基于前面的对话片断中的文本，就足以生成一个有意义的对话。</li>\n</ul>\n<h2 id=\"3-1-学习语言模型\"><a href=\"#3-1-学习语言模型\" class=\"headerlink\" title=\"3.1 学习语言模型\"></a>3.1 学习语言模型</h2><ul>\n<li><p>我们面对的问题是如何对一个文档，甚至是一个词元序列进行建模。<br>$$P(x_ 1,x_ 2,…,x_ T) &#x3D; \\prod_ { t &#x3D; 1 }^ T P(x_ t|x_ 1,x_ 2,…,x_ {t-1})$$</p>\n</li>\n<li><p>例如，包含了四个单词的一个文本序列的概率是：<br>$$P(deep, learning, is, fun) &#x3D; P(deep)P(learning|deep)P(is|deep, learning)P(fun|deep, learning, is)$$</p>\n</li>\n<li><p>为了训练语言模型，我们需要计算单词的概率，以及给定前面几个单词后出现某个单词的条件概率。这些概率本质上就是语言模型的参数。</p>\n</li>\n<li><p>假设训练数据集是一个大型的文本语料库。训练数据集中词的概率可以根据给定词的相对词频来计算。例如，可以将估计值$\\hat P(deep)$计算为任何以单词“deep”开头的句子的概率，或“deep”在数据集中的出现次数。接下来，我们可以尝试估计：<br>$$\\hat P(learning|deep) &#x3D; \\frac{n(deep, learning)}{n(deep)}$$</p>\n</li>\n<li><p>其中$n(deep,learning)$是’ deep learning’连续出现的次数。由于连续单词对“deep learning”的出现频率要低得多，所以估计这类单词正确的概率要困难得多。许多合理的三个单词组合可能是存在的，但是在数据集中却找不到。</p>\n</li>\n<li><p>一种常见的策略是执行某种形式的拉普拉斯平滑（Laplace smoothing）：在所有计数中添加一个小常量。用n表示训练集中的单词总数，用m表示唯一单词的数量。此解决方案有助于处理单元素问题：<br>$$\\begin{aligned} \\hat P(x) &amp; &#x3D; \\frac{n(x) + \\frac{ \\epsilon_1 }{m}}{n + \\epsilon_1} \\ \\hat P(x’|x) &amp; &#x3D; \\frac{ n(x,x’ ) + \\epsilon_2 \\hat P(x’) }{n(x) + \\epsilon_2} \\ \\hat P(x’’|x,x’) &amp; &#x3D; \\frac{ n(x,x’,x’’) + \\epsilon_3 \\hat P(x’’) }{n(x,x’) + \\epsilon_3} \\end{aligned}$$</p>\n</li>\n<li><p>其中$\\epsilon$是一个超参数，当$\\epsilon &#x3D; 0$时，不应用平滑；当$\\epsilon$接近正无穷时，$\\hat P(x)$接近均匀分布 $\\frac{1}{m}$。</p>\n</li>\n<li><p>这样的模型很容易变得无效，首先，我们需要存储所有的计数；其次，这完全忽略了单词的意思。例如，“猫”（cat）和“猫科动物”（feline）可能出现在相关的上下文中，但是想根据上下文调整这类模型其实是相当困难的。最后，长单词序列大部分是没出现过的，因此一个模型如果只是简单地统计先前“看到”的单词序列频率，那么模型面对这种问题肯定是表现不佳的。</p>\n</li>\n</ul>\n<h2 id=\"3-2-马尔可夫模型与n元语法\"><a href=\"#3-2-马尔可夫模型与n元语法\" class=\"headerlink\" title=\"3.2 马尔可夫模型与n元语法\"></a>3.2 马尔可夫模型与n元语法</h2><ul>\n<li><p>如果$P(x_ {t+1} | X_ t, \\cdots, x_ 1) &#x3D; P(x_ {t+1} | x_ t)$,则序列上的分布满足一阶马尔可夫性质。阶数越高，对应的依赖关系就越长。这种性质推导出了许多可以应用于序列建模的近似公式：<br>$$\\begin{aligned} P(x_ 1,x_ 2,x_ 3,x_ 4) &amp; &#x3D; P(x_ 1)P(x_ 2)P(x_ 3)P(x_ 4|x_ 3) \\ P(x_ 1,x_ 2,x_ 3,x_ 4) &amp; &#x3D; P(x_ 1)P(x_ 2|x_ 1)P(x_ 3|x_ 2)P(x_ 4|x_ 3) \\ P(x_ 1,x_ 2,x_ 3,x_ 4) &amp; &#x3D; P(x_ 1)P(x_ 2|x_ 1)P(x_ 3|x_ 1,x_ 2)P(x_ 4|x_ 2,x_ 3) \\end{aligned}$$</p>\n</li>\n<li><p>涉及一个、两个和三个变量的概率公式分别被称为 一元语法（unigram）、二元语法（bigram）和三元语法（trigram）模型。</p>\n</li>\n</ul>\n<h2 id=\"3-3-自然语言统计\"><a href=\"#3-3-自然语言统计\" class=\"headerlink\" title=\"3.3 自然语言统计\"></a>3.3 自然语言统计</h2><ul>\n<li>在真实数据上如果进行自然语言统计。根据数据集构建词表，并打印前10个最常用的（频率最高的）单词。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> random</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span>  torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">tokens = d2l.tokenize(d2l.read_time_machine())</span><br><span class=\"line\"><span class=\"comment\"># 因为每个文本行不一定是一个句子或一个段落，因此我们把所有文本行拼接到一起</span></span><br><span class=\"line\">corpus = [token <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> tokens <span class=\"keyword\">for</span> token <span class=\"keyword\">in</span> line]</span><br><span class=\"line\">vocab = d2l.Vocab(corpus)</span><br><span class=\"line\">vocab.token_freqs[:<span class=\"number\">10</span>]</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>[(&#39;the&#39;, 2261),\n (&#39;i&#39;, 1267),\n (&#39;and&#39;, 1245),\n (&#39;of&#39;, 1155),\n (&#39;a&#39;, 816),\n (&#39;to&#39;, 695),\n (&#39;was&#39;, 552),\n (&#39;in&#39;, 541),\n (&#39;that&#39;, 443),\n (&#39;my&#39;, 440)]\n</code></pre>\n<ul>\n<li>最流行的词看起来很无聊，这些词通常被称为停用词（stop words），因此可以被过滤掉。还有个明显的问题是词频衰减的速度相当地快。例如，最常用单词的词频对比，第10个还不到第1个的1&#x2F;5。</li>\n<li>词频图：</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>]=<span class=\"string\">&quot;TRUE&quot;</span></span><br><span class=\"line\">freqs = [freq <span class=\"keyword\">for</span> token, freq <span class=\"keyword\">in</span> vocab.token_freqs]</span><br><span class=\"line\">d2l.plot(freqs, xlabel=<span class=\"string\">&#x27;token: x&#x27;</span>, ylabel=<span class=\"string\">&#x27;frequency: n(x)&#x27;</span>, xscale=<span class=\"string\">&#x27;log&#x27;</span>, yscale=<span class=\"string\">&#x27;log&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/3_lmdataset_files/3_lmdataset_3_0.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/7_rnn/3_lmdataset_files/3_lmdataset_3_0.svg\"></p>\n<ul>\n<li><p>将前几个单词作为例外消除后，剩余的所有单词大致遵循双对数坐标图上的一条直线。这意味着单词的频率满足齐普夫定律即第i个最常用单词的频率ni为：<br>$$ n_ i \\propto \\frac{1}{i^ \\alpha}$$</p>\n</li>\n<li><p>等价于：<br>$$ \\log(n_ i) &#x3D; - \\alpha \\log(i) + c$$</p>\n</li>\n<li><p>这告诉我们想要通过计数统计和平滑来建模单词是不可行的，因为这样建模的结果会大大高估尾部单词的频率，也就是所谓的不常用单词。</p>\n</li>\n<li><p>我们来看看二元语法的频率是否与一元语法的频率表现出相同的行为方式。</p>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bigram_tokens = [pair <span class=\"keyword\">for</span> pair <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(corpus[:-<span class=\"number\">1</span>], corpus[<span class=\"number\">1</span>:])] <span class=\"comment\">#[0,1,2] + [1,2,3] = [(0,1),(1,2),(2,3)]</span></span><br><span class=\"line\">bigram_vocab = d2l.Vocab(bigram_tokens)</span><br><span class=\"line\">bigram_vocab.token_freqs[:<span class=\"number\">10</span>]</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>[((&#39;of&#39;, &#39;the&#39;), 309),\n ((&#39;in&#39;, &#39;the&#39;), 169),\n ((&#39;i&#39;, &#39;had&#39;), 130),\n ((&#39;i&#39;, &#39;was&#39;), 112),\n ((&#39;and&#39;, &#39;the&#39;), 109),\n ((&#39;the&#39;, &#39;time&#39;), 102),\n ((&#39;it&#39;, &#39;was&#39;), 99),\n ((&#39;to&#39;, &#39;the&#39;), 85),\n ((&#39;as&#39;, &#39;i&#39;), 78),\n ((&#39;of&#39;, &#39;a&#39;), 73)]\n</code></pre>\n<ul>\n<li>发现在十个最频繁的词对中，有九个是由两个停用词组成的，只有一个与“the time”有关。</li>\n<li>三元语法：</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">trigram_tokens = [triple <span class=\"keyword\">for</span> triple <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(corpus[:-<span class=\"number\">2</span>], corpus[<span class=\"number\">1</span>:-<span class=\"number\">1</span>], corpus[<span class=\"number\">2</span>:])]</span><br><span class=\"line\">trigram_vocab = d2l.Vocab(trigram_tokens)</span><br><span class=\"line\">trigram_vocab.token_freqs[:<span class=\"number\">10</span>]</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>[((&#39;the&#39;, &#39;time&#39;, &#39;traveller&#39;), 59),\n ((&#39;the&#39;, &#39;time&#39;, &#39;machine&#39;), 30),\n ((&#39;the&#39;, &#39;medical&#39;, &#39;man&#39;), 24),\n ((&#39;it&#39;, &#39;seemed&#39;, &#39;to&#39;), 16),\n ((&#39;it&#39;, &#39;was&#39;, &#39;a&#39;), 15),\n ((&#39;here&#39;, &#39;and&#39;, &#39;there&#39;), 15),\n ((&#39;seemed&#39;, &#39;to&#39;, &#39;me&#39;), 14),\n ((&#39;i&#39;, &#39;did&#39;, &#39;not&#39;), 14),\n ((&#39;i&#39;, &#39;saw&#39;, &#39;the&#39;), 13),\n ((&#39;i&#39;, &#39;began&#39;, &#39;to&#39;), 13)]\n</code></pre>\n<ul>\n<li>直观地对比三种模型中的词元频率：一元语法、二元语法和三元语法。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bigram_freqs = [freq <span class=\"keyword\">for</span> token, freq <span class=\"keyword\">in</span> bigram_vocab.token_freqs]</span><br><span class=\"line\">trigram_freqs = [freq <span class=\"keyword\">for</span> token, freq <span class=\"keyword\">in</span> trigram_vocab.token_freqs]</span><br><span class=\"line\">d2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel=<span class=\"string\">&#x27;token: x&#x27;</span>, </span><br><span class=\"line\">         ylabel=<span class=\"string\">&#x27;frequency: n(x)&#x27;</span>, xscale=<span class=\"string\">&#x27;log&#x27;</span>, yscale=<span class=\"string\">&#x27;log&#x27;</span>,</span><br><span class=\"line\">         legend=[<span class=\"string\">&#x27;unigram&#x27;</span>, <span class=\"string\">&#x27;bigram&#x27;</span>, <span class=\"string\">&#x27;trigram&#x27;</span>])</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/3_lmdataset_files/3_lmdataset_9_0.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/7_rnn/3_lmdataset_files/3_lmdataset_9_0.svg\"></p>\n<ul>\n<li>我们发现：<ul>\n<li>除了一元语法词，单词序列似乎也遵循齐普夫定律</li>\n<li>词表中n元组的数量并没有那么大，这说明语言中存在相当多的结构，这些结构给了我们应用模型的希望</li>\n<li>很多n元组很少出现，这使得拉普拉斯平滑非常不适合语言建模。作为代替，我们将使用基于深度学习的模型。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"3-4-读取长序列数据\"><a href=\"#3-4-读取长序列数据\" class=\"headerlink\" title=\"3.4 读取长序列数据\"></a>3.4 读取长序列数据</h2><ul>\n<li>由于序列数据本质上是连续的，因此我们在处理数据时需要解决这个问题。一种方法：当序列变得太长而不能被模型一次性全部处理时，我们可能希望拆分这样的序列方便模型读取。</li>\n<li>总体策略：<ul>\n<li>假设我们将使用神经网络来训练语言模型，模型中的网络一次处理具有预定义长度（例如n个时间步）的一个小批量序列。现在的问题是如何随机生成一个小批量数据的特征和标签以供读取。</li>\n<li>由于文本序列可以是任意长的，任意长的序列可以被我们划分为具有相同时间步数的子序列。训练时，这样的小批量子序列将被输入到模型中。如下图n&#x3D;5，每个时间步的词元对应于一个字符，可以选择任意偏移量来指示初始位置<br>  <img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/7_rnn/3_lmdataset/1.png\"><br>  <img src=\"/img/deeplearning/code/pytorch/7_rnn/3_lmdataset/1.png\"></li>\n<li>选择图中的哪一个呢？都差不多。如果我们只选择一个偏移量，那么用于训练网络的、所有可能的子序列的覆盖范围将是有限的。因此，我们可以从随机偏移量开始划分序列，以同时获得覆盖性（coverage）和随机性（randomness）。如何实现随机采样（random sampling）和顺序分区（sequential partitioning）策略。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"3-4-1-随机采样\"><a href=\"#3-4-1-随机采样\" class=\"headerlink\" title=\"3.4.1 随机采样\"></a>3.4.1 随机采样</h3><ul>\n<li>对于语言建模，目标是基于到目前为止我们看到的词元来预测下一个词元，因此标签是移位了一个词元的原始序列。</li>\n<li>每次从数据中随机生成一个小批量。batch_size指定了每个小批量中子序列样本的数目，num_steps是每个子序列中预定义的时间步数(n)。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">seq_data_iter_random</span>(<span class=\"params\">corpus, batch_size, num_steps</span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;使用随机抽样生成一个小批量子序列&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"comment\"># 从随机偏移量开始对序列进行分区，随机范围包括num_steps -1</span></span><br><span class=\"line\">    corpus = corpus[random.randint(<span class=\"number\">0</span>, num_steps -<span class=\"number\">1</span>):]</span><br><span class=\"line\">    <span class=\"comment\"># 减去1，是因为我们需要考虑标签</span></span><br><span class=\"line\">    num_subseqs = (<span class=\"built_in\">len</span>(corpus) -<span class=\"number\">1</span>) // num_steps <span class=\"comment\"># 多少个子序列</span></span><br><span class=\"line\">    <span class=\"comment\"># 长度为num_steps的子序列的起始索引</span></span><br><span class=\"line\">    initial_indices = <span class=\"built_in\">list</span>(<span class=\"built_in\">range</span>(<span class=\"number\">0</span>, num_subseqs*num_steps, num_steps))</span><br><span class=\"line\">    <span class=\"comment\"># 在随机抽样的迭代过程中，来自两个相邻的、随机的、小批量的子序列不一定在原始序列上相邻</span></span><br><span class=\"line\">    random.shuffle(initial_indices)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">data</span>(<span class=\"params\">pos</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 返回从pos开始的长度为num_steps的序列</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> corpus[pos: pos + num_steps]</span><br><span class=\"line\">    </span><br><span class=\"line\">    num_batches = num_subseqs // batch_size <span class=\"comment\"># 将整个数据集分成多少个batch</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">0</span>, batch_size*num_batches, batch_size):</span><br><span class=\"line\">        <span class=\"comment\"># initial_indices包含子序列的随机起始索引</span></span><br><span class=\"line\">        initial_indices_per_batch = initial_indices[i: i + batch_size] <span class=\"comment\"># 每个batch的起始索引</span></span><br><span class=\"line\">        X = [data(j) <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> initial_indices_per_batch] <span class=\"comment\"># 获得batch_size个子序列</span></span><br><span class=\"line\">        Y = [data(j + <span class=\"number\">1</span>) <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> initial_indices_per_batch]</span><br><span class=\"line\">        <span class=\"keyword\">yield</span> torch.tensor(X), torch.tensor(Y)</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>生成一个从0到34的序列。batch_size&#x3D;2，num_steps&#x3D;5。可以生成(35-1)&#x2F;5&#x3D;6个子序列对，6&#x2F;2&#x3D;3个小批量。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">my_seq = <span class=\"built_in\">list</span>(<span class=\"built_in\">range</span>(<span class=\"number\">35</span>))</span><br><span class=\"line\"><span class=\"keyword\">for</span> X,Y <span class=\"keyword\">in</span> seq_data_iter_random(my_seq, batch_size=<span class=\"number\">2</span>, num_steps=<span class=\"number\">5</span>):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;X: &#x27;</span>, X, <span class=\"string\">&#x27;\\nY: &#x27;</span>, Y)</span><br></pre></td></tr></table></figure>\n\n<pre><code>X:  tensor([[ 7,  8,  9, 10, 11],\n        [27, 28, 29, 30, 31]]) \nY:  tensor([[ 8,  9, 10, 11, 12],\n        [28, 29, 30, 31, 32]])\nX:  tensor([[12, 13, 14, 15, 16],\n        [ 2,  3,  4,  5,  6]]) \nY:  tensor([[13, 14, 15, 16, 17],\n        [ 3,  4,  5,  6,  7]])\nX:  tensor([[22, 23, 24, 25, 26],\n        [17, 18, 19, 20, 21]]) \nY:  tensor([[23, 24, 25, 26, 27],\n        [18, 19, 20, 21, 22]])\n</code></pre>\n<h3 id=\"3-4-2-顺序分区\"><a href=\"#3-4-2-顺序分区\" class=\"headerlink\" title=\"3.4.2 顺序分区\"></a>3.4.2 顺序分区</h3><ul>\n<li>迭代过程中，除了对原始序列可以随机抽样外，我们还可以保证两个相邻的小批量中的子序列在原始序列上也是相邻的。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">seq_data_iter_sequential</span>(<span class=\"params\">corpus, batch_size, num_steps</span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;使用顺序分区生成一个小批量子序列&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"comment\"># 从随机偏移量开始划分序列</span></span><br><span class=\"line\">    offset = random.randint(<span class=\"number\">0</span>, num_steps)</span><br><span class=\"line\">    num_tokens = ((<span class=\"built_in\">len</span>(corpus) - offset - <span class=\"number\">1</span>) // batch_size) * batch_size <span class=\"comment\"># 可用于训练的tokens数量</span></span><br><span class=\"line\">    Xs = torch.tensor(corpus[offset: offset + num_tokens])</span><br><span class=\"line\">    Ys = torch.tensor(corpus[offset + <span class=\"number\">1</span>: offset + <span class=\"number\">1</span> + num_tokens])</span><br><span class=\"line\">    Xs, Ys = Xs.reshape(batch_size, -<span class=\"number\">1</span>), Ys.reshape(batch_size, -<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Xs：&#x27;</span>,Xs)</span><br><span class=\"line\">    num_batches = Xs.shape[<span class=\"number\">1</span>] // num_steps <span class=\"comment\"># batch_size是子序列个数，不是token个数</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">0</span>, num_steps*num_batches, num_steps):</span><br><span class=\"line\">        X = Xs[:, i: i+num_steps] <span class=\"comment\">#一个batch里面包含token数：num_steps*batch_size</span></span><br><span class=\"line\">        Y = Ys[:, i: i+num_steps]</span><br><span class=\"line\">        <span class=\"keyword\">yield</span> X, Y</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> X,Y <span class=\"keyword\">in</span> seq_data_iter_sequential(my_seq, batch_size=<span class=\"number\">2</span>, num_steps=<span class=\"number\">5</span>):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;X: &#x27;</span>, X, <span class=\"string\">&#x27;\\nY: &#x27;</span>, Y)</span><br></pre></td></tr></table></figure>\n\n<pre><code>Xs： tensor([[ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17],\n        [18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]])\nX:  tensor([[ 2,  3,  4,  5,  6],\n        [18, 19, 20, 21, 22]]) \nY:  tensor([[ 3,  4,  5,  6,  7],\n        [19, 20, 21, 22, 23]])\nX:  tensor([[ 7,  8,  9, 10, 11],\n        [23, 24, 25, 26, 27]]) \nY:  tensor([[ 8,  9, 10, 11, 12],\n        [24, 25, 26, 27, 28]])\nX:  tensor([[12, 13, 14, 15, 16],\n        [28, 29, 30, 31, 32]]) \nY:  tensor([[13, 14, 15, 16, 17],\n        [29, 30, 31, 32, 33]])\n</code></pre>\n<ul>\n<li>将两个采样函数包装到一个类中，以便稍后可以将其用作数据迭代器。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">SeqDataLoader</span>: <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;加载序列数据的迭代器&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, batch_size, num_steps, use_random_iter, max_tokens</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> use_random_iter:</span><br><span class=\"line\">            self.data_iter_fn = d2l.seq_data_iter_random</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            self.data_iter_fn = d2l.seq_data_iter_sequential</span><br><span class=\"line\">        self.corpus, self.vocab = d2l.load_corpus_time_machine(max_tokens)</span><br><span class=\"line\">        self.batch_size, self.num_steps = batch_size, num_steps</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__iter__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)</span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"comment\"># 定义函数，同时返回数据迭代器和词表，因此可以与其他带有load_data前缀的函数（如d2l.load_data_fashion_mnist）类似地使用。</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">load_data_time_machine</span>(<span class=\"params\">batch_size, num_steps, <span class=\"comment\">#@save</span></span></span><br><span class=\"line\"><span class=\"params\">                           use_random_iter=<span class=\"literal\">False</span>, max_tokens=<span class=\"number\">10000</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;返回时光机数据集的迭代器和词表&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    data_iter = SeqDataLoader(batch_size, num_steps, use_random_iter, max_tokens)</span><br></pre></td></tr></table></figure>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"3-语言模型和数据集\"><a href=\"#3-语言模型和数据集\" class=\"headerlink\" title=\"3 语言模型和数据集\"></a>3 语言模型和数据集</h1><ul>\n<li>假设长度为T的文本序列中的词元依次为$x_ 1$,$x_ 2$,…,$x_ T$。$x_ t$可以被认为是文本序列在时间步t处的观测或标签。语言模型（language model）的目标是估计序列的联合概率：<br>$$P(x_ 1,x_ 2,…,x_ T)$$</li>\n<li>只需要一次抽取一个词元$x_t ~ P(x_t|x_ {t-1},…,x_ 1)$，一个理想的语言模型就能够基于模型本身生成自然文本。只需要基于前面的对话片断中的文本，就足以生成一个有意义的对话。</li>\n</ul>\n<h2 id=\"3-1-学习语言模型\"><a href=\"#3-1-学习语言模型\" class=\"headerlink\" title=\"3.1 学习语言模型\"></a>3.1 学习语言模型</h2><ul>\n<li><p>我们面对的问题是如何对一个文档，甚至是一个词元序列进行建模。<br>$$P(x_ 1,x_ 2,…,x_ T) &#x3D; \\prod_ { t &#x3D; 1 }^ T P(x_ t|x_ 1,x_ 2,…,x_ {t-1})$$</p>\n</li>\n<li><p>例如，包含了四个单词的一个文本序列的概率是：<br>$$P(deep, learning, is, fun) &#x3D; P(deep)P(learning|deep)P(is|deep, learning)P(fun|deep, learning, is)$$</p>\n</li>\n<li><p>为了训练语言模型，我们需要计算单词的概率，以及给定前面几个单词后出现某个单词的条件概率。这些概率本质上就是语言模型的参数。</p>\n</li>\n<li><p>假设训练数据集是一个大型的文本语料库。训练数据集中词的概率可以根据给定词的相对词频来计算。例如，可以将估计值$\\hat P(deep)$计算为任何以单词“deep”开头的句子的概率，或“deep”在数据集中的出现次数。接下来，我们可以尝试估计：<br>$$\\hat P(learning|deep) &#x3D; \\frac{n(deep, learning)}{n(deep)}$$</p>\n</li>\n<li><p>其中$n(deep,learning)$是’ deep learning’连续出现的次数。由于连续单词对“deep learning”的出现频率要低得多，所以估计这类单词正确的概率要困难得多。许多合理的三个单词组合可能是存在的，但是在数据集中却找不到。</p>\n</li>\n<li><p>一种常见的策略是执行某种形式的拉普拉斯平滑（Laplace smoothing）：在所有计数中添加一个小常量。用n表示训练集中的单词总数，用m表示唯一单词的数量。此解决方案有助于处理单元素问题：<br>$$\\begin{aligned} \\hat P(x) &amp; &#x3D; \\frac{n(x) + \\frac{ \\epsilon_1 }{m}}{n + \\epsilon_1} \\ \\hat P(x’|x) &amp; &#x3D; \\frac{ n(x,x’ ) + \\epsilon_2 \\hat P(x’) }{n(x) + \\epsilon_2} \\ \\hat P(x’’|x,x’) &amp; &#x3D; \\frac{ n(x,x’,x’’) + \\epsilon_3 \\hat P(x’’) }{n(x,x’) + \\epsilon_3} \\end{aligned}$$</p>\n</li>\n<li><p>其中$\\epsilon$是一个超参数，当$\\epsilon &#x3D; 0$时，不应用平滑；当$\\epsilon$接近正无穷时，$\\hat P(x)$接近均匀分布 $\\frac{1}{m}$。</p>\n</li>\n<li><p>这样的模型很容易变得无效，首先，我们需要存储所有的计数；其次，这完全忽略了单词的意思。例如，“猫”（cat）和“猫科动物”（feline）可能出现在相关的上下文中，但是想根据上下文调整这类模型其实是相当困难的。最后，长单词序列大部分是没出现过的，因此一个模型如果只是简单地统计先前“看到”的单词序列频率，那么模型面对这种问题肯定是表现不佳的。</p>\n</li>\n</ul>\n<h2 id=\"3-2-马尔可夫模型与n元语法\"><a href=\"#3-2-马尔可夫模型与n元语法\" class=\"headerlink\" title=\"3.2 马尔可夫模型与n元语法\"></a>3.2 马尔可夫模型与n元语法</h2><ul>\n<li><p>如果$P(x_ {t+1} | X_ t, \\cdots, x_ 1) &#x3D; P(x_ {t+1} | x_ t)$,则序列上的分布满足一阶马尔可夫性质。阶数越高，对应的依赖关系就越长。这种性质推导出了许多可以应用于序列建模的近似公式：<br>$$\\begin{aligned} P(x_ 1,x_ 2,x_ 3,x_ 4) &amp; &#x3D; P(x_ 1)P(x_ 2)P(x_ 3)P(x_ 4|x_ 3) \\ P(x_ 1,x_ 2,x_ 3,x_ 4) &amp; &#x3D; P(x_ 1)P(x_ 2|x_ 1)P(x_ 3|x_ 2)P(x_ 4|x_ 3) \\ P(x_ 1,x_ 2,x_ 3,x_ 4) &amp; &#x3D; P(x_ 1)P(x_ 2|x_ 1)P(x_ 3|x_ 1,x_ 2)P(x_ 4|x_ 2,x_ 3) \\end{aligned}$$</p>\n</li>\n<li><p>涉及一个、两个和三个变量的概率公式分别被称为 一元语法（unigram）、二元语法（bigram）和三元语法（trigram）模型。</p>\n</li>\n</ul>\n<h2 id=\"3-3-自然语言统计\"><a href=\"#3-3-自然语言统计\" class=\"headerlink\" title=\"3.3 自然语言统计\"></a>3.3 自然语言统计</h2><ul>\n<li>在真实数据上如果进行自然语言统计。根据数据集构建词表，并打印前10个最常用的（频率最高的）单词。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> random</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span>  torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">tokens = d2l.tokenize(d2l.read_time_machine())</span><br><span class=\"line\"><span class=\"comment\"># 因为每个文本行不一定是一个句子或一个段落，因此我们把所有文本行拼接到一起</span></span><br><span class=\"line\">corpus = [token <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> tokens <span class=\"keyword\">for</span> token <span class=\"keyword\">in</span> line]</span><br><span class=\"line\">vocab = d2l.Vocab(corpus)</span><br><span class=\"line\">vocab.token_freqs[:<span class=\"number\">10</span>]</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>[(&#39;the&#39;, 2261),\n (&#39;i&#39;, 1267),\n (&#39;and&#39;, 1245),\n (&#39;of&#39;, 1155),\n (&#39;a&#39;, 816),\n (&#39;to&#39;, 695),\n (&#39;was&#39;, 552),\n (&#39;in&#39;, 541),\n (&#39;that&#39;, 443),\n (&#39;my&#39;, 440)]\n</code></pre>\n<ul>\n<li>最流行的词看起来很无聊，这些词通常被称为停用词（stop words），因此可以被过滤掉。还有个明显的问题是词频衰减的速度相当地快。例如，最常用单词的词频对比，第10个还不到第1个的1&#x2F;5。</li>\n<li>词频图：</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>]=<span class=\"string\">&quot;TRUE&quot;</span></span><br><span class=\"line\">freqs = [freq <span class=\"keyword\">for</span> token, freq <span class=\"keyword\">in</span> vocab.token_freqs]</span><br><span class=\"line\">d2l.plot(freqs, xlabel=<span class=\"string\">&#x27;token: x&#x27;</span>, ylabel=<span class=\"string\">&#x27;frequency: n(x)&#x27;</span>, xscale=<span class=\"string\">&#x27;log&#x27;</span>, yscale=<span class=\"string\">&#x27;log&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/3_lmdataset_files/3_lmdataset_3_0.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/7_rnn/3_lmdataset_files/3_lmdataset_3_0.svg\"></p>\n<ul>\n<li><p>将前几个单词作为例外消除后，剩余的所有单词大致遵循双对数坐标图上的一条直线。这意味着单词的频率满足齐普夫定律即第i个最常用单词的频率ni为：<br>$$ n_ i \\propto \\frac{1}{i^ \\alpha}$$</p>\n</li>\n<li><p>等价于：<br>$$ \\log(n_ i) &#x3D; - \\alpha \\log(i) + c$$</p>\n</li>\n<li><p>这告诉我们想要通过计数统计和平滑来建模单词是不可行的，因为这样建模的结果会大大高估尾部单词的频率，也就是所谓的不常用单词。</p>\n</li>\n<li><p>我们来看看二元语法的频率是否与一元语法的频率表现出相同的行为方式。</p>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bigram_tokens = [pair <span class=\"keyword\">for</span> pair <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(corpus[:-<span class=\"number\">1</span>], corpus[<span class=\"number\">1</span>:])] <span class=\"comment\">#[0,1,2] + [1,2,3] = [(0,1),(1,2),(2,3)]</span></span><br><span class=\"line\">bigram_vocab = d2l.Vocab(bigram_tokens)</span><br><span class=\"line\">bigram_vocab.token_freqs[:<span class=\"number\">10</span>]</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>[((&#39;of&#39;, &#39;the&#39;), 309),\n ((&#39;in&#39;, &#39;the&#39;), 169),\n ((&#39;i&#39;, &#39;had&#39;), 130),\n ((&#39;i&#39;, &#39;was&#39;), 112),\n ((&#39;and&#39;, &#39;the&#39;), 109),\n ((&#39;the&#39;, &#39;time&#39;), 102),\n ((&#39;it&#39;, &#39;was&#39;), 99),\n ((&#39;to&#39;, &#39;the&#39;), 85),\n ((&#39;as&#39;, &#39;i&#39;), 78),\n ((&#39;of&#39;, &#39;a&#39;), 73)]\n</code></pre>\n<ul>\n<li>发现在十个最频繁的词对中，有九个是由两个停用词组成的，只有一个与“the time”有关。</li>\n<li>三元语法：</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">trigram_tokens = [triple <span class=\"keyword\">for</span> triple <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(corpus[:-<span class=\"number\">2</span>], corpus[<span class=\"number\">1</span>:-<span class=\"number\">1</span>], corpus[<span class=\"number\">2</span>:])]</span><br><span class=\"line\">trigram_vocab = d2l.Vocab(trigram_tokens)</span><br><span class=\"line\">trigram_vocab.token_freqs[:<span class=\"number\">10</span>]</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>[((&#39;the&#39;, &#39;time&#39;, &#39;traveller&#39;), 59),\n ((&#39;the&#39;, &#39;time&#39;, &#39;machine&#39;), 30),\n ((&#39;the&#39;, &#39;medical&#39;, &#39;man&#39;), 24),\n ((&#39;it&#39;, &#39;seemed&#39;, &#39;to&#39;), 16),\n ((&#39;it&#39;, &#39;was&#39;, &#39;a&#39;), 15),\n ((&#39;here&#39;, &#39;and&#39;, &#39;there&#39;), 15),\n ((&#39;seemed&#39;, &#39;to&#39;, &#39;me&#39;), 14),\n ((&#39;i&#39;, &#39;did&#39;, &#39;not&#39;), 14),\n ((&#39;i&#39;, &#39;saw&#39;, &#39;the&#39;), 13),\n ((&#39;i&#39;, &#39;began&#39;, &#39;to&#39;), 13)]\n</code></pre>\n<ul>\n<li>直观地对比三种模型中的词元频率：一元语法、二元语法和三元语法。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bigram_freqs = [freq <span class=\"keyword\">for</span> token, freq <span class=\"keyword\">in</span> bigram_vocab.token_freqs]</span><br><span class=\"line\">trigram_freqs = [freq <span class=\"keyword\">for</span> token, freq <span class=\"keyword\">in</span> trigram_vocab.token_freqs]</span><br><span class=\"line\">d2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel=<span class=\"string\">&#x27;token: x&#x27;</span>, </span><br><span class=\"line\">         ylabel=<span class=\"string\">&#x27;frequency: n(x)&#x27;</span>, xscale=<span class=\"string\">&#x27;log&#x27;</span>, yscale=<span class=\"string\">&#x27;log&#x27;</span>,</span><br><span class=\"line\">         legend=[<span class=\"string\">&#x27;unigram&#x27;</span>, <span class=\"string\">&#x27;bigram&#x27;</span>, <span class=\"string\">&#x27;trigram&#x27;</span>])</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/3_lmdataset_files/3_lmdataset_9_0.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/7_rnn/3_lmdataset_files/3_lmdataset_9_0.svg\"></p>\n<ul>\n<li>我们发现：<ul>\n<li>除了一元语法词，单词序列似乎也遵循齐普夫定律</li>\n<li>词表中n元组的数量并没有那么大，这说明语言中存在相当多的结构，这些结构给了我们应用模型的希望</li>\n<li>很多n元组很少出现，这使得拉普拉斯平滑非常不适合语言建模。作为代替，我们将使用基于深度学习的模型。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"3-4-读取长序列数据\"><a href=\"#3-4-读取长序列数据\" class=\"headerlink\" title=\"3.4 读取长序列数据\"></a>3.4 读取长序列数据</h2><ul>\n<li>由于序列数据本质上是连续的，因此我们在处理数据时需要解决这个问题。一种方法：当序列变得太长而不能被模型一次性全部处理时，我们可能希望拆分这样的序列方便模型读取。</li>\n<li>总体策略：<ul>\n<li>假设我们将使用神经网络来训练语言模型，模型中的网络一次处理具有预定义长度（例如n个时间步）的一个小批量序列。现在的问题是如何随机生成一个小批量数据的特征和标签以供读取。</li>\n<li>由于文本序列可以是任意长的，任意长的序列可以被我们划分为具有相同时间步数的子序列。训练时，这样的小批量子序列将被输入到模型中。如下图n&#x3D;5，每个时间步的词元对应于一个字符，可以选择任意偏移量来指示初始位置<br>  <img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/7_rnn/3_lmdataset/1.png\"><br>  <img src=\"/img/deeplearning/code/pytorch/7_rnn/3_lmdataset/1.png\"></li>\n<li>选择图中的哪一个呢？都差不多。如果我们只选择一个偏移量，那么用于训练网络的、所有可能的子序列的覆盖范围将是有限的。因此，我们可以从随机偏移量开始划分序列，以同时获得覆盖性（coverage）和随机性（randomness）。如何实现随机采样（random sampling）和顺序分区（sequential partitioning）策略。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"3-4-1-随机采样\"><a href=\"#3-4-1-随机采样\" class=\"headerlink\" title=\"3.4.1 随机采样\"></a>3.4.1 随机采样</h3><ul>\n<li>对于语言建模，目标是基于到目前为止我们看到的词元来预测下一个词元，因此标签是移位了一个词元的原始序列。</li>\n<li>每次从数据中随机生成一个小批量。batch_size指定了每个小批量中子序列样本的数目，num_steps是每个子序列中预定义的时间步数(n)。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">seq_data_iter_random</span>(<span class=\"params\">corpus, batch_size, num_steps</span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;使用随机抽样生成一个小批量子序列&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"comment\"># 从随机偏移量开始对序列进行分区，随机范围包括num_steps -1</span></span><br><span class=\"line\">    corpus = corpus[random.randint(<span class=\"number\">0</span>, num_steps -<span class=\"number\">1</span>):]</span><br><span class=\"line\">    <span class=\"comment\"># 减去1，是因为我们需要考虑标签</span></span><br><span class=\"line\">    num_subseqs = (<span class=\"built_in\">len</span>(corpus) -<span class=\"number\">1</span>) // num_steps <span class=\"comment\"># 多少个子序列</span></span><br><span class=\"line\">    <span class=\"comment\"># 长度为num_steps的子序列的起始索引</span></span><br><span class=\"line\">    initial_indices = <span class=\"built_in\">list</span>(<span class=\"built_in\">range</span>(<span class=\"number\">0</span>, num_subseqs*num_steps, num_steps))</span><br><span class=\"line\">    <span class=\"comment\"># 在随机抽样的迭代过程中，来自两个相邻的、随机的、小批量的子序列不一定在原始序列上相邻</span></span><br><span class=\"line\">    random.shuffle(initial_indices)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">data</span>(<span class=\"params\">pos</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 返回从pos开始的长度为num_steps的序列</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> corpus[pos: pos + num_steps]</span><br><span class=\"line\">    </span><br><span class=\"line\">    num_batches = num_subseqs // batch_size <span class=\"comment\"># 将整个数据集分成多少个batch</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">0</span>, batch_size*num_batches, batch_size):</span><br><span class=\"line\">        <span class=\"comment\"># initial_indices包含子序列的随机起始索引</span></span><br><span class=\"line\">        initial_indices_per_batch = initial_indices[i: i + batch_size] <span class=\"comment\"># 每个batch的起始索引</span></span><br><span class=\"line\">        X = [data(j) <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> initial_indices_per_batch] <span class=\"comment\"># 获得batch_size个子序列</span></span><br><span class=\"line\">        Y = [data(j + <span class=\"number\">1</span>) <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> initial_indices_per_batch]</span><br><span class=\"line\">        <span class=\"keyword\">yield</span> torch.tensor(X), torch.tensor(Y)</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>生成一个从0到34的序列。batch_size&#x3D;2，num_steps&#x3D;5。可以生成(35-1)&#x2F;5&#x3D;6个子序列对，6&#x2F;2&#x3D;3个小批量。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">my_seq = <span class=\"built_in\">list</span>(<span class=\"built_in\">range</span>(<span class=\"number\">35</span>))</span><br><span class=\"line\"><span class=\"keyword\">for</span> X,Y <span class=\"keyword\">in</span> seq_data_iter_random(my_seq, batch_size=<span class=\"number\">2</span>, num_steps=<span class=\"number\">5</span>):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;X: &#x27;</span>, X, <span class=\"string\">&#x27;\\nY: &#x27;</span>, Y)</span><br></pre></td></tr></table></figure>\n\n<pre><code>X:  tensor([[ 7,  8,  9, 10, 11],\n        [27, 28, 29, 30, 31]]) \nY:  tensor([[ 8,  9, 10, 11, 12],\n        [28, 29, 30, 31, 32]])\nX:  tensor([[12, 13, 14, 15, 16],\n        [ 2,  3,  4,  5,  6]]) \nY:  tensor([[13, 14, 15, 16, 17],\n        [ 3,  4,  5,  6,  7]])\nX:  tensor([[22, 23, 24, 25, 26],\n        [17, 18, 19, 20, 21]]) \nY:  tensor([[23, 24, 25, 26, 27],\n        [18, 19, 20, 21, 22]])\n</code></pre>\n<h3 id=\"3-4-2-顺序分区\"><a href=\"#3-4-2-顺序分区\" class=\"headerlink\" title=\"3.4.2 顺序分区\"></a>3.4.2 顺序分区</h3><ul>\n<li>迭代过程中，除了对原始序列可以随机抽样外，我们还可以保证两个相邻的小批量中的子序列在原始序列上也是相邻的。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">seq_data_iter_sequential</span>(<span class=\"params\">corpus, batch_size, num_steps</span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;使用顺序分区生成一个小批量子序列&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"comment\"># 从随机偏移量开始划分序列</span></span><br><span class=\"line\">    offset = random.randint(<span class=\"number\">0</span>, num_steps)</span><br><span class=\"line\">    num_tokens = ((<span class=\"built_in\">len</span>(corpus) - offset - <span class=\"number\">1</span>) // batch_size) * batch_size <span class=\"comment\"># 可用于训练的tokens数量</span></span><br><span class=\"line\">    Xs = torch.tensor(corpus[offset: offset + num_tokens])</span><br><span class=\"line\">    Ys = torch.tensor(corpus[offset + <span class=\"number\">1</span>: offset + <span class=\"number\">1</span> + num_tokens])</span><br><span class=\"line\">    Xs, Ys = Xs.reshape(batch_size, -<span class=\"number\">1</span>), Ys.reshape(batch_size, -<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Xs：&#x27;</span>,Xs)</span><br><span class=\"line\">    num_batches = Xs.shape[<span class=\"number\">1</span>] // num_steps <span class=\"comment\"># batch_size是子序列个数，不是token个数</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">0</span>, num_steps*num_batches, num_steps):</span><br><span class=\"line\">        X = Xs[:, i: i+num_steps] <span class=\"comment\">#一个batch里面包含token数：num_steps*batch_size</span></span><br><span class=\"line\">        Y = Ys[:, i: i+num_steps]</span><br><span class=\"line\">        <span class=\"keyword\">yield</span> X, Y</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> X,Y <span class=\"keyword\">in</span> seq_data_iter_sequential(my_seq, batch_size=<span class=\"number\">2</span>, num_steps=<span class=\"number\">5</span>):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;X: &#x27;</span>, X, <span class=\"string\">&#x27;\\nY: &#x27;</span>, Y)</span><br></pre></td></tr></table></figure>\n\n<pre><code>Xs： tensor([[ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17],\n        [18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]])\nX:  tensor([[ 2,  3,  4,  5,  6],\n        [18, 19, 20, 21, 22]]) \nY:  tensor([[ 3,  4,  5,  6,  7],\n        [19, 20, 21, 22, 23]])\nX:  tensor([[ 7,  8,  9, 10, 11],\n        [23, 24, 25, 26, 27]]) \nY:  tensor([[ 8,  9, 10, 11, 12],\n        [24, 25, 26, 27, 28]])\nX:  tensor([[12, 13, 14, 15, 16],\n        [28, 29, 30, 31, 32]]) \nY:  tensor([[13, 14, 15, 16, 17],\n        [29, 30, 31, 32, 33]])\n</code></pre>\n<ul>\n<li>将两个采样函数包装到一个类中，以便稍后可以将其用作数据迭代器。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">SeqDataLoader</span>: <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;加载序列数据的迭代器&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, batch_size, num_steps, use_random_iter, max_tokens</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> use_random_iter:</span><br><span class=\"line\">            self.data_iter_fn = d2l.seq_data_iter_random</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            self.data_iter_fn = d2l.seq_data_iter_sequential</span><br><span class=\"line\">        self.corpus, self.vocab = d2l.load_corpus_time_machine(max_tokens)</span><br><span class=\"line\">        self.batch_size, self.num_steps = batch_size, num_steps</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__iter__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)</span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"comment\"># 定义函数，同时返回数据迭代器和词表，因此可以与其他带有load_data前缀的函数（如d2l.load_data_fashion_mnist）类似地使用。</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">load_data_time_machine</span>(<span class=\"params\">batch_size, num_steps, <span class=\"comment\">#@save</span></span></span><br><span class=\"line\"><span class=\"params\">                           use_random_iter=<span class=\"literal\">False</span>, max_tokens=<span class=\"number\">10000</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;返回时光机数据集的迭代器和词表&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    data_iter = SeqDataLoader(batch_size, num_steps, use_random_iter, max_tokens)</span><br></pre></td></tr></table></figure>"},{"title":"7.2 文本预处理","date":"2024-05-01T00:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n\n## 2 文本预处理\n- 将文本作为字符串加载到内存中。\n- 将字符串拆分为词元（如单词和字符）。\n- 建立一个词表，将拆分的词元映射到数字索引。\n- 将文本转换为数字索引序列，方便模型操作。\n\n\n```python\nimport collections\nimport re\nfrom d2l import torch as d2l\n```\n\n## 2.1 读取数据集\n- 从一个小语料库加载文本，包含30000多个单词\n- 将数据集读取到由多条文本行组成的列表中，其中每条文本行都是一个字符串。\n\n\n```python\n#@save\nd2l.DATA_HUB['time_machine'] = (\n    d2l.DATA_URL + 'timemachine.txt', '090b5e7e70c295757f55df93cb0a180b9691891a'\n)\n\ndef read_time_machine(): #@save\n    '''将时间机器数据集加载到文本的列表中'''\n    with open(d2l.download('time_machine'), 'r') as f:\n        lines = f.readlines()\n    return [re.sub('[^A-Za-z]+',' ', line).strip().lower() for line in lines] #忽略标点符号和大小写\n\nlines = read_time_machine()\nprint(f'# 文本总行数：{len(lines)}')\nprint(lines[0])\nprint(lines[10])\n```\n\n    Downloading ..\\data\\timemachine.txt from http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt...\n    # 文本总行数：3221\n    the time machine by h g wells\n    twinkled and his usually pale face was flushed and animated the\n    \n\n## 2.2 词元化\n- 将文本行列表（lines）作为输入，列表中的每个元素是一个文本序列（如一条文本行）\n- 每个文本序列又被拆分成一个词元列表，词元（token）是文本的基本单位。\n\n\n```python\ndef tokenize(lines, token='word'): #@save\n    '''将文本行拆分为单词或字符词元'''\n    if token == 'word':\n        return [line.split() for line in lines]\n    elif token == 'char':\n        return [list(line) for line in lines]\n    else:\n        print('错误，未知词元类型：' + token)\n\ntokens = tokenize(lines)\nfor i in range(11):\n    print(tokens[i])\n```\n\n    ['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n    []\n    []\n    []\n    []\n    ['i']\n    []\n    []\n    ['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him']\n    ['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']\n    ['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n    \n\n## 2.3 词表\n- 词元的类型是字符串，而模型需要的输入是数字。我们需要构建一个字典，通常也叫做词表（vocabulary），用来将字符串类型的词元映射到从0开始的数字索引中。\n- 先将训练集中的所有文档合并在一起，对它们的唯一词元进行统计，得到的统计结果称之为语料（corpus）。然后根据每个唯一词元的出现频率，为其分配一个数字索引。很少出现的词元通常被移除，这可以降低复杂性。语料库中不存在或已删除的任何词元都将映射到一个特定的未知词元“<unk>”。\n- 可以选择增加一个列表，用于保存那些被保留的词元，例如：填充词元（“<pad>”）；序列开始词元（“<bos>”）；序列结束词元（“<eos>”）\n\n\n```python\nclass Vocab: #@save\n    '''文本词表'''\n    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n        if tokens is None:\n            tokens = []\n        if reserved_tokens is None:\n            reserved_tokens = []\n        # 按出现频率排序\n        counter = count_corpus(tokens) # Counter({'the': 2261, 'i': 1267})\n        self._token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True) # [('the', 2261), ('i', 1267)]\n        # 未知词元的索引为0\n        self.idx_to_token = ['<unk>'] + reserved_tokens #['<unk>']\n        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)} # {'<unk>': 0}\n        for token, freq in self._token_freqs:\n            if freq< min_freq: #出现频率小于min_freq的词元被忽略\n                continue\n            if token not in self.token_to_idx:\n                self.idx_to_token.append(token)\n                self.token_to_idx[token] = len(self.idx_to_token) -1\n    def __len__(self):\n        return len(self.idx_to_token)\n    def __getitem__(self, tokens):\n        if not isinstance(tokens, (list, tuple)):\n            return self.token_to_idx.get(tokens, self.unk) # 返回token_to_idx[tokens]，无则返回self.unk\n        return [self.__getitem__(token) for token in tokens]\n    def to_tokens(self, indices):\n        if not isinstance(indices, (list, tuple)):\n            return self.idx_to_token[indices]\n        return [self.idx_to_token[index] for index in indices]\n    @property\n    def unk(self): # 未知词元的索引为0\n        return 0\n    @property\n    def token_freqs(self):\n        return self._token_freqs\n    \ndef count_corpus(tokens): #@save\n    '''统计词元的频率'''\n    # 这里的tokens是1D列表或2D列表\n    if len(tokens) == 0 or isinstance(tokens[0], list):\n        # 将词元列表展平成一个列表\n        tokens = [token for line in tokens for token in line]\n    return collections.Counter(tokens) # Counter({'the': 2261, 'i': 1267})\n```\n\n- 用语料库来构建词表，然后打印前几个高频词元及其索引。\n\n\n```python\nvocab = Vocab(tokens)\nprint(list(vocab.token_to_idx.items())[:10])\n```\n\n    [('<unk>', 0), ('the', 1), ('i', 2), ('and', 3), ('of', 4), ('a', 5), ('to', 6), ('was', 7), ('in', 8), ('that', 9)]\n    \n\n- 将每一条文本行转换成一个数字索引列表。\n\n\n```python\nfor i in [0, 10]:\n    print('文本：',tokens[i])\n    print('索引：',vocab[tokens[i]])\n```\n\n    文本： ['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n    索引： [1, 19, 50, 40, 2183, 2184, 400]\n    文本： ['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n    索引： [2186, 3, 25, 1044, 362, 113, 7, 1421, 3, 1045, 1]\n    \n\n## 2.4 整合所有功能\n- 将所有功能打包到load_corpus_time_machine函数中，该函数返回corpus（词元索引列表）和vocab（时光机器语料库的词表）。\n- 为了简化训练，使用字符（而不是单词）实现文本词元化；\n- 数据集中的每个文本行不一定是一个句子或一个段落，还可能是一个单词，因此返回的corpus仅处理为单个列表，而不是使用多词元列表构成的一个列表。\n\n\n```python\ndef load_corpus_time_machine(max_tokens=-1): #@save\n    '''返回时光机数据集的词元索引列表和词表'''\n    lines = read_time_machine()\n    tokens = tokenize(lines, 'char')\n    vocab = Vocab(tokens)\n    # 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，所以将所有文本行展平到一个列表中\n    corpus = [vocab[token] for line in tokens for token in line]\n    if max_tokens > 0:\n        corpus = corpus[:max_tokens]\n    return corpus, vocab\n\ncorpus, vocab = load_corpus_time_machine()\nprint(len(corpus), len(vocab))\n```\n\n    170580 28\n    \n","source":"_posts/deeplearning/code/pytorch/7_rnn/2_textprecess.md","raw":"---\ntitle: 7.2 文本预处理\ndate: 2024-5-1 08:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n\n## 2 文本预处理\n- 将文本作为字符串加载到内存中。\n- 将字符串拆分为词元（如单词和字符）。\n- 建立一个词表，将拆分的词元映射到数字索引。\n- 将文本转换为数字索引序列，方便模型操作。\n\n\n```python\nimport collections\nimport re\nfrom d2l import torch as d2l\n```\n\n## 2.1 读取数据集\n- 从一个小语料库加载文本，包含30000多个单词\n- 将数据集读取到由多条文本行组成的列表中，其中每条文本行都是一个字符串。\n\n\n```python\n#@save\nd2l.DATA_HUB['time_machine'] = (\n    d2l.DATA_URL + 'timemachine.txt', '090b5e7e70c295757f55df93cb0a180b9691891a'\n)\n\ndef read_time_machine(): #@save\n    '''将时间机器数据集加载到文本的列表中'''\n    with open(d2l.download('time_machine'), 'r') as f:\n        lines = f.readlines()\n    return [re.sub('[^A-Za-z]+',' ', line).strip().lower() for line in lines] #忽略标点符号和大小写\n\nlines = read_time_machine()\nprint(f'# 文本总行数：{len(lines)}')\nprint(lines[0])\nprint(lines[10])\n```\n\n    Downloading ..\\data\\timemachine.txt from http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt...\n    # 文本总行数：3221\n    the time machine by h g wells\n    twinkled and his usually pale face was flushed and animated the\n    \n\n## 2.2 词元化\n- 将文本行列表（lines）作为输入，列表中的每个元素是一个文本序列（如一条文本行）\n- 每个文本序列又被拆分成一个词元列表，词元（token）是文本的基本单位。\n\n\n```python\ndef tokenize(lines, token='word'): #@save\n    '''将文本行拆分为单词或字符词元'''\n    if token == 'word':\n        return [line.split() for line in lines]\n    elif token == 'char':\n        return [list(line) for line in lines]\n    else:\n        print('错误，未知词元类型：' + token)\n\ntokens = tokenize(lines)\nfor i in range(11):\n    print(tokens[i])\n```\n\n    ['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n    []\n    []\n    []\n    []\n    ['i']\n    []\n    []\n    ['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him']\n    ['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']\n    ['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n    \n\n## 2.3 词表\n- 词元的类型是字符串，而模型需要的输入是数字。我们需要构建一个字典，通常也叫做词表（vocabulary），用来将字符串类型的词元映射到从0开始的数字索引中。\n- 先将训练集中的所有文档合并在一起，对它们的唯一词元进行统计，得到的统计结果称之为语料（corpus）。然后根据每个唯一词元的出现频率，为其分配一个数字索引。很少出现的词元通常被移除，这可以降低复杂性。语料库中不存在或已删除的任何词元都将映射到一个特定的未知词元“<unk>”。\n- 可以选择增加一个列表，用于保存那些被保留的词元，例如：填充词元（“<pad>”）；序列开始词元（“<bos>”）；序列结束词元（“<eos>”）\n\n\n```python\nclass Vocab: #@save\n    '''文本词表'''\n    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n        if tokens is None:\n            tokens = []\n        if reserved_tokens is None:\n            reserved_tokens = []\n        # 按出现频率排序\n        counter = count_corpus(tokens) # Counter({'the': 2261, 'i': 1267})\n        self._token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True) # [('the', 2261), ('i', 1267)]\n        # 未知词元的索引为0\n        self.idx_to_token = ['<unk>'] + reserved_tokens #['<unk>']\n        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)} # {'<unk>': 0}\n        for token, freq in self._token_freqs:\n            if freq< min_freq: #出现频率小于min_freq的词元被忽略\n                continue\n            if token not in self.token_to_idx:\n                self.idx_to_token.append(token)\n                self.token_to_idx[token] = len(self.idx_to_token) -1\n    def __len__(self):\n        return len(self.idx_to_token)\n    def __getitem__(self, tokens):\n        if not isinstance(tokens, (list, tuple)):\n            return self.token_to_idx.get(tokens, self.unk) # 返回token_to_idx[tokens]，无则返回self.unk\n        return [self.__getitem__(token) for token in tokens]\n    def to_tokens(self, indices):\n        if not isinstance(indices, (list, tuple)):\n            return self.idx_to_token[indices]\n        return [self.idx_to_token[index] for index in indices]\n    @property\n    def unk(self): # 未知词元的索引为0\n        return 0\n    @property\n    def token_freqs(self):\n        return self._token_freqs\n    \ndef count_corpus(tokens): #@save\n    '''统计词元的频率'''\n    # 这里的tokens是1D列表或2D列表\n    if len(tokens) == 0 or isinstance(tokens[0], list):\n        # 将词元列表展平成一个列表\n        tokens = [token for line in tokens for token in line]\n    return collections.Counter(tokens) # Counter({'the': 2261, 'i': 1267})\n```\n\n- 用语料库来构建词表，然后打印前几个高频词元及其索引。\n\n\n```python\nvocab = Vocab(tokens)\nprint(list(vocab.token_to_idx.items())[:10])\n```\n\n    [('<unk>', 0), ('the', 1), ('i', 2), ('and', 3), ('of', 4), ('a', 5), ('to', 6), ('was', 7), ('in', 8), ('that', 9)]\n    \n\n- 将每一条文本行转换成一个数字索引列表。\n\n\n```python\nfor i in [0, 10]:\n    print('文本：',tokens[i])\n    print('索引：',vocab[tokens[i]])\n```\n\n    文本： ['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n    索引： [1, 19, 50, 40, 2183, 2184, 400]\n    文本： ['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n    索引： [2186, 3, 25, 1044, 362, 113, 7, 1421, 3, 1045, 1]\n    \n\n## 2.4 整合所有功能\n- 将所有功能打包到load_corpus_time_machine函数中，该函数返回corpus（词元索引列表）和vocab（时光机器语料库的词表）。\n- 为了简化训练，使用字符（而不是单词）实现文本词元化；\n- 数据集中的每个文本行不一定是一个句子或一个段落，还可能是一个单词，因此返回的corpus仅处理为单个列表，而不是使用多词元列表构成的一个列表。\n\n\n```python\ndef load_corpus_time_machine(max_tokens=-1): #@save\n    '''返回时光机数据集的词元索引列表和词表'''\n    lines = read_time_machine()\n    tokens = tokenize(lines, 'char')\n    vocab = Vocab(tokens)\n    # 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，所以将所有文本行展平到一个列表中\n    corpus = [vocab[token] for line in tokens for token in line]\n    if max_tokens > 0:\n        corpus = corpus[:max_tokens]\n    return corpus, vocab\n\ncorpus, vocab = load_corpus_time_machine()\nprint(len(corpus), len(vocab))\n```\n\n    170580 28\n    \n","slug":"deeplearning/code/pytorch/7_rnn/2_textprecess","published":1,"updated":"2024-05-02T08:46:43.550Z","comments":1,"layout":"post","photos":[],"_id":"clvuli6uy0092zsvwa8jk06o1","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h2 id=\"2-文本预处理\"><a href=\"#2-文本预处理\" class=\"headerlink\" title=\"2 文本预处理\"></a>2 文本预处理</h2><ul>\n<li>将文本作为字符串加载到内存中。</li>\n<li>将字符串拆分为词元（如单词和字符）。</li>\n<li>建立一个词表，将拆分的词元映射到数字索引。</li>\n<li>将文本转换为数字索引序列，方便模型操作。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> collections</span><br><span class=\"line\"><span class=\"keyword\">import</span> re</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"2-1-读取数据集\"><a href=\"#2-1-读取数据集\" class=\"headerlink\" title=\"2.1 读取数据集\"></a>2.1 读取数据集</h2><ul>\n<li>从一个小语料库加载文本，包含30000多个单词</li>\n<li>将数据集读取到由多条文本行组成的列表中，其中每条文本行都是一个字符串。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#@save</span></span><br><span class=\"line\">d2l.DATA_HUB[<span class=\"string\">&#x27;time_machine&#x27;</span>] = (</span><br><span class=\"line\">    d2l.DATA_URL + <span class=\"string\">&#x27;timemachine.txt&#x27;</span>, <span class=\"string\">&#x27;090b5e7e70c295757f55df93cb0a180b9691891a&#x27;</span></span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">read_time_machine</span>(): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;将时间机器数据集加载到文本的列表中&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(d2l.download(<span class=\"string\">&#x27;time_machine&#x27;</span>), <span class=\"string\">&#x27;r&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">        lines = f.readlines()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> [re.sub(<span class=\"string\">&#x27;[^A-Za-z]+&#x27;</span>,<span class=\"string\">&#x27; &#x27;</span>, line).strip().lower() <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> lines] <span class=\"comment\">#忽略标点符号和大小写</span></span><br><span class=\"line\"></span><br><span class=\"line\">lines = read_time_machine()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;# 文本总行数：<span class=\"subst\">&#123;<span class=\"built_in\">len</span>(lines)&#125;</span>&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(lines[<span class=\"number\">0</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(lines[<span class=\"number\">10</span>])</span><br></pre></td></tr></table></figure>\n\n<pre><code>Downloading ..\\data\\timemachine.txt from http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt...\n# 文本总行数：3221\nthe time machine by h g wells\ntwinkled and his usually pale face was flushed and animated the\n</code></pre>\n<h2 id=\"2-2-词元化\"><a href=\"#2-2-词元化\" class=\"headerlink\" title=\"2.2 词元化\"></a>2.2 词元化</h2><ul>\n<li>将文本行列表（lines）作为输入，列表中的每个元素是一个文本序列（如一条文本行）</li>\n<li>每个文本序列又被拆分成一个词元列表，词元（token）是文本的基本单位。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">tokenize</span>(<span class=\"params\">lines, token=<span class=\"string\">&#x27;word&#x27;</span></span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;将文本行拆分为单词或字符词元&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> token == <span class=\"string\">&#x27;word&#x27;</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> [line.split() <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> lines]</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> token == <span class=\"string\">&#x27;char&#x27;</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> [<span class=\"built_in\">list</span>(line) <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> lines]</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;错误，未知词元类型：&#x27;</span> + token)</span><br><span class=\"line\"></span><br><span class=\"line\">tokens = tokenize(lines)</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">11</span>):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(tokens[i])</span><br></pre></td></tr></table></figure>\n\n<pre><code>[&#39;the&#39;, &#39;time&#39;, &#39;machine&#39;, &#39;by&#39;, &#39;h&#39;, &#39;g&#39;, &#39;wells&#39;]\n[]\n[]\n[]\n[]\n[&#39;i&#39;]\n[]\n[]\n[&#39;the&#39;, &#39;time&#39;, &#39;traveller&#39;, &#39;for&#39;, &#39;so&#39;, &#39;it&#39;, &#39;will&#39;, &#39;be&#39;, &#39;convenient&#39;, &#39;to&#39;, &#39;speak&#39;, &#39;of&#39;, &#39;him&#39;]\n[&#39;was&#39;, &#39;expounding&#39;, &#39;a&#39;, &#39;recondite&#39;, &#39;matter&#39;, &#39;to&#39;, &#39;us&#39;, &#39;his&#39;, &#39;grey&#39;, &#39;eyes&#39;, &#39;shone&#39;, &#39;and&#39;]\n[&#39;twinkled&#39;, &#39;and&#39;, &#39;his&#39;, &#39;usually&#39;, &#39;pale&#39;, &#39;face&#39;, &#39;was&#39;, &#39;flushed&#39;, &#39;and&#39;, &#39;animated&#39;, &#39;the&#39;]\n</code></pre>\n<h2 id=\"2-3-词表\"><a href=\"#2-3-词表\" class=\"headerlink\" title=\"2.3 词表\"></a>2.3 词表</h2><ul>\n<li>词元的类型是字符串，而模型需要的输入是数字。我们需要构建一个字典，通常也叫做词表（vocabulary），用来将字符串类型的词元映射到从0开始的数字索引中。</li>\n<li>先将训练集中的所有文档合并在一起，对它们的唯一词元进行统计，得到的统计结果称之为语料（corpus）。然后根据每个唯一词元的出现频率，为其分配一个数字索引。很少出现的词元通常被移除，这可以降低复杂性。语料库中不存在或已删除的任何词元都将映射到一个特定的未知词元“<unk>”。</li>\n<li>可以选择增加一个列表，用于保存那些被保留的词元，例如：填充词元（“<pad>”）；序列开始词元（“<bos>”）；序列结束词元（“<eos>”）</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Vocab</span>: <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;文本词表&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, tokens=<span class=\"literal\">None</span>, min_freq=<span class=\"number\">0</span>, reserved_tokens=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> tokens <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            tokens = []</span><br><span class=\"line\">        <span class=\"keyword\">if</span> reserved_tokens <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            reserved_tokens = []</span><br><span class=\"line\">        <span class=\"comment\"># 按出现频率排序</span></span><br><span class=\"line\">        counter = count_corpus(tokens) <span class=\"comment\"># Counter(&#123;&#x27;the&#x27;: 2261, &#x27;i&#x27;: 1267&#125;)</span></span><br><span class=\"line\">        self._token_freqs = <span class=\"built_in\">sorted</span>(counter.items(), key=<span class=\"keyword\">lambda</span> x: x[<span class=\"number\">1</span>], reverse=<span class=\"literal\">True</span>) <span class=\"comment\"># [(&#x27;the&#x27;, 2261), (&#x27;i&#x27;, 1267)]</span></span><br><span class=\"line\">        <span class=\"comment\"># 未知词元的索引为0</span></span><br><span class=\"line\">        self.idx_to_token = [<span class=\"string\">&#x27;&lt;unk&gt;&#x27;</span>] + reserved_tokens <span class=\"comment\">#[&#x27;&lt;unk&gt;&#x27;]</span></span><br><span class=\"line\">        self.token_to_idx = &#123;token: idx <span class=\"keyword\">for</span> idx, token <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(self.idx_to_token)&#125; <span class=\"comment\"># &#123;&#x27;&lt;unk&gt;&#x27;: 0&#125;</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> token, freq <span class=\"keyword\">in</span> self._token_freqs:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> freq&lt; min_freq: <span class=\"comment\">#出现频率小于min_freq的词元被忽略</span></span><br><span class=\"line\">                <span class=\"keyword\">continue</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> token <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> self.token_to_idx:</span><br><span class=\"line\">                self.idx_to_token.append(token)</span><br><span class=\"line\">                self.token_to_idx[token] = <span class=\"built_in\">len</span>(self.idx_to_token) -<span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__len__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"built_in\">len</span>(self.idx_to_token)</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__getitem__</span>(<span class=\"params\">self, tokens</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> <span class=\"built_in\">isinstance</span>(tokens, (<span class=\"built_in\">list</span>, <span class=\"built_in\">tuple</span>)):</span><br><span class=\"line\">            <span class=\"keyword\">return</span> self.token_to_idx.get(tokens, self.unk) <span class=\"comment\"># 返回token_to_idx[tokens]，无则返回self.unk</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> [self.__getitem__(token) <span class=\"keyword\">for</span> token <span class=\"keyword\">in</span> tokens]</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">to_tokens</span>(<span class=\"params\">self, indices</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> <span class=\"built_in\">isinstance</span>(indices, (<span class=\"built_in\">list</span>, <span class=\"built_in\">tuple</span>)):</span><br><span class=\"line\">            <span class=\"keyword\">return</span> self.idx_to_token[indices]</span><br><span class=\"line\">        <span class=\"keyword\">return</span> [self.idx_to_token[index] <span class=\"keyword\">for</span> index <span class=\"keyword\">in</span> indices]</span><br><span class=\"line\"><span class=\"meta\">    @property</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">unk</span>(<span class=\"params\">self</span>): <span class=\"comment\"># 未知词元的索引为0</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"meta\">    @property</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">token_freqs</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self._token_freqs</span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">count_corpus</span>(<span class=\"params\">tokens</span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;统计词元的频率&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"comment\"># 这里的tokens是1D列表或2D列表</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(tokens) == <span class=\"number\">0</span> <span class=\"keyword\">or</span> <span class=\"built_in\">isinstance</span>(tokens[<span class=\"number\">0</span>], <span class=\"built_in\">list</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 将词元列表展平成一个列表</span></span><br><span class=\"line\">        tokens = [token <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> tokens <span class=\"keyword\">for</span> token <span class=\"keyword\">in</span> line]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> collections.Counter(tokens) <span class=\"comment\"># Counter(&#123;&#x27;the&#x27;: 2261, &#x27;i&#x27;: 1267&#125;)</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>用语料库来构建词表，然后打印前几个高频词元及其索引。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vocab = Vocab(tokens)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">list</span>(vocab.token_to_idx.items())[:<span class=\"number\">10</span>])</span><br></pre></td></tr></table></figure>\n\n<pre><code>[(&#39;&lt;unk&gt;&#39;, 0), (&#39;the&#39;, 1), (&#39;i&#39;, 2), (&#39;and&#39;, 3), (&#39;of&#39;, 4), (&#39;a&#39;, 5), (&#39;to&#39;, 6), (&#39;was&#39;, 7), (&#39;in&#39;, 8), (&#39;that&#39;, 9)]\n</code></pre>\n<ul>\n<li>将每一条文本行转换成一个数字索引列表。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> [<span class=\"number\">0</span>, <span class=\"number\">10</span>]:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;文本：&#x27;</span>,tokens[i])</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;索引：&#x27;</span>,vocab[tokens[i]])</span><br></pre></td></tr></table></figure>\n\n<pre><code>文本： [&#39;the&#39;, &#39;time&#39;, &#39;machine&#39;, &#39;by&#39;, &#39;h&#39;, &#39;g&#39;, &#39;wells&#39;]\n索引： [1, 19, 50, 40, 2183, 2184, 400]\n文本： [&#39;twinkled&#39;, &#39;and&#39;, &#39;his&#39;, &#39;usually&#39;, &#39;pale&#39;, &#39;face&#39;, &#39;was&#39;, &#39;flushed&#39;, &#39;and&#39;, &#39;animated&#39;, &#39;the&#39;]\n索引： [2186, 3, 25, 1044, 362, 113, 7, 1421, 3, 1045, 1]\n</code></pre>\n<h2 id=\"2-4-整合所有功能\"><a href=\"#2-4-整合所有功能\" class=\"headerlink\" title=\"2.4 整合所有功能\"></a>2.4 整合所有功能</h2><ul>\n<li>将所有功能打包到load_corpus_time_machine函数中，该函数返回corpus（词元索引列表）和vocab（时光机器语料库的词表）。</li>\n<li>为了简化训练，使用字符（而不是单词）实现文本词元化；</li>\n<li>数据集中的每个文本行不一定是一个句子或一个段落，还可能是一个单词，因此返回的corpus仅处理为单个列表，而不是使用多词元列表构成的一个列表。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">load_corpus_time_machine</span>(<span class=\"params\">max_tokens=-<span class=\"number\">1</span></span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;返回时光机数据集的词元索引列表和词表&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    lines = read_time_machine()</span><br><span class=\"line\">    tokens = tokenize(lines, <span class=\"string\">&#x27;char&#x27;</span>)</span><br><span class=\"line\">    vocab = Vocab(tokens)</span><br><span class=\"line\">    <span class=\"comment\"># 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，所以将所有文本行展平到一个列表中</span></span><br><span class=\"line\">    corpus = [vocab[token] <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> tokens <span class=\"keyword\">for</span> token <span class=\"keyword\">in</span> line]</span><br><span class=\"line\">    <span class=\"keyword\">if</span> max_tokens &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">        corpus = corpus[:max_tokens]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> corpus, vocab</span><br><span class=\"line\"></span><br><span class=\"line\">corpus, vocab = load_corpus_time_machine()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">len</span>(corpus), <span class=\"built_in\">len</span>(vocab))</span><br></pre></td></tr></table></figure>\n\n<pre><code>170580 28\n</code></pre>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h2 id=\"2-文本预处理\"><a href=\"#2-文本预处理\" class=\"headerlink\" title=\"2 文本预处理\"></a>2 文本预处理</h2><ul>\n<li>将文本作为字符串加载到内存中。</li>\n<li>将字符串拆分为词元（如单词和字符）。</li>\n<li>建立一个词表，将拆分的词元映射到数字索引。</li>\n<li>将文本转换为数字索引序列，方便模型操作。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> collections</span><br><span class=\"line\"><span class=\"keyword\">import</span> re</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"2-1-读取数据集\"><a href=\"#2-1-读取数据集\" class=\"headerlink\" title=\"2.1 读取数据集\"></a>2.1 读取数据集</h2><ul>\n<li>从一个小语料库加载文本，包含30000多个单词</li>\n<li>将数据集读取到由多条文本行组成的列表中，其中每条文本行都是一个字符串。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#@save</span></span><br><span class=\"line\">d2l.DATA_HUB[<span class=\"string\">&#x27;time_machine&#x27;</span>] = (</span><br><span class=\"line\">    d2l.DATA_URL + <span class=\"string\">&#x27;timemachine.txt&#x27;</span>, <span class=\"string\">&#x27;090b5e7e70c295757f55df93cb0a180b9691891a&#x27;</span></span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">read_time_machine</span>(): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;将时间机器数据集加载到文本的列表中&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(d2l.download(<span class=\"string\">&#x27;time_machine&#x27;</span>), <span class=\"string\">&#x27;r&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">        lines = f.readlines()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> [re.sub(<span class=\"string\">&#x27;[^A-Za-z]+&#x27;</span>,<span class=\"string\">&#x27; &#x27;</span>, line).strip().lower() <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> lines] <span class=\"comment\">#忽略标点符号和大小写</span></span><br><span class=\"line\"></span><br><span class=\"line\">lines = read_time_machine()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;# 文本总行数：<span class=\"subst\">&#123;<span class=\"built_in\">len</span>(lines)&#125;</span>&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(lines[<span class=\"number\">0</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(lines[<span class=\"number\">10</span>])</span><br></pre></td></tr></table></figure>\n\n<pre><code>Downloading ..\\data\\timemachine.txt from http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt...\n# 文本总行数：3221\nthe time machine by h g wells\ntwinkled and his usually pale face was flushed and animated the\n</code></pre>\n<h2 id=\"2-2-词元化\"><a href=\"#2-2-词元化\" class=\"headerlink\" title=\"2.2 词元化\"></a>2.2 词元化</h2><ul>\n<li>将文本行列表（lines）作为输入，列表中的每个元素是一个文本序列（如一条文本行）</li>\n<li>每个文本序列又被拆分成一个词元列表，词元（token）是文本的基本单位。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">tokenize</span>(<span class=\"params\">lines, token=<span class=\"string\">&#x27;word&#x27;</span></span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;将文本行拆分为单词或字符词元&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> token == <span class=\"string\">&#x27;word&#x27;</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> [line.split() <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> lines]</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> token == <span class=\"string\">&#x27;char&#x27;</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> [<span class=\"built_in\">list</span>(line) <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> lines]</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;错误，未知词元类型：&#x27;</span> + token)</span><br><span class=\"line\"></span><br><span class=\"line\">tokens = tokenize(lines)</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">11</span>):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(tokens[i])</span><br></pre></td></tr></table></figure>\n\n<pre><code>[&#39;the&#39;, &#39;time&#39;, &#39;machine&#39;, &#39;by&#39;, &#39;h&#39;, &#39;g&#39;, &#39;wells&#39;]\n[]\n[]\n[]\n[]\n[&#39;i&#39;]\n[]\n[]\n[&#39;the&#39;, &#39;time&#39;, &#39;traveller&#39;, &#39;for&#39;, &#39;so&#39;, &#39;it&#39;, &#39;will&#39;, &#39;be&#39;, &#39;convenient&#39;, &#39;to&#39;, &#39;speak&#39;, &#39;of&#39;, &#39;him&#39;]\n[&#39;was&#39;, &#39;expounding&#39;, &#39;a&#39;, &#39;recondite&#39;, &#39;matter&#39;, &#39;to&#39;, &#39;us&#39;, &#39;his&#39;, &#39;grey&#39;, &#39;eyes&#39;, &#39;shone&#39;, &#39;and&#39;]\n[&#39;twinkled&#39;, &#39;and&#39;, &#39;his&#39;, &#39;usually&#39;, &#39;pale&#39;, &#39;face&#39;, &#39;was&#39;, &#39;flushed&#39;, &#39;and&#39;, &#39;animated&#39;, &#39;the&#39;]\n</code></pre>\n<h2 id=\"2-3-词表\"><a href=\"#2-3-词表\" class=\"headerlink\" title=\"2.3 词表\"></a>2.3 词表</h2><ul>\n<li>词元的类型是字符串，而模型需要的输入是数字。我们需要构建一个字典，通常也叫做词表（vocabulary），用来将字符串类型的词元映射到从0开始的数字索引中。</li>\n<li>先将训练集中的所有文档合并在一起，对它们的唯一词元进行统计，得到的统计结果称之为语料（corpus）。然后根据每个唯一词元的出现频率，为其分配一个数字索引。很少出现的词元通常被移除，这可以降低复杂性。语料库中不存在或已删除的任何词元都将映射到一个特定的未知词元“<unk>”。</li>\n<li>可以选择增加一个列表，用于保存那些被保留的词元，例如：填充词元（“<pad>”）；序列开始词元（“<bos>”）；序列结束词元（“<eos>”）</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Vocab</span>: <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;文本词表&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, tokens=<span class=\"literal\">None</span>, min_freq=<span class=\"number\">0</span>, reserved_tokens=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> tokens <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            tokens = []</span><br><span class=\"line\">        <span class=\"keyword\">if</span> reserved_tokens <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            reserved_tokens = []</span><br><span class=\"line\">        <span class=\"comment\"># 按出现频率排序</span></span><br><span class=\"line\">        counter = count_corpus(tokens) <span class=\"comment\"># Counter(&#123;&#x27;the&#x27;: 2261, &#x27;i&#x27;: 1267&#125;)</span></span><br><span class=\"line\">        self._token_freqs = <span class=\"built_in\">sorted</span>(counter.items(), key=<span class=\"keyword\">lambda</span> x: x[<span class=\"number\">1</span>], reverse=<span class=\"literal\">True</span>) <span class=\"comment\"># [(&#x27;the&#x27;, 2261), (&#x27;i&#x27;, 1267)]</span></span><br><span class=\"line\">        <span class=\"comment\"># 未知词元的索引为0</span></span><br><span class=\"line\">        self.idx_to_token = [<span class=\"string\">&#x27;&lt;unk&gt;&#x27;</span>] + reserved_tokens <span class=\"comment\">#[&#x27;&lt;unk&gt;&#x27;]</span></span><br><span class=\"line\">        self.token_to_idx = &#123;token: idx <span class=\"keyword\">for</span> idx, token <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(self.idx_to_token)&#125; <span class=\"comment\"># &#123;&#x27;&lt;unk&gt;&#x27;: 0&#125;</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> token, freq <span class=\"keyword\">in</span> self._token_freqs:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> freq&lt; min_freq: <span class=\"comment\">#出现频率小于min_freq的词元被忽略</span></span><br><span class=\"line\">                <span class=\"keyword\">continue</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> token <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> self.token_to_idx:</span><br><span class=\"line\">                self.idx_to_token.append(token)</span><br><span class=\"line\">                self.token_to_idx[token] = <span class=\"built_in\">len</span>(self.idx_to_token) -<span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__len__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"built_in\">len</span>(self.idx_to_token)</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__getitem__</span>(<span class=\"params\">self, tokens</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> <span class=\"built_in\">isinstance</span>(tokens, (<span class=\"built_in\">list</span>, <span class=\"built_in\">tuple</span>)):</span><br><span class=\"line\">            <span class=\"keyword\">return</span> self.token_to_idx.get(tokens, self.unk) <span class=\"comment\"># 返回token_to_idx[tokens]，无则返回self.unk</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> [self.__getitem__(token) <span class=\"keyword\">for</span> token <span class=\"keyword\">in</span> tokens]</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">to_tokens</span>(<span class=\"params\">self, indices</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> <span class=\"built_in\">isinstance</span>(indices, (<span class=\"built_in\">list</span>, <span class=\"built_in\">tuple</span>)):</span><br><span class=\"line\">            <span class=\"keyword\">return</span> self.idx_to_token[indices]</span><br><span class=\"line\">        <span class=\"keyword\">return</span> [self.idx_to_token[index] <span class=\"keyword\">for</span> index <span class=\"keyword\">in</span> indices]</span><br><span class=\"line\"><span class=\"meta\">    @property</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">unk</span>(<span class=\"params\">self</span>): <span class=\"comment\"># 未知词元的索引为0</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"meta\">    @property</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">token_freqs</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self._token_freqs</span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">count_corpus</span>(<span class=\"params\">tokens</span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;统计词元的频率&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"comment\"># 这里的tokens是1D列表或2D列表</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(tokens) == <span class=\"number\">0</span> <span class=\"keyword\">or</span> <span class=\"built_in\">isinstance</span>(tokens[<span class=\"number\">0</span>], <span class=\"built_in\">list</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 将词元列表展平成一个列表</span></span><br><span class=\"line\">        tokens = [token <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> tokens <span class=\"keyword\">for</span> token <span class=\"keyword\">in</span> line]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> collections.Counter(tokens) <span class=\"comment\"># Counter(&#123;&#x27;the&#x27;: 2261, &#x27;i&#x27;: 1267&#125;)</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>用语料库来构建词表，然后打印前几个高频词元及其索引。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vocab = Vocab(tokens)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">list</span>(vocab.token_to_idx.items())[:<span class=\"number\">10</span>])</span><br></pre></td></tr></table></figure>\n\n<pre><code>[(&#39;&lt;unk&gt;&#39;, 0), (&#39;the&#39;, 1), (&#39;i&#39;, 2), (&#39;and&#39;, 3), (&#39;of&#39;, 4), (&#39;a&#39;, 5), (&#39;to&#39;, 6), (&#39;was&#39;, 7), (&#39;in&#39;, 8), (&#39;that&#39;, 9)]\n</code></pre>\n<ul>\n<li>将每一条文本行转换成一个数字索引列表。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> [<span class=\"number\">0</span>, <span class=\"number\">10</span>]:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;文本：&#x27;</span>,tokens[i])</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;索引：&#x27;</span>,vocab[tokens[i]])</span><br></pre></td></tr></table></figure>\n\n<pre><code>文本： [&#39;the&#39;, &#39;time&#39;, &#39;machine&#39;, &#39;by&#39;, &#39;h&#39;, &#39;g&#39;, &#39;wells&#39;]\n索引： [1, 19, 50, 40, 2183, 2184, 400]\n文本： [&#39;twinkled&#39;, &#39;and&#39;, &#39;his&#39;, &#39;usually&#39;, &#39;pale&#39;, &#39;face&#39;, &#39;was&#39;, &#39;flushed&#39;, &#39;and&#39;, &#39;animated&#39;, &#39;the&#39;]\n索引： [2186, 3, 25, 1044, 362, 113, 7, 1421, 3, 1045, 1]\n</code></pre>\n<h2 id=\"2-4-整合所有功能\"><a href=\"#2-4-整合所有功能\" class=\"headerlink\" title=\"2.4 整合所有功能\"></a>2.4 整合所有功能</h2><ul>\n<li>将所有功能打包到load_corpus_time_machine函数中，该函数返回corpus（词元索引列表）和vocab（时光机器语料库的词表）。</li>\n<li>为了简化训练，使用字符（而不是单词）实现文本词元化；</li>\n<li>数据集中的每个文本行不一定是一个句子或一个段落，还可能是一个单词，因此返回的corpus仅处理为单个列表，而不是使用多词元列表构成的一个列表。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">load_corpus_time_machine</span>(<span class=\"params\">max_tokens=-<span class=\"number\">1</span></span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;返回时光机数据集的词元索引列表和词表&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    lines = read_time_machine()</span><br><span class=\"line\">    tokens = tokenize(lines, <span class=\"string\">&#x27;char&#x27;</span>)</span><br><span class=\"line\">    vocab = Vocab(tokens)</span><br><span class=\"line\">    <span class=\"comment\"># 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，所以将所有文本行展平到一个列表中</span></span><br><span class=\"line\">    corpus = [vocab[token] <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> tokens <span class=\"keyword\">for</span> token <span class=\"keyword\">in</span> line]</span><br><span class=\"line\">    <span class=\"keyword\">if</span> max_tokens &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">        corpus = corpus[:max_tokens]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> corpus, vocab</span><br><span class=\"line\"></span><br><span class=\"line\">corpus, vocab = load_corpus_time_machine()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">len</span>(corpus), <span class=\"built_in\">len</span>(vocab))</span><br></pre></td></tr></table></figure>\n\n<pre><code>170580 28\n</code></pre>"},{"title":"7.4 循环神经网络","date":"2024-05-03T03:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 4 循环神经网络\n- n元语法模型中，单词$x_ t$在时间步t的条件概率仅取决于前面n − 1个单词。对于时间步t − (n − 1)之前的单词，如果我们想将其可能产生的影响合并到$x_ t$上，需要增加n，然而模型参数的数量也会随之呈指数增长，因为词表$V$需要存储$|V|^ n$个数字，因此与其将$P(x_ t| x_ {t-1}, \\cdots , x_ {t-n+1})$模型化，不如使用隐变量模型：\n$$P(x_ t | x_ {t-1}, \\cdots , x_ 1) \\approx P(x_ t | h_ {t-1})$$\n\n- $h_ {t-1}$是隐状态（hidden state），也称为隐藏变量（hidden variable），它存储了到时间步t − 1的序列信息。通常，我们可以基于当前输入xt和先前隐状态ht−1 来计算时间步t处的任何时间的隐状态：\n$$h_ t = f(x_ t, h_ {t-1})$$\n\n- 对于函数$f$，隐变量模型不是近似值。毕竟$h_ t$是可以仅仅存储到目前为止观察到的所有数据，然而这样的操作可能会使计算和存储的代价都变得昂贵。\n- 隐藏层和隐状态指的是两个截然不同的概念。隐藏层是在从输入到输出的路径上（以观测角度来理解）的隐藏的层，而隐状态则是在给定步骤所做的任何事情（以技术角度来定义）的输入，并且这些状态只能通过先前时间步的数据来计算。\n## 4.1 无隐状态的神经网络\n- 对于只有单隐藏层的多层感知机，设隐藏层的激活函数为$\\phi$，给定一个小批量样本$\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$，其中$n$是批量大小，$d$是输入维度，隐藏层的输出为：\n$$\\mathbf{H} = \\phi(\\mathbf{X} \\mathbf{W}_ {xh} + \\mathbf{b}_ h)$$\n\n- 其中，隐藏层权重$\\mathbf{W}_ {xh} \\in \\mathbb{R}^{d \\times h}$，隐藏层偏置$\\mathbf{b}_ h \\in \\mathbb{R}^{1 \\times h}$，$h$是隐藏单元的个数。然后将隐藏变量$\\mathbf{H}$作为输出层的输入，输出层的输出为：\n$$\\mathbf{O} = \\mathbf{H} \\mathbf{W}_ {hq} + \\mathbf{b}_ q$$\n\n- 其中，$\\mathbf{O}$是输出变量。如果是分类问题，可以用$\\text{softmax}(\\mathbf{O})$来计算输出类别的概率分布。\n\n## 4.2 有隐状态的循环神经网络\n- 假设我们在时间步t有小批量输入$\\mathbf{X}_ t \\in \\mathbb{R}^{n \\times d}$（样本数为n，$\\mathbf{X}_ t$的每一行对应于来自该序列的时间步t处的一个样本。\n- 用$\\mathbf{H}_ t \\in \\mathbb{R}^{n \\times h}$表示时间步t的隐藏变量。与mlp不同的是，我们在这里保存了前一个时间步的隐藏变量$\\mathbf{H}_ {t-1}$，并引入了一个新的权重参数$\\mathbf{W}_ {hh} \\in \\mathbb{R}^{h \\times h}$来描述如何在当前时间步中使用前一个时间步的隐藏变量。\n- 当前时间步隐藏变量由当前时间步的输入与前一个时间步的隐藏变量一起计算得出：\n$$\\mathbf{H}_ t = \\phi(\\mathbf{X}_ t \\mathbf{W}_ {xh} + \\mathbf{H}_ {t-1} \\mathbf{W}_ {hh} + \\mathbf{b}_ h)$$\n\n- 与之前的相比，多了一项$\\mathbf{H}_ {t-1} \\mathbf{W}_ {hh}$，这一项描述了如何利用前一个时间步的隐藏变量。从相邻时间步的隐藏变量$\\mathbf{H}_ {t}$和$\\mathbf{H}_ {t-1}$之间的关系可知，这些变量捕获并保留了序列直到其当前时间步的历史信息，因此这样的隐藏变量被称为隐状态（hidden state）。由于在当前时间步中，隐状态使用的定义与前一个时间步中使用的定义相同，因此上式的计算是循环的。于是基于循环计算的隐状态神经网络被命名为 循环神经网络（recurrent neural network）。在循环神经网络中执行上式计算的层称为循环层。\n- 有许多不同的方法可以构建循环神经网络，由上式定义的隐状态的循环神经网络是非常常见的一种。对于时间步t，输出层的输出类似于多层感知机中的计算：\n$$\\mathbf{O}_ t = \\mathbf{H}_ t \\mathbf{W}_ {hq} + \\mathbf{b}_ q$$\n\n- 循环神经网络的参数包括:\n    - 隐藏层权重：$\\mathbf{W}_ {xh} \\in \\mathbb{R}^{d \\times h}$，$\\mathbf{W}_ {hh} \\in \\mathbb{R}^{h \\times h}$\n    - 隐藏层偏置：$\\mathbf{b}_ h \\in \\mathbb{R}^{1 \\times h}$\n    - 输出层权重：$\\mathbf{W}_ {hq} \\in \\mathbb{R}^{h \\times q}$\n    - 输出层偏置：$\\mathbf{b}_ q \\in \\mathbb{R}^{1 \\times q}$\n    - 在不同的时间步，循环神经网络也总是使用这些模型参数。因此，循环神经网络的参数开销不会随着时间步的增加而增加。\n- 下图展示了循环神经网络在三个相邻时间步的计算逻辑。\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/7_rnn/4_rnn/1.png)\n![](img/deeplearning/code/pytorch/7_rnn/4_rnn/1.png)\n\n- 在任意时间步t，隐状态的计算可以被视为：\n    1. 拼接当前时间步t的输入$\\mathbf{X}_ t$和前一个时间步t−1的隐状态$\\mathbf{H}_ {t-1}$；\n    2. 将拼接的结果送入一个带有激活函数$\\phi$的全连接层。全连接层的输出是当前时间步t的隐状态$\\mathbf{H}_ t$；\n- 在上图中，模型参数是$\\mathbf{W}_ {xh}$和$\\mathbf{W}_ {hh}$的拼接，以及$\\mathbf{b}_ h$的偏置。$\\mathbf{H}_ t$参与计算$\\mathbf{H}_ {t+1}$，同时将$\\mathbf{H}_ t$送入全连接输出层，计算$\\mathbf{O}_ t$。\n\n- 公式中的$\\mathbf{X}_ t \\mathbf{W}_ {xh} + \\mathbf{H}_ {t-1} \\mathbf{W}_ {hh}$相当于将$\\mathbf{X}_ t$和$\\mathbf{H}_ {t-1}$的列连接起来，然后与$\\mathbf{W}_ {xh}$和$\\mathbf{W}_ {hh}$的拼接做矩阵乘法。\n\n\n```python\nimport torch\nfrom d2l import torch as d2l\n\nX, W_xh = torch.normal(0, 1, (3, 1)), torch.normal(0, 1, (1, 4)) # X: (3, 1), W_xh: (1, 4)\nH, W_hh = torch.normal(0, 1, (3, 4)), torch.normal(0, 1, (4, 4)) # H: (3, 4), W_hh: (4, 4)\ns1 = torch.matmul(X, W_xh) + torch.matmul(H, W_hh)\ns2 = torch.matmul(torch.cat((X, H), 1), torch.cat((W_xh, W_hh), 0))\nprint(s1)\nprint(s2)\n```\n\n    tensor([[ 1.6472,  0.7583,  0.6199, -1.7095],\n            [ 3.6499, -3.2390, -0.4522, -3.4590],\n            [ 0.1226, -0.8202, -0.3588, -1.6686]])\n    tensor([[ 1.6472,  0.7583,  0.6199, -1.7095],\n            [ 3.6499, -3.2390, -0.4522, -3.4590],\n            [ 0.1226, -0.8202, -0.3588, -1.6686]])\n    \n\n## 4.3 基于循环神经网络的字符级语言模型\n- 设：batch_size=1, 批量中的文本为“machine”。使用字符级语言模型而不是单词级。使用当前的和先前的字符预测下一个字符：\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/7_rnn/4_rnn/2.png)\n![](img/deeplearning/code/pytorch/7_rnn/4_rnn/2.png)\n\n- 输入序列和标签序列分别为“machin”和“achine”。在训练过程中，我们对每个时间步的输出层的输出进行softmax操作，然后利用交叉熵损失计算模型输出和标签之间的误差。\n\n## 4.4 困惑度（Perplexity）\n- 如何度量语言模型的质量，考虑一下由不同的语言模型给出的对“It is raining ⋯”的续写：\n    1. “It is raining outside”（外面下雨了）；\n    2. “It is raining banana tree”（香蕉树下雨了）；\n    3. “It is raining piouw;kcj pwepoiut”（piouw;kcj pwepoiut下雨了）。\n- 1显然是最合乎情理。2则要糟糕得多，因为其产生了一个无意义的续写。尽管如此，至少该模型已经学会了如何拼写单词，以及单词之间的某种程度的相关性。3表明了训练不足的模型是无法正确地拟合数据的。\n- 可以通过计算序列的似然概率来度量模型的质量。然而这是一个难以理解、难以比较的数字。毕竟，较短的序列比较长的序列更有可能出现。\n- 如果想要压缩文本，我们可以根据当前词元集预测的下一个词元。一个更好的语言模型应该能让我们更准确地预测下一个词元。因此，它应该允许我们在压缩序列时花费更少的比特。所以我们可以通过一个序列中所有的n个词元的交叉熵损失的平均值来衡量：\n$$ \\frac{1}{n} \\sum_{t=1}^ n -\\log P(x_ t | x_ {t-1}, \\cdots , x_ 1)$$\n\n- 其中P由语言模型给出，将上式做指数处理得到更常用的困量：困惑度（perplexity）\n    - 困惑度可以理解为：下一个词元的实际选择数的调和平均数\n    - 最好情况下，模型总是完美地估计标签词元的概率为1。在这种情况下，模型的困惑度为1。\n    - 最坏情况下，模型总是预测标签词元的概率为0。在这种情况下，困惑度为正无穷。\n    - 在基线上，该模型的预测是词表的所有可用词元上的均匀分布。在这种情况下，困惑度等于词表中唯一词元的数量。事实上，如果我们在没有任何压缩的情况下存储序列，这将是我们能做的最好的编码方式。因此，这种方式提供了一个重要的上限，而任何实际模型都必须超越这个上限。\n\n","source":"_posts/deeplearning/code/pytorch/7_rnn/4_rnn.md","raw":"---\ntitle: 7.4 循环神经网络\ndate: 2024-5-3 11:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 4 循环神经网络\n- n元语法模型中，单词$x_ t$在时间步t的条件概率仅取决于前面n − 1个单词。对于时间步t − (n − 1)之前的单词，如果我们想将其可能产生的影响合并到$x_ t$上，需要增加n，然而模型参数的数量也会随之呈指数增长，因为词表$V$需要存储$|V|^ n$个数字，因此与其将$P(x_ t| x_ {t-1}, \\cdots , x_ {t-n+1})$模型化，不如使用隐变量模型：\n$$P(x_ t | x_ {t-1}, \\cdots , x_ 1) \\approx P(x_ t | h_ {t-1})$$\n\n- $h_ {t-1}$是隐状态（hidden state），也称为隐藏变量（hidden variable），它存储了到时间步t − 1的序列信息。通常，我们可以基于当前输入xt和先前隐状态ht−1 来计算时间步t处的任何时间的隐状态：\n$$h_ t = f(x_ t, h_ {t-1})$$\n\n- 对于函数$f$，隐变量模型不是近似值。毕竟$h_ t$是可以仅仅存储到目前为止观察到的所有数据，然而这样的操作可能会使计算和存储的代价都变得昂贵。\n- 隐藏层和隐状态指的是两个截然不同的概念。隐藏层是在从输入到输出的路径上（以观测角度来理解）的隐藏的层，而隐状态则是在给定步骤所做的任何事情（以技术角度来定义）的输入，并且这些状态只能通过先前时间步的数据来计算。\n## 4.1 无隐状态的神经网络\n- 对于只有单隐藏层的多层感知机，设隐藏层的激活函数为$\\phi$，给定一个小批量样本$\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$，其中$n$是批量大小，$d$是输入维度，隐藏层的输出为：\n$$\\mathbf{H} = \\phi(\\mathbf{X} \\mathbf{W}_ {xh} + \\mathbf{b}_ h)$$\n\n- 其中，隐藏层权重$\\mathbf{W}_ {xh} \\in \\mathbb{R}^{d \\times h}$，隐藏层偏置$\\mathbf{b}_ h \\in \\mathbb{R}^{1 \\times h}$，$h$是隐藏单元的个数。然后将隐藏变量$\\mathbf{H}$作为输出层的输入，输出层的输出为：\n$$\\mathbf{O} = \\mathbf{H} \\mathbf{W}_ {hq} + \\mathbf{b}_ q$$\n\n- 其中，$\\mathbf{O}$是输出变量。如果是分类问题，可以用$\\text{softmax}(\\mathbf{O})$来计算输出类别的概率分布。\n\n## 4.2 有隐状态的循环神经网络\n- 假设我们在时间步t有小批量输入$\\mathbf{X}_ t \\in \\mathbb{R}^{n \\times d}$（样本数为n，$\\mathbf{X}_ t$的每一行对应于来自该序列的时间步t处的一个样本。\n- 用$\\mathbf{H}_ t \\in \\mathbb{R}^{n \\times h}$表示时间步t的隐藏变量。与mlp不同的是，我们在这里保存了前一个时间步的隐藏变量$\\mathbf{H}_ {t-1}$，并引入了一个新的权重参数$\\mathbf{W}_ {hh} \\in \\mathbb{R}^{h \\times h}$来描述如何在当前时间步中使用前一个时间步的隐藏变量。\n- 当前时间步隐藏变量由当前时间步的输入与前一个时间步的隐藏变量一起计算得出：\n$$\\mathbf{H}_ t = \\phi(\\mathbf{X}_ t \\mathbf{W}_ {xh} + \\mathbf{H}_ {t-1} \\mathbf{W}_ {hh} + \\mathbf{b}_ h)$$\n\n- 与之前的相比，多了一项$\\mathbf{H}_ {t-1} \\mathbf{W}_ {hh}$，这一项描述了如何利用前一个时间步的隐藏变量。从相邻时间步的隐藏变量$\\mathbf{H}_ {t}$和$\\mathbf{H}_ {t-1}$之间的关系可知，这些变量捕获并保留了序列直到其当前时间步的历史信息，因此这样的隐藏变量被称为隐状态（hidden state）。由于在当前时间步中，隐状态使用的定义与前一个时间步中使用的定义相同，因此上式的计算是循环的。于是基于循环计算的隐状态神经网络被命名为 循环神经网络（recurrent neural network）。在循环神经网络中执行上式计算的层称为循环层。\n- 有许多不同的方法可以构建循环神经网络，由上式定义的隐状态的循环神经网络是非常常见的一种。对于时间步t，输出层的输出类似于多层感知机中的计算：\n$$\\mathbf{O}_ t = \\mathbf{H}_ t \\mathbf{W}_ {hq} + \\mathbf{b}_ q$$\n\n- 循环神经网络的参数包括:\n    - 隐藏层权重：$\\mathbf{W}_ {xh} \\in \\mathbb{R}^{d \\times h}$，$\\mathbf{W}_ {hh} \\in \\mathbb{R}^{h \\times h}$\n    - 隐藏层偏置：$\\mathbf{b}_ h \\in \\mathbb{R}^{1 \\times h}$\n    - 输出层权重：$\\mathbf{W}_ {hq} \\in \\mathbb{R}^{h \\times q}$\n    - 输出层偏置：$\\mathbf{b}_ q \\in \\mathbb{R}^{1 \\times q}$\n    - 在不同的时间步，循环神经网络也总是使用这些模型参数。因此，循环神经网络的参数开销不会随着时间步的增加而增加。\n- 下图展示了循环神经网络在三个相邻时间步的计算逻辑。\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/7_rnn/4_rnn/1.png)\n![](img/deeplearning/code/pytorch/7_rnn/4_rnn/1.png)\n\n- 在任意时间步t，隐状态的计算可以被视为：\n    1. 拼接当前时间步t的输入$\\mathbf{X}_ t$和前一个时间步t−1的隐状态$\\mathbf{H}_ {t-1}$；\n    2. 将拼接的结果送入一个带有激活函数$\\phi$的全连接层。全连接层的输出是当前时间步t的隐状态$\\mathbf{H}_ t$；\n- 在上图中，模型参数是$\\mathbf{W}_ {xh}$和$\\mathbf{W}_ {hh}$的拼接，以及$\\mathbf{b}_ h$的偏置。$\\mathbf{H}_ t$参与计算$\\mathbf{H}_ {t+1}$，同时将$\\mathbf{H}_ t$送入全连接输出层，计算$\\mathbf{O}_ t$。\n\n- 公式中的$\\mathbf{X}_ t \\mathbf{W}_ {xh} + \\mathbf{H}_ {t-1} \\mathbf{W}_ {hh}$相当于将$\\mathbf{X}_ t$和$\\mathbf{H}_ {t-1}$的列连接起来，然后与$\\mathbf{W}_ {xh}$和$\\mathbf{W}_ {hh}$的拼接做矩阵乘法。\n\n\n```python\nimport torch\nfrom d2l import torch as d2l\n\nX, W_xh = torch.normal(0, 1, (3, 1)), torch.normal(0, 1, (1, 4)) # X: (3, 1), W_xh: (1, 4)\nH, W_hh = torch.normal(0, 1, (3, 4)), torch.normal(0, 1, (4, 4)) # H: (3, 4), W_hh: (4, 4)\ns1 = torch.matmul(X, W_xh) + torch.matmul(H, W_hh)\ns2 = torch.matmul(torch.cat((X, H), 1), torch.cat((W_xh, W_hh), 0))\nprint(s1)\nprint(s2)\n```\n\n    tensor([[ 1.6472,  0.7583,  0.6199, -1.7095],\n            [ 3.6499, -3.2390, -0.4522, -3.4590],\n            [ 0.1226, -0.8202, -0.3588, -1.6686]])\n    tensor([[ 1.6472,  0.7583,  0.6199, -1.7095],\n            [ 3.6499, -3.2390, -0.4522, -3.4590],\n            [ 0.1226, -0.8202, -0.3588, -1.6686]])\n    \n\n## 4.3 基于循环神经网络的字符级语言模型\n- 设：batch_size=1, 批量中的文本为“machine”。使用字符级语言模型而不是单词级。使用当前的和先前的字符预测下一个字符：\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/7_rnn/4_rnn/2.png)\n![](img/deeplearning/code/pytorch/7_rnn/4_rnn/2.png)\n\n- 输入序列和标签序列分别为“machin”和“achine”。在训练过程中，我们对每个时间步的输出层的输出进行softmax操作，然后利用交叉熵损失计算模型输出和标签之间的误差。\n\n## 4.4 困惑度（Perplexity）\n- 如何度量语言模型的质量，考虑一下由不同的语言模型给出的对“It is raining ⋯”的续写：\n    1. “It is raining outside”（外面下雨了）；\n    2. “It is raining banana tree”（香蕉树下雨了）；\n    3. “It is raining piouw;kcj pwepoiut”（piouw;kcj pwepoiut下雨了）。\n- 1显然是最合乎情理。2则要糟糕得多，因为其产生了一个无意义的续写。尽管如此，至少该模型已经学会了如何拼写单词，以及单词之间的某种程度的相关性。3表明了训练不足的模型是无法正确地拟合数据的。\n- 可以通过计算序列的似然概率来度量模型的质量。然而这是一个难以理解、难以比较的数字。毕竟，较短的序列比较长的序列更有可能出现。\n- 如果想要压缩文本，我们可以根据当前词元集预测的下一个词元。一个更好的语言模型应该能让我们更准确地预测下一个词元。因此，它应该允许我们在压缩序列时花费更少的比特。所以我们可以通过一个序列中所有的n个词元的交叉熵损失的平均值来衡量：\n$$ \\frac{1}{n} \\sum_{t=1}^ n -\\log P(x_ t | x_ {t-1}, \\cdots , x_ 1)$$\n\n- 其中P由语言模型给出，将上式做指数处理得到更常用的困量：困惑度（perplexity）\n    - 困惑度可以理解为：下一个词元的实际选择数的调和平均数\n    - 最好情况下，模型总是完美地估计标签词元的概率为1。在这种情况下，模型的困惑度为1。\n    - 最坏情况下，模型总是预测标签词元的概率为0。在这种情况下，困惑度为正无穷。\n    - 在基线上，该模型的预测是词表的所有可用词元上的均匀分布。在这种情况下，困惑度等于词表中唯一词元的数量。事实上，如果我们在没有任何压缩的情况下存储序列，这将是我们能做的最好的编码方式。因此，这种方式提供了一个重要的上限，而任何实际模型都必须超越这个上限。\n\n","slug":"deeplearning/code/pytorch/7_rnn/4_rnn","published":1,"updated":"2024-05-03T15:00:46.969Z","comments":1,"layout":"post","photos":[],"_id":"clvuli6uz0094zsvw8gwm4y2j","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"4-循环神经网络\"><a href=\"#4-循环神经网络\" class=\"headerlink\" title=\"4 循环神经网络\"></a>4 循环神经网络</h1><ul>\n<li><p>n元语法模型中，单词$x_ t$在时间步t的条件概率仅取决于前面n − 1个单词。对于时间步t − (n − 1)之前的单词，如果我们想将其可能产生的影响合并到$x_ t$上，需要增加n，然而模型参数的数量也会随之呈指数增长，因为词表$V$需要存储$|V|^ n$个数字，因此与其将$P(x_ t| x_ {t-1}, \\cdots , x_ {t-n+1})$模型化，不如使用隐变量模型：<br>$$P(x_ t | x_ {t-1}, \\cdots , x_ 1) \\approx P(x_ t | h_ {t-1})$$</p>\n</li>\n<li><p>$h_ {t-1}$是隐状态（hidden state），也称为隐藏变量（hidden variable），它存储了到时间步t − 1的序列信息。通常，我们可以基于当前输入xt和先前隐状态ht−1 来计算时间步t处的任何时间的隐状态：<br>$$h_ t &#x3D; f(x_ t, h_ {t-1})$$</p>\n</li>\n<li><p>对于函数$f$，隐变量模型不是近似值。毕竟$h_ t$是可以仅仅存储到目前为止观察到的所有数据，然而这样的操作可能会使计算和存储的代价都变得昂贵。</p>\n</li>\n<li><p>隐藏层和隐状态指的是两个截然不同的概念。隐藏层是在从输入到输出的路径上（以观测角度来理解）的隐藏的层，而隐状态则是在给定步骤所做的任何事情（以技术角度来定义）的输入，并且这些状态只能通过先前时间步的数据来计算。</p>\n</li>\n</ul>\n<h2 id=\"4-1-无隐状态的神经网络\"><a href=\"#4-1-无隐状态的神经网络\" class=\"headerlink\" title=\"4.1 无隐状态的神经网络\"></a>4.1 无隐状态的神经网络</h2><ul>\n<li><p>对于只有单隐藏层的多层感知机，设隐藏层的激活函数为$\\phi$，给定一个小批量样本$\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$，其中$n$是批量大小，$d$是输入维度，隐藏层的输出为：<br>$$\\mathbf{H} &#x3D; \\phi(\\mathbf{X} \\mathbf{W}_ {xh} + \\mathbf{b}_ h)$$</p>\n</li>\n<li><p>其中，隐藏层权重$\\mathbf{W}_ {xh} \\in \\mathbb{R}^{d \\times h}$，隐藏层偏置$\\mathbf{b}_ h \\in \\mathbb{R}^{1 \\times h}$，$h$是隐藏单元的个数。然后将隐藏变量$\\mathbf{H}$作为输出层的输入，输出层的输出为：<br>$$\\mathbf{O} &#x3D; \\mathbf{H} \\mathbf{W}_ {hq} + \\mathbf{b}_ q$$</p>\n</li>\n<li><p>其中，$\\mathbf{O}$是输出变量。如果是分类问题，可以用$\\text{softmax}(\\mathbf{O})$来计算输出类别的概率分布。</p>\n</li>\n</ul>\n<h2 id=\"4-2-有隐状态的循环神经网络\"><a href=\"#4-2-有隐状态的循环神经网络\" class=\"headerlink\" title=\"4.2 有隐状态的循环神经网络\"></a>4.2 有隐状态的循环神经网络</h2><ul>\n<li><p>假设我们在时间步t有小批量输入$\\mathbf{X}_ t \\in \\mathbb{R}^{n \\times d}$（样本数为n，$\\mathbf{X}_ t$的每一行对应于来自该序列的时间步t处的一个样本。</p>\n</li>\n<li><p>用$\\mathbf{H}_ t \\in \\mathbb{R}^{n \\times h}$表示时间步t的隐藏变量。与mlp不同的是，我们在这里保存了前一个时间步的隐藏变量$\\mathbf{H}_ {t-1}$，并引入了一个新的权重参数$\\mathbf{W}_ {hh} \\in \\mathbb{R}^{h \\times h}$来描述如何在当前时间步中使用前一个时间步的隐藏变量。</p>\n</li>\n<li><p>当前时间步隐藏变量由当前时间步的输入与前一个时间步的隐藏变量一起计算得出：<br>$$\\mathbf{H}_ t &#x3D; \\phi(\\mathbf{X}_ t \\mathbf{W}_ {xh} + \\mathbf{H}_ {t-1} \\mathbf{W}_ {hh} + \\mathbf{b}_ h)$$</p>\n</li>\n<li><p>与之前的相比，多了一项$\\mathbf{H}_ {t-1} \\mathbf{W}_ {hh}$，这一项描述了如何利用前一个时间步的隐藏变量。从相邻时间步的隐藏变量$\\mathbf{H}_ {t}$和$\\mathbf{H}_ {t-1}$之间的关系可知，这些变量捕获并保留了序列直到其当前时间步的历史信息，因此这样的隐藏变量被称为隐状态（hidden state）。由于在当前时间步中，隐状态使用的定义与前一个时间步中使用的定义相同，因此上式的计算是循环的。于是基于循环计算的隐状态神经网络被命名为 循环神经网络（recurrent neural network）。在循环神经网络中执行上式计算的层称为循环层。</p>\n</li>\n<li><p>有许多不同的方法可以构建循环神经网络，由上式定义的隐状态的循环神经网络是非常常见的一种。对于时间步t，输出层的输出类似于多层感知机中的计算：<br>$$\\mathbf{O}_ t &#x3D; \\mathbf{H}_ t \\mathbf{W}_ {hq} + \\mathbf{b}_ q$$</p>\n</li>\n<li><p>循环神经网络的参数包括:</p>\n<ul>\n<li>隐藏层权重：$\\mathbf{W}_ {xh} \\in \\mathbb{R}^{d \\times h}$，$\\mathbf{W}_ {hh} \\in \\mathbb{R}^{h \\times h}$</li>\n<li>隐藏层偏置：$\\mathbf{b}_ h \\in \\mathbb{R}^{1 \\times h}$</li>\n<li>输出层权重：$\\mathbf{W}_ {hq} \\in \\mathbb{R}^{h \\times q}$</li>\n<li>输出层偏置：$\\mathbf{b}_ q \\in \\mathbb{R}^{1 \\times q}$</li>\n<li>在不同的时间步，循环神经网络也总是使用这些模型参数。因此，循环神经网络的参数开销不会随着时间步的增加而增加。</li>\n</ul>\n</li>\n<li><p>下图展示了循环神经网络在三个相邻时间步的计算逻辑。<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/7_rnn/4_rnn/1.png\"><br><img src=\"/img/deeplearning/code/pytorch/7_rnn/4_rnn/1.png\"></p>\n</li>\n<li><p>在任意时间步t，隐状态的计算可以被视为：</p>\n<ol>\n<li>拼接当前时间步t的输入$\\mathbf{X}_ t$和前一个时间步t−1的隐状态$\\mathbf{H}_ {t-1}$；</li>\n<li>将拼接的结果送入一个带有激活函数$\\phi$的全连接层。全连接层的输出是当前时间步t的隐状态$\\mathbf{H}_ t$；</li>\n</ol>\n</li>\n<li><p>在上图中，模型参数是$\\mathbf{W}_ {xh}$和$\\mathbf{W}_ {hh}$的拼接，以及$\\mathbf{b}_ h$的偏置。$\\mathbf{H}_ t$参与计算$\\mathbf{H}_ {t+1}$，同时将$\\mathbf{H}_ t$送入全连接输出层，计算$\\mathbf{O}_ t$。</p>\n</li>\n<li><p>公式中的$\\mathbf{X}_ t \\mathbf{W}_ {xh} + \\mathbf{H}_ {t-1} \\mathbf{W}_ {hh}$相当于将$\\mathbf{X}_ t$和$\\mathbf{H}_ {t-1}$的列连接起来，然后与$\\mathbf{W}_ {xh}$和$\\mathbf{W}_ {hh}$的拼接做矩阵乘法。</p>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">X, W_xh = torch.normal(<span class=\"number\">0</span>, <span class=\"number\">1</span>, (<span class=\"number\">3</span>, <span class=\"number\">1</span>)), torch.normal(<span class=\"number\">0</span>, <span class=\"number\">1</span>, (<span class=\"number\">1</span>, <span class=\"number\">4</span>)) <span class=\"comment\"># X: (3, 1), W_xh: (1, 4)</span></span><br><span class=\"line\">H, W_hh = torch.normal(<span class=\"number\">0</span>, <span class=\"number\">1</span>, (<span class=\"number\">3</span>, <span class=\"number\">4</span>)), torch.normal(<span class=\"number\">0</span>, <span class=\"number\">1</span>, (<span class=\"number\">4</span>, <span class=\"number\">4</span>)) <span class=\"comment\"># H: (3, 4), W_hh: (4, 4)</span></span><br><span class=\"line\">s1 = torch.matmul(X, W_xh) + torch.matmul(H, W_hh)</span><br><span class=\"line\">s2 = torch.matmul(torch.cat((X, H), <span class=\"number\">1</span>), torch.cat((W_xh, W_hh), <span class=\"number\">0</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(s1)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(s2)</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[ 1.6472,  0.7583,  0.6199, -1.7095],\n        [ 3.6499, -3.2390, -0.4522, -3.4590],\n        [ 0.1226, -0.8202, -0.3588, -1.6686]])\ntensor([[ 1.6472,  0.7583,  0.6199, -1.7095],\n        [ 3.6499, -3.2390, -0.4522, -3.4590],\n        [ 0.1226, -0.8202, -0.3588, -1.6686]])\n</code></pre>\n<h2 id=\"4-3-基于循环神经网络的字符级语言模型\"><a href=\"#4-3-基于循环神经网络的字符级语言模型\" class=\"headerlink\" title=\"4.3 基于循环神经网络的字符级语言模型\"></a>4.3 基于循环神经网络的字符级语言模型</h2><ul>\n<li><p>设：batch_size&#x3D;1, 批量中的文本为“machine”。使用字符级语言模型而不是单词级。使用当前的和先前的字符预测下一个字符：<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/7_rnn/4_rnn/2.png\"><br><img src=\"/img/deeplearning/code/pytorch/7_rnn/4_rnn/2.png\"></p>\n</li>\n<li><p>输入序列和标签序列分别为“machin”和“achine”。在训练过程中，我们对每个时间步的输出层的输出进行softmax操作，然后利用交叉熵损失计算模型输出和标签之间的误差。</p>\n</li>\n</ul>\n<h2 id=\"4-4-困惑度（Perplexity）\"><a href=\"#4-4-困惑度（Perplexity）\" class=\"headerlink\" title=\"4.4 困惑度（Perplexity）\"></a>4.4 困惑度（Perplexity）</h2><ul>\n<li><p>如何度量语言模型的质量，考虑一下由不同的语言模型给出的对“It is raining ⋯”的续写：</p>\n<ol>\n<li>“It is raining outside”（外面下雨了）；</li>\n<li>“It is raining banana tree”（香蕉树下雨了）；</li>\n<li>“It is raining piouw;kcj pwepoiut”（piouw;kcj pwepoiut下雨了）。</li>\n</ol>\n</li>\n<li><p>1显然是最合乎情理。2则要糟糕得多，因为其产生了一个无意义的续写。尽管如此，至少该模型已经学会了如何拼写单词，以及单词之间的某种程度的相关性。3表明了训练不足的模型是无法正确地拟合数据的。</p>\n</li>\n<li><p>可以通过计算序列的似然概率来度量模型的质量。然而这是一个难以理解、难以比较的数字。毕竟，较短的序列比较长的序列更有可能出现。</p>\n</li>\n<li><p>如果想要压缩文本，我们可以根据当前词元集预测的下一个词元。一个更好的语言模型应该能让我们更准确地预测下一个词元。因此，它应该允许我们在压缩序列时花费更少的比特。所以我们可以通过一个序列中所有的n个词元的交叉熵损失的平均值来衡量：<br>$$ \\frac{1}{n} \\sum_{t&#x3D;1}^ n -\\log P(x_ t | x_ {t-1}, \\cdots , x_ 1)$$</p>\n</li>\n<li><p>其中P由语言模型给出，将上式做指数处理得到更常用的困量：困惑度（perplexity）</p>\n<ul>\n<li>困惑度可以理解为：下一个词元的实际选择数的调和平均数</li>\n<li>最好情况下，模型总是完美地估计标签词元的概率为1。在这种情况下，模型的困惑度为1。</li>\n<li>最坏情况下，模型总是预测标签词元的概率为0。在这种情况下，困惑度为正无穷。</li>\n<li>在基线上，该模型的预测是词表的所有可用词元上的均匀分布。在这种情况下，困惑度等于词表中唯一词元的数量。事实上，如果我们在没有任何压缩的情况下存储序列，这将是我们能做的最好的编码方式。因此，这种方式提供了一个重要的上限，而任何实际模型都必须超越这个上限。</li>\n</ul>\n</li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"4-循环神经网络\"><a href=\"#4-循环神经网络\" class=\"headerlink\" title=\"4 循环神经网络\"></a>4 循环神经网络</h1><ul>\n<li><p>n元语法模型中，单词$x_ t$在时间步t的条件概率仅取决于前面n − 1个单词。对于时间步t − (n − 1)之前的单词，如果我们想将其可能产生的影响合并到$x_ t$上，需要增加n，然而模型参数的数量也会随之呈指数增长，因为词表$V$需要存储$|V|^ n$个数字，因此与其将$P(x_ t| x_ {t-1}, \\cdots , x_ {t-n+1})$模型化，不如使用隐变量模型：<br>$$P(x_ t | x_ {t-1}, \\cdots , x_ 1) \\approx P(x_ t | h_ {t-1})$$</p>\n</li>\n<li><p>$h_ {t-1}$是隐状态（hidden state），也称为隐藏变量（hidden variable），它存储了到时间步t − 1的序列信息。通常，我们可以基于当前输入xt和先前隐状态ht−1 来计算时间步t处的任何时间的隐状态：<br>$$h_ t &#x3D; f(x_ t, h_ {t-1})$$</p>\n</li>\n<li><p>对于函数$f$，隐变量模型不是近似值。毕竟$h_ t$是可以仅仅存储到目前为止观察到的所有数据，然而这样的操作可能会使计算和存储的代价都变得昂贵。</p>\n</li>\n<li><p>隐藏层和隐状态指的是两个截然不同的概念。隐藏层是在从输入到输出的路径上（以观测角度来理解）的隐藏的层，而隐状态则是在给定步骤所做的任何事情（以技术角度来定义）的输入，并且这些状态只能通过先前时间步的数据来计算。</p>\n</li>\n</ul>\n<h2 id=\"4-1-无隐状态的神经网络\"><a href=\"#4-1-无隐状态的神经网络\" class=\"headerlink\" title=\"4.1 无隐状态的神经网络\"></a>4.1 无隐状态的神经网络</h2><ul>\n<li><p>对于只有单隐藏层的多层感知机，设隐藏层的激活函数为$\\phi$，给定一个小批量样本$\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$，其中$n$是批量大小，$d$是输入维度，隐藏层的输出为：<br>$$\\mathbf{H} &#x3D; \\phi(\\mathbf{X} \\mathbf{W}_ {xh} + \\mathbf{b}_ h)$$</p>\n</li>\n<li><p>其中，隐藏层权重$\\mathbf{W}_ {xh} \\in \\mathbb{R}^{d \\times h}$，隐藏层偏置$\\mathbf{b}_ h \\in \\mathbb{R}^{1 \\times h}$，$h$是隐藏单元的个数。然后将隐藏变量$\\mathbf{H}$作为输出层的输入，输出层的输出为：<br>$$\\mathbf{O} &#x3D; \\mathbf{H} \\mathbf{W}_ {hq} + \\mathbf{b}_ q$$</p>\n</li>\n<li><p>其中，$\\mathbf{O}$是输出变量。如果是分类问题，可以用$\\text{softmax}(\\mathbf{O})$来计算输出类别的概率分布。</p>\n</li>\n</ul>\n<h2 id=\"4-2-有隐状态的循环神经网络\"><a href=\"#4-2-有隐状态的循环神经网络\" class=\"headerlink\" title=\"4.2 有隐状态的循环神经网络\"></a>4.2 有隐状态的循环神经网络</h2><ul>\n<li><p>假设我们在时间步t有小批量输入$\\mathbf{X}_ t \\in \\mathbb{R}^{n \\times d}$（样本数为n，$\\mathbf{X}_ t$的每一行对应于来自该序列的时间步t处的一个样本。</p>\n</li>\n<li><p>用$\\mathbf{H}_ t \\in \\mathbb{R}^{n \\times h}$表示时间步t的隐藏变量。与mlp不同的是，我们在这里保存了前一个时间步的隐藏变量$\\mathbf{H}_ {t-1}$，并引入了一个新的权重参数$\\mathbf{W}_ {hh} \\in \\mathbb{R}^{h \\times h}$来描述如何在当前时间步中使用前一个时间步的隐藏变量。</p>\n</li>\n<li><p>当前时间步隐藏变量由当前时间步的输入与前一个时间步的隐藏变量一起计算得出：<br>$$\\mathbf{H}_ t &#x3D; \\phi(\\mathbf{X}_ t \\mathbf{W}_ {xh} + \\mathbf{H}_ {t-1} \\mathbf{W}_ {hh} + \\mathbf{b}_ h)$$</p>\n</li>\n<li><p>与之前的相比，多了一项$\\mathbf{H}_ {t-1} \\mathbf{W}_ {hh}$，这一项描述了如何利用前一个时间步的隐藏变量。从相邻时间步的隐藏变量$\\mathbf{H}_ {t}$和$\\mathbf{H}_ {t-1}$之间的关系可知，这些变量捕获并保留了序列直到其当前时间步的历史信息，因此这样的隐藏变量被称为隐状态（hidden state）。由于在当前时间步中，隐状态使用的定义与前一个时间步中使用的定义相同，因此上式的计算是循环的。于是基于循环计算的隐状态神经网络被命名为 循环神经网络（recurrent neural network）。在循环神经网络中执行上式计算的层称为循环层。</p>\n</li>\n<li><p>有许多不同的方法可以构建循环神经网络，由上式定义的隐状态的循环神经网络是非常常见的一种。对于时间步t，输出层的输出类似于多层感知机中的计算：<br>$$\\mathbf{O}_ t &#x3D; \\mathbf{H}_ t \\mathbf{W}_ {hq} + \\mathbf{b}_ q$$</p>\n</li>\n<li><p>循环神经网络的参数包括:</p>\n<ul>\n<li>隐藏层权重：$\\mathbf{W}_ {xh} \\in \\mathbb{R}^{d \\times h}$，$\\mathbf{W}_ {hh} \\in \\mathbb{R}^{h \\times h}$</li>\n<li>隐藏层偏置：$\\mathbf{b}_ h \\in \\mathbb{R}^{1 \\times h}$</li>\n<li>输出层权重：$\\mathbf{W}_ {hq} \\in \\mathbb{R}^{h \\times q}$</li>\n<li>输出层偏置：$\\mathbf{b}_ q \\in \\mathbb{R}^{1 \\times q}$</li>\n<li>在不同的时间步，循环神经网络也总是使用这些模型参数。因此，循环神经网络的参数开销不会随着时间步的增加而增加。</li>\n</ul>\n</li>\n<li><p>下图展示了循环神经网络在三个相邻时间步的计算逻辑。<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/7_rnn/4_rnn/1.png\"><br><img src=\"/img/deeplearning/code/pytorch/7_rnn/4_rnn/1.png\"></p>\n</li>\n<li><p>在任意时间步t，隐状态的计算可以被视为：</p>\n<ol>\n<li>拼接当前时间步t的输入$\\mathbf{X}_ t$和前一个时间步t−1的隐状态$\\mathbf{H}_ {t-1}$；</li>\n<li>将拼接的结果送入一个带有激活函数$\\phi$的全连接层。全连接层的输出是当前时间步t的隐状态$\\mathbf{H}_ t$；</li>\n</ol>\n</li>\n<li><p>在上图中，模型参数是$\\mathbf{W}_ {xh}$和$\\mathbf{W}_ {hh}$的拼接，以及$\\mathbf{b}_ h$的偏置。$\\mathbf{H}_ t$参与计算$\\mathbf{H}_ {t+1}$，同时将$\\mathbf{H}_ t$送入全连接输出层，计算$\\mathbf{O}_ t$。</p>\n</li>\n<li><p>公式中的$\\mathbf{X}_ t \\mathbf{W}_ {xh} + \\mathbf{H}_ {t-1} \\mathbf{W}_ {hh}$相当于将$\\mathbf{X}_ t$和$\\mathbf{H}_ {t-1}$的列连接起来，然后与$\\mathbf{W}_ {xh}$和$\\mathbf{W}_ {hh}$的拼接做矩阵乘法。</p>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">X, W_xh = torch.normal(<span class=\"number\">0</span>, <span class=\"number\">1</span>, (<span class=\"number\">3</span>, <span class=\"number\">1</span>)), torch.normal(<span class=\"number\">0</span>, <span class=\"number\">1</span>, (<span class=\"number\">1</span>, <span class=\"number\">4</span>)) <span class=\"comment\"># X: (3, 1), W_xh: (1, 4)</span></span><br><span class=\"line\">H, W_hh = torch.normal(<span class=\"number\">0</span>, <span class=\"number\">1</span>, (<span class=\"number\">3</span>, <span class=\"number\">4</span>)), torch.normal(<span class=\"number\">0</span>, <span class=\"number\">1</span>, (<span class=\"number\">4</span>, <span class=\"number\">4</span>)) <span class=\"comment\"># H: (3, 4), W_hh: (4, 4)</span></span><br><span class=\"line\">s1 = torch.matmul(X, W_xh) + torch.matmul(H, W_hh)</span><br><span class=\"line\">s2 = torch.matmul(torch.cat((X, H), <span class=\"number\">1</span>), torch.cat((W_xh, W_hh), <span class=\"number\">0</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(s1)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(s2)</span><br></pre></td></tr></table></figure>\n\n<pre><code>tensor([[ 1.6472,  0.7583,  0.6199, -1.7095],\n        [ 3.6499, -3.2390, -0.4522, -3.4590],\n        [ 0.1226, -0.8202, -0.3588, -1.6686]])\ntensor([[ 1.6472,  0.7583,  0.6199, -1.7095],\n        [ 3.6499, -3.2390, -0.4522, -3.4590],\n        [ 0.1226, -0.8202, -0.3588, -1.6686]])\n</code></pre>\n<h2 id=\"4-3-基于循环神经网络的字符级语言模型\"><a href=\"#4-3-基于循环神经网络的字符级语言模型\" class=\"headerlink\" title=\"4.3 基于循环神经网络的字符级语言模型\"></a>4.3 基于循环神经网络的字符级语言模型</h2><ul>\n<li><p>设：batch_size&#x3D;1, 批量中的文本为“machine”。使用字符级语言模型而不是单词级。使用当前的和先前的字符预测下一个字符：<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/7_rnn/4_rnn/2.png\"><br><img src=\"/img/deeplearning/code/pytorch/7_rnn/4_rnn/2.png\"></p>\n</li>\n<li><p>输入序列和标签序列分别为“machin”和“achine”。在训练过程中，我们对每个时间步的输出层的输出进行softmax操作，然后利用交叉熵损失计算模型输出和标签之间的误差。</p>\n</li>\n</ul>\n<h2 id=\"4-4-困惑度（Perplexity）\"><a href=\"#4-4-困惑度（Perplexity）\" class=\"headerlink\" title=\"4.4 困惑度（Perplexity）\"></a>4.4 困惑度（Perplexity）</h2><ul>\n<li><p>如何度量语言模型的质量，考虑一下由不同的语言模型给出的对“It is raining ⋯”的续写：</p>\n<ol>\n<li>“It is raining outside”（外面下雨了）；</li>\n<li>“It is raining banana tree”（香蕉树下雨了）；</li>\n<li>“It is raining piouw;kcj pwepoiut”（piouw;kcj pwepoiut下雨了）。</li>\n</ol>\n</li>\n<li><p>1显然是最合乎情理。2则要糟糕得多，因为其产生了一个无意义的续写。尽管如此，至少该模型已经学会了如何拼写单词，以及单词之间的某种程度的相关性。3表明了训练不足的模型是无法正确地拟合数据的。</p>\n</li>\n<li><p>可以通过计算序列的似然概率来度量模型的质量。然而这是一个难以理解、难以比较的数字。毕竟，较短的序列比较长的序列更有可能出现。</p>\n</li>\n<li><p>如果想要压缩文本，我们可以根据当前词元集预测的下一个词元。一个更好的语言模型应该能让我们更准确地预测下一个词元。因此，它应该允许我们在压缩序列时花费更少的比特。所以我们可以通过一个序列中所有的n个词元的交叉熵损失的平均值来衡量：<br>$$ \\frac{1}{n} \\sum_{t&#x3D;1}^ n -\\log P(x_ t | x_ {t-1}, \\cdots , x_ 1)$$</p>\n</li>\n<li><p>其中P由语言模型给出，将上式做指数处理得到更常用的困量：困惑度（perplexity）</p>\n<ul>\n<li>困惑度可以理解为：下一个词元的实际选择数的调和平均数</li>\n<li>最好情况下，模型总是完美地估计标签词元的概率为1。在这种情况下，模型的困惑度为1。</li>\n<li>最坏情况下，模型总是预测标签词元的概率为0。在这种情况下，困惑度为正无穷。</li>\n<li>在基线上，该模型的预测是词表的所有可用词元上的均匀分布。在这种情况下，困惑度等于词表中唯一词元的数量。事实上，如果我们在没有任何压缩的情况下存储序列，这将是我们能做的最好的编码方式。因此，这种方式提供了一个重要的上限，而任何实际模型都必须超越这个上限。</li>\n</ul>\n</li>\n</ul>"},{"title":"7.5 循环神经网络从零开始实现","date":"2024-05-03T04:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 5 循环神经网络从零开始实现\n- 读取数据集\n\n\n```python\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK']='True'\nimport math\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom d2l import torch as d2l\n\nbatch_size, num_steps = 32, 35\ntrain_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)\n```\n\n## 5.1 独热编码\n- 在train_iter中，每个词元都表示为一个数字索引，将这些索引直接输入神经网络可能会使学习变得困难。我们通常将每个词元表示为更具表现力的特征向量。最简单的表示称为独热编码（one‐hot encoding）。\n    - 将每个索引映射为相互不同的单位向量：假设词表中不同词元的数目为N（即len(vocab)），词元索引的范围为0到N − 1。\n    - 如果词元的索引是整数i，那么我们将创建一个长度为N的全0向量，并将第i处的元素设置为1。此向量是原始词元的一个独热向量。\n\n\n```python\n# 索引为0和2的独热向量\nF.one_hot(torch.tensor([0, 2]), len(vocab))\n```\n\n\n\n\n    tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n             0, 0, 0, 0],\n            [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n             0, 0, 0, 0]])\n\n\n\n- 每次采样的小批量数据形状是二维张量：（batch_size, num_steps）。one_hot函数将这样一个小批量数据转换成三维张量，张量的最后一个维度等于词表大小（len(vocab)）。我们经常转换输入的维度，以便获得形状为（时间步数，批量大小，词表大小）的输出。这将使我们能够更方便地通过最外层的维度，一步一步地更新小批量数据的隐状态。\n\n\n\n```python\nX = torch.arange(10).reshape((2, 5))\nF.one_hot(X.T, 28).shape\n```\n\n\n\n\n    torch.Size([5, 2, 28])\n\n\n\n## 5.2 初始化模型参数\n- 隐藏单元数num_hiddens是一个可调的超参数。当训练语言模型时，输入和输出来自相同的词表。因此，它们具有相同的维度，即词表的大小。\n\n\n```python\ndef get_params(vocab_size, num_hiddens, device):\n    num_inputs = num_outputs = vocab_size\n\n    def normal(shape):\n        return torch.randn(size=shape, device=device) * 0.01\n    \n    # 隐藏层参数\n    W_xh = normal((num_inputs, num_hiddens))\n    W_hh = normal((num_hiddens, num_hiddens))\n    b_h = torch.zeros(num_hiddens, device=device)\n    # 输出层参数\n    W_hq = normal((num_hiddens, num_outputs))\n    b_q = torch.zeros(num_outputs, device=device)\n    # 附加梯度\n    params = [W_xh, W_hh, b_h, W_hq, b_q]\n    for param in params:\n        param.requires_grad_(True)\n    return params\n```\n\n## 5.3 循环神经网络模型\n- 为了定义循环神经网络模型，我们首先需要一个init_rnn_state函数在初始化时返回隐状态。这个函数的返回是一个张量，张量全用0填充，形状为（批量大小，隐藏单元数）。\n\n\n```python\ndef init_rnn_state(batch_size, num_hiddens, device):\n    return (torch.zeros((batch_size, num_hiddens), device=device), )\n```\n\n- 如何在一个时间步内计算隐状态和输出。循环神经网络模型通过inputs最外层的维度实现循环，以便逐时间步更新小批量数据的隐状态H。这里使用tanh函数作为激活函数。当元素在实数上满足均匀分布时，tanh函数的平均值为0。\n\n\n```python\ndef rnn(inputs, state, params):\n    # `inputs`的形状：(`时间步数量`, `批量大小`, `词表大小`)\n    W_xh, W_hh, b_h, W_hq, b_q = params\n    H, = state\n    outputs = []\n    # `X`的形状：(`批量大小`, `词表大小`)\n    for X in inputs:\n        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)\n        Y = torch.mm(H, W_hq) + b_q\n        outputs.append(Y)\n    return torch.cat(outputs, dim=0), (H,)\n```\n\n- 创建一个类来包装这些函数，并存储从零开始实现的循环神经网络模型的参数。\n\n\n```python\nclass RNNModelScratch: #@save\n    \"\"\"从零开始实现的循环神经网络模型\"\"\"\n    def __init__(self, vocab_size, num_hiddens, device, get_params, init_state, forward_fn):\n        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n        self.params = get_params(vocab_size, num_hiddens, device)\n        self.init_state, self.forward_fn = init_state, forward_fn\n\n    def __call__(self, X, state):\n        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)\n        return self.forward_fn(X, state, self.params)\n\n    def begin_state(self, batch_size, device):\n        return self.init_state(batch_size, self.num_hiddens, device)\n```\n\n- 检查输出是否具有正确的形状。例如，隐状态的维数是否保持不变。\n\n\n```python\nnum_hiddens = 512\nnet = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params, init_rnn_state, rnn)\nstate = net.begin_state(X.shape[0], d2l.try_gpu())\nY, new_state = net(X.to(d2l.try_gpu()), state)\nY.shape, len(new_state), new_state[0].shape\n\n```\n\n\n\n\n    (torch.Size([10, 28]), 1, torch.Size([2, 512]))\n\n\n\n## 5.4 预测\n- 定义预测函数来生成prefix之后的新字符，在循环遍历prefix中的开始字符时，我们不断地将隐状态传递到下一个时间步，但是不生成任何输出。这被称为预热（warm‐up）期，因为在此期间模型会自我更新（例如，更新隐状态），但不会进行预测。预热期结束后，隐状态的值通常比刚开始的初始值更适合预测，从而预测字符并输出它们。\n\n\n\n```python\ndef predict_ch8(prefix, num_preds, net, vocab, device):  #@save\n    \"\"\"在`prefix`后面生成新字符。\"\"\"\n    state = net.begin_state(batch_size=1, device=device)\n    outputs = [vocab[prefix[0]]]\n    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))\n    for y in prefix[1:]:  # 预热期\n        _, state = net(get_input(), state)\n        outputs.append(vocab[y])\n    for _ in range(num_preds):  # 预测`num_preds`步\n        y, state = net(get_input(), state)\n        outputs.append(int(y.argmax(dim=1).reshape(1)))\n    return ''.join([vocab.idx_to_token[i] for i in outputs])\n```\n\n- 测试predict_ch8函数。我们将前缀指定为time traveller，并基于这个前缀生成10个后续字符。鉴于我们还没有训练网络，它会生成荒谬的预测结果。\n\n\n```python\npredict_ch8('time traveller ', 10, net, vocab, d2l.try_gpu())\n```\n\n\n\n\n    'time traveller uvf uvf uv'\n\n\n\n## 5.5 梯度裁剪\n- 对于长度为T的序列，我们在迭代中计算这T个时间步上的梯度，将会在反向传播过程中产生长度为O(T)的矩阵乘法链。当T较大时，它可能导致数值不稳定，例如可能导致梯度爆炸或梯度消失。因此，循环神经网络模型往往需要额外的方式来支持稳定训练。\n- 当解决优化问题时，我们对模型参数采用更新步骤。假定在向量形式的$\\mathbf{x}$中，或者在小批量数据的负梯度g方向上。假设学习率$\\eta$>0,，在一次迭代中，我们将$\\mathbf{x}$更新为$\\mathbf{x}-\\eta \\mathbf{g}$。如果我们进一步假设目标函数f表现良好，即函数f在常数L下是利普希茨连续的（Lipschitz continuous）。也就是说，对于任$u$和$v$，我们有：\n$$|f(\\mathbf{ x } - f(\\mathbf{ y })| \\leq L||\\mathbf{ x } - \\mathbf{ y }||$$\n\n- 在这种情况下，我们可以安全地假设：如果我们通过$\\eta \\mathbf{ g }$更新参数向量，则：\n$$|f(\\mathbf{ x } - f(\\mathbf{ x - \\eta \\mathbf{ g } })| \\leq L \\eta ||\\mathbf{ g }||$$\n\n- 这意味着我们不会观察到超过$L \\eta ||\\mathbf{ g }||$的变化。这既是坏事也是好事。坏的方面，它限制了取得进展的速度；好的方面，它限制了事情变糟的程度，尤其当我们朝着错误的方向前进时。\n- 有时梯度可能很大，优化算法可能无法收敛。我们可以通过降低学习率$\\eta$来解决这个问题。但是如果我们很少得到大的梯度呢？在这种情况下，这种做法似乎毫无道理。一个流行的替代方案是通过将梯度$\\mathbf{ g }$投影回给定半径(例如$\\theta$)的球来裁剪梯度：\n$$\\mathbf{ g } \\leftarrow \\text{ min }(1, \\frac{\\theta}{||\\mathbf{ g }||}) \\mathbf{ g }$$\n\n- 通过这样做，我们知道梯度范数永远不会超过$\\theta$，并且更新后的梯度完全与$\\mathbf{ g }$的原始方向对齐。这是梯度裁剪的基本思想。它还有一个值得拥有的副作用，即限制任何给定的小批量数据（以及其中任何给定的样本）对参数向量的影响，这赋予了模型一定程度的稳定性。梯度裁剪提供了一个快速修复梯度爆炸的方法，虽然它并不能完全解决问题，但它是众多有效的技术之一。\n- 定义一个函数来裁剪模型的梯度，计算了所有模型参数的梯度的范数。\n\n\n```python\ndef grad_clipping(net, theta): #@save\n    \"\"\"裁剪梯度。\"\"\"\n    if isinstance(net, nn.Module):\n        params = [p for p in net.parameters() if p.requires_grad]\n    else:\n        params = net.params\n    norm = torch.sqrt(sum(torch.sum((p.grad**2)) for p in params))\n    if norm > theta:\n        for param in params:\n            param.grad[:] *= theta / norm\n```\n\n## 5.6 训练\n- 定义一个函数在一个迭代周期内训练模型。它有三个不同之处。\n    - 序列数据的不同采样方法（随机采样和顺序分区）将导致隐状态初始化的差异。\n    - 更新模型参数之前裁剪梯度。这样的操作的目的是，即使训练过程中某个点上发生了梯度爆炸，也能保证模型不会发散。\n    - 用困惑度来评价模型。这样的度量确保了不同长度的序列具有可比性。\n- 当使用顺序分区时，我们只在每个迭代周期的开始位置初始化隐状态。由于下一个小批量数据中的第i个子序列样本与当前第i个子序列样本相邻，因此当前小批量数据最后一个样本的隐状态，将用于初始化下一个小批量数据第一个样本的隐状态。这样，存储在隐状态中的序列的历史信息可以在一个迭代周期内流经相邻的子序列。然而，在任何一点隐状态的计算，都依赖于同一迭代周期中前面所有的小批量数据，这使得梯度计算变得复杂。为了降低计算量，在处理任何一个小批量数据之前，我们先分离梯度，使得隐状态的梯度计算总是限制在一个小批量数据的时间步内。\n- 当使用随机抽样时，因为每个样本都是在一个随机位置抽样的，因此需要为每个迭代周期重新初始化隐状态。updater是更新模型参数的常用函数。它既可以是从头开始实现的d2l.sgd函数，也可以是深度学习框架中内置的优化函数。\n\n\n```python\n#@save\ndef train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):\n    \"\"\"训练模型一个迭代周期（定义见第8章）。\"\"\"\n    state, timer = None, d2l.Timer()\n    metric = d2l.Accumulator(2)  # 训练损失之和, 词元数量\n    for X, Y in train_iter:\n        if state is None or use_random_iter:\n            # 在第一个迭代周期，随机初始化隐藏状态\n            state = net.begin_state(batch_size=X.shape[0], device=device)\n        else:\n            if isinstance(net, nn.Module) and not isinstance(state, tuple):\n                # `state`对于`nn.GRU`是个张量\n                state.detach_()\n            else:\n                # `state`对于`nn.LSTM`或对于我们从零开始实现的模型是个张量\n                for s in state:\n                    s.detach_()\n        y = Y.T.reshape(-1)\n        X, y = X.to(device), y.to(device)\n        y_hat, state = net(X, state)\n        l = loss(y_hat, y.long()).mean()\n        if isinstance(updater, torch.optim.Optimizer):\n            updater.zero_grad()\n            l.backward()\n            grad_clipping(net, 1)\n            updater.step()\n        else:\n            l.backward()\n            grad_clipping(net, 1)\n            # 因为已经调用了`mean`函数\n            updater(batch_size=1)\n        metric.add(l * y.numel(), y.numel())\n    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()\n```\n\n- 循环神经网络模型的训练函数既支持从零开始实现，也可以使用高级API来实现。\n\n\n```python\n#@save\ndef train_ch8(net, train_iter, vocab, lr, num_epochs, device, use_random_iter=False):\n    \"\"\"训练模型。\"\"\"\n    loss = nn.CrossEntropyLoss()\n    animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',\n                            legend=['train'], xlim=[10, num_epochs])\n    # 初始化\n    if isinstance(net, nn.Module):\n        updater = torch.optim.SGD(net.parameters(), lr)\n    else:\n        updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size)\n    predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device)\n    # 训练和预测\n    for epoch in range(num_epochs):\n        ppl, speed = train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter)\n        if (epoch + 1) % 10 == 0:\n            print(predict('time traveller'))\n            animator.add(epoch + 1, [ppl])\n    print(f'困惑度 {ppl:.1f}, {speed:.1f} 词元/秒 {str(device)}')\n    print(predict('time traveller'))\n    print(predict('traveller'))\n```\n\n- 训练循环神经网络模型。因为我们在数据集中只使用了10000个词元，所以模型需要更多的迭代周期来更好地收敛。\n\n\n```python\nnum_epochs, lr = 500, 1\ntrain_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu())\n```\n\n    困惑度 1.0, 28075.0 词元/秒 cpu\n    time traveller for so it will be convenient to speak of himwas e\n    travelleryou can show black is white by argument said filby\n    \n\n\n    \n![svg](5_rnn_realize_files/5_rnn_realize_27_1.svg)\n![](img/deeplearning/code/pytorch/7_rnn/5_rnn_realize_files/5_rnn_realize_27_1.svg)\n\n\n- 检查使用随机抽样方法的结果。\n\n\n```python\nnet = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params, init_rnn_state, rnn)\ntrain_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu(), use_random_iter=True)\n```\n\n    困惑度 1.4, 27814.1 词元/秒 cpu\n    time travellerit s against reason said filbycan a cube that does\n    travelleryou can show black is white by argument said filby\n    \n\n\n    \n![svg](5_rnn_realize_files/5_rnn_realize_29_1.svg)\n![](img/deeplearning/code/pytorch/7_rnn/5_rnn_realize_files/5_rnn_realize_29_1.svg)\n\n","source":"_posts/deeplearning/code/pytorch/7_rnn/5_rnn_realize.md","raw":"---\ntitle: 7.5 循环神经网络从零开始实现\ndate: 2024-5-3 12:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 5 循环神经网络从零开始实现\n- 读取数据集\n\n\n```python\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK']='True'\nimport math\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom d2l import torch as d2l\n\nbatch_size, num_steps = 32, 35\ntrain_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)\n```\n\n## 5.1 独热编码\n- 在train_iter中，每个词元都表示为一个数字索引，将这些索引直接输入神经网络可能会使学习变得困难。我们通常将每个词元表示为更具表现力的特征向量。最简单的表示称为独热编码（one‐hot encoding）。\n    - 将每个索引映射为相互不同的单位向量：假设词表中不同词元的数目为N（即len(vocab)），词元索引的范围为0到N − 1。\n    - 如果词元的索引是整数i，那么我们将创建一个长度为N的全0向量，并将第i处的元素设置为1。此向量是原始词元的一个独热向量。\n\n\n```python\n# 索引为0和2的独热向量\nF.one_hot(torch.tensor([0, 2]), len(vocab))\n```\n\n\n\n\n    tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n             0, 0, 0, 0],\n            [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n             0, 0, 0, 0]])\n\n\n\n- 每次采样的小批量数据形状是二维张量：（batch_size, num_steps）。one_hot函数将这样一个小批量数据转换成三维张量，张量的最后一个维度等于词表大小（len(vocab)）。我们经常转换输入的维度，以便获得形状为（时间步数，批量大小，词表大小）的输出。这将使我们能够更方便地通过最外层的维度，一步一步地更新小批量数据的隐状态。\n\n\n\n```python\nX = torch.arange(10).reshape((2, 5))\nF.one_hot(X.T, 28).shape\n```\n\n\n\n\n    torch.Size([5, 2, 28])\n\n\n\n## 5.2 初始化模型参数\n- 隐藏单元数num_hiddens是一个可调的超参数。当训练语言模型时，输入和输出来自相同的词表。因此，它们具有相同的维度，即词表的大小。\n\n\n```python\ndef get_params(vocab_size, num_hiddens, device):\n    num_inputs = num_outputs = vocab_size\n\n    def normal(shape):\n        return torch.randn(size=shape, device=device) * 0.01\n    \n    # 隐藏层参数\n    W_xh = normal((num_inputs, num_hiddens))\n    W_hh = normal((num_hiddens, num_hiddens))\n    b_h = torch.zeros(num_hiddens, device=device)\n    # 输出层参数\n    W_hq = normal((num_hiddens, num_outputs))\n    b_q = torch.zeros(num_outputs, device=device)\n    # 附加梯度\n    params = [W_xh, W_hh, b_h, W_hq, b_q]\n    for param in params:\n        param.requires_grad_(True)\n    return params\n```\n\n## 5.3 循环神经网络模型\n- 为了定义循环神经网络模型，我们首先需要一个init_rnn_state函数在初始化时返回隐状态。这个函数的返回是一个张量，张量全用0填充，形状为（批量大小，隐藏单元数）。\n\n\n```python\ndef init_rnn_state(batch_size, num_hiddens, device):\n    return (torch.zeros((batch_size, num_hiddens), device=device), )\n```\n\n- 如何在一个时间步内计算隐状态和输出。循环神经网络模型通过inputs最外层的维度实现循环，以便逐时间步更新小批量数据的隐状态H。这里使用tanh函数作为激活函数。当元素在实数上满足均匀分布时，tanh函数的平均值为0。\n\n\n```python\ndef rnn(inputs, state, params):\n    # `inputs`的形状：(`时间步数量`, `批量大小`, `词表大小`)\n    W_xh, W_hh, b_h, W_hq, b_q = params\n    H, = state\n    outputs = []\n    # `X`的形状：(`批量大小`, `词表大小`)\n    for X in inputs:\n        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)\n        Y = torch.mm(H, W_hq) + b_q\n        outputs.append(Y)\n    return torch.cat(outputs, dim=0), (H,)\n```\n\n- 创建一个类来包装这些函数，并存储从零开始实现的循环神经网络模型的参数。\n\n\n```python\nclass RNNModelScratch: #@save\n    \"\"\"从零开始实现的循环神经网络模型\"\"\"\n    def __init__(self, vocab_size, num_hiddens, device, get_params, init_state, forward_fn):\n        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n        self.params = get_params(vocab_size, num_hiddens, device)\n        self.init_state, self.forward_fn = init_state, forward_fn\n\n    def __call__(self, X, state):\n        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)\n        return self.forward_fn(X, state, self.params)\n\n    def begin_state(self, batch_size, device):\n        return self.init_state(batch_size, self.num_hiddens, device)\n```\n\n- 检查输出是否具有正确的形状。例如，隐状态的维数是否保持不变。\n\n\n```python\nnum_hiddens = 512\nnet = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params, init_rnn_state, rnn)\nstate = net.begin_state(X.shape[0], d2l.try_gpu())\nY, new_state = net(X.to(d2l.try_gpu()), state)\nY.shape, len(new_state), new_state[0].shape\n\n```\n\n\n\n\n    (torch.Size([10, 28]), 1, torch.Size([2, 512]))\n\n\n\n## 5.4 预测\n- 定义预测函数来生成prefix之后的新字符，在循环遍历prefix中的开始字符时，我们不断地将隐状态传递到下一个时间步，但是不生成任何输出。这被称为预热（warm‐up）期，因为在此期间模型会自我更新（例如，更新隐状态），但不会进行预测。预热期结束后，隐状态的值通常比刚开始的初始值更适合预测，从而预测字符并输出它们。\n\n\n\n```python\ndef predict_ch8(prefix, num_preds, net, vocab, device):  #@save\n    \"\"\"在`prefix`后面生成新字符。\"\"\"\n    state = net.begin_state(batch_size=1, device=device)\n    outputs = [vocab[prefix[0]]]\n    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))\n    for y in prefix[1:]:  # 预热期\n        _, state = net(get_input(), state)\n        outputs.append(vocab[y])\n    for _ in range(num_preds):  # 预测`num_preds`步\n        y, state = net(get_input(), state)\n        outputs.append(int(y.argmax(dim=1).reshape(1)))\n    return ''.join([vocab.idx_to_token[i] for i in outputs])\n```\n\n- 测试predict_ch8函数。我们将前缀指定为time traveller，并基于这个前缀生成10个后续字符。鉴于我们还没有训练网络，它会生成荒谬的预测结果。\n\n\n```python\npredict_ch8('time traveller ', 10, net, vocab, d2l.try_gpu())\n```\n\n\n\n\n    'time traveller uvf uvf uv'\n\n\n\n## 5.5 梯度裁剪\n- 对于长度为T的序列，我们在迭代中计算这T个时间步上的梯度，将会在反向传播过程中产生长度为O(T)的矩阵乘法链。当T较大时，它可能导致数值不稳定，例如可能导致梯度爆炸或梯度消失。因此，循环神经网络模型往往需要额外的方式来支持稳定训练。\n- 当解决优化问题时，我们对模型参数采用更新步骤。假定在向量形式的$\\mathbf{x}$中，或者在小批量数据的负梯度g方向上。假设学习率$\\eta$>0,，在一次迭代中，我们将$\\mathbf{x}$更新为$\\mathbf{x}-\\eta \\mathbf{g}$。如果我们进一步假设目标函数f表现良好，即函数f在常数L下是利普希茨连续的（Lipschitz continuous）。也就是说，对于任$u$和$v$，我们有：\n$$|f(\\mathbf{ x } - f(\\mathbf{ y })| \\leq L||\\mathbf{ x } - \\mathbf{ y }||$$\n\n- 在这种情况下，我们可以安全地假设：如果我们通过$\\eta \\mathbf{ g }$更新参数向量，则：\n$$|f(\\mathbf{ x } - f(\\mathbf{ x - \\eta \\mathbf{ g } })| \\leq L \\eta ||\\mathbf{ g }||$$\n\n- 这意味着我们不会观察到超过$L \\eta ||\\mathbf{ g }||$的变化。这既是坏事也是好事。坏的方面，它限制了取得进展的速度；好的方面，它限制了事情变糟的程度，尤其当我们朝着错误的方向前进时。\n- 有时梯度可能很大，优化算法可能无法收敛。我们可以通过降低学习率$\\eta$来解决这个问题。但是如果我们很少得到大的梯度呢？在这种情况下，这种做法似乎毫无道理。一个流行的替代方案是通过将梯度$\\mathbf{ g }$投影回给定半径(例如$\\theta$)的球来裁剪梯度：\n$$\\mathbf{ g } \\leftarrow \\text{ min }(1, \\frac{\\theta}{||\\mathbf{ g }||}) \\mathbf{ g }$$\n\n- 通过这样做，我们知道梯度范数永远不会超过$\\theta$，并且更新后的梯度完全与$\\mathbf{ g }$的原始方向对齐。这是梯度裁剪的基本思想。它还有一个值得拥有的副作用，即限制任何给定的小批量数据（以及其中任何给定的样本）对参数向量的影响，这赋予了模型一定程度的稳定性。梯度裁剪提供了一个快速修复梯度爆炸的方法，虽然它并不能完全解决问题，但它是众多有效的技术之一。\n- 定义一个函数来裁剪模型的梯度，计算了所有模型参数的梯度的范数。\n\n\n```python\ndef grad_clipping(net, theta): #@save\n    \"\"\"裁剪梯度。\"\"\"\n    if isinstance(net, nn.Module):\n        params = [p for p in net.parameters() if p.requires_grad]\n    else:\n        params = net.params\n    norm = torch.sqrt(sum(torch.sum((p.grad**2)) for p in params))\n    if norm > theta:\n        for param in params:\n            param.grad[:] *= theta / norm\n```\n\n## 5.6 训练\n- 定义一个函数在一个迭代周期内训练模型。它有三个不同之处。\n    - 序列数据的不同采样方法（随机采样和顺序分区）将导致隐状态初始化的差异。\n    - 更新模型参数之前裁剪梯度。这样的操作的目的是，即使训练过程中某个点上发生了梯度爆炸，也能保证模型不会发散。\n    - 用困惑度来评价模型。这样的度量确保了不同长度的序列具有可比性。\n- 当使用顺序分区时，我们只在每个迭代周期的开始位置初始化隐状态。由于下一个小批量数据中的第i个子序列样本与当前第i个子序列样本相邻，因此当前小批量数据最后一个样本的隐状态，将用于初始化下一个小批量数据第一个样本的隐状态。这样，存储在隐状态中的序列的历史信息可以在一个迭代周期内流经相邻的子序列。然而，在任何一点隐状态的计算，都依赖于同一迭代周期中前面所有的小批量数据，这使得梯度计算变得复杂。为了降低计算量，在处理任何一个小批量数据之前，我们先分离梯度，使得隐状态的梯度计算总是限制在一个小批量数据的时间步内。\n- 当使用随机抽样时，因为每个样本都是在一个随机位置抽样的，因此需要为每个迭代周期重新初始化隐状态。updater是更新模型参数的常用函数。它既可以是从头开始实现的d2l.sgd函数，也可以是深度学习框架中内置的优化函数。\n\n\n```python\n#@save\ndef train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):\n    \"\"\"训练模型一个迭代周期（定义见第8章）。\"\"\"\n    state, timer = None, d2l.Timer()\n    metric = d2l.Accumulator(2)  # 训练损失之和, 词元数量\n    for X, Y in train_iter:\n        if state is None or use_random_iter:\n            # 在第一个迭代周期，随机初始化隐藏状态\n            state = net.begin_state(batch_size=X.shape[0], device=device)\n        else:\n            if isinstance(net, nn.Module) and not isinstance(state, tuple):\n                # `state`对于`nn.GRU`是个张量\n                state.detach_()\n            else:\n                # `state`对于`nn.LSTM`或对于我们从零开始实现的模型是个张量\n                for s in state:\n                    s.detach_()\n        y = Y.T.reshape(-1)\n        X, y = X.to(device), y.to(device)\n        y_hat, state = net(X, state)\n        l = loss(y_hat, y.long()).mean()\n        if isinstance(updater, torch.optim.Optimizer):\n            updater.zero_grad()\n            l.backward()\n            grad_clipping(net, 1)\n            updater.step()\n        else:\n            l.backward()\n            grad_clipping(net, 1)\n            # 因为已经调用了`mean`函数\n            updater(batch_size=1)\n        metric.add(l * y.numel(), y.numel())\n    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()\n```\n\n- 循环神经网络模型的训练函数既支持从零开始实现，也可以使用高级API来实现。\n\n\n```python\n#@save\ndef train_ch8(net, train_iter, vocab, lr, num_epochs, device, use_random_iter=False):\n    \"\"\"训练模型。\"\"\"\n    loss = nn.CrossEntropyLoss()\n    animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',\n                            legend=['train'], xlim=[10, num_epochs])\n    # 初始化\n    if isinstance(net, nn.Module):\n        updater = torch.optim.SGD(net.parameters(), lr)\n    else:\n        updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size)\n    predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device)\n    # 训练和预测\n    for epoch in range(num_epochs):\n        ppl, speed = train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter)\n        if (epoch + 1) % 10 == 0:\n            print(predict('time traveller'))\n            animator.add(epoch + 1, [ppl])\n    print(f'困惑度 {ppl:.1f}, {speed:.1f} 词元/秒 {str(device)}')\n    print(predict('time traveller'))\n    print(predict('traveller'))\n```\n\n- 训练循环神经网络模型。因为我们在数据集中只使用了10000个词元，所以模型需要更多的迭代周期来更好地收敛。\n\n\n```python\nnum_epochs, lr = 500, 1\ntrain_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu())\n```\n\n    困惑度 1.0, 28075.0 词元/秒 cpu\n    time traveller for so it will be convenient to speak of himwas e\n    travelleryou can show black is white by argument said filby\n    \n\n\n    \n![svg](5_rnn_realize_files/5_rnn_realize_27_1.svg)\n![](img/deeplearning/code/pytorch/7_rnn/5_rnn_realize_files/5_rnn_realize_27_1.svg)\n\n\n- 检查使用随机抽样方法的结果。\n\n\n```python\nnet = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params, init_rnn_state, rnn)\ntrain_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu(), use_random_iter=True)\n```\n\n    困惑度 1.4, 27814.1 词元/秒 cpu\n    time travellerit s against reason said filbycan a cube that does\n    travelleryou can show black is white by argument said filby\n    \n\n\n    \n![svg](5_rnn_realize_files/5_rnn_realize_29_1.svg)\n![](img/deeplearning/code/pytorch/7_rnn/5_rnn_realize_files/5_rnn_realize_29_1.svg)\n\n","slug":"deeplearning/code/pytorch/7_rnn/5_rnn_realize","published":1,"updated":"2024-05-05T14:41:55.315Z","comments":1,"layout":"post","photos":[],"_id":"clvuli6uz0097zsvw4rv2acyw","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"5-循环神经网络从零开始实现\"><a href=\"#5-循环神经网络从零开始实现\" class=\"headerlink\" title=\"5 循环神经网络从零开始实现\"></a>5 循环神经网络从零开始实现</h1><ul>\n<li>读取数据集</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>]=<span class=\"string\">&#x27;True&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> math</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.nn <span class=\"keyword\">import</span> functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">batch_size, num_steps = <span class=\"number\">32</span>, <span class=\"number\">35</span></span><br><span class=\"line\">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"5-1-独热编码\"><a href=\"#5-1-独热编码\" class=\"headerlink\" title=\"5.1 独热编码\"></a>5.1 独热编码</h2><ul>\n<li>在train_iter中，每个词元都表示为一个数字索引，将这些索引直接输入神经网络可能会使学习变得困难。我们通常将每个词元表示为更具表现力的特征向量。最简单的表示称为独热编码（one‐hot encoding）。<ul>\n<li>将每个索引映射为相互不同的单位向量：假设词表中不同词元的数目为N（即len(vocab)），词元索引的范围为0到N − 1。</li>\n<li>如果词元的索引是整数i，那么我们将创建一个长度为N的全0向量，并将第i处的元素设置为1。此向量是原始词元的一个独热向量。</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 索引为0和2的独热向量</span></span><br><span class=\"line\">F.one_hot(torch.tensor([<span class=\"number\">0</span>, <span class=\"number\">2</span>]), <span class=\"built_in\">len</span>(vocab))</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0],\n        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0]])\n</code></pre>\n<ul>\n<li>每次采样的小批量数据形状是二维张量：（batch_size, num_steps）。one_hot函数将这样一个小批量数据转换成三维张量，张量的最后一个维度等于词表大小（len(vocab)）。我们经常转换输入的维度，以便获得形状为（时间步数，批量大小，词表大小）的输出。这将使我们能够更方便地通过最外层的维度，一步一步地更新小批量数据的隐状态。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = torch.arange(<span class=\"number\">10</span>).reshape((<span class=\"number\">2</span>, <span class=\"number\">5</span>))</span><br><span class=\"line\">F.one_hot(X.T, <span class=\"number\">28</span>).shape</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>torch.Size([5, 2, 28])\n</code></pre>\n<h2 id=\"5-2-初始化模型参数\"><a href=\"#5-2-初始化模型参数\" class=\"headerlink\" title=\"5.2 初始化模型参数\"></a>5.2 初始化模型参数</h2><ul>\n<li>隐藏单元数num_hiddens是一个可调的超参数。当训练语言模型时，输入和输出来自相同的词表。因此，它们具有相同的维度，即词表的大小。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">get_params</span>(<span class=\"params\">vocab_size, num_hiddens, device</span>):</span><br><span class=\"line\">    num_inputs = num_outputs = vocab_size</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">normal</span>(<span class=\"params\">shape</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> torch.randn(size=shape, device=device) * <span class=\"number\">0.01</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 隐藏层参数</span></span><br><span class=\"line\">    W_xh = normal((num_inputs, num_hiddens))</span><br><span class=\"line\">    W_hh = normal((num_hiddens, num_hiddens))</span><br><span class=\"line\">    b_h = torch.zeros(num_hiddens, device=device)</span><br><span class=\"line\">    <span class=\"comment\"># 输出层参数</span></span><br><span class=\"line\">    W_hq = normal((num_hiddens, num_outputs))</span><br><span class=\"line\">    b_q = torch.zeros(num_outputs, device=device)</span><br><span class=\"line\">    <span class=\"comment\"># 附加梯度</span></span><br><span class=\"line\">    params = [W_xh, W_hh, b_h, W_hq, b_q]</span><br><span class=\"line\">    <span class=\"keyword\">for</span> param <span class=\"keyword\">in</span> params:</span><br><span class=\"line\">        param.requires_grad_(<span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> params</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"5-3-循环神经网络模型\"><a href=\"#5-3-循环神经网络模型\" class=\"headerlink\" title=\"5.3 循环神经网络模型\"></a>5.3 循环神经网络模型</h2><ul>\n<li>为了定义循环神经网络模型，我们首先需要一个init_rnn_state函数在初始化时返回隐状态。这个函数的返回是一个张量，张量全用0填充，形状为（批量大小，隐藏单元数）。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_rnn_state</span>(<span class=\"params\">batch_size, num_hiddens, device</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> (torch.zeros((batch_size, num_hiddens), device=device), )</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>如何在一个时间步内计算隐状态和输出。循环神经网络模型通过inputs最外层的维度实现循环，以便逐时间步更新小批量数据的隐状态H。这里使用tanh函数作为激活函数。当元素在实数上满足均匀分布时，tanh函数的平均值为0。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">rnn</span>(<span class=\"params\">inputs, state, params</span>):</span><br><span class=\"line\">    <span class=\"comment\"># `inputs`的形状：(`时间步数量`, `批量大小`, `词表大小`)</span></span><br><span class=\"line\">    W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class=\"line\">    H, = state</span><br><span class=\"line\">    outputs = []</span><br><span class=\"line\">    <span class=\"comment\"># `X`的形状：(`批量大小`, `词表大小`)</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> X <span class=\"keyword\">in</span> inputs:</span><br><span class=\"line\">        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)</span><br><span class=\"line\">        Y = torch.mm(H, W_hq) + b_q</span><br><span class=\"line\">        outputs.append(Y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.cat(outputs, dim=<span class=\"number\">0</span>), (H,)</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>创建一个类来包装这些函数，并存储从零开始实现的循环神经网络模型的参数。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">RNNModelScratch</span>: <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;从零开始实现的循环神经网络模型&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, vocab_size, num_hiddens, device, get_params, init_state, forward_fn</span>):</span><br><span class=\"line\">        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens</span><br><span class=\"line\">        self.params = get_params(vocab_size, num_hiddens, device)</span><br><span class=\"line\">        self.init_state, self.forward_fn = init_state, forward_fn</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__call__</span>(<span class=\"params\">self, X, state</span>):</span><br><span class=\"line\">        X = F.one_hot(X.T, self.vocab_size).<span class=\"built_in\">type</span>(torch.float32)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.forward_fn(X, state, self.params)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">begin_state</span>(<span class=\"params\">self, batch_size, device</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.init_state(batch_size, self.num_hiddens, device)</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>检查输出是否具有正确的形状。例如，隐状态的维数是否保持不变。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_hiddens = <span class=\"number\">512</span></span><br><span class=\"line\">net = RNNModelScratch(<span class=\"built_in\">len</span>(vocab), num_hiddens, d2l.try_gpu(), get_params, init_rnn_state, rnn)</span><br><span class=\"line\">state = net.begin_state(X.shape[<span class=\"number\">0</span>], d2l.try_gpu())</span><br><span class=\"line\">Y, new_state = net(X.to(d2l.try_gpu()), state)</span><br><span class=\"line\">Y.shape, <span class=\"built_in\">len</span>(new_state), new_state[<span class=\"number\">0</span>].shape</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>(torch.Size([10, 28]), 1, torch.Size([2, 512]))\n</code></pre>\n<h2 id=\"5-4-预测\"><a href=\"#5-4-预测\" class=\"headerlink\" title=\"5.4 预测\"></a>5.4 预测</h2><ul>\n<li>定义预测函数来生成prefix之后的新字符，在循环遍历prefix中的开始字符时，我们不断地将隐状态传递到下一个时间步，但是不生成任何输出。这被称为预热（warm‐up）期，因为在此期间模型会自我更新（例如，更新隐状态），但不会进行预测。预热期结束后，隐状态的值通常比刚开始的初始值更适合预测，从而预测字符并输出它们。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">predict_ch8</span>(<span class=\"params\">prefix, num_preds, net, vocab, device</span>):  <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;在`prefix`后面生成新字符。&quot;&quot;&quot;</span></span><br><span class=\"line\">    state = net.begin_state(batch_size=<span class=\"number\">1</span>, device=device)</span><br><span class=\"line\">    outputs = [vocab[prefix[<span class=\"number\">0</span>]]]</span><br><span class=\"line\">    get_input = <span class=\"keyword\">lambda</span>: torch.tensor([outputs[-<span class=\"number\">1</span>]], device=device).reshape((<span class=\"number\">1</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">    <span class=\"keyword\">for</span> y <span class=\"keyword\">in</span> prefix[<span class=\"number\">1</span>:]:  <span class=\"comment\"># 预热期</span></span><br><span class=\"line\">        _, state = net(get_input(), state)</span><br><span class=\"line\">        outputs.append(vocab[y])</span><br><span class=\"line\">    <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_preds):  <span class=\"comment\"># 预测`num_preds`步</span></span><br><span class=\"line\">        y, state = net(get_input(), state)</span><br><span class=\"line\">        outputs.append(<span class=\"built_in\">int</span>(y.argmax(dim=<span class=\"number\">1</span>).reshape(<span class=\"number\">1</span>)))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"string\">&#x27;&#x27;</span>.join([vocab.idx_to_token[i] <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> outputs])</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>测试predict_ch8函数。我们将前缀指定为time traveller，并基于这个前缀生成10个后续字符。鉴于我们还没有训练网络，它会生成荒谬的预测结果。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">predict_ch8(<span class=\"string\">&#x27;time traveller &#x27;</span>, <span class=\"number\">10</span>, net, vocab, d2l.try_gpu())</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>&#39;time traveller uvf uvf uv&#39;\n</code></pre>\n<h2 id=\"5-5-梯度裁剪\"><a href=\"#5-5-梯度裁剪\" class=\"headerlink\" title=\"5.5 梯度裁剪\"></a>5.5 梯度裁剪</h2><ul>\n<li><p>对于长度为T的序列，我们在迭代中计算这T个时间步上的梯度，将会在反向传播过程中产生长度为O(T)的矩阵乘法链。当T较大时，它可能导致数值不稳定，例如可能导致梯度爆炸或梯度消失。因此，循环神经网络模型往往需要额外的方式来支持稳定训练。</p>\n</li>\n<li><p>当解决优化问题时，我们对模型参数采用更新步骤。假定在向量形式的$\\mathbf{x}$中，或者在小批量数据的负梯度g方向上。假设学习率$\\eta$&gt;0,，在一次迭代中，我们将$\\mathbf{x}$更新为$\\mathbf{x}-\\eta \\mathbf{g}$。如果我们进一步假设目标函数f表现良好，即函数f在常数L下是利普希茨连续的（Lipschitz continuous）。也就是说，对于任$u$和$v$，我们有：<br>$$|f(\\mathbf{ x } - f(\\mathbf{ y })| \\leq L||\\mathbf{ x } - \\mathbf{ y }||$$</p>\n</li>\n<li><p>在这种情况下，我们可以安全地假设：如果我们通过$\\eta \\mathbf{ g }$更新参数向量，则：<br>$$|f(\\mathbf{ x } - f(\\mathbf{ x - \\eta \\mathbf{ g } })| \\leq L \\eta ||\\mathbf{ g }||$$</p>\n</li>\n<li><p>这意味着我们不会观察到超过$L \\eta ||\\mathbf{ g }||$的变化。这既是坏事也是好事。坏的方面，它限制了取得进展的速度；好的方面，它限制了事情变糟的程度，尤其当我们朝着错误的方向前进时。</p>\n</li>\n<li><p>有时梯度可能很大，优化算法可能无法收敛。我们可以通过降低学习率$\\eta$来解决这个问题。但是如果我们很少得到大的梯度呢？在这种情况下，这种做法似乎毫无道理。一个流行的替代方案是通过将梯度$\\mathbf{ g }$投影回给定半径(例如$\\theta$)的球来裁剪梯度：<br>$$\\mathbf{ g } \\leftarrow \\text{ min }(1, \\frac{\\theta}{||\\mathbf{ g }||}) \\mathbf{ g }$$</p>\n</li>\n<li><p>通过这样做，我们知道梯度范数永远不会超过$\\theta$，并且更新后的梯度完全与$\\mathbf{ g }$的原始方向对齐。这是梯度裁剪的基本思想。它还有一个值得拥有的副作用，即限制任何给定的小批量数据（以及其中任何给定的样本）对参数向量的影响，这赋予了模型一定程度的稳定性。梯度裁剪提供了一个快速修复梯度爆炸的方法，虽然它并不能完全解决问题，但它是众多有效的技术之一。</p>\n</li>\n<li><p>定义一个函数来裁剪模型的梯度，计算了所有模型参数的梯度的范数。</p>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">grad_clipping</span>(<span class=\"params\">net, theta</span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;裁剪梯度。&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(net, nn.Module):</span><br><span class=\"line\">        params = [p <span class=\"keyword\">for</span> p <span class=\"keyword\">in</span> net.parameters() <span class=\"keyword\">if</span> p.requires_grad]</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        params = net.params</span><br><span class=\"line\">    norm = torch.sqrt(<span class=\"built_in\">sum</span>(torch.<span class=\"built_in\">sum</span>((p.grad**<span class=\"number\">2</span>)) <span class=\"keyword\">for</span> p <span class=\"keyword\">in</span> params))</span><br><span class=\"line\">    <span class=\"keyword\">if</span> norm &gt; theta:</span><br><span class=\"line\">        <span class=\"keyword\">for</span> param <span class=\"keyword\">in</span> params:</span><br><span class=\"line\">            param.grad[:] *= theta / norm</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"5-6-训练\"><a href=\"#5-6-训练\" class=\"headerlink\" title=\"5.6 训练\"></a>5.6 训练</h2><ul>\n<li>定义一个函数在一个迭代周期内训练模型。它有三个不同之处。<ul>\n<li>序列数据的不同采样方法（随机采样和顺序分区）将导致隐状态初始化的差异。</li>\n<li>更新模型参数之前裁剪梯度。这样的操作的目的是，即使训练过程中某个点上发生了梯度爆炸，也能保证模型不会发散。</li>\n<li>用困惑度来评价模型。这样的度量确保了不同长度的序列具有可比性。</li>\n</ul>\n</li>\n<li>当使用顺序分区时，我们只在每个迭代周期的开始位置初始化隐状态。由于下一个小批量数据中的第i个子序列样本与当前第i个子序列样本相邻，因此当前小批量数据最后一个样本的隐状态，将用于初始化下一个小批量数据第一个样本的隐状态。这样，存储在隐状态中的序列的历史信息可以在一个迭代周期内流经相邻的子序列。然而，在任何一点隐状态的计算，都依赖于同一迭代周期中前面所有的小批量数据，这使得梯度计算变得复杂。为了降低计算量，在处理任何一个小批量数据之前，我们先分离梯度，使得隐状态的梯度计算总是限制在一个小批量数据的时间步内。</li>\n<li>当使用随机抽样时，因为每个样本都是在一个随机位置抽样的，因此需要为每个迭代周期重新初始化隐状态。updater是更新模型参数的常用函数。它既可以是从头开始实现的d2l.sgd函数，也可以是深度学习框架中内置的优化函数。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#@save</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train_epoch_ch8</span>(<span class=\"params\">net, train_iter, loss, updater, device, use_random_iter</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;训练模型一个迭代周期（定义见第8章）。&quot;&quot;&quot;</span></span><br><span class=\"line\">    state, timer = <span class=\"literal\">None</span>, d2l.Timer()</span><br><span class=\"line\">    metric = d2l.Accumulator(<span class=\"number\">2</span>)  <span class=\"comment\"># 训练损失之和, 词元数量</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> X, Y <span class=\"keyword\">in</span> train_iter:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> state <span class=\"keyword\">is</span> <span class=\"literal\">None</span> <span class=\"keyword\">or</span> use_random_iter:</span><br><span class=\"line\">            <span class=\"comment\"># 在第一个迭代周期，随机初始化隐藏状态</span></span><br><span class=\"line\">            state = net.begin_state(batch_size=X.shape[<span class=\"number\">0</span>], device=device)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(net, nn.Module) <span class=\"keyword\">and</span> <span class=\"keyword\">not</span> <span class=\"built_in\">isinstance</span>(state, <span class=\"built_in\">tuple</span>):</span><br><span class=\"line\">                <span class=\"comment\"># `state`对于`nn.GRU`是个张量</span></span><br><span class=\"line\">                state.detach_()</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                <span class=\"comment\"># `state`对于`nn.LSTM`或对于我们从零开始实现的模型是个张量</span></span><br><span class=\"line\">                <span class=\"keyword\">for</span> s <span class=\"keyword\">in</span> state:</span><br><span class=\"line\">                    s.detach_()</span><br><span class=\"line\">        y = Y.T.reshape(-<span class=\"number\">1</span>)</span><br><span class=\"line\">        X, y = X.to(device), y.to(device)</span><br><span class=\"line\">        y_hat, state = net(X, state)</span><br><span class=\"line\">        l = loss(y_hat, y.long()).mean()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(updater, torch.optim.Optimizer):</span><br><span class=\"line\">            updater.zero_grad()</span><br><span class=\"line\">            l.backward()</span><br><span class=\"line\">            grad_clipping(net, <span class=\"number\">1</span>)</span><br><span class=\"line\">            updater.step()</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            l.backward()</span><br><span class=\"line\">            grad_clipping(net, <span class=\"number\">1</span>)</span><br><span class=\"line\">            <span class=\"comment\"># 因为已经调用了`mean`函数</span></span><br><span class=\"line\">            updater(batch_size=<span class=\"number\">1</span>)</span><br><span class=\"line\">        metric.add(l * y.numel(), y.numel())</span><br><span class=\"line\">    <span class=\"keyword\">return</span> math.exp(metric[<span class=\"number\">0</span>] / metric[<span class=\"number\">1</span>]), metric[<span class=\"number\">1</span>] / timer.stop()</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>循环神经网络模型的训练函数既支持从零开始实现，也可以使用高级API来实现。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#@save</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train_ch8</span>(<span class=\"params\">net, train_iter, vocab, lr, num_epochs, device, use_random_iter=<span class=\"literal\">False</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;训练模型。&quot;&quot;&quot;</span></span><br><span class=\"line\">    loss = nn.CrossEntropyLoss()</span><br><span class=\"line\">    animator = d2l.Animator(xlabel=<span class=\"string\">&#x27;epoch&#x27;</span>, ylabel=<span class=\"string\">&#x27;perplexity&#x27;</span>,</span><br><span class=\"line\">                            legend=[<span class=\"string\">&#x27;train&#x27;</span>], xlim=[<span class=\"number\">10</span>, num_epochs])</span><br><span class=\"line\">    <span class=\"comment\"># 初始化</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(net, nn.Module):</span><br><span class=\"line\">        updater = torch.optim.SGD(net.parameters(), lr)</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        updater = <span class=\"keyword\">lambda</span> batch_size: d2l.sgd(net.params, lr, batch_size)</span><br><span class=\"line\">    predict = <span class=\"keyword\">lambda</span> prefix: predict_ch8(prefix, <span class=\"number\">50</span>, net, vocab, device)</span><br><span class=\"line\">    <span class=\"comment\"># 训练和预测</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        ppl, speed = train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (epoch + <span class=\"number\">1</span>) % <span class=\"number\">10</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(predict(<span class=\"string\">&#x27;time traveller&#x27;</span>))</span><br><span class=\"line\">            animator.add(epoch + <span class=\"number\">1</span>, [ppl])</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;困惑度 <span class=\"subst\">&#123;ppl:<span class=\"number\">.1</span>f&#125;</span>, <span class=\"subst\">&#123;speed:<span class=\"number\">.1</span>f&#125;</span> 词元/秒 <span class=\"subst\">&#123;<span class=\"built_in\">str</span>(device)&#125;</span>&#x27;</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(predict(<span class=\"string\">&#x27;time traveller&#x27;</span>))</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(predict(<span class=\"string\">&#x27;traveller&#x27;</span>))</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>训练循环神经网络模型。因为我们在数据集中只使用了10000个词元，所以模型需要更多的迭代周期来更好地收敛。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_epochs, lr = <span class=\"number\">500</span>, <span class=\"number\">1</span></span><br><span class=\"line\">train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu())</span><br></pre></td></tr></table></figure>\n\n<pre><code>困惑度 1.0, 28075.0 词元/秒 cpu\ntime traveller for so it will be convenient to speak of himwas e\ntravelleryou can show black is white by argument said filby\n</code></pre>\n<p><img src=\"/5_rnn_realize_files/5_rnn_realize_27_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/7_rnn/5_rnn_realize_files/5_rnn_realize_27_1.svg\"></p>\n<ul>\n<li>检查使用随机抽样方法的结果。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net = RNNModelScratch(<span class=\"built_in\">len</span>(vocab), num_hiddens, d2l.try_gpu(), get_params, init_rnn_state, rnn)</span><br><span class=\"line\">train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu(), use_random_iter=<span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>困惑度 1.4, 27814.1 词元/秒 cpu\ntime travellerit s against reason said filbycan a cube that does\ntravelleryou can show black is white by argument said filby\n</code></pre>\n<p><img src=\"/5_rnn_realize_files/5_rnn_realize_29_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/7_rnn/5_rnn_realize_files/5_rnn_realize_29_1.svg\"></p>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"5-循环神经网络从零开始实现\"><a href=\"#5-循环神经网络从零开始实现\" class=\"headerlink\" title=\"5 循环神经网络从零开始实现\"></a>5 循环神经网络从零开始实现</h1><ul>\n<li>读取数据集</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>]=<span class=\"string\">&#x27;True&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> math</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.nn <span class=\"keyword\">import</span> functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">batch_size, num_steps = <span class=\"number\">32</span>, <span class=\"number\">35</span></span><br><span class=\"line\">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"5-1-独热编码\"><a href=\"#5-1-独热编码\" class=\"headerlink\" title=\"5.1 独热编码\"></a>5.1 独热编码</h2><ul>\n<li>在train_iter中，每个词元都表示为一个数字索引，将这些索引直接输入神经网络可能会使学习变得困难。我们通常将每个词元表示为更具表现力的特征向量。最简单的表示称为独热编码（one‐hot encoding）。<ul>\n<li>将每个索引映射为相互不同的单位向量：假设词表中不同词元的数目为N（即len(vocab)），词元索引的范围为0到N − 1。</li>\n<li>如果词元的索引是整数i，那么我们将创建一个长度为N的全0向量，并将第i处的元素设置为1。此向量是原始词元的一个独热向量。</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 索引为0和2的独热向量</span></span><br><span class=\"line\">F.one_hot(torch.tensor([<span class=\"number\">0</span>, <span class=\"number\">2</span>]), <span class=\"built_in\">len</span>(vocab))</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0],\n        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0]])\n</code></pre>\n<ul>\n<li>每次采样的小批量数据形状是二维张量：（batch_size, num_steps）。one_hot函数将这样一个小批量数据转换成三维张量，张量的最后一个维度等于词表大小（len(vocab)）。我们经常转换输入的维度，以便获得形状为（时间步数，批量大小，词表大小）的输出。这将使我们能够更方便地通过最外层的维度，一步一步地更新小批量数据的隐状态。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = torch.arange(<span class=\"number\">10</span>).reshape((<span class=\"number\">2</span>, <span class=\"number\">5</span>))</span><br><span class=\"line\">F.one_hot(X.T, <span class=\"number\">28</span>).shape</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>torch.Size([5, 2, 28])\n</code></pre>\n<h2 id=\"5-2-初始化模型参数\"><a href=\"#5-2-初始化模型参数\" class=\"headerlink\" title=\"5.2 初始化模型参数\"></a>5.2 初始化模型参数</h2><ul>\n<li>隐藏单元数num_hiddens是一个可调的超参数。当训练语言模型时，输入和输出来自相同的词表。因此，它们具有相同的维度，即词表的大小。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">get_params</span>(<span class=\"params\">vocab_size, num_hiddens, device</span>):</span><br><span class=\"line\">    num_inputs = num_outputs = vocab_size</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">normal</span>(<span class=\"params\">shape</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> torch.randn(size=shape, device=device) * <span class=\"number\">0.01</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 隐藏层参数</span></span><br><span class=\"line\">    W_xh = normal((num_inputs, num_hiddens))</span><br><span class=\"line\">    W_hh = normal((num_hiddens, num_hiddens))</span><br><span class=\"line\">    b_h = torch.zeros(num_hiddens, device=device)</span><br><span class=\"line\">    <span class=\"comment\"># 输出层参数</span></span><br><span class=\"line\">    W_hq = normal((num_hiddens, num_outputs))</span><br><span class=\"line\">    b_q = torch.zeros(num_outputs, device=device)</span><br><span class=\"line\">    <span class=\"comment\"># 附加梯度</span></span><br><span class=\"line\">    params = [W_xh, W_hh, b_h, W_hq, b_q]</span><br><span class=\"line\">    <span class=\"keyword\">for</span> param <span class=\"keyword\">in</span> params:</span><br><span class=\"line\">        param.requires_grad_(<span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> params</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"5-3-循环神经网络模型\"><a href=\"#5-3-循环神经网络模型\" class=\"headerlink\" title=\"5.3 循环神经网络模型\"></a>5.3 循环神经网络模型</h2><ul>\n<li>为了定义循环神经网络模型，我们首先需要一个init_rnn_state函数在初始化时返回隐状态。这个函数的返回是一个张量，张量全用0填充，形状为（批量大小，隐藏单元数）。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_rnn_state</span>(<span class=\"params\">batch_size, num_hiddens, device</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> (torch.zeros((batch_size, num_hiddens), device=device), )</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>如何在一个时间步内计算隐状态和输出。循环神经网络模型通过inputs最外层的维度实现循环，以便逐时间步更新小批量数据的隐状态H。这里使用tanh函数作为激活函数。当元素在实数上满足均匀分布时，tanh函数的平均值为0。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">rnn</span>(<span class=\"params\">inputs, state, params</span>):</span><br><span class=\"line\">    <span class=\"comment\"># `inputs`的形状：(`时间步数量`, `批量大小`, `词表大小`)</span></span><br><span class=\"line\">    W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class=\"line\">    H, = state</span><br><span class=\"line\">    outputs = []</span><br><span class=\"line\">    <span class=\"comment\"># `X`的形状：(`批量大小`, `词表大小`)</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> X <span class=\"keyword\">in</span> inputs:</span><br><span class=\"line\">        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)</span><br><span class=\"line\">        Y = torch.mm(H, W_hq) + b_q</span><br><span class=\"line\">        outputs.append(Y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.cat(outputs, dim=<span class=\"number\">0</span>), (H,)</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>创建一个类来包装这些函数，并存储从零开始实现的循环神经网络模型的参数。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">RNNModelScratch</span>: <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;从零开始实现的循环神经网络模型&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, vocab_size, num_hiddens, device, get_params, init_state, forward_fn</span>):</span><br><span class=\"line\">        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens</span><br><span class=\"line\">        self.params = get_params(vocab_size, num_hiddens, device)</span><br><span class=\"line\">        self.init_state, self.forward_fn = init_state, forward_fn</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__call__</span>(<span class=\"params\">self, X, state</span>):</span><br><span class=\"line\">        X = F.one_hot(X.T, self.vocab_size).<span class=\"built_in\">type</span>(torch.float32)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.forward_fn(X, state, self.params)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">begin_state</span>(<span class=\"params\">self, batch_size, device</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.init_state(batch_size, self.num_hiddens, device)</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>检查输出是否具有正确的形状。例如，隐状态的维数是否保持不变。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_hiddens = <span class=\"number\">512</span></span><br><span class=\"line\">net = RNNModelScratch(<span class=\"built_in\">len</span>(vocab), num_hiddens, d2l.try_gpu(), get_params, init_rnn_state, rnn)</span><br><span class=\"line\">state = net.begin_state(X.shape[<span class=\"number\">0</span>], d2l.try_gpu())</span><br><span class=\"line\">Y, new_state = net(X.to(d2l.try_gpu()), state)</span><br><span class=\"line\">Y.shape, <span class=\"built_in\">len</span>(new_state), new_state[<span class=\"number\">0</span>].shape</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>(torch.Size([10, 28]), 1, torch.Size([2, 512]))\n</code></pre>\n<h2 id=\"5-4-预测\"><a href=\"#5-4-预测\" class=\"headerlink\" title=\"5.4 预测\"></a>5.4 预测</h2><ul>\n<li>定义预测函数来生成prefix之后的新字符，在循环遍历prefix中的开始字符时，我们不断地将隐状态传递到下一个时间步，但是不生成任何输出。这被称为预热（warm‐up）期，因为在此期间模型会自我更新（例如，更新隐状态），但不会进行预测。预热期结束后，隐状态的值通常比刚开始的初始值更适合预测，从而预测字符并输出它们。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">predict_ch8</span>(<span class=\"params\">prefix, num_preds, net, vocab, device</span>):  <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;在`prefix`后面生成新字符。&quot;&quot;&quot;</span></span><br><span class=\"line\">    state = net.begin_state(batch_size=<span class=\"number\">1</span>, device=device)</span><br><span class=\"line\">    outputs = [vocab[prefix[<span class=\"number\">0</span>]]]</span><br><span class=\"line\">    get_input = <span class=\"keyword\">lambda</span>: torch.tensor([outputs[-<span class=\"number\">1</span>]], device=device).reshape((<span class=\"number\">1</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">    <span class=\"keyword\">for</span> y <span class=\"keyword\">in</span> prefix[<span class=\"number\">1</span>:]:  <span class=\"comment\"># 预热期</span></span><br><span class=\"line\">        _, state = net(get_input(), state)</span><br><span class=\"line\">        outputs.append(vocab[y])</span><br><span class=\"line\">    <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_preds):  <span class=\"comment\"># 预测`num_preds`步</span></span><br><span class=\"line\">        y, state = net(get_input(), state)</span><br><span class=\"line\">        outputs.append(<span class=\"built_in\">int</span>(y.argmax(dim=<span class=\"number\">1</span>).reshape(<span class=\"number\">1</span>)))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"string\">&#x27;&#x27;</span>.join([vocab.idx_to_token[i] <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> outputs])</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>测试predict_ch8函数。我们将前缀指定为time traveller，并基于这个前缀生成10个后续字符。鉴于我们还没有训练网络，它会生成荒谬的预测结果。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">predict_ch8(<span class=\"string\">&#x27;time traveller &#x27;</span>, <span class=\"number\">10</span>, net, vocab, d2l.try_gpu())</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>&#39;time traveller uvf uvf uv&#39;\n</code></pre>\n<h2 id=\"5-5-梯度裁剪\"><a href=\"#5-5-梯度裁剪\" class=\"headerlink\" title=\"5.5 梯度裁剪\"></a>5.5 梯度裁剪</h2><ul>\n<li><p>对于长度为T的序列，我们在迭代中计算这T个时间步上的梯度，将会在反向传播过程中产生长度为O(T)的矩阵乘法链。当T较大时，它可能导致数值不稳定，例如可能导致梯度爆炸或梯度消失。因此，循环神经网络模型往往需要额外的方式来支持稳定训练。</p>\n</li>\n<li><p>当解决优化问题时，我们对模型参数采用更新步骤。假定在向量形式的$\\mathbf{x}$中，或者在小批量数据的负梯度g方向上。假设学习率$\\eta$&gt;0,，在一次迭代中，我们将$\\mathbf{x}$更新为$\\mathbf{x}-\\eta \\mathbf{g}$。如果我们进一步假设目标函数f表现良好，即函数f在常数L下是利普希茨连续的（Lipschitz continuous）。也就是说，对于任$u$和$v$，我们有：<br>$$|f(\\mathbf{ x } - f(\\mathbf{ y })| \\leq L||\\mathbf{ x } - \\mathbf{ y }||$$</p>\n</li>\n<li><p>在这种情况下，我们可以安全地假设：如果我们通过$\\eta \\mathbf{ g }$更新参数向量，则：<br>$$|f(\\mathbf{ x } - f(\\mathbf{ x - \\eta \\mathbf{ g } })| \\leq L \\eta ||\\mathbf{ g }||$$</p>\n</li>\n<li><p>这意味着我们不会观察到超过$L \\eta ||\\mathbf{ g }||$的变化。这既是坏事也是好事。坏的方面，它限制了取得进展的速度；好的方面，它限制了事情变糟的程度，尤其当我们朝着错误的方向前进时。</p>\n</li>\n<li><p>有时梯度可能很大，优化算法可能无法收敛。我们可以通过降低学习率$\\eta$来解决这个问题。但是如果我们很少得到大的梯度呢？在这种情况下，这种做法似乎毫无道理。一个流行的替代方案是通过将梯度$\\mathbf{ g }$投影回给定半径(例如$\\theta$)的球来裁剪梯度：<br>$$\\mathbf{ g } \\leftarrow \\text{ min }(1, \\frac{\\theta}{||\\mathbf{ g }||}) \\mathbf{ g }$$</p>\n</li>\n<li><p>通过这样做，我们知道梯度范数永远不会超过$\\theta$，并且更新后的梯度完全与$\\mathbf{ g }$的原始方向对齐。这是梯度裁剪的基本思想。它还有一个值得拥有的副作用，即限制任何给定的小批量数据（以及其中任何给定的样本）对参数向量的影响，这赋予了模型一定程度的稳定性。梯度裁剪提供了一个快速修复梯度爆炸的方法，虽然它并不能完全解决问题，但它是众多有效的技术之一。</p>\n</li>\n<li><p>定义一个函数来裁剪模型的梯度，计算了所有模型参数的梯度的范数。</p>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">grad_clipping</span>(<span class=\"params\">net, theta</span>): <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;裁剪梯度。&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(net, nn.Module):</span><br><span class=\"line\">        params = [p <span class=\"keyword\">for</span> p <span class=\"keyword\">in</span> net.parameters() <span class=\"keyword\">if</span> p.requires_grad]</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        params = net.params</span><br><span class=\"line\">    norm = torch.sqrt(<span class=\"built_in\">sum</span>(torch.<span class=\"built_in\">sum</span>((p.grad**<span class=\"number\">2</span>)) <span class=\"keyword\">for</span> p <span class=\"keyword\">in</span> params))</span><br><span class=\"line\">    <span class=\"keyword\">if</span> norm &gt; theta:</span><br><span class=\"line\">        <span class=\"keyword\">for</span> param <span class=\"keyword\">in</span> params:</span><br><span class=\"line\">            param.grad[:] *= theta / norm</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"5-6-训练\"><a href=\"#5-6-训练\" class=\"headerlink\" title=\"5.6 训练\"></a>5.6 训练</h2><ul>\n<li>定义一个函数在一个迭代周期内训练模型。它有三个不同之处。<ul>\n<li>序列数据的不同采样方法（随机采样和顺序分区）将导致隐状态初始化的差异。</li>\n<li>更新模型参数之前裁剪梯度。这样的操作的目的是，即使训练过程中某个点上发生了梯度爆炸，也能保证模型不会发散。</li>\n<li>用困惑度来评价模型。这样的度量确保了不同长度的序列具有可比性。</li>\n</ul>\n</li>\n<li>当使用顺序分区时，我们只在每个迭代周期的开始位置初始化隐状态。由于下一个小批量数据中的第i个子序列样本与当前第i个子序列样本相邻，因此当前小批量数据最后一个样本的隐状态，将用于初始化下一个小批量数据第一个样本的隐状态。这样，存储在隐状态中的序列的历史信息可以在一个迭代周期内流经相邻的子序列。然而，在任何一点隐状态的计算，都依赖于同一迭代周期中前面所有的小批量数据，这使得梯度计算变得复杂。为了降低计算量，在处理任何一个小批量数据之前，我们先分离梯度，使得隐状态的梯度计算总是限制在一个小批量数据的时间步内。</li>\n<li>当使用随机抽样时，因为每个样本都是在一个随机位置抽样的，因此需要为每个迭代周期重新初始化隐状态。updater是更新模型参数的常用函数。它既可以是从头开始实现的d2l.sgd函数，也可以是深度学习框架中内置的优化函数。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#@save</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train_epoch_ch8</span>(<span class=\"params\">net, train_iter, loss, updater, device, use_random_iter</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;训练模型一个迭代周期（定义见第8章）。&quot;&quot;&quot;</span></span><br><span class=\"line\">    state, timer = <span class=\"literal\">None</span>, d2l.Timer()</span><br><span class=\"line\">    metric = d2l.Accumulator(<span class=\"number\">2</span>)  <span class=\"comment\"># 训练损失之和, 词元数量</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> X, Y <span class=\"keyword\">in</span> train_iter:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> state <span class=\"keyword\">is</span> <span class=\"literal\">None</span> <span class=\"keyword\">or</span> use_random_iter:</span><br><span class=\"line\">            <span class=\"comment\"># 在第一个迭代周期，随机初始化隐藏状态</span></span><br><span class=\"line\">            state = net.begin_state(batch_size=X.shape[<span class=\"number\">0</span>], device=device)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(net, nn.Module) <span class=\"keyword\">and</span> <span class=\"keyword\">not</span> <span class=\"built_in\">isinstance</span>(state, <span class=\"built_in\">tuple</span>):</span><br><span class=\"line\">                <span class=\"comment\"># `state`对于`nn.GRU`是个张量</span></span><br><span class=\"line\">                state.detach_()</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                <span class=\"comment\"># `state`对于`nn.LSTM`或对于我们从零开始实现的模型是个张量</span></span><br><span class=\"line\">                <span class=\"keyword\">for</span> s <span class=\"keyword\">in</span> state:</span><br><span class=\"line\">                    s.detach_()</span><br><span class=\"line\">        y = Y.T.reshape(-<span class=\"number\">1</span>)</span><br><span class=\"line\">        X, y = X.to(device), y.to(device)</span><br><span class=\"line\">        y_hat, state = net(X, state)</span><br><span class=\"line\">        l = loss(y_hat, y.long()).mean()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(updater, torch.optim.Optimizer):</span><br><span class=\"line\">            updater.zero_grad()</span><br><span class=\"line\">            l.backward()</span><br><span class=\"line\">            grad_clipping(net, <span class=\"number\">1</span>)</span><br><span class=\"line\">            updater.step()</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            l.backward()</span><br><span class=\"line\">            grad_clipping(net, <span class=\"number\">1</span>)</span><br><span class=\"line\">            <span class=\"comment\"># 因为已经调用了`mean`函数</span></span><br><span class=\"line\">            updater(batch_size=<span class=\"number\">1</span>)</span><br><span class=\"line\">        metric.add(l * y.numel(), y.numel())</span><br><span class=\"line\">    <span class=\"keyword\">return</span> math.exp(metric[<span class=\"number\">0</span>] / metric[<span class=\"number\">1</span>]), metric[<span class=\"number\">1</span>] / timer.stop()</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>循环神经网络模型的训练函数既支持从零开始实现，也可以使用高级API来实现。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#@save</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train_ch8</span>(<span class=\"params\">net, train_iter, vocab, lr, num_epochs, device, use_random_iter=<span class=\"literal\">False</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;训练模型。&quot;&quot;&quot;</span></span><br><span class=\"line\">    loss = nn.CrossEntropyLoss()</span><br><span class=\"line\">    animator = d2l.Animator(xlabel=<span class=\"string\">&#x27;epoch&#x27;</span>, ylabel=<span class=\"string\">&#x27;perplexity&#x27;</span>,</span><br><span class=\"line\">                            legend=[<span class=\"string\">&#x27;train&#x27;</span>], xlim=[<span class=\"number\">10</span>, num_epochs])</span><br><span class=\"line\">    <span class=\"comment\"># 初始化</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(net, nn.Module):</span><br><span class=\"line\">        updater = torch.optim.SGD(net.parameters(), lr)</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        updater = <span class=\"keyword\">lambda</span> batch_size: d2l.sgd(net.params, lr, batch_size)</span><br><span class=\"line\">    predict = <span class=\"keyword\">lambda</span> prefix: predict_ch8(prefix, <span class=\"number\">50</span>, net, vocab, device)</span><br><span class=\"line\">    <span class=\"comment\"># 训练和预测</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        ppl, speed = train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (epoch + <span class=\"number\">1</span>) % <span class=\"number\">10</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(predict(<span class=\"string\">&#x27;time traveller&#x27;</span>))</span><br><span class=\"line\">            animator.add(epoch + <span class=\"number\">1</span>, [ppl])</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;困惑度 <span class=\"subst\">&#123;ppl:<span class=\"number\">.1</span>f&#125;</span>, <span class=\"subst\">&#123;speed:<span class=\"number\">.1</span>f&#125;</span> 词元/秒 <span class=\"subst\">&#123;<span class=\"built_in\">str</span>(device)&#125;</span>&#x27;</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(predict(<span class=\"string\">&#x27;time traveller&#x27;</span>))</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(predict(<span class=\"string\">&#x27;traveller&#x27;</span>))</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>训练循环神经网络模型。因为我们在数据集中只使用了10000个词元，所以模型需要更多的迭代周期来更好地收敛。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_epochs, lr = <span class=\"number\">500</span>, <span class=\"number\">1</span></span><br><span class=\"line\">train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu())</span><br></pre></td></tr></table></figure>\n\n<pre><code>困惑度 1.0, 28075.0 词元/秒 cpu\ntime traveller for so it will be convenient to speak of himwas e\ntravelleryou can show black is white by argument said filby\n</code></pre>\n<p><img src=\"/5_rnn_realize_files/5_rnn_realize_27_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/7_rnn/5_rnn_realize_files/5_rnn_realize_27_1.svg\"></p>\n<ul>\n<li>检查使用随机抽样方法的结果。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net = RNNModelScratch(<span class=\"built_in\">len</span>(vocab), num_hiddens, d2l.try_gpu(), get_params, init_rnn_state, rnn)</span><br><span class=\"line\">train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu(), use_random_iter=<span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>困惑度 1.4, 27814.1 词元/秒 cpu\ntime travellerit s against reason said filbycan a cube that does\ntravelleryou can show black is white by argument said filby\n</code></pre>\n<p><img src=\"/5_rnn_realize_files/5_rnn_realize_29_1.svg\" alt=\"svg\"><br><img src=\"/img/deeplearning/code/pytorch/7_rnn/5_rnn_realize_files/5_rnn_realize_29_1.svg\"></p>"},{"title":"7.6 循环神经网络的简洁实现","date":"2024-05-04T04:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 6 循环神经网络的简洁实现\n\n\n\n```python\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom d2l import torch as d2l\n\nbatch_size, num_steps = 32, 35\ntrain_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)\n```\n\n## 6.1 定义模型\n- 我们构造一个具有256个隐藏单元的单隐藏层的循环神经网络层rnn_layer。\n\n\n```python\nnum_hiddens = 256\nrnn_layer = nn.RNN(len(vocab), num_hiddens)\n```\n\n- 使用张量来初始化隐状态，它的形状是（隐藏层数，批量大小，隐藏单元数）。\n\n\n```python\nstate = torch.zeros((1, batch_size, num_hiddens))\nstate.shape\n```\n\n\n\n\n    torch.Size([1, 32, 256])\n\n\n\n- 通过一个隐状态和一个输入，我们就可以用更新后的隐状态计算输出。rnn_layer的“输出”（Y）不涉及输出层的计算：它是指每个时间步的隐状态，这些隐状态可以用作后续输出层的输入。\n\n\n```python\nX = torch.rand(size=(num_steps, batch_size, len(vocab)))\nY, state_new = rnn_layer(X, state)\nY.shape, state_new.shape\n```\n\n\n\n\n    (torch.Size([35, 32, 256]), torch.Size([1, 32, 256]))\n\n\n\n- 为一个完整的循环神经网络模型定义了一个RNNModel类。注意，rnn_layer只包含隐藏的循环层，我们还需要创建一个单独的输出层。\n\n\n```python\n#@save\nclass RNNModel(nn.Module):\n    \"\"\"循环神经网络模型\"\"\"\n    def __init__(self, rnn_layer, vocab_size, **kwargs):\n        super(RNNModel, self).__init__(**kwargs)\n        self.rnn = rnn_layer\n        self.vocab_size = vocab_size\n        self.num_hiddens = self.rnn.hidden_size\n        # 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1\n        if not self.rnn.bidirectional:\n            self.num_directions = 1\n            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n        else:\n            self.num_directions = 2\n            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)\n\n    def forward(self, inputs, state):\n        X = F.one_hot(inputs.T.long(), self.vocab_size)\n        X = X.to(torch.float32)\n        Y, state = self.rnn(X, state)\n        # 全连接层首先将Y的形状改为(num_steps * batch_size, num_hiddens)\n        # 它的输出形状是(num_steps * batch_size, vocab_size)\n        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n        return output, state\n    \n    def begin_state(self, device, batch_size=1):\n        if not isinstance(self.rnn, nn.LSTM):\n            # `nn.GRU` 以张量作为隐藏状态\n            return torch.zeros((self.num_directions * self.rnn.num_layers,\n                                batch_size, self.num_hiddens), device=device)\n        else:\n            # `nn.LSTM` 包含两个隐藏状态\n            return (torch.zeros((self.num_directions * self.rnn.num_layers,\n                                 batch_size, self.num_hiddens), device=device),\n                    torch.zeros((self.num_directions * self.rnn.num_layers,\n                                 batch_size, self.num_hiddens), device=device))\n```\n\n## 6.2 训练与预测\n\n\n```python\ndevice = d2l.try_gpu()\nnet = RNNModel(rnn_layer, vocab_size=len(vocab))\nnet = net.to(device)\nd2l.predict_ch8('time traveller', 10, net, vocab, device)\n```\n\n\n\n\n    'time travelleruttwwputtw'\n\n\n\n- 效果不好，训练：\n\n\n```python\nimport os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\nnum_epochs, lr = 500, 1\nd2l.train_ch8(net, train_iter, vocab, lr, num_epochs, device)\n```\n\n    perplexity 1.3, 93581.6 tokens/sec on cpu\n    time travellerit would be relarkably oole pare whe grove to trov\n    traveller after the pars of ate that ink of the incand sce \n\n\n\n\n![svg](6_rnn_sample_realize_files/6_rnn_sample_realize_13_1.svg)\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/7_rnn/6_rnn_sample_realize_files/6_rnn_sample_realize_13_1.svg)\n![](img/deeplearning/code/pytorch/7_rnn/6_rnn_sample_realize_files/6_rnn_sample_realize_13_1.svg)\n\n- 与上一节相比，由于深度学习框架的高级API对代码进行了更多的优化，该模型在较短的时间内达到了较低的困惑度。\n","source":"_posts/deeplearning/code/pytorch/7_rnn/6_rnn_sample_realize.md","raw":"---\ntitle: 7.6 循环神经网络的简洁实现\ndate: 2024-5-4 12:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 6 循环神经网络的简洁实现\n\n\n\n```python\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom d2l import torch as d2l\n\nbatch_size, num_steps = 32, 35\ntrain_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)\n```\n\n## 6.1 定义模型\n- 我们构造一个具有256个隐藏单元的单隐藏层的循环神经网络层rnn_layer。\n\n\n```python\nnum_hiddens = 256\nrnn_layer = nn.RNN(len(vocab), num_hiddens)\n```\n\n- 使用张量来初始化隐状态，它的形状是（隐藏层数，批量大小，隐藏单元数）。\n\n\n```python\nstate = torch.zeros((1, batch_size, num_hiddens))\nstate.shape\n```\n\n\n\n\n    torch.Size([1, 32, 256])\n\n\n\n- 通过一个隐状态和一个输入，我们就可以用更新后的隐状态计算输出。rnn_layer的“输出”（Y）不涉及输出层的计算：它是指每个时间步的隐状态，这些隐状态可以用作后续输出层的输入。\n\n\n```python\nX = torch.rand(size=(num_steps, batch_size, len(vocab)))\nY, state_new = rnn_layer(X, state)\nY.shape, state_new.shape\n```\n\n\n\n\n    (torch.Size([35, 32, 256]), torch.Size([1, 32, 256]))\n\n\n\n- 为一个完整的循环神经网络模型定义了一个RNNModel类。注意，rnn_layer只包含隐藏的循环层，我们还需要创建一个单独的输出层。\n\n\n```python\n#@save\nclass RNNModel(nn.Module):\n    \"\"\"循环神经网络模型\"\"\"\n    def __init__(self, rnn_layer, vocab_size, **kwargs):\n        super(RNNModel, self).__init__(**kwargs)\n        self.rnn = rnn_layer\n        self.vocab_size = vocab_size\n        self.num_hiddens = self.rnn.hidden_size\n        # 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1\n        if not self.rnn.bidirectional:\n            self.num_directions = 1\n            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n        else:\n            self.num_directions = 2\n            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)\n\n    def forward(self, inputs, state):\n        X = F.one_hot(inputs.T.long(), self.vocab_size)\n        X = X.to(torch.float32)\n        Y, state = self.rnn(X, state)\n        # 全连接层首先将Y的形状改为(num_steps * batch_size, num_hiddens)\n        # 它的输出形状是(num_steps * batch_size, vocab_size)\n        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n        return output, state\n    \n    def begin_state(self, device, batch_size=1):\n        if not isinstance(self.rnn, nn.LSTM):\n            # `nn.GRU` 以张量作为隐藏状态\n            return torch.zeros((self.num_directions * self.rnn.num_layers,\n                                batch_size, self.num_hiddens), device=device)\n        else:\n            # `nn.LSTM` 包含两个隐藏状态\n            return (torch.zeros((self.num_directions * self.rnn.num_layers,\n                                 batch_size, self.num_hiddens), device=device),\n                    torch.zeros((self.num_directions * self.rnn.num_layers,\n                                 batch_size, self.num_hiddens), device=device))\n```\n\n## 6.2 训练与预测\n\n\n```python\ndevice = d2l.try_gpu()\nnet = RNNModel(rnn_layer, vocab_size=len(vocab))\nnet = net.to(device)\nd2l.predict_ch8('time traveller', 10, net, vocab, device)\n```\n\n\n\n\n    'time travelleruttwwputtw'\n\n\n\n- 效果不好，训练：\n\n\n```python\nimport os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\nnum_epochs, lr = 500, 1\nd2l.train_ch8(net, train_iter, vocab, lr, num_epochs, device)\n```\n\n    perplexity 1.3, 93581.6 tokens/sec on cpu\n    time travellerit would be relarkably oole pare whe grove to trov\n    traveller after the pars of ate that ink of the incand sce \n\n\n\n\n![svg](6_rnn_sample_realize_files/6_rnn_sample_realize_13_1.svg)\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/7_rnn/6_rnn_sample_realize_files/6_rnn_sample_realize_13_1.svg)\n![](img/deeplearning/code/pytorch/7_rnn/6_rnn_sample_realize_files/6_rnn_sample_realize_13_1.svg)\n\n- 与上一节相比，由于深度学习框架的高级API对代码进行了更多的优化，该模型在较短的时间内达到了较低的困惑度。\n","slug":"deeplearning/code/pytorch/7_rnn/6_rnn_sample_realize","published":1,"updated":"2024-05-06T06:51:57.343Z","_id":"clvuli6v0009azsvwbt70gpu3","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"6-循环神经网络的简洁实现\"><a href=\"#6-循环神经网络的简洁实现\" class=\"headerlink\" title=\"6 循环神经网络的简洁实现\"></a>6 循环神经网络的简洁实现</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.nn <span class=\"keyword\">import</span> functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">batch_size, num_steps = <span class=\"number\">32</span>, <span class=\"number\">35</span></span><br><span class=\"line\">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"6-1-定义模型\"><a href=\"#6-1-定义模型\" class=\"headerlink\" title=\"6.1 定义模型\"></a>6.1 定义模型</h2><ul>\n<li>我们构造一个具有256个隐藏单元的单隐藏层的循环神经网络层rnn_layer。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_hiddens = <span class=\"number\">256</span></span><br><span class=\"line\">rnn_layer = nn.RNN(<span class=\"built_in\">len</span>(vocab), num_hiddens)</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>使用张量来初始化隐状态，它的形状是（隐藏层数，批量大小，隐藏单元数）。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">state = torch.zeros((<span class=\"number\">1</span>, batch_size, num_hiddens))</span><br><span class=\"line\">state.shape</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>torch.Size([1, 32, 256])\n</code></pre>\n<ul>\n<li>通过一个隐状态和一个输入，我们就可以用更新后的隐状态计算输出。rnn_layer的“输出”（Y）不涉及输出层的计算：它是指每个时间步的隐状态，这些隐状态可以用作后续输出层的输入。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = torch.rand(size=(num_steps, batch_size, <span class=\"built_in\">len</span>(vocab)))</span><br><span class=\"line\">Y, state_new = rnn_layer(X, state)</span><br><span class=\"line\">Y.shape, state_new.shape</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>(torch.Size([35, 32, 256]), torch.Size([1, 32, 256]))\n</code></pre>\n<ul>\n<li>为一个完整的循环神经网络模型定义了一个RNNModel类。注意，rnn_layer只包含隐藏的循环层，我们还需要创建一个单独的输出层。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#@save</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">RNNModel</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;循环神经网络模型&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, rnn_layer, vocab_size, **kwargs</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(RNNModel, self).__init__(**kwargs)</span><br><span class=\"line\">        self.rnn = rnn_layer</span><br><span class=\"line\">        self.vocab_size = vocab_size</span><br><span class=\"line\">        self.num_hiddens = self.rnn.hidden_size</span><br><span class=\"line\">        <span class=\"comment\"># 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> self.rnn.bidirectional:</span><br><span class=\"line\">            self.num_directions = <span class=\"number\">1</span></span><br><span class=\"line\">            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            self.num_directions = <span class=\"number\">2</span></span><br><span class=\"line\">            self.linear = nn.Linear(self.num_hiddens * <span class=\"number\">2</span>, self.vocab_size)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, inputs, state</span>):</span><br><span class=\"line\">        X = F.one_hot(inputs.T.long(), self.vocab_size)</span><br><span class=\"line\">        X = X.to(torch.float32)</span><br><span class=\"line\">        Y, state = self.rnn(X, state)</span><br><span class=\"line\">        <span class=\"comment\"># 全连接层首先将Y的形状改为(num_steps * batch_size, num_hiddens)</span></span><br><span class=\"line\">        <span class=\"comment\"># 它的输出形状是(num_steps * batch_size, vocab_size)</span></span><br><span class=\"line\">        output = self.linear(Y.reshape((-<span class=\"number\">1</span>, Y.shape[-<span class=\"number\">1</span>])))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output, state</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">begin_state</span>(<span class=\"params\">self, device, batch_size=<span class=\"number\">1</span></span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> <span class=\"built_in\">isinstance</span>(self.rnn, nn.LSTM):</span><br><span class=\"line\">            <span class=\"comment\"># `nn.GRU` 以张量作为隐藏状态</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span> torch.zeros((self.num_directions * self.rnn.num_layers,</span><br><span class=\"line\">                                batch_size, self.num_hiddens), device=device)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"comment\"># `nn.LSTM` 包含两个隐藏状态</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span> (torch.zeros((self.num_directions * self.rnn.num_layers,</span><br><span class=\"line\">                                 batch_size, self.num_hiddens), device=device),</span><br><span class=\"line\">                    torch.zeros((self.num_directions * self.rnn.num_layers,</span><br><span class=\"line\">                                 batch_size, self.num_hiddens), device=device))</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"6-2-训练与预测\"><a href=\"#6-2-训练与预测\" class=\"headerlink\" title=\"6.2 训练与预测\"></a>6.2 训练与预测</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">device = d2l.try_gpu()</span><br><span class=\"line\">net = RNNModel(rnn_layer, vocab_size=<span class=\"built_in\">len</span>(vocab))</span><br><span class=\"line\">net = net.to(device)</span><br><span class=\"line\">d2l.predict_ch8(<span class=\"string\">&#x27;time traveller&#x27;</span>, <span class=\"number\">10</span>, net, vocab, device)</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>&#39;time travelleruttwwputtw&#39;\n</code></pre>\n<ul>\n<li>效果不好，训练：</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>] = <span class=\"string\">&quot;TRUE&quot;</span></span><br><span class=\"line\">num_epochs, lr = <span class=\"number\">500</span>, <span class=\"number\">1</span></span><br><span class=\"line\">d2l.train_ch8(net, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></table></figure>\n\n<pre><code>perplexity 1.3, 93581.6 tokens/sec on cpu\ntime travellerit would be relarkably oole pare whe grove to trov\ntraveller after the pars of ate that ink of the incand sce \n</code></pre>\n<p><img src=\"/6_rnn_sample_realize_files/6_rnn_sample_realize_13_1.svg\" alt=\"svg\"><br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/7_rnn/6_rnn_sample_realize_files/6_rnn_sample_realize_13_1.svg\"><br><img src=\"/img/deeplearning/code/pytorch/7_rnn/6_rnn_sample_realize_files/6_rnn_sample_realize_13_1.svg\"></p>\n<ul>\n<li>与上一节相比，由于深度学习框架的高级API对代码进行了更多的优化，该模型在较短的时间内达到了较低的困惑度。</li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"6-循环神经网络的简洁实现\"><a href=\"#6-循环神经网络的简洁实现\" class=\"headerlink\" title=\"6 循环神经网络的简洁实现\"></a>6 循环神经网络的简洁实现</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.nn <span class=\"keyword\">import</span> functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">batch_size, num_steps = <span class=\"number\">32</span>, <span class=\"number\">35</span></span><br><span class=\"line\">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"6-1-定义模型\"><a href=\"#6-1-定义模型\" class=\"headerlink\" title=\"6.1 定义模型\"></a>6.1 定义模型</h2><ul>\n<li>我们构造一个具有256个隐藏单元的单隐藏层的循环神经网络层rnn_layer。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_hiddens = <span class=\"number\">256</span></span><br><span class=\"line\">rnn_layer = nn.RNN(<span class=\"built_in\">len</span>(vocab), num_hiddens)</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>使用张量来初始化隐状态，它的形状是（隐藏层数，批量大小，隐藏单元数）。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">state = torch.zeros((<span class=\"number\">1</span>, batch_size, num_hiddens))</span><br><span class=\"line\">state.shape</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>torch.Size([1, 32, 256])\n</code></pre>\n<ul>\n<li>通过一个隐状态和一个输入，我们就可以用更新后的隐状态计算输出。rnn_layer的“输出”（Y）不涉及输出层的计算：它是指每个时间步的隐状态，这些隐状态可以用作后续输出层的输入。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = torch.rand(size=(num_steps, batch_size, <span class=\"built_in\">len</span>(vocab)))</span><br><span class=\"line\">Y, state_new = rnn_layer(X, state)</span><br><span class=\"line\">Y.shape, state_new.shape</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>(torch.Size([35, 32, 256]), torch.Size([1, 32, 256]))\n</code></pre>\n<ul>\n<li>为一个完整的循环神经网络模型定义了一个RNNModel类。注意，rnn_layer只包含隐藏的循环层，我们还需要创建一个单独的输出层。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#@save</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">RNNModel</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;循环神经网络模型&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, rnn_layer, vocab_size, **kwargs</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(RNNModel, self).__init__(**kwargs)</span><br><span class=\"line\">        self.rnn = rnn_layer</span><br><span class=\"line\">        self.vocab_size = vocab_size</span><br><span class=\"line\">        self.num_hiddens = self.rnn.hidden_size</span><br><span class=\"line\">        <span class=\"comment\"># 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> self.rnn.bidirectional:</span><br><span class=\"line\">            self.num_directions = <span class=\"number\">1</span></span><br><span class=\"line\">            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            self.num_directions = <span class=\"number\">2</span></span><br><span class=\"line\">            self.linear = nn.Linear(self.num_hiddens * <span class=\"number\">2</span>, self.vocab_size)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, inputs, state</span>):</span><br><span class=\"line\">        X = F.one_hot(inputs.T.long(), self.vocab_size)</span><br><span class=\"line\">        X = X.to(torch.float32)</span><br><span class=\"line\">        Y, state = self.rnn(X, state)</span><br><span class=\"line\">        <span class=\"comment\"># 全连接层首先将Y的形状改为(num_steps * batch_size, num_hiddens)</span></span><br><span class=\"line\">        <span class=\"comment\"># 它的输出形状是(num_steps * batch_size, vocab_size)</span></span><br><span class=\"line\">        output = self.linear(Y.reshape((-<span class=\"number\">1</span>, Y.shape[-<span class=\"number\">1</span>])))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output, state</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">begin_state</span>(<span class=\"params\">self, device, batch_size=<span class=\"number\">1</span></span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> <span class=\"built_in\">isinstance</span>(self.rnn, nn.LSTM):</span><br><span class=\"line\">            <span class=\"comment\"># `nn.GRU` 以张量作为隐藏状态</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span> torch.zeros((self.num_directions * self.rnn.num_layers,</span><br><span class=\"line\">                                batch_size, self.num_hiddens), device=device)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"comment\"># `nn.LSTM` 包含两个隐藏状态</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span> (torch.zeros((self.num_directions * self.rnn.num_layers,</span><br><span class=\"line\">                                 batch_size, self.num_hiddens), device=device),</span><br><span class=\"line\">                    torch.zeros((self.num_directions * self.rnn.num_layers,</span><br><span class=\"line\">                                 batch_size, self.num_hiddens), device=device))</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"6-2-训练与预测\"><a href=\"#6-2-训练与预测\" class=\"headerlink\" title=\"6.2 训练与预测\"></a>6.2 训练与预测</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">device = d2l.try_gpu()</span><br><span class=\"line\">net = RNNModel(rnn_layer, vocab_size=<span class=\"built_in\">len</span>(vocab))</span><br><span class=\"line\">net = net.to(device)</span><br><span class=\"line\">d2l.predict_ch8(<span class=\"string\">&#x27;time traveller&#x27;</span>, <span class=\"number\">10</span>, net, vocab, device)</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>&#39;time travelleruttwwputtw&#39;\n</code></pre>\n<ul>\n<li>效果不好，训练：</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.environ[<span class=\"string\">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>] = <span class=\"string\">&quot;TRUE&quot;</span></span><br><span class=\"line\">num_epochs, lr = <span class=\"number\">500</span>, <span class=\"number\">1</span></span><br><span class=\"line\">d2l.train_ch8(net, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></table></figure>\n\n<pre><code>perplexity 1.3, 93581.6 tokens/sec on cpu\ntime travellerit would be relarkably oole pare whe grove to trov\ntraveller after the pars of ate that ink of the incand sce \n</code></pre>\n<p><img src=\"/6_rnn_sample_realize_files/6_rnn_sample_realize_13_1.svg\" alt=\"svg\"><br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/7_rnn/6_rnn_sample_realize_files/6_rnn_sample_realize_13_1.svg\"><br><img src=\"/img/deeplearning/code/pytorch/7_rnn/6_rnn_sample_realize_files/6_rnn_sample_realize_13_1.svg\"></p>\n<ul>\n<li>与上一节相比，由于深度学习框架的高级API对代码进行了更多的优化，该模型在较短的时间内达到了较低的困惑度。</li>\n</ul>"},{"title":"7.7 通过时间反向传播","date":"2024-05-05T04:00:00.000Z","comment":true,"toc":true,"_content":"#  \n<!--more-->\n# 7 通过时间反向传播\n- 对于梯度爆炸或梯度消失，以及需要对循环神经网络分离梯度。例如，我们在序列上调用了detach函数。为了能够快速构建模型并了解其工作原理，本节将更深入地探讨序列模型反向传播的细节，以及相关的数学原理。\n\n- 循环神经网络中的前向传播相对简单。通过时间反向传播（backpropagation through time，BPTT）实际上是循环神经网络中反向传播技术的一个特定应用。它要求我们将循环神经网络的计算图一次展开一个时间步，以获得模型变量和参数之间的依赖关系。然后，基于链式法则，应用反向传播来计算和存储梯度。由于序列可能相当长，因此依赖关系也可能相当长。例如，某个1000个字符的序列，其第一个词元可能会对最后位置的词元产生重大影响。这在计算上是不可行的（它需要的时间和内存都太多了），并且还需要超过1000个矩阵的乘积才能得到非常难以捉摸的梯度。这个过程充满了计算与统计的不确定性。\n\n\n## 7.1 循环神经网络的梯度分析\n- 从一个循环神经网络的简化模型开始，此模型忽略了隐状态的特性及其更新方式的细节。这里的数学表示没有像过去那样明确地区分标量、向量和矩阵，因为这些细节对于分析并不重要\n\n- 我们将时间步t的隐状态表示为$h_ t$，输入表示为$x_ t$，输出表示为$o_ t$。输入和隐状态可以拼接后与隐藏层中的一个权重变量相乘。。因此，我们分别使用$w_ h$和$w_ o$来表示隐藏层和输出层的权重。每个时间步的隐状态和输出可以写为：\n$$ \\begin{aligned} h_ t &= f(x_t, h_{t-1}, w_h),\\\\ o_ t &= g(h_ t, w_o), \\end{aligned} $$\n\n- 其中f和g分别是隐藏层和输出层的变换。因此，我们有一个链$ \\{ \\dots , (x_ {t-1}, h_ {t-1}, o_ {t-1}), (x_ t, h_ t, o_ t),  \\dots \\}$，它们通过循环计算彼此依赖。前向传播相当简单，一次一个时间步的遍历三元组$(x_ t, h_ t, o_ t)$，然后通过一个目标函数在所有T个时间步内评估输出$o_ t$和对应的标签$y_ t$之间的误差:\n$$ L(x_ 1, \\dots, x_ T, y_ 1, \\dots, y_ T, w_ h, w_ o) = \\frac{1}{T} \\sum_ {t=1} ^ T l(o_ t, y_ t). $$\n\n- 对于反向传播，问题则有点棘手，特别是当我们计算目标函数L关于参数wh的梯度时。具体来说，按照链式法则：\n$$ \\begin{aligned} \\frac{\\partial L}{\\partial w_ h} &= \\frac{1}{T} \\sum_ {t=1} ^ T \\frac{\\partial l(o_ t, y_ t)}{\\partial w_ h} \\\\ &= \\frac{1}{T} \\sum_ {t=1} ^ T \\frac{\\partial l(o_ t, y_ t)}{\\partial o_ t} \\frac{\\partial g(h_ t, w_ o)}{\\partial h_ t} \\frac{\\partial h_ t}{\\partial w_ h}. \\end{aligned} $$\n\n- 其中第一项和第二项很容易计算，而第三项$ \\frac{\\partial h_ t}{\\partial w_ h}$则相当复杂。因为我们需要循环地计算参数$w_ h$对$h_ t$的影响，因为$h_ t = f(x_t, h_{t-1}, w_h)$，所以$h_ t$既依赖于$h_ {t-1}$又依赖于$w_ h$，其中$h_ {t-1}$的计算也依赖于$w_ h$。因此，使用链式法则产生：\n$$ \\frac{\\partial h_ t}{\\partial w_ h} = \\frac{\\partial f(x_ t, h_ {t-1}, w_ h)}{\\partial w_ h} + \\frac{\\partial f(x_ t, h_ {t-1}, w_ h)}{\\partial h_ {t-1}} \\frac{\\partial h_ {t-1}}{\\partial w_ h}.  $$\n\n- 为了导出上述梯度，假设我们有三个序列$ \\{ a_ t \\} $，$ \\{ b_ t \\} $，$ \\{ c_ t \\} $，当t=1,2,...时，序列满足$a_ 0 = 0$，且$a_ t = b_ t + c_ t a_ {t-1}$。对于$t \\geq 1$，我们有：\n$$ a_ t = b_ t + \\sum_ {i=1} ^ {t-1}(\\prod_ {j=i+1} ^ t c_ j) b_ i $$\n\n- 不妨基于下列公式替换$a_ t$、$b_ t$和$c_ t$:\n$$ \\begin{aligned} a_ t &= \\frac{\\partial h_ t}{\\partial w_ h},\\\\ b_ t &= \\frac{\\partial f(x_ t, h_ {t-1}, w_ h)}{\\partial w_ h},\\\\ c_ t &= \\frac{\\partial f(x_ t, h_ {t-1}, w_ h)}{\\partial h_ {t-1}}, \\end{aligned} $$\n\n- 通过：\n$$ \\frac{\\partial h_ t}{\\partial w_ h} = \\frac{\\partial f(x_ t, h_ {t-1}, w_ h)}{\\partial w_ h} + \\frac{\\partial f(x_ t, h_ {t-1}, w_ h)}{\\partial h_ {t-1}} \\frac{\\partial h_ {t-1}}{\\partial w_ h}.  $$\n\n$$ a_ t = b_ t + c_ t a_ {t-1} $$\n\n$$ a_ t = b_ t + \\sum_ {i=1} ^ {t-1}(\\prod_ {j=i+1} ^ t c_ j) b_ i $$\n\n- 移除循环计算：\n<span id='goto7'></span>$$ \\frac{\\partial h_ t}{\\partial w_ h} = \\frac{\\partial f(x_ t, h_ {t-1}, w_ h)}{\\partial w_ h} + \\sum_ {i=1} ^ {t-1}(\\prod_ {j=i+1} ^ t \\frac{\\partial f(x_ j, h_ {j-1}, w_ h)}{\\partial h_ {j-1}}) \\frac{\\partial f(x_ i, h_ {i-1}, w_ h)}{\\partial w_ h}. (7)  $$\n\n- 虽然我们可以使用链式法则递归地计算$\\frac{\\partial h_ t}{\\partial w_ h}$，但当t很大时这个链就会变得很长。我们需要想想办法来处理这一问题.\n\n### 7.1.1 完全计算\n- 显然，我们可以计算<a href=\"#goto7\">（7）</a>中的全部总和，然而，这样的计算非常缓慢，并且可能会发生梯度爆炸，因为初始条件的微小变化就可能会对结果产生巨大的影响。也就是说，我们可以观察到类似于蝴蝶效应的现象，即初始条件的很小变化就会导致结果发生不成比例的变化。这对于我们想要估计的模型而言是非常不可取的。毕竟，我们正在寻找的是能够很好地泛化高稳定性模型的估计器。因此，在实践中，这种方法几乎从未使用过。\n\n### 7.1.2 截断时间步\n- 或者，我们可以在τ步后截断<a href=\"#goto7\">（7）</a>中的求和计算。例如之前的梯度裁剪。这会带来真实梯度的近似，只需将求和终止为$\\frac{ \\partial h_ {t - \\tau}}{\\partial w_ h}$。在实践中，这种方式工作得很好。它通常被称为截断的通过时间反向传播。这样做导致该模型主要侧重于短期影响，而不是长期影响。这在现实中是可取的，因为它会将估计值偏向更简单和更稳定的模型。\n\n### 7.1.3 随机截断\n- 我们可以用一个随机变量替换$\\frac{\\partial h_ t }{\\partial w_ h}$，该随机变量在预期中是正确的，但是会截断序列。这个随机变量是通过使用序列 $\\xi_ t$来实现的，序列预定义了$0 \\leq \\pi_ t \\leq 1$，其中$P (\\xi_ t = 0 ) = 1 - \\pi_ t$，且$P (\\xi_ t = \\pi_ t ^ {-1}) = \\pi_ t$，因此$E [ \\xi_ t ] = 1$。我们使用它来替换<a href=\"#goto7\">（7）</a>中的梯度$\\frac{\\partial h_ t}{\\partial w_ h}$得到：\n$$ z_ t = \\frac{\\partial f(x_ t, h_ {t-1}, w_ h)}{\\partial w_ h} + \\xi_ t \\frac{\\partial f(x_ t, h_ {t-1}, w_ h)}{\\partial h_ {t-1}} \\frac{\\partial h_ {t-1}}{\\partial w_ h}. $$\n\n- 从$\\xi_ t$的定义中推导出来$E [ z_ t ] = \\frac{\\partial h_ t}{\\partial w_ h}$。每当$ \\xi_ t = 0 $时，递归计算终止在这个t时间步。这导致了不同长度序列的加权和，其中长序列出现的很少，所以将适当地加大权重。\n\n### 7.1.4 比较策略\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/7_rnn/7_backward/1.png)\n![](img/deeplearning/code/pytorch/7_rnn/7_backward/1.png)\n- 第一行采用随机截断，方法是将文本划分为不同长度的片断；\n- 第二行采用常规截断，方法是将文本分解为相同长度的子序列。这也是我们在循环神经网络实验中一直在做的；\n- 第三行采用通过时间的完全反向传播，结果是产生了在计算上不可行的表达式。\n- 虽然随机截断在理论上具有吸引力，但很可能是由于多种因素在实践中并不比常规截断更好。首先，在对过去若干个时间步经过反向传播后，观测结果足以捕获实际的依赖关系。其次，增加的方差抵消了时间步数越多梯度越精确的事实。第三，我们真正想要的是只有短范围交互的模型。因此，模型需要的正是截断的通过时间反向传播方法所具备的轻度正则化效果。\n\n\n## 7.2 通过时间反向传播的细节\n- 如何计算目标函数相对于所有分解模型参数的梯度。为了保持简单，我们考虑一个没有偏置参数的循环神经网络，其在隐藏层中的激活函数使用恒等映射（$\\phi(x) = x$）。对于时间步t，设单个样本的输入及其对应的标签分别为$\\mathbf{ x }_ t \\in \\mathbb{ R }^{d} $和$y_ t $。计算隐状态$ \\mathbf{ h }_ t \\in \\mathbb{ R }^{h} $和输出$ \\mathbf{ o }_ t \\in \\mathbb{ R }^{q} $的公式如下：\n$$ \\begin{aligned} \\mathbf{ h }_ t &= \\mathbf{ W }_ { hx } \\mathbf{ x }_ t + \\mathbf{ W }_ { hh } \\mathbf{ h }_ { t-1},\\\\ \\mathbf{ o }_ t &= \\mathbf{ W }_ { qh } \\mathbf{ h }_ t. \\end{aligned} $$\n\n- 其中权重参数为$\\mathbf{ W }_ { hx } \\in \\mathbb{ R }^{h \\times d} $，$\\mathbf{ W }_ { hh } \\in \\mathbb{ R }^{h \\times h} $和$\\mathbf{ W }_ { qh } \\in \\mathbb{ R }^{q \\times h} $。用$l(\\mathbf{ o }_ t, y_ t) $表示时间步t处（即从序列开始起的超过T个时间步）的损失函数，则我们的目标函数的总体损失是：\n$$ L = \\frac{1}{T} \\sum_ {t=1} ^ T l(\\mathbf{ o }_ t, y_ t). $$\n\n- 为了在循环神经网络的计算过程中可视化模型变量和参数之间的依赖关系，我们可以为模型绘制一个计算图：例如，时间步3的隐状态$ \\mathbf{ h }_ 3 $的计算依赖于模型参数$\\mathbf{ W }_ { hx } $和$\\mathbf{ W }_ { hh } $，以及最终时间步的隐状态$ \\mathbf{ h }_ 2 $和当前时间步的输入$ \\mathbf{ x }_ 3 $。\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/7_rnn/7_backward/2.png)\n![](img/deeplearning/code/pytorch/7_rnn/7_backward/2.png)\n\n- 未着色的方框表示变量，着色的方框表示参数，圆表示运算符。模型参数是：$\\mathbf{ W }_ { hx } $，$\\mathbf{ W }_ { hh } $和$\\mathbf{ W }_ { qh } $。通常，训练该模型需要对这些参数进行梯度计算：$ \\frac{\\partial L}{\\partial \\mathbf{ W }_ { hx } } $，$ \\frac{\\partial L}{\\partial \\mathbf{ W }_ { hh } } $和$ \\frac{\\partial L}{\\partial \\mathbf{ W }_ { qh } } $。根据上图的依赖关系，我们可以沿箭头的相反方向遍历计算图，依次计算和存储梯度。为了灵活地表示链式法则中不同形状的矩阵、向量和标量的乘法，我们继续使用prod运算符。\n\n- 首先，在任意时间步t，目标函数关于模型输出的微分计算是相当简单的：\n$$ \\frac{\\partial L}{\\partial \\mathbf{ o }_ t} = \\frac{1}{T} \\frac{\\partial l(\\mathbf{ o }_ t, y_ t)}{\\partial \\mathbf{ o }_ t}. $$\n\n- 现在，我们可以计算目标函数关于输出层中参数$\\mathbf{ W }_ { qh } $的梯度：$ \\frac{\\partial L}{\\partial \\mathbf{ W }_ { qh } } $。由上图可知，目标函数L通过$\\mathbf{ o }_ 1, \\ldots, \\mathbf{ o }_ T $依赖于$\\mathbf{ W }_ { qh } $。根据链式法则，我们得到：\n$$ \\begin{aligned} \\frac{\\partial L}{\\partial \\mathbf{ W }_ { qh } } &= \\sum_ {t=1} ^ T prod \\left( \\frac{\\partial L}{\\partial \\mathbf{ o }_ t}, \\frac{\\partial \\mathbf{ o }_ t}{\\partial \\mathbf{ W }_ { qh } } \\right) \\\\ &= \\sum_ {t=1} ^ T  \\frac{\\partial L}{\\partial \\mathbf{ o }_ t} \\mathbf{ h }_ t ^ T . \\end{aligned} $$\n\n- 其中$ \\frac{ \\partial L}{\\partial \\mathbf{ o }_ t} $已经在上面计算过了。接下来，在最后的时间步T，目标函数L仅通过$\\mathbf{ o }_ T $依赖于隐状态$\\mathbf{ h }_ T $。因此，使用链式法可以很容易地得到梯度$ \\frac{ \\partial L}{\\partial \\mathbf{ h }_ T} $：\n<span id='goto13'></span>$$ \\begin{aligned} \\frac{\\partial L}{\\partial \\mathbf{ h }_ T} &= prod \\left( \\frac{\\partial L}{\\partial \\mathbf{ o }_ T}, \\frac{\\partial \\mathbf{ o }_ T}{\\partial \\mathbf{ h }_ T} \\right) \\\\ &= \\mathbf{ W }_ { qh } ^ T \\frac{\\partial L}{\\partial \\mathbf{ o }_ T}. \\end{aligned} （13） $$\n\n- 当目标函数L通过$\\mathbf{ h }_ { t+1 } $和$\\mathbf{ o }_ t $依赖于$\\mathbf{ h }_ t $时，对任意时间步t < T来说都变得更加棘手。根据链式法则，隐状态的梯度$ \\frac{ \\partial L}{\\partial \\mathbf{ h }_ t} $在任何时间步骤t < T时都可以递归地计算为：\n<span id='goto14'></span>$$ \\begin{aligned} \\frac{\\partial L}{\\partial \\mathbf{ h }_ t} &= prod \\left( \\frac{\\partial L}{\\partial \\mathbf{ h }_ { t+1 }}, \\frac{\\partial \\mathbf{ h }_ { t+1 }}{\\partial \\mathbf{ h }_ t} \\right) + prod \\left( \\frac{\\partial L}{\\partial \\mathbf{ o }_ t}, \\frac{\\partial \\mathbf{ o }_ t}{\\partial \\mathbf{ h }_ t} \\right) \\\\ &= \\mathbf{ W }_ { hh } ^ T \\frac{\\partial L}{\\partial \\mathbf{ h }_ { t+1 }} + \\mathbf{ W }_ { qh } ^ T \\frac{\\partial L}{\\partial \\mathbf{ o }_ t}. \\end{aligned} （14） $$\n\n- 为了进行分析，对于任何时间步1 ≤ t ≤ T展开递归计算得:\n$$ \\frac{\\partial L}{\\partial \\mathbf{ h }_ t} = \\sum_ {i=t} ^ T \\left( \\mathbf{ W }_ { hh } ^ T \\right) ^ {T-i} \\mathbf{ W }_ { qh } ^ T \\frac{\\partial L}{\\partial \\mathbf{ o }_ { T+t-i }}. $$\n\n- 从上式看到，这个简单的线性例子已经展现了长序列模型的一些关键问题：它陷入到$ \\mathbf{ W }_ { hh } ^ T $的潜在的非常大的幂。在这个幂中，小于1的特征值将会消失，大于1的特征值将会发散。这在数值上是不稳定的，表现形式为梯度消失或梯度爆炸。解决此问题的一种方法是按照计算方便的需要截断时间步长的尺寸。实际上，这种截断是通过在给定数量的时间步之后分离梯度来实现的。\n\n- 最后，上图表明：目标函数L通过隐状态$\\mathbf{ h }_ 1, \\ldots, \\mathbf{ h }_ T $依赖于隐藏层中的模型参数$\\mathbf{ W }_ { hx } $和$\\mathbf{ W }_ { hh } $。为了计算有关这些参数的梯度$ \\frac{ \\partial L}{\\partial \\mathbf{ W }_ { hx } } $和$ \\frac{ \\partial L}{\\partial \\mathbf{ W }_ { hh } } $，我们应用链式规则得：\n$$ \\begin{aligned} \\frac{\\partial L}{\\partial \\mathbf{ W }_ { hx } } &= \\sum_ {t=1} ^ T prod \\left( \\frac{\\partial L}{\\partial \\mathbf{ h }_ t}, \\frac{\\partial \\mathbf{ h }_ t}{\\partial \\mathbf{ W }_ { hx } } \\right) = \\sum_ {t=1} ^ T \\frac{\\partial L}{\\partial \\mathbf{ h }_ t} \\mathbf{ x }_ t ^ T, \\\\ \\frac{\\partial L}{\\partial \\mathbf{ W }_ { hh } } &= \\sum_ {t=1} ^ T prod \\left( \\frac{\\partial L}{\\partial \\mathbf{ h }_ t}, \\frac{\\partial \\mathbf{ h }_ t}{\\partial \\mathbf{ W }_ { hh } } \\right) = \\sum_ {t=1} ^ T \\frac{\\partial L}{\\partial \\mathbf{ h }_ t} \\mathbf{ h }_ { t-1 } ^ T. \\end{aligned} $$\n\n- 其中$ \\frac{ \\partial L}{\\partial \\mathbf{ h }_ t} $是由<a href=\"#goto13\">（13）</a>和<a href=\"#goto14\">（14）</a>递归计算得到的，是影响数值稳定性的关键量。\n\n- 由于通过时间反向传播是反向传播在循环神经网络中的应用方式，所以训练循环神经网络交替使用前向传播和通过时间反向传播。通过时间反向传播依次计算并存储上述梯度。具体而言，存储的中间值会被重复使用，以避免重复计算，例如存储$ \\frac{ \\partial L}{\\partial \\mathbf{ h }_ t} $，以便在计算$ \\frac{ \\partial L}{\\partial \\mathbf{ W }_ { hx } } $和$ \\frac{ \\partial L}{\\partial \\mathbf{ W }_ { hh } } $时重复使用。\n","source":"_posts/deeplearning/code/pytorch/7_rnn/7_backward.md","raw":"---\ntitle: 7.7 通过时间反向传播\ndate: 2024-5-5 12:00:00\ntags: [机器学习,pytorch]\ncategories: [机器学习]\ncomment: true\ntoc: true\n---\n#  \n<!--more-->\n# 7 通过时间反向传播\n- 对于梯度爆炸或梯度消失，以及需要对循环神经网络分离梯度。例如，我们在序列上调用了detach函数。为了能够快速构建模型并了解其工作原理，本节将更深入地探讨序列模型反向传播的细节，以及相关的数学原理。\n\n- 循环神经网络中的前向传播相对简单。通过时间反向传播（backpropagation through time，BPTT）实际上是循环神经网络中反向传播技术的一个特定应用。它要求我们将循环神经网络的计算图一次展开一个时间步，以获得模型变量和参数之间的依赖关系。然后，基于链式法则，应用反向传播来计算和存储梯度。由于序列可能相当长，因此依赖关系也可能相当长。例如，某个1000个字符的序列，其第一个词元可能会对最后位置的词元产生重大影响。这在计算上是不可行的（它需要的时间和内存都太多了），并且还需要超过1000个矩阵的乘积才能得到非常难以捉摸的梯度。这个过程充满了计算与统计的不确定性。\n\n\n## 7.1 循环神经网络的梯度分析\n- 从一个循环神经网络的简化模型开始，此模型忽略了隐状态的特性及其更新方式的细节。这里的数学表示没有像过去那样明确地区分标量、向量和矩阵，因为这些细节对于分析并不重要\n\n- 我们将时间步t的隐状态表示为$h_ t$，输入表示为$x_ t$，输出表示为$o_ t$。输入和隐状态可以拼接后与隐藏层中的一个权重变量相乘。。因此，我们分别使用$w_ h$和$w_ o$来表示隐藏层和输出层的权重。每个时间步的隐状态和输出可以写为：\n$$ \\begin{aligned} h_ t &= f(x_t, h_{t-1}, w_h),\\\\ o_ t &= g(h_ t, w_o), \\end{aligned} $$\n\n- 其中f和g分别是隐藏层和输出层的变换。因此，我们有一个链$ \\{ \\dots , (x_ {t-1}, h_ {t-1}, o_ {t-1}), (x_ t, h_ t, o_ t),  \\dots \\}$，它们通过循环计算彼此依赖。前向传播相当简单，一次一个时间步的遍历三元组$(x_ t, h_ t, o_ t)$，然后通过一个目标函数在所有T个时间步内评估输出$o_ t$和对应的标签$y_ t$之间的误差:\n$$ L(x_ 1, \\dots, x_ T, y_ 1, \\dots, y_ T, w_ h, w_ o) = \\frac{1}{T} \\sum_ {t=1} ^ T l(o_ t, y_ t). $$\n\n- 对于反向传播，问题则有点棘手，特别是当我们计算目标函数L关于参数wh的梯度时。具体来说，按照链式法则：\n$$ \\begin{aligned} \\frac{\\partial L}{\\partial w_ h} &= \\frac{1}{T} \\sum_ {t=1} ^ T \\frac{\\partial l(o_ t, y_ t)}{\\partial w_ h} \\\\ &= \\frac{1}{T} \\sum_ {t=1} ^ T \\frac{\\partial l(o_ t, y_ t)}{\\partial o_ t} \\frac{\\partial g(h_ t, w_ o)}{\\partial h_ t} \\frac{\\partial h_ t}{\\partial w_ h}. \\end{aligned} $$\n\n- 其中第一项和第二项很容易计算，而第三项$ \\frac{\\partial h_ t}{\\partial w_ h}$则相当复杂。因为我们需要循环地计算参数$w_ h$对$h_ t$的影响，因为$h_ t = f(x_t, h_{t-1}, w_h)$，所以$h_ t$既依赖于$h_ {t-1}$又依赖于$w_ h$，其中$h_ {t-1}$的计算也依赖于$w_ h$。因此，使用链式法则产生：\n$$ \\frac{\\partial h_ t}{\\partial w_ h} = \\frac{\\partial f(x_ t, h_ {t-1}, w_ h)}{\\partial w_ h} + \\frac{\\partial f(x_ t, h_ {t-1}, w_ h)}{\\partial h_ {t-1}} \\frac{\\partial h_ {t-1}}{\\partial w_ h}.  $$\n\n- 为了导出上述梯度，假设我们有三个序列$ \\{ a_ t \\} $，$ \\{ b_ t \\} $，$ \\{ c_ t \\} $，当t=1,2,...时，序列满足$a_ 0 = 0$，且$a_ t = b_ t + c_ t a_ {t-1}$。对于$t \\geq 1$，我们有：\n$$ a_ t = b_ t + \\sum_ {i=1} ^ {t-1}(\\prod_ {j=i+1} ^ t c_ j) b_ i $$\n\n- 不妨基于下列公式替换$a_ t$、$b_ t$和$c_ t$:\n$$ \\begin{aligned} a_ t &= \\frac{\\partial h_ t}{\\partial w_ h},\\\\ b_ t &= \\frac{\\partial f(x_ t, h_ {t-1}, w_ h)}{\\partial w_ h},\\\\ c_ t &= \\frac{\\partial f(x_ t, h_ {t-1}, w_ h)}{\\partial h_ {t-1}}, \\end{aligned} $$\n\n- 通过：\n$$ \\frac{\\partial h_ t}{\\partial w_ h} = \\frac{\\partial f(x_ t, h_ {t-1}, w_ h)}{\\partial w_ h} + \\frac{\\partial f(x_ t, h_ {t-1}, w_ h)}{\\partial h_ {t-1}} \\frac{\\partial h_ {t-1}}{\\partial w_ h}.  $$\n\n$$ a_ t = b_ t + c_ t a_ {t-1} $$\n\n$$ a_ t = b_ t + \\sum_ {i=1} ^ {t-1}(\\prod_ {j=i+1} ^ t c_ j) b_ i $$\n\n- 移除循环计算：\n<span id='goto7'></span>$$ \\frac{\\partial h_ t}{\\partial w_ h} = \\frac{\\partial f(x_ t, h_ {t-1}, w_ h)}{\\partial w_ h} + \\sum_ {i=1} ^ {t-1}(\\prod_ {j=i+1} ^ t \\frac{\\partial f(x_ j, h_ {j-1}, w_ h)}{\\partial h_ {j-1}}) \\frac{\\partial f(x_ i, h_ {i-1}, w_ h)}{\\partial w_ h}. (7)  $$\n\n- 虽然我们可以使用链式法则递归地计算$\\frac{\\partial h_ t}{\\partial w_ h}$，但当t很大时这个链就会变得很长。我们需要想想办法来处理这一问题.\n\n### 7.1.1 完全计算\n- 显然，我们可以计算<a href=\"#goto7\">（7）</a>中的全部总和，然而，这样的计算非常缓慢，并且可能会发生梯度爆炸，因为初始条件的微小变化就可能会对结果产生巨大的影响。也就是说，我们可以观察到类似于蝴蝶效应的现象，即初始条件的很小变化就会导致结果发生不成比例的变化。这对于我们想要估计的模型而言是非常不可取的。毕竟，我们正在寻找的是能够很好地泛化高稳定性模型的估计器。因此，在实践中，这种方法几乎从未使用过。\n\n### 7.1.2 截断时间步\n- 或者，我们可以在τ步后截断<a href=\"#goto7\">（7）</a>中的求和计算。例如之前的梯度裁剪。这会带来真实梯度的近似，只需将求和终止为$\\frac{ \\partial h_ {t - \\tau}}{\\partial w_ h}$。在实践中，这种方式工作得很好。它通常被称为截断的通过时间反向传播。这样做导致该模型主要侧重于短期影响，而不是长期影响。这在现实中是可取的，因为它会将估计值偏向更简单和更稳定的模型。\n\n### 7.1.3 随机截断\n- 我们可以用一个随机变量替换$\\frac{\\partial h_ t }{\\partial w_ h}$，该随机变量在预期中是正确的，但是会截断序列。这个随机变量是通过使用序列 $\\xi_ t$来实现的，序列预定义了$0 \\leq \\pi_ t \\leq 1$，其中$P (\\xi_ t = 0 ) = 1 - \\pi_ t$，且$P (\\xi_ t = \\pi_ t ^ {-1}) = \\pi_ t$，因此$E [ \\xi_ t ] = 1$。我们使用它来替换<a href=\"#goto7\">（7）</a>中的梯度$\\frac{\\partial h_ t}{\\partial w_ h}$得到：\n$$ z_ t = \\frac{\\partial f(x_ t, h_ {t-1}, w_ h)}{\\partial w_ h} + \\xi_ t \\frac{\\partial f(x_ t, h_ {t-1}, w_ h)}{\\partial h_ {t-1}} \\frac{\\partial h_ {t-1}}{\\partial w_ h}. $$\n\n- 从$\\xi_ t$的定义中推导出来$E [ z_ t ] = \\frac{\\partial h_ t}{\\partial w_ h}$。每当$ \\xi_ t = 0 $时，递归计算终止在这个t时间步。这导致了不同长度序列的加权和，其中长序列出现的很少，所以将适当地加大权重。\n\n### 7.1.4 比较策略\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/7_rnn/7_backward/1.png)\n![](img/deeplearning/code/pytorch/7_rnn/7_backward/1.png)\n- 第一行采用随机截断，方法是将文本划分为不同长度的片断；\n- 第二行采用常规截断，方法是将文本分解为相同长度的子序列。这也是我们在循环神经网络实验中一直在做的；\n- 第三行采用通过时间的完全反向传播，结果是产生了在计算上不可行的表达式。\n- 虽然随机截断在理论上具有吸引力，但很可能是由于多种因素在实践中并不比常规截断更好。首先，在对过去若干个时间步经过反向传播后，观测结果足以捕获实际的依赖关系。其次，增加的方差抵消了时间步数越多梯度越精确的事实。第三，我们真正想要的是只有短范围交互的模型。因此，模型需要的正是截断的通过时间反向传播方法所具备的轻度正则化效果。\n\n\n## 7.2 通过时间反向传播的细节\n- 如何计算目标函数相对于所有分解模型参数的梯度。为了保持简单，我们考虑一个没有偏置参数的循环神经网络，其在隐藏层中的激活函数使用恒等映射（$\\phi(x) = x$）。对于时间步t，设单个样本的输入及其对应的标签分别为$\\mathbf{ x }_ t \\in \\mathbb{ R }^{d} $和$y_ t $。计算隐状态$ \\mathbf{ h }_ t \\in \\mathbb{ R }^{h} $和输出$ \\mathbf{ o }_ t \\in \\mathbb{ R }^{q} $的公式如下：\n$$ \\begin{aligned} \\mathbf{ h }_ t &= \\mathbf{ W }_ { hx } \\mathbf{ x }_ t + \\mathbf{ W }_ { hh } \\mathbf{ h }_ { t-1},\\\\ \\mathbf{ o }_ t &= \\mathbf{ W }_ { qh } \\mathbf{ h }_ t. \\end{aligned} $$\n\n- 其中权重参数为$\\mathbf{ W }_ { hx } \\in \\mathbb{ R }^{h \\times d} $，$\\mathbf{ W }_ { hh } \\in \\mathbb{ R }^{h \\times h} $和$\\mathbf{ W }_ { qh } \\in \\mathbb{ R }^{q \\times h} $。用$l(\\mathbf{ o }_ t, y_ t) $表示时间步t处（即从序列开始起的超过T个时间步）的损失函数，则我们的目标函数的总体损失是：\n$$ L = \\frac{1}{T} \\sum_ {t=1} ^ T l(\\mathbf{ o }_ t, y_ t). $$\n\n- 为了在循环神经网络的计算过程中可视化模型变量和参数之间的依赖关系，我们可以为模型绘制一个计算图：例如，时间步3的隐状态$ \\mathbf{ h }_ 3 $的计算依赖于模型参数$\\mathbf{ W }_ { hx } $和$\\mathbf{ W }_ { hh } $，以及最终时间步的隐状态$ \\mathbf{ h }_ 2 $和当前时间步的输入$ \\mathbf{ x }_ 3 $。\n![](../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/7_rnn/7_backward/2.png)\n![](img/deeplearning/code/pytorch/7_rnn/7_backward/2.png)\n\n- 未着色的方框表示变量，着色的方框表示参数，圆表示运算符。模型参数是：$\\mathbf{ W }_ { hx } $，$\\mathbf{ W }_ { hh } $和$\\mathbf{ W }_ { qh } $。通常，训练该模型需要对这些参数进行梯度计算：$ \\frac{\\partial L}{\\partial \\mathbf{ W }_ { hx } } $，$ \\frac{\\partial L}{\\partial \\mathbf{ W }_ { hh } } $和$ \\frac{\\partial L}{\\partial \\mathbf{ W }_ { qh } } $。根据上图的依赖关系，我们可以沿箭头的相反方向遍历计算图，依次计算和存储梯度。为了灵活地表示链式法则中不同形状的矩阵、向量和标量的乘法，我们继续使用prod运算符。\n\n- 首先，在任意时间步t，目标函数关于模型输出的微分计算是相当简单的：\n$$ \\frac{\\partial L}{\\partial \\mathbf{ o }_ t} = \\frac{1}{T} \\frac{\\partial l(\\mathbf{ o }_ t, y_ t)}{\\partial \\mathbf{ o }_ t}. $$\n\n- 现在，我们可以计算目标函数关于输出层中参数$\\mathbf{ W }_ { qh } $的梯度：$ \\frac{\\partial L}{\\partial \\mathbf{ W }_ { qh } } $。由上图可知，目标函数L通过$\\mathbf{ o }_ 1, \\ldots, \\mathbf{ o }_ T $依赖于$\\mathbf{ W }_ { qh } $。根据链式法则，我们得到：\n$$ \\begin{aligned} \\frac{\\partial L}{\\partial \\mathbf{ W }_ { qh } } &= \\sum_ {t=1} ^ T prod \\left( \\frac{\\partial L}{\\partial \\mathbf{ o }_ t}, \\frac{\\partial \\mathbf{ o }_ t}{\\partial \\mathbf{ W }_ { qh } } \\right) \\\\ &= \\sum_ {t=1} ^ T  \\frac{\\partial L}{\\partial \\mathbf{ o }_ t} \\mathbf{ h }_ t ^ T . \\end{aligned} $$\n\n- 其中$ \\frac{ \\partial L}{\\partial \\mathbf{ o }_ t} $已经在上面计算过了。接下来，在最后的时间步T，目标函数L仅通过$\\mathbf{ o }_ T $依赖于隐状态$\\mathbf{ h }_ T $。因此，使用链式法可以很容易地得到梯度$ \\frac{ \\partial L}{\\partial \\mathbf{ h }_ T} $：\n<span id='goto13'></span>$$ \\begin{aligned} \\frac{\\partial L}{\\partial \\mathbf{ h }_ T} &= prod \\left( \\frac{\\partial L}{\\partial \\mathbf{ o }_ T}, \\frac{\\partial \\mathbf{ o }_ T}{\\partial \\mathbf{ h }_ T} \\right) \\\\ &= \\mathbf{ W }_ { qh } ^ T \\frac{\\partial L}{\\partial \\mathbf{ o }_ T}. \\end{aligned} （13） $$\n\n- 当目标函数L通过$\\mathbf{ h }_ { t+1 } $和$\\mathbf{ o }_ t $依赖于$\\mathbf{ h }_ t $时，对任意时间步t < T来说都变得更加棘手。根据链式法则，隐状态的梯度$ \\frac{ \\partial L}{\\partial \\mathbf{ h }_ t} $在任何时间步骤t < T时都可以递归地计算为：\n<span id='goto14'></span>$$ \\begin{aligned} \\frac{\\partial L}{\\partial \\mathbf{ h }_ t} &= prod \\left( \\frac{\\partial L}{\\partial \\mathbf{ h }_ { t+1 }}, \\frac{\\partial \\mathbf{ h }_ { t+1 }}{\\partial \\mathbf{ h }_ t} \\right) + prod \\left( \\frac{\\partial L}{\\partial \\mathbf{ o }_ t}, \\frac{\\partial \\mathbf{ o }_ t}{\\partial \\mathbf{ h }_ t} \\right) \\\\ &= \\mathbf{ W }_ { hh } ^ T \\frac{\\partial L}{\\partial \\mathbf{ h }_ { t+1 }} + \\mathbf{ W }_ { qh } ^ T \\frac{\\partial L}{\\partial \\mathbf{ o }_ t}. \\end{aligned} （14） $$\n\n- 为了进行分析，对于任何时间步1 ≤ t ≤ T展开递归计算得:\n$$ \\frac{\\partial L}{\\partial \\mathbf{ h }_ t} = \\sum_ {i=t} ^ T \\left( \\mathbf{ W }_ { hh } ^ T \\right) ^ {T-i} \\mathbf{ W }_ { qh } ^ T \\frac{\\partial L}{\\partial \\mathbf{ o }_ { T+t-i }}. $$\n\n- 从上式看到，这个简单的线性例子已经展现了长序列模型的一些关键问题：它陷入到$ \\mathbf{ W }_ { hh } ^ T $的潜在的非常大的幂。在这个幂中，小于1的特征值将会消失，大于1的特征值将会发散。这在数值上是不稳定的，表现形式为梯度消失或梯度爆炸。解决此问题的一种方法是按照计算方便的需要截断时间步长的尺寸。实际上，这种截断是通过在给定数量的时间步之后分离梯度来实现的。\n\n- 最后，上图表明：目标函数L通过隐状态$\\mathbf{ h }_ 1, \\ldots, \\mathbf{ h }_ T $依赖于隐藏层中的模型参数$\\mathbf{ W }_ { hx } $和$\\mathbf{ W }_ { hh } $。为了计算有关这些参数的梯度$ \\frac{ \\partial L}{\\partial \\mathbf{ W }_ { hx } } $和$ \\frac{ \\partial L}{\\partial \\mathbf{ W }_ { hh } } $，我们应用链式规则得：\n$$ \\begin{aligned} \\frac{\\partial L}{\\partial \\mathbf{ W }_ { hx } } &= \\sum_ {t=1} ^ T prod \\left( \\frac{\\partial L}{\\partial \\mathbf{ h }_ t}, \\frac{\\partial \\mathbf{ h }_ t}{\\partial \\mathbf{ W }_ { hx } } \\right) = \\sum_ {t=1} ^ T \\frac{\\partial L}{\\partial \\mathbf{ h }_ t} \\mathbf{ x }_ t ^ T, \\\\ \\frac{\\partial L}{\\partial \\mathbf{ W }_ { hh } } &= \\sum_ {t=1} ^ T prod \\left( \\frac{\\partial L}{\\partial \\mathbf{ h }_ t}, \\frac{\\partial \\mathbf{ h }_ t}{\\partial \\mathbf{ W }_ { hh } } \\right) = \\sum_ {t=1} ^ T \\frac{\\partial L}{\\partial \\mathbf{ h }_ t} \\mathbf{ h }_ { t-1 } ^ T. \\end{aligned} $$\n\n- 其中$ \\frac{ \\partial L}{\\partial \\mathbf{ h }_ t} $是由<a href=\"#goto13\">（13）</a>和<a href=\"#goto14\">（14）</a>递归计算得到的，是影响数值稳定性的关键量。\n\n- 由于通过时间反向传播是反向传播在循环神经网络中的应用方式，所以训练循环神经网络交替使用前向传播和通过时间反向传播。通过时间反向传播依次计算并存储上述梯度。具体而言，存储的中间值会被重复使用，以避免重复计算，例如存储$ \\frac{ \\partial L}{\\partial \\mathbf{ h }_ t} $，以便在计算$ \\frac{ \\partial L}{\\partial \\mathbf{ W }_ { hx } } $和$ \\frac{ \\partial L}{\\partial \\mathbf{ W }_ { hh } } $时重复使用。\n","slug":"deeplearning/code/pytorch/7_rnn/7_backward","published":1,"updated":"2024-05-05T14:26:16.652Z","comments":1,"layout":"post","photos":[],"_id":"clvuli6v0009dzsvwe9fq72tk","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n<h1 id=\"7-通过时间反向传播\"><a href=\"#7-通过时间反向传播\" class=\"headerlink\" title=\"7 通过时间反向传播\"></a>7 通过时间反向传播</h1><ul>\n<li><p>对于梯度爆炸或梯度消失，以及需要对循环神经网络分离梯度。例如，我们在序列上调用了detach函数。为了能够快速构建模型并了解其工作原理，本节将更深入地探讨序列模型反向传播的细节，以及相关的数学原理。</p>\n</li>\n<li><p>循环神经网络中的前向传播相对简单。通过时间反向传播（backpropagation through time，BPTT）实际上是循环神经网络中反向传播技术的一个特定应用。它要求我们将循环神经网络的计算图一次展开一个时间步，以获得模型变量和参数之间的依赖关系。然后，基于链式法则，应用反向传播来计算和存储梯度。由于序列可能相当长，因此依赖关系也可能相当长。例如，某个1000个字符的序列，其第一个词元可能会对最后位置的词元产生重大影响。这在计算上是不可行的（它需要的时间和内存都太多了），并且还需要超过1000个矩阵的乘积才能得到非常难以捉摸的梯度。这个过程充满了计算与统计的不确定性。</p>\n</li>\n</ul>\n<h2 id=\"7-1-循环神经网络的梯度分析\"><a href=\"#7-1-循环神经网络的梯度分析\" class=\"headerlink\" title=\"7.1 循环神经网络的梯度分析\"></a>7.1 循环神经网络的梯度分析</h2><ul>\n<li><p>从一个循环神经网络的简化模型开始，此模型忽略了隐状态的特性及其更新方式的细节。这里的数学表示没有像过去那样明确地区分标量、向量和矩阵，因为这些细节对于分析并不重要</p>\n</li>\n<li><p>我们将时间步t的隐状态表示为$h_ t$，输入表示为$x_ t$，输出表示为$o_ t$。输入和隐状态可以拼接后与隐藏层中的一个权重变量相乘。。因此，我们分别使用$w_ h$和$w_ o$来表示隐藏层和输出层的权重。每个时间步的隐状态和输出可以写为：<br>$$ \\begin{aligned} h_ t &amp;&#x3D; f(x_t, h_{t-1}, w_h),\\ o_ t &amp;&#x3D; g(h_ t, w_o), \\end{aligned} $$</p>\n</li>\n<li><p>其中f和g分别是隐藏层和输出层的变换。因此，我们有一个链$ { \\dots , (x_ {t-1}, h_ {t-1}, o_ {t-1}), (x_ t, h_ t, o_ t),  \\dots }$，它们通过循环计算彼此依赖。前向传播相当简单，一次一个时间步的遍历三元组$(x_ t, h_ t, o_ t)$，然后通过一个目标函数在所有T个时间步内评估输出$o_ t$和对应的标签$y_ t$之间的误差:<br>$$ L(x_ 1, \\dots, x_ T, y_ 1, \\dots, y_ T, w_ h, w_ o) &#x3D; \\frac{1}{T} \\sum_ {t&#x3D;1} ^ T l(o_ t, y_ t). $$</p>\n</li>\n<li><p>对于反向传播，问题则有点棘手，特别是当我们计算目标函数L关于参数wh的梯度时。具体来说，按照链式法则：<br>$$ \\begin{aligned} \\frac{\\partial L}{\\partial w_ h} &amp;&#x3D; \\frac{1}{T} \\sum_ {t&#x3D;1} ^ T \\frac{\\partial l(o_ t, y_ t)}{\\partial w_ h} \\ &amp;&#x3D; \\frac{1}{T} \\sum_ {t&#x3D;1} ^ T \\frac{\\partial l(o_ t, y_ t)}{\\partial o_ t} \\frac{\\partial g(h_ t, w_ o)}{\\partial h_ t} \\frac{\\partial h_ t}{\\partial w_ h}. \\end{aligned} $$</p>\n</li>\n<li><p>其中第一项和第二项很容易计算，而第三项$ \\frac{\\partial h_ t}{\\partial w_ h}$则相当复杂。因为我们需要循环地计算参数$w_ h$对$h_ t$的影响，因为$h_ t &#x3D; f(x_t, h_{t-1}, w_h)$，所以$h_ t$既依赖于$h_ {t-1}$又依赖于$w_ h$，其中$h_ {t-1}$的计算也依赖于$w_ h$。因此，使用链式法则产生：<br>$$ \\frac{\\partial h_ t}{\\partial w_ h} &#x3D; \\frac{\\partial f(x_ t, h_ {t-1}, w_ h)}{\\partial w_ h} + \\frac{\\partial f(x_ t, h_ {t-1}, w_ h)}{\\partial h_ {t-1}} \\frac{\\partial h_ {t-1}}{\\partial w_ h}.  $$</p>\n</li>\n<li><p>为了导出上述梯度，假设我们有三个序列$ { a_ t } $，$ { b_ t } $，$ { c_ t } $，当t&#x3D;1,2,…时，序列满足$a_ 0 &#x3D; 0$，且$a_ t &#x3D; b_ t + c_ t a_ {t-1}$。对于$t \\geq 1$，我们有：<br>$$ a_ t &#x3D; b_ t + \\sum_ {i&#x3D;1} ^ {t-1}(\\prod_ {j&#x3D;i+1} ^ t c_ j) b_ i $$</p>\n</li>\n<li><p>不妨基于下列公式替换$a_ t$、$b_ t$和$c_ t$:<br>$$ \\begin{aligned} a_ t &amp;&#x3D; \\frac{\\partial h_ t}{\\partial w_ h},\\ b_ t &amp;&#x3D; \\frac{\\partial f(x_ t, h_ {t-1}, w_ h)}{\\partial w_ h},\\ c_ t &amp;&#x3D; \\frac{\\partial f(x_ t, h_ {t-1}, w_ h)}{\\partial h_ {t-1}}, \\end{aligned} $$</p>\n</li>\n<li><p>通过：<br>$$ \\frac{\\partial h_ t}{\\partial w_ h} &#x3D; \\frac{\\partial f(x_ t, h_ {t-1}, w_ h)}{\\partial w_ h} + \\frac{\\partial f(x_ t, h_ {t-1}, w_ h)}{\\partial h_ {t-1}} \\frac{\\partial h_ {t-1}}{\\partial w_ h}.  $$</p>\n</li>\n</ul>\n<p>$$ a_ t &#x3D; b_ t + c_ t a_ {t-1} $$</p>\n<p>$$ a_ t &#x3D; b_ t + \\sum_ {i&#x3D;1} ^ {t-1}(\\prod_ {j&#x3D;i+1} ^ t c_ j) b_ i $$</p>\n<ul>\n<li><p>移除循环计算：<br><span id='goto7'></span>$$ \\frac{\\partial h_ t}{\\partial w_ h} &#x3D; \\frac{\\partial f(x_ t, h_ {t-1}, w_ h)}{\\partial w_ h} + \\sum_ {i&#x3D;1} ^ {t-1}(\\prod_ {j&#x3D;i+1} ^ t \\frac{\\partial f(x_ j, h_ {j-1}, w_ h)}{\\partial h_ {j-1}}) \\frac{\\partial f(x_ i, h_ {i-1}, w_ h)}{\\partial w_ h}. (7)  $$</p>\n</li>\n<li><p>虽然我们可以使用链式法则递归地计算$\\frac{\\partial h_ t}{\\partial w_ h}$，但当t很大时这个链就会变得很长。我们需要想想办法来处理这一问题.</p>\n</li>\n</ul>\n<h3 id=\"7-1-1-完全计算\"><a href=\"#7-1-1-完全计算\" class=\"headerlink\" title=\"7.1.1 完全计算\"></a>7.1.1 完全计算</h3><ul>\n<li>显然，我们可以计算<a href=\"#goto7\">（7）</a>中的全部总和，然而，这样的计算非常缓慢，并且可能会发生梯度爆炸，因为初始条件的微小变化就可能会对结果产生巨大的影响。也就是说，我们可以观察到类似于蝴蝶效应的现象，即初始条件的很小变化就会导致结果发生不成比例的变化。这对于我们想要估计的模型而言是非常不可取的。毕竟，我们正在寻找的是能够很好地泛化高稳定性模型的估计器。因此，在实践中，这种方法几乎从未使用过。</li>\n</ul>\n<h3 id=\"7-1-2-截断时间步\"><a href=\"#7-1-2-截断时间步\" class=\"headerlink\" title=\"7.1.2 截断时间步\"></a>7.1.2 截断时间步</h3><ul>\n<li>或者，我们可以在τ步后截断<a href=\"#goto7\">（7）</a>中的求和计算。例如之前的梯度裁剪。这会带来真实梯度的近似，只需将求和终止为$\\frac{ \\partial h_ {t - \\tau}}{\\partial w_ h}$。在实践中，这种方式工作得很好。它通常被称为截断的通过时间反向传播。这样做导致该模型主要侧重于短期影响，而不是长期影响。这在现实中是可取的，因为它会将估计值偏向更简单和更稳定的模型。</li>\n</ul>\n<h3 id=\"7-1-3-随机截断\"><a href=\"#7-1-3-随机截断\" class=\"headerlink\" title=\"7.1.3 随机截断\"></a>7.1.3 随机截断</h3><ul>\n<li><p>我们可以用一个随机变量替换$\\frac{\\partial h_ t }{\\partial w_ h}$，该随机变量在预期中是正确的，但是会截断序列。这个随机变量是通过使用序列 $\\xi_ t$来实现的，序列预定义了$0 \\leq \\pi_ t \\leq 1$，其中$P (\\xi_ t &#x3D; 0 ) &#x3D; 1 - \\pi_ t$，且$P (\\xi_ t &#x3D; \\pi_ t ^ {-1}) &#x3D; \\pi_ t$，因此$E [ \\xi_ t ] &#x3D; 1$。我们使用它来替换<a href=\"#goto7\">（7）</a>中的梯度$\\frac{\\partial h_ t}{\\partial w_ h}$得到：<br>$$ z_ t &#x3D; \\frac{\\partial f(x_ t, h_ {t-1}, w_ h)}{\\partial w_ h} + \\xi_ t \\frac{\\partial f(x_ t, h_ {t-1}, w_ h)}{\\partial h_ {t-1}} \\frac{\\partial h_ {t-1}}{\\partial w_ h}. $$</p>\n</li>\n<li><p>从$\\xi_ t$的定义中推导出来$E [ z_ t ] &#x3D; \\frac{\\partial h_ t}{\\partial w_ h}$。每当$ \\xi_ t &#x3D; 0 $时，递归计算终止在这个t时间步。这导致了不同长度序列的加权和，其中长序列出现的很少，所以将适当地加大权重。</p>\n</li>\n</ul>\n<h3 id=\"7-1-4-比较策略\"><a href=\"#7-1-4-比较策略\" class=\"headerlink\" title=\"7.1.4 比较策略\"></a>7.1.4 比较策略</h3><p><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/7_rnn/7_backward/1.png\"><br><img src=\"/img/deeplearning/code/pytorch/7_rnn/7_backward/1.png\"></p>\n<ul>\n<li>第一行采用随机截断，方法是将文本划分为不同长度的片断；</li>\n<li>第二行采用常规截断，方法是将文本分解为相同长度的子序列。这也是我们在循环神经网络实验中一直在做的；</li>\n<li>第三行采用通过时间的完全反向传播，结果是产生了在计算上不可行的表达式。</li>\n<li>虽然随机截断在理论上具有吸引力，但很可能是由于多种因素在实践中并不比常规截断更好。首先，在对过去若干个时间步经过反向传播后，观测结果足以捕获实际的依赖关系。其次，增加的方差抵消了时间步数越多梯度越精确的事实。第三，我们真正想要的是只有短范围交互的模型。因此，模型需要的正是截断的通过时间反向传播方法所具备的轻度正则化效果。</li>\n</ul>\n<h2 id=\"7-2-通过时间反向传播的细节\"><a href=\"#7-2-通过时间反向传播的细节\" class=\"headerlink\" title=\"7.2 通过时间反向传播的细节\"></a>7.2 通过时间反向传播的细节</h2><ul>\n<li><p>如何计算目标函数相对于所有分解模型参数的梯度。为了保持简单，我们考虑一个没有偏置参数的循环神经网络，其在隐藏层中的激活函数使用恒等映射（$\\phi(x) &#x3D; x$）。对于时间步t，设单个样本的输入及其对应的标签分别为$\\mathbf{ x }_ t \\in \\mathbb{ R }^{d} $和$y_ t $。计算隐状态$ \\mathbf{ h }_ t \\in \\mathbb{ R }^{h} $和输出$ \\mathbf{ o }_ t \\in \\mathbb{ R }^{q} $的公式如下：<br>$$ \\begin{aligned} \\mathbf{ h }_ t &amp;&#x3D; \\mathbf{ W }_ { hx } \\mathbf{ x }_ t + \\mathbf{ W }_ { hh } \\mathbf{ h }_ { t-1},\\ \\mathbf{ o }_ t &amp;&#x3D; \\mathbf{ W }_ { qh } \\mathbf{ h }_ t. \\end{aligned} $$</p>\n</li>\n<li><p>其中权重参数为$\\mathbf{ W }_ { hx } \\in \\mathbb{ R }^{h \\times d} $，$\\mathbf{ W }_ { hh } \\in \\mathbb{ R }^{h \\times h} $和$\\mathbf{ W }_ { qh } \\in \\mathbb{ R }^{q \\times h} $。用$l(\\mathbf{ o }_ t, y_ t) $表示时间步t处（即从序列开始起的超过T个时间步）的损失函数，则我们的目标函数的总体损失是：<br>$$ L &#x3D; \\frac{1}{T} \\sum_ {t&#x3D;1} ^ T l(\\mathbf{ o }_ t, y_ t). $$</p>\n</li>\n<li><p>为了在循环神经网络的计算过程中可视化模型变量和参数之间的依赖关系，我们可以为模型绘制一个计算图：例如，时间步3的隐状态$ \\mathbf{ h }_ 3 $的计算依赖于模型参数$\\mathbf{ W }_ { hx } $和$\\mathbf{ W }_ { hh } $，以及最终时间步的隐状态$ \\mathbf{ h }_ 2 $和当前时间步的输入$ \\mathbf{ x }_ 3 $。<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/7_rnn/7_backward/2.png\"><br><img src=\"/img/deeplearning/code/pytorch/7_rnn/7_backward/2.png\"></p>\n</li>\n<li><p>未着色的方框表示变量，着色的方框表示参数，圆表示运算符。模型参数是：$\\mathbf{ W }_ { hx } $，$\\mathbf{ W }_ { hh } $和$\\mathbf{ W }_ { qh } $。通常，训练该模型需要对这些参数进行梯度计算：$ \\frac{\\partial L}{\\partial \\mathbf{ W }_ { hx } } $，$ \\frac{\\partial L}{\\partial \\mathbf{ W }_ { hh } } $和$ \\frac{\\partial L}{\\partial \\mathbf{ W }_ { qh } } $。根据上图的依赖关系，我们可以沿箭头的相反方向遍历计算图，依次计算和存储梯度。为了灵活地表示链式法则中不同形状的矩阵、向量和标量的乘法，我们继续使用prod运算符。</p>\n</li>\n<li><p>首先，在任意时间步t，目标函数关于模型输出的微分计算是相当简单的：<br>$$ \\frac{\\partial L}{\\partial \\mathbf{ o }_ t} &#x3D; \\frac{1}{T} \\frac{\\partial l(\\mathbf{ o }_ t, y_ t)}{\\partial \\mathbf{ o }_ t}. $$</p>\n</li>\n<li><p>现在，我们可以计算目标函数关于输出层中参数$\\mathbf{ W }_ { qh } $的梯度：$ \\frac{\\partial L}{\\partial \\mathbf{ W }_ { qh } } $。由上图可知，目标函数L通过$\\mathbf{ o }_ 1, \\ldots, \\mathbf{ o }_ T $依赖于$\\mathbf{ W }_ { qh } $。根据链式法则，我们得到：<br>$$ \\begin{aligned} \\frac{\\partial L}{\\partial \\mathbf{ W }_ { qh } } &amp;&#x3D; \\sum_ {t&#x3D;1} ^ T prod \\left( \\frac{\\partial L}{\\partial \\mathbf{ o }_ t}, \\frac{\\partial \\mathbf{ o }_ t}{\\partial \\mathbf{ W }_ { qh } } \\right) \\ &amp;&#x3D; \\sum_ {t&#x3D;1} ^ T  \\frac{\\partial L}{\\partial \\mathbf{ o }_ t} \\mathbf{ h }_ t ^ T . \\end{aligned} $$</p>\n</li>\n<li><p>其中$ \\frac{ \\partial L}{\\partial \\mathbf{ o }_ t} $已经在上面计算过了。接下来，在最后的时间步T，目标函数L仅通过$\\mathbf{ o }_ T $依赖于隐状态$\\mathbf{ h }_ T $。因此，使用链式法可以很容易地得到梯度$ \\frac{ \\partial L}{\\partial \\mathbf{ h }_ T} $：<br><span id='goto13'></span>$$ \\begin{aligned} \\frac{\\partial L}{\\partial \\mathbf{ h }_ T} &amp;&#x3D; prod \\left( \\frac{\\partial L}{\\partial \\mathbf{ o }_ T}, \\frac{\\partial \\mathbf{ o }_ T}{\\partial \\mathbf{ h }_ T} \\right) \\ &amp;&#x3D; \\mathbf{ W }_ { qh } ^ T \\frac{\\partial L}{\\partial \\mathbf{ o }_ T}. \\end{aligned} （13） $$</p>\n</li>\n<li><p>当目标函数L通过$\\mathbf{ h }_ { t+1 } $和$\\mathbf{ o }_ t $依赖于$\\mathbf{ h }_ t $时，对任意时间步t &lt; T来说都变得更加棘手。根据链式法则，隐状态的梯度$ \\frac{ \\partial L}{\\partial \\mathbf{ h }_ t} $在任何时间步骤t &lt; T时都可以递归地计算为：<br><span id='goto14'></span>$$ \\begin{aligned} \\frac{\\partial L}{\\partial \\mathbf{ h }_ t} &amp;&#x3D; prod \\left( \\frac{\\partial L}{\\partial \\mathbf{ h }_ { t+1 }}, \\frac{\\partial \\mathbf{ h }_ { t+1 }}{\\partial \\mathbf{ h }_ t} \\right) + prod \\left( \\frac{\\partial L}{\\partial \\mathbf{ o }_ t}, \\frac{\\partial \\mathbf{ o }_ t}{\\partial \\mathbf{ h }_ t} \\right) \\ &amp;&#x3D; \\mathbf{ W }_ { hh } ^ T \\frac{\\partial L}{\\partial \\mathbf{ h }_ { t+1 }} + \\mathbf{ W }_ { qh } ^ T \\frac{\\partial L}{\\partial \\mathbf{ o }_ t}. \\end{aligned} （14） $$</p>\n</li>\n<li><p>为了进行分析，对于任何时间步1 ≤ t ≤ T展开递归计算得:<br>$$ \\frac{\\partial L}{\\partial \\mathbf{ h }_ t} &#x3D; \\sum_ {i&#x3D;t} ^ T \\left( \\mathbf{ W }_ { hh } ^ T \\right) ^ {T-i} \\mathbf{ W }_ { qh } ^ T \\frac{\\partial L}{\\partial \\mathbf{ o }_ { T+t-i }}. $$</p>\n</li>\n<li><p>从上式看到，这个简单的线性例子已经展现了长序列模型的一些关键问题：它陷入到$ \\mathbf{ W }_ { hh } ^ T $的潜在的非常大的幂。在这个幂中，小于1的特征值将会消失，大于1的特征值将会发散。这在数值上是不稳定的，表现形式为梯度消失或梯度爆炸。解决此问题的一种方法是按照计算方便的需要截断时间步长的尺寸。实际上，这种截断是通过在给定数量的时间步之后分离梯度来实现的。</p>\n</li>\n<li><p>最后，上图表明：目标函数L通过隐状态$\\mathbf{ h }_ 1, \\ldots, \\mathbf{ h }_ T $依赖于隐藏层中的模型参数$\\mathbf{ W }_ { hx } $和$\\mathbf{ W }_ { hh } $。为了计算有关这些参数的梯度$ \\frac{ \\partial L}{\\partial \\mathbf{ W }_ { hx } } $和$ \\frac{ \\partial L}{\\partial \\mathbf{ W }_ { hh } } $，我们应用链式规则得：<br>$$ \\begin{aligned} \\frac{\\partial L}{\\partial \\mathbf{ W }_ { hx } } &amp;&#x3D; \\sum_ {t&#x3D;1} ^ T prod \\left( \\frac{\\partial L}{\\partial \\mathbf{ h }_ t}, \\frac{\\partial \\mathbf{ h }_ t}{\\partial \\mathbf{ W }_ { hx } } \\right) &#x3D; \\sum_ {t&#x3D;1} ^ T \\frac{\\partial L}{\\partial \\mathbf{ h }_ t} \\mathbf{ x }_ t ^ T, \\ \\frac{\\partial L}{\\partial \\mathbf{ W }_ { hh } } &amp;&#x3D; \\sum_ {t&#x3D;1} ^ T prod \\left( \\frac{\\partial L}{\\partial \\mathbf{ h }_ t}, \\frac{\\partial \\mathbf{ h }_ t}{\\partial \\mathbf{ W }_ { hh } } \\right) &#x3D; \\sum_ {t&#x3D;1} ^ T \\frac{\\partial L}{\\partial \\mathbf{ h }_ t} \\mathbf{ h }_ { t-1 } ^ T. \\end{aligned} $$</p>\n</li>\n<li><p>其中$ \\frac{ \\partial L}{\\partial \\mathbf{ h }_ t} $是由<a href=\"#goto13\">（13）</a>和<a href=\"#goto14\">（14）</a>递归计算得到的，是影响数值稳定性的关键量。</p>\n</li>\n<li><p>由于通过时间反向传播是反向传播在循环神经网络中的应用方式，所以训练循环神经网络交替使用前向传播和通过时间反向传播。通过时间反向传播依次计算并存储上述梯度。具体而言，存储的中间值会被重复使用，以避免重复计算，例如存储$ \\frac{ \\partial L}{\\partial \\mathbf{ h }_ t} $，以便在计算$ \\frac{ \\partial L}{\\partial \\mathbf{ W }_ { hx } } $和$ \\frac{ \\partial L}{\\partial \\mathbf{ W }_ { hh } } $时重复使用。</p>\n</li>\n</ul>\n","excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h1 id=\"7-通过时间反向传播\"><a href=\"#7-通过时间反向传播\" class=\"headerlink\" title=\"7 通过时间反向传播\"></a>7 通过时间反向传播</h1><ul>\n<li><p>对于梯度爆炸或梯度消失，以及需要对循环神经网络分离梯度。例如，我们在序列上调用了detach函数。为了能够快速构建模型并了解其工作原理，本节将更深入地探讨序列模型反向传播的细节，以及相关的数学原理。</p>\n</li>\n<li><p>循环神经网络中的前向传播相对简单。通过时间反向传播（backpropagation through time，BPTT）实际上是循环神经网络中反向传播技术的一个特定应用。它要求我们将循环神经网络的计算图一次展开一个时间步，以获得模型变量和参数之间的依赖关系。然后，基于链式法则，应用反向传播来计算和存储梯度。由于序列可能相当长，因此依赖关系也可能相当长。例如，某个1000个字符的序列，其第一个词元可能会对最后位置的词元产生重大影响。这在计算上是不可行的（它需要的时间和内存都太多了），并且还需要超过1000个矩阵的乘积才能得到非常难以捉摸的梯度。这个过程充满了计算与统计的不确定性。</p>\n</li>\n</ul>\n<h2 id=\"7-1-循环神经网络的梯度分析\"><a href=\"#7-1-循环神经网络的梯度分析\" class=\"headerlink\" title=\"7.1 循环神经网络的梯度分析\"></a>7.1 循环神经网络的梯度分析</h2><ul>\n<li><p>从一个循环神经网络的简化模型开始，此模型忽略了隐状态的特性及其更新方式的细节。这里的数学表示没有像过去那样明确地区分标量、向量和矩阵，因为这些细节对于分析并不重要</p>\n</li>\n<li><p>我们将时间步t的隐状态表示为$h_ t$，输入表示为$x_ t$，输出表示为$o_ t$。输入和隐状态可以拼接后与隐藏层中的一个权重变量相乘。。因此，我们分别使用$w_ h$和$w_ o$来表示隐藏层和输出层的权重。每个时间步的隐状态和输出可以写为：<br>$$ \\begin{aligned} h_ t &amp;&#x3D; f(x_t, h_{t-1}, w_h),\\ o_ t &amp;&#x3D; g(h_ t, w_o), \\end{aligned} $$</p>\n</li>\n<li><p>其中f和g分别是隐藏层和输出层的变换。因此，我们有一个链$ { \\dots , (x_ {t-1}, h_ {t-1}, o_ {t-1}), (x_ t, h_ t, o_ t),  \\dots }$，它们通过循环计算彼此依赖。前向传播相当简单，一次一个时间步的遍历三元组$(x_ t, h_ t, o_ t)$，然后通过一个目标函数在所有T个时间步内评估输出$o_ t$和对应的标签$y_ t$之间的误差:<br>$$ L(x_ 1, \\dots, x_ T, y_ 1, \\dots, y_ T, w_ h, w_ o) &#x3D; \\frac{1}{T} \\sum_ {t&#x3D;1} ^ T l(o_ t, y_ t). $$</p>\n</li>\n<li><p>对于反向传播，问题则有点棘手，特别是当我们计算目标函数L关于参数wh的梯度时。具体来说，按照链式法则：<br>$$ \\begin{aligned} \\frac{\\partial L}{\\partial w_ h} &amp;&#x3D; \\frac{1}{T} \\sum_ {t&#x3D;1} ^ T \\frac{\\partial l(o_ t, y_ t)}{\\partial w_ h} \\ &amp;&#x3D; \\frac{1}{T} \\sum_ {t&#x3D;1} ^ T \\frac{\\partial l(o_ t, y_ t)}{\\partial o_ t} \\frac{\\partial g(h_ t, w_ o)}{\\partial h_ t} \\frac{\\partial h_ t}{\\partial w_ h}. \\end{aligned} $$</p>\n</li>\n<li><p>其中第一项和第二项很容易计算，而第三项$ \\frac{\\partial h_ t}{\\partial w_ h}$则相当复杂。因为我们需要循环地计算参数$w_ h$对$h_ t$的影响，因为$h_ t &#x3D; f(x_t, h_{t-1}, w_h)$，所以$h_ t$既依赖于$h_ {t-1}$又依赖于$w_ h$，其中$h_ {t-1}$的计算也依赖于$w_ h$。因此，使用链式法则产生：<br>$$ \\frac{\\partial h_ t}{\\partial w_ h} &#x3D; \\frac{\\partial f(x_ t, h_ {t-1}, w_ h)}{\\partial w_ h} + \\frac{\\partial f(x_ t, h_ {t-1}, w_ h)}{\\partial h_ {t-1}} \\frac{\\partial h_ {t-1}}{\\partial w_ h}.  $$</p>\n</li>\n<li><p>为了导出上述梯度，假设我们有三个序列$ { a_ t } $，$ { b_ t } $，$ { c_ t } $，当t&#x3D;1,2,…时，序列满足$a_ 0 &#x3D; 0$，且$a_ t &#x3D; b_ t + c_ t a_ {t-1}$。对于$t \\geq 1$，我们有：<br>$$ a_ t &#x3D; b_ t + \\sum_ {i&#x3D;1} ^ {t-1}(\\prod_ {j&#x3D;i+1} ^ t c_ j) b_ i $$</p>\n</li>\n<li><p>不妨基于下列公式替换$a_ t$、$b_ t$和$c_ t$:<br>$$ \\begin{aligned} a_ t &amp;&#x3D; \\frac{\\partial h_ t}{\\partial w_ h},\\ b_ t &amp;&#x3D; \\frac{\\partial f(x_ t, h_ {t-1}, w_ h)}{\\partial w_ h},\\ c_ t &amp;&#x3D; \\frac{\\partial f(x_ t, h_ {t-1}, w_ h)}{\\partial h_ {t-1}}, \\end{aligned} $$</p>\n</li>\n<li><p>通过：<br>$$ \\frac{\\partial h_ t}{\\partial w_ h} &#x3D; \\frac{\\partial f(x_ t, h_ {t-1}, w_ h)}{\\partial w_ h} + \\frac{\\partial f(x_ t, h_ {t-1}, w_ h)}{\\partial h_ {t-1}} \\frac{\\partial h_ {t-1}}{\\partial w_ h}.  $$</p>\n</li>\n</ul>\n<p>$$ a_ t &#x3D; b_ t + c_ t a_ {t-1} $$</p>\n<p>$$ a_ t &#x3D; b_ t + \\sum_ {i&#x3D;1} ^ {t-1}(\\prod_ {j&#x3D;i+1} ^ t c_ j) b_ i $$</p>\n<ul>\n<li><p>移除循环计算：<br><span id='goto7'></span>$$ \\frac{\\partial h_ t}{\\partial w_ h} &#x3D; \\frac{\\partial f(x_ t, h_ {t-1}, w_ h)}{\\partial w_ h} + \\sum_ {i&#x3D;1} ^ {t-1}(\\prod_ {j&#x3D;i+1} ^ t \\frac{\\partial f(x_ j, h_ {j-1}, w_ h)}{\\partial h_ {j-1}}) \\frac{\\partial f(x_ i, h_ {i-1}, w_ h)}{\\partial w_ h}. (7)  $$</p>\n</li>\n<li><p>虽然我们可以使用链式法则递归地计算$\\frac{\\partial h_ t}{\\partial w_ h}$，但当t很大时这个链就会变得很长。我们需要想想办法来处理这一问题.</p>\n</li>\n</ul>\n<h3 id=\"7-1-1-完全计算\"><a href=\"#7-1-1-完全计算\" class=\"headerlink\" title=\"7.1.1 完全计算\"></a>7.1.1 完全计算</h3><ul>\n<li>显然，我们可以计算<a href=\"#goto7\">（7）</a>中的全部总和，然而，这样的计算非常缓慢，并且可能会发生梯度爆炸，因为初始条件的微小变化就可能会对结果产生巨大的影响。也就是说，我们可以观察到类似于蝴蝶效应的现象，即初始条件的很小变化就会导致结果发生不成比例的变化。这对于我们想要估计的模型而言是非常不可取的。毕竟，我们正在寻找的是能够很好地泛化高稳定性模型的估计器。因此，在实践中，这种方法几乎从未使用过。</li>\n</ul>\n<h3 id=\"7-1-2-截断时间步\"><a href=\"#7-1-2-截断时间步\" class=\"headerlink\" title=\"7.1.2 截断时间步\"></a>7.1.2 截断时间步</h3><ul>\n<li>或者，我们可以在τ步后截断<a href=\"#goto7\">（7）</a>中的求和计算。例如之前的梯度裁剪。这会带来真实梯度的近似，只需将求和终止为$\\frac{ \\partial h_ {t - \\tau}}{\\partial w_ h}$。在实践中，这种方式工作得很好。它通常被称为截断的通过时间反向传播。这样做导致该模型主要侧重于短期影响，而不是长期影响。这在现实中是可取的，因为它会将估计值偏向更简单和更稳定的模型。</li>\n</ul>\n<h3 id=\"7-1-3-随机截断\"><a href=\"#7-1-3-随机截断\" class=\"headerlink\" title=\"7.1.3 随机截断\"></a>7.1.3 随机截断</h3><ul>\n<li><p>我们可以用一个随机变量替换$\\frac{\\partial h_ t }{\\partial w_ h}$，该随机变量在预期中是正确的，但是会截断序列。这个随机变量是通过使用序列 $\\xi_ t$来实现的，序列预定义了$0 \\leq \\pi_ t \\leq 1$，其中$P (\\xi_ t &#x3D; 0 ) &#x3D; 1 - \\pi_ t$，且$P (\\xi_ t &#x3D; \\pi_ t ^ {-1}) &#x3D; \\pi_ t$，因此$E [ \\xi_ t ] &#x3D; 1$。我们使用它来替换<a href=\"#goto7\">（7）</a>中的梯度$\\frac{\\partial h_ t}{\\partial w_ h}$得到：<br>$$ z_ t &#x3D; \\frac{\\partial f(x_ t, h_ {t-1}, w_ h)}{\\partial w_ h} + \\xi_ t \\frac{\\partial f(x_ t, h_ {t-1}, w_ h)}{\\partial h_ {t-1}} \\frac{\\partial h_ {t-1}}{\\partial w_ h}. $$</p>\n</li>\n<li><p>从$\\xi_ t$的定义中推导出来$E [ z_ t ] &#x3D; \\frac{\\partial h_ t}{\\partial w_ h}$。每当$ \\xi_ t &#x3D; 0 $时，递归计算终止在这个t时间步。这导致了不同长度序列的加权和，其中长序列出现的很少，所以将适当地加大权重。</p>\n</li>\n</ul>\n<h3 id=\"7-1-4-比较策略\"><a href=\"#7-1-4-比较策略\" class=\"headerlink\" title=\"7.1.4 比较策略\"></a>7.1.4 比较策略</h3><p><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/7_rnn/7_backward/1.png\"><br><img src=\"/img/deeplearning/code/pytorch/7_rnn/7_backward/1.png\"></p>\n<ul>\n<li>第一行采用随机截断，方法是将文本划分为不同长度的片断；</li>\n<li>第二行采用常规截断，方法是将文本分解为相同长度的子序列。这也是我们在循环神经网络实验中一直在做的；</li>\n<li>第三行采用通过时间的完全反向传播，结果是产生了在计算上不可行的表达式。</li>\n<li>虽然随机截断在理论上具有吸引力，但很可能是由于多种因素在实践中并不比常规截断更好。首先，在对过去若干个时间步经过反向传播后，观测结果足以捕获实际的依赖关系。其次，增加的方差抵消了时间步数越多梯度越精确的事实。第三，我们真正想要的是只有短范围交互的模型。因此，模型需要的正是截断的通过时间反向传播方法所具备的轻度正则化效果。</li>\n</ul>\n<h2 id=\"7-2-通过时间反向传播的细节\"><a href=\"#7-2-通过时间反向传播的细节\" class=\"headerlink\" title=\"7.2 通过时间反向传播的细节\"></a>7.2 通过时间反向传播的细节</h2><ul>\n<li><p>如何计算目标函数相对于所有分解模型参数的梯度。为了保持简单，我们考虑一个没有偏置参数的循环神经网络，其在隐藏层中的激活函数使用恒等映射（$\\phi(x) &#x3D; x$）。对于时间步t，设单个样本的输入及其对应的标签分别为$\\mathbf{ x }_ t \\in \\mathbb{ R }^{d} $和$y_ t $。计算隐状态$ \\mathbf{ h }_ t \\in \\mathbb{ R }^{h} $和输出$ \\mathbf{ o }_ t \\in \\mathbb{ R }^{q} $的公式如下：<br>$$ \\begin{aligned} \\mathbf{ h }_ t &amp;&#x3D; \\mathbf{ W }_ { hx } \\mathbf{ x }_ t + \\mathbf{ W }_ { hh } \\mathbf{ h }_ { t-1},\\ \\mathbf{ o }_ t &amp;&#x3D; \\mathbf{ W }_ { qh } \\mathbf{ h }_ t. \\end{aligned} $$</p>\n</li>\n<li><p>其中权重参数为$\\mathbf{ W }_ { hx } \\in \\mathbb{ R }^{h \\times d} $，$\\mathbf{ W }_ { hh } \\in \\mathbb{ R }^{h \\times h} $和$\\mathbf{ W }_ { qh } \\in \\mathbb{ R }^{q \\times h} $。用$l(\\mathbf{ o }_ t, y_ t) $表示时间步t处（即从序列开始起的超过T个时间步）的损失函数，则我们的目标函数的总体损失是：<br>$$ L &#x3D; \\frac{1}{T} \\sum_ {t&#x3D;1} ^ T l(\\mathbf{ o }_ t, y_ t). $$</p>\n</li>\n<li><p>为了在循环神经网络的计算过程中可视化模型变量和参数之间的依赖关系，我们可以为模型绘制一个计算图：例如，时间步3的隐状态$ \\mathbf{ h }_ 3 $的计算依赖于模型参数$\\mathbf{ W }_ { hx } $和$\\mathbf{ W }_ { hh } $，以及最终时间步的隐状态$ \\mathbf{ h }_ 2 $和当前时间步的输入$ \\mathbf{ x }_ 3 $。<br><img src=\"/../../../../../../themes/yilia/source/img/deeplearning/code/pytorch/7_rnn/7_backward/2.png\"><br><img src=\"/img/deeplearning/code/pytorch/7_rnn/7_backward/2.png\"></p>\n</li>\n<li><p>未着色的方框表示变量，着色的方框表示参数，圆表示运算符。模型参数是：$\\mathbf{ W }_ { hx } $，$\\mathbf{ W }_ { hh } $和$\\mathbf{ W }_ { qh } $。通常，训练该模型需要对这些参数进行梯度计算：$ \\frac{\\partial L}{\\partial \\mathbf{ W }_ { hx } } $，$ \\frac{\\partial L}{\\partial \\mathbf{ W }_ { hh } } $和$ \\frac{\\partial L}{\\partial \\mathbf{ W }_ { qh } } $。根据上图的依赖关系，我们可以沿箭头的相反方向遍历计算图，依次计算和存储梯度。为了灵活地表示链式法则中不同形状的矩阵、向量和标量的乘法，我们继续使用prod运算符。</p>\n</li>\n<li><p>首先，在任意时间步t，目标函数关于模型输出的微分计算是相当简单的：<br>$$ \\frac{\\partial L}{\\partial \\mathbf{ o }_ t} &#x3D; \\frac{1}{T} \\frac{\\partial l(\\mathbf{ o }_ t, y_ t)}{\\partial \\mathbf{ o }_ t}. $$</p>\n</li>\n<li><p>现在，我们可以计算目标函数关于输出层中参数$\\mathbf{ W }_ { qh } $的梯度：$ \\frac{\\partial L}{\\partial \\mathbf{ W }_ { qh } } $。由上图可知，目标函数L通过$\\mathbf{ o }_ 1, \\ldots, \\mathbf{ o }_ T $依赖于$\\mathbf{ W }_ { qh } $。根据链式法则，我们得到：<br>$$ \\begin{aligned} \\frac{\\partial L}{\\partial \\mathbf{ W }_ { qh } } &amp;&#x3D; \\sum_ {t&#x3D;1} ^ T prod \\left( \\frac{\\partial L}{\\partial \\mathbf{ o }_ t}, \\frac{\\partial \\mathbf{ o }_ t}{\\partial \\mathbf{ W }_ { qh } } \\right) \\ &amp;&#x3D; \\sum_ {t&#x3D;1} ^ T  \\frac{\\partial L}{\\partial \\mathbf{ o }_ t} \\mathbf{ h }_ t ^ T . \\end{aligned} $$</p>\n</li>\n<li><p>其中$ \\frac{ \\partial L}{\\partial \\mathbf{ o }_ t} $已经在上面计算过了。接下来，在最后的时间步T，目标函数L仅通过$\\mathbf{ o }_ T $依赖于隐状态$\\mathbf{ h }_ T $。因此，使用链式法可以很容易地得到梯度$ \\frac{ \\partial L}{\\partial \\mathbf{ h }_ T} $：<br><span id='goto13'></span>$$ \\begin{aligned} \\frac{\\partial L}{\\partial \\mathbf{ h }_ T} &amp;&#x3D; prod \\left( \\frac{\\partial L}{\\partial \\mathbf{ o }_ T}, \\frac{\\partial \\mathbf{ o }_ T}{\\partial \\mathbf{ h }_ T} \\right) \\ &amp;&#x3D; \\mathbf{ W }_ { qh } ^ T \\frac{\\partial L}{\\partial \\mathbf{ o }_ T}. \\end{aligned} （13） $$</p>\n</li>\n<li><p>当目标函数L通过$\\mathbf{ h }_ { t+1 } $和$\\mathbf{ o }_ t $依赖于$\\mathbf{ h }_ t $时，对任意时间步t &lt; T来说都变得更加棘手。根据链式法则，隐状态的梯度$ \\frac{ \\partial L}{\\partial \\mathbf{ h }_ t} $在任何时间步骤t &lt; T时都可以递归地计算为：<br><span id='goto14'></span>$$ \\begin{aligned} \\frac{\\partial L}{\\partial \\mathbf{ h }_ t} &amp;&#x3D; prod \\left( \\frac{\\partial L}{\\partial \\mathbf{ h }_ { t+1 }}, \\frac{\\partial \\mathbf{ h }_ { t+1 }}{\\partial \\mathbf{ h }_ t} \\right) + prod \\left( \\frac{\\partial L}{\\partial \\mathbf{ o }_ t}, \\frac{\\partial \\mathbf{ o }_ t}{\\partial \\mathbf{ h }_ t} \\right) \\ &amp;&#x3D; \\mathbf{ W }_ { hh } ^ T \\frac{\\partial L}{\\partial \\mathbf{ h }_ { t+1 }} + \\mathbf{ W }_ { qh } ^ T \\frac{\\partial L}{\\partial \\mathbf{ o }_ t}. \\end{aligned} （14） $$</p>\n</li>\n<li><p>为了进行分析，对于任何时间步1 ≤ t ≤ T展开递归计算得:<br>$$ \\frac{\\partial L}{\\partial \\mathbf{ h }_ t} &#x3D; \\sum_ {i&#x3D;t} ^ T \\left( \\mathbf{ W }_ { hh } ^ T \\right) ^ {T-i} \\mathbf{ W }_ { qh } ^ T \\frac{\\partial L}{\\partial \\mathbf{ o }_ { T+t-i }}. $$</p>\n</li>\n<li><p>从上式看到，这个简单的线性例子已经展现了长序列模型的一些关键问题：它陷入到$ \\mathbf{ W }_ { hh } ^ T $的潜在的非常大的幂。在这个幂中，小于1的特征值将会消失，大于1的特征值将会发散。这在数值上是不稳定的，表现形式为梯度消失或梯度爆炸。解决此问题的一种方法是按照计算方便的需要截断时间步长的尺寸。实际上，这种截断是通过在给定数量的时间步之后分离梯度来实现的。</p>\n</li>\n<li><p>最后，上图表明：目标函数L通过隐状态$\\mathbf{ h }_ 1, \\ldots, \\mathbf{ h }_ T $依赖于隐藏层中的模型参数$\\mathbf{ W }_ { hx } $和$\\mathbf{ W }_ { hh } $。为了计算有关这些参数的梯度$ \\frac{ \\partial L}{\\partial \\mathbf{ W }_ { hx } } $和$ \\frac{ \\partial L}{\\partial \\mathbf{ W }_ { hh } } $，我们应用链式规则得：<br>$$ \\begin{aligned} \\frac{\\partial L}{\\partial \\mathbf{ W }_ { hx } } &amp;&#x3D; \\sum_ {t&#x3D;1} ^ T prod \\left( \\frac{\\partial L}{\\partial \\mathbf{ h }_ t}, \\frac{\\partial \\mathbf{ h }_ t}{\\partial \\mathbf{ W }_ { hx } } \\right) &#x3D; \\sum_ {t&#x3D;1} ^ T \\frac{\\partial L}{\\partial \\mathbf{ h }_ t} \\mathbf{ x }_ t ^ T, \\ \\frac{\\partial L}{\\partial \\mathbf{ W }_ { hh } } &amp;&#x3D; \\sum_ {t&#x3D;1} ^ T prod \\left( \\frac{\\partial L}{\\partial \\mathbf{ h }_ t}, \\frac{\\partial \\mathbf{ h }_ t}{\\partial \\mathbf{ W }_ { hh } } \\right) &#x3D; \\sum_ {t&#x3D;1} ^ T \\frac{\\partial L}{\\partial \\mathbf{ h }_ t} \\mathbf{ h }_ { t-1 } ^ T. \\end{aligned} $$</p>\n</li>\n<li><p>其中$ \\frac{ \\partial L}{\\partial \\mathbf{ h }_ t} $是由<a href=\"#goto13\">（13）</a>和<a href=\"#goto14\">（14）</a>递归计算得到的，是影响数值稳定性的关键量。</p>\n</li>\n<li><p>由于通过时间反向传播是反向传播在循环神经网络中的应用方式，所以训练循环神经网络交替使用前向传播和通过时间反向传播。通过时间反向传播依次计算并存储上述梯度。具体而言，存储的中间值会被重复使用，以避免重复计算，例如存储$ \\frac{ \\partial L}{\\partial \\mathbf{ h }_ t} $，以便在计算$ \\frac{ \\partial L}{\\partial \\mathbf{ W }_ { hx } } $和$ \\frac{ \\partial L}{\\partial \\mathbf{ W }_ { hh } } $时重复使用。</p>\n</li>\n</ul>"}],"PostAsset":[],"PostCategory":[{"post_id":"clvulgd5f00055ovw7lud76kl","category_id":"clvulgd5h00075ovw26ovfpxr","_id":"clvulgd5n000h5ovw3dgdhog6"},{"post_id":"clvulgd5i00095ovw1otn5y7u","category_id":"clvulgd5h00075ovw26ovfpxr","_id":"clvulgd5p000m5ovwei7nhl1g"},{"post_id":"clvulgd5j000a5ovw5s6w9cbx","category_id":"clvulgd5n000g5ovw3dvd4zg9","_id":"clvulgd5r000s5ovway5r0102"},{"post_id":"clvulgd5k000b5ovw5zpo42gm","category_id":"clvulgd5p000o5ovw1zz30awb","_id":"clvulgd5s00105ovw7f5s7u4w"},{"post_id":"clvulgd5m000e5ovwh89e8uhh","category_id":"clvulgd5n000g5ovw3dvd4zg9","_id":"clvulgd5u00165ovw69si8g4g"},{"post_id":"clvulgd5m000f5ovw92jebf5u","category_id":"clvulgd5n000g5ovw3dvd4zg9","_id":"clvulgd5v001b5ovw64abe4um"},{"post_id":"clvulgd5u00185ovwdgnf79h0","category_id":"clvulgd5u00155ovw5nj3asuk","_id":"clvulgd5x001h5ovw8ysn251f"},{"post_id":"clvulgd5o000j5ovw9s5o0sam","category_id":"clvulgd5u00155ovw5nj3asuk","_id":"clvulgd5y001l5ovwfvjq939c"},{"post_id":"clvulgd5v001a5ovw8qtq7tw0","category_id":"clvulgd5u00155ovw5nj3asuk","_id":"clvulgd5y001o5ovw2tmnde7j"},{"post_id":"clvulgd5v001e5ovw4y6h7sly","category_id":"clvulgd5u00155ovw5nj3asuk","_id":"clvulgd5z001r5ovwfrpogqoz"},{"post_id":"clvulgd5p000l5ovwdki71pig","category_id":"clvulgd5u00155ovw5nj3asuk","_id":"clvulgd60001u5ovw06y5dk9a"},{"post_id":"clvulgd5w001g5ovwfi7zd0sc","category_id":"clvulgd5u00155ovw5nj3asuk","_id":"clvulgd60001x5ovw262g2etd"},{"post_id":"clvulgd5x001k5ovwd7192w0p","category_id":"clvulgd5u00155ovw5nj3asuk","_id":"clvulgd6100205ovw8s4h0f34"},{"post_id":"clvulgd5q000q5ovw711kdnsv","category_id":"clvulgd5u00155ovw5nj3asuk","_id":"clvulgd6200235ovw8rwl13xp"},{"post_id":"clvulgd5y001n5ovw0o8lcnb5","category_id":"clvulgd5u00155ovw5nj3asuk","_id":"clvulgd6300265ovw9do3accu"},{"post_id":"clvulgd5z001q5ovwbsozai6g","category_id":"clvulgd5u00155ovw5nj3asuk","_id":"clvulgd64002a5ovw5utv3wgp"},{"post_id":"clvulgd5q000r5ovw922r30zr","category_id":"clvulgd5u00155ovw5nj3asuk","_id":"clvulgd65002e5ovw5rg8aelc"},{"post_id":"clvulgd5z001t5ovwasrobibp","category_id":"clvulgd5u00155ovw5nj3asuk","_id":"clvulgd66002i5ovw9q508hh4"},{"post_id":"clvulgd60001w5ovw9psv8bnw","category_id":"clvulgd5u00155ovw5nj3asuk","_id":"clvulgd66002l5ovw5lhbh9e6"},{"post_id":"clvulgd5r000v5ovwell8gto0","category_id":"clvulgd5u00155ovw5nj3asuk","_id":"clvulgd67002n5ovw0grq2h93"},{"post_id":"clvulgd61001z5ovwf8v0bsfj","category_id":"clvulgd5u00155ovw5nj3asuk","_id":"clvulgd68002r5ovw4yblfjao"},{"post_id":"clvulgd6200225ovwg1jbc0l5","category_id":"clvulgd5u00155ovw5nj3asuk","_id":"clvulgd68002u5ovw1a20hx4u"},{"post_id":"clvulgd5s000x5ovw2vjt9c6j","category_id":"clvulgd5u00155ovw5nj3asuk","_id":"clvulgd69002z5ovwf6z114bm"},{"post_id":"clvulgd6300255ovw2lu89hru","category_id":"clvulgd5u00155ovw5nj3asuk","_id":"clvulgd6a00325ovw990z1vwm"},{"post_id":"clvulgd6400295ovwf5ym0vcu","category_id":"clvulgd5u00155ovw5nj3asuk","_id":"clvulgd6a00355ovwfcns897y"},{"post_id":"clvulgd5t00125ovwd7qf9rm1","category_id":"clvulgd5u00155ovw5nj3asuk","_id":"clvulgd6b00395ovw9nleetmz"},{"post_id":"clvulgd65002d5ovwg99l48gu","category_id":"clvulgd5u00155ovw5nj3asuk","_id":"clvulgd6c003d5ovwaubwhgog"},{"post_id":"clvulgd65002h5ovw3zwq9mls","category_id":"clvulgd5u00155ovw5nj3asuk","_id":"clvulgd6c003g5ovwgr0p9iyh"},{"post_id":"clvulgd5t00145ovw4k4d9z5y","category_id":"clvulgd5u00155ovw5nj3asuk","_id":"clvulgd6d003i5ovw2tkn4pak"},{"post_id":"clvulgd66002k5ovw3oiq0s1j","category_id":"clvulgd67002o5ovw8wmy9d78","_id":"clvulgd6d003m5ovw3bic25rg"},{"post_id":"clvulgd69002x5ovwad858886","category_id":"clvulgd6d003k5ovwai2g3qxs","_id":"clvulgd6f003v5ovw8gulhg0x"},{"post_id":"clvulgd6a00315ovw6j3r5f1e","category_id":"clvulgd6d003k5ovwai2g3qxs","_id":"clvulgd6f00405ovwam5008xe"},{"post_id":"clvulgd6a00345ovwh29708fk","category_id":"clvulgd6d003k5ovwai2g3qxs","_id":"clvulgd6g00445ovwgcssa9cc"},{"post_id":"clvulgd6b00385ovwfka40h8y","category_id":"clvulgd6d003k5ovwai2g3qxs","_id":"clvulgd6g00475ovw819u9gs4"},{"post_id":"clvulgd6b003c5ovw4qm6gvjw","category_id":"clvulgd6d003k5ovwai2g3qxs","_id":"clvulgd6h004c5ovw4ivid13s"},{"post_id":"clvuli5p50000zsvwaswv0eae","category_id":"clvulgd67002o5ovw8wmy9d78","_id":"clvuli5py0002zsvwd9717uvk"},{"post_id":"clvuli5qi0003zsvwfe0a1deh","category_id":"clvulgd67002o5ovw8wmy9d78","_id":"clvuli5qk0007zsvw330u6pal"},{"post_id":"clvuli5qj0004zsvw90t2hkf5","category_id":"clvulgd67002o5ovw8wmy9d78","_id":"clvuli5qk0008zsvw7xbmheql"},{"post_id":"clvuli5r0000fzsvwbe0hgd80","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli5r3000lzsvwdyn0fsg3"},{"post_id":"clvuli5qy0009zsvw4zcaeia8","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli5r3000ozsvw43owasyj"},{"post_id":"clvuli5r1000hzsvw6a1wex7x","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli5r3000qzsvw20egeufd"},{"post_id":"clvuli5qz000azsvw34t1cfb7","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli5r3000szsvw0divciye"},{"post_id":"clvuli5r0000dzsvwf76e8bno","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli5r3000uzsvw6wz1gg12"},{"post_id":"clvuli5sv000wzsvwa9xncfth","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli5t5000zzsvw0ohh1alg"},{"post_id":"clvuli5vy0010zsvw6s0yhr6g","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli5w30013zsvw6mu5hig0"},{"post_id":"clvuli5zc0014zsvwb31setbq","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli5zf0019zsvw1iz50fj6"},{"post_id":"clvuli5zd0015zsvw74e5co72","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli5zf001bzsvwebxeawco"},{"post_id":"clvuli5ze0017zsvwevac35jc","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli5zg001dzsvw6mop2isq"},{"post_id":"clvuli60d001gzsvw8h9dgxao","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli60f001kzsvwgacq7xgp"},{"post_id":"clvuli60e001hzsvw5ydaf8j7","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli60f001mzsvw6igbbfmk"},{"post_id":"clvuli66n001ozsvwbw6kanwy","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli66p001uzsvwdd8udikl"},{"post_id":"clvuli66o001pzsvw2hg1azdc","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli66q001xzsvw422073pq"},{"post_id":"clvuli66o001rzsvwgqpsf2my","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli66r0020zsvw8msg7nuw"},{"post_id":"clvuli66p001tzsvwdh4s5r7i","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli66r0022zsvwf6ghg1qv"},{"post_id":"clvuli66q001wzsvw62c4dag9","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli66r0024zsvw4xnpcgqb"},{"post_id":"clvuli66r001zzsvweltc9e4r","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli66s0026zsvw3w2uby60"},{"post_id":"clvuli68r002czsvw4ro12gs7","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli68t002izsvw54gra31h"},{"post_id":"clvuli68r002dzsvw5mwa9j07","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli68t002lzsvwdmoh8c33"},{"post_id":"clvuli68s002fzsvwc6fyauki","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli68u002ozsvw9d9n2q4o"},{"post_id":"clvuli68s002hzsvw1fwxg3ha","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli68u002qzsvw9mn625ql"},{"post_id":"clvuli68t002kzsvw1iqd4yri","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli68u002szsvw95h6a5og"},{"post_id":"clvuli68u002nzsvwcyccalod","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli68v002uzsvw9hm2ebf9"},{"post_id":"clvuli6b10030zsvw3ikwbcd9","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli6ct0036zsvwghsfe27a"},{"post_id":"clvuli6b20031zsvw0eny14nb","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli6cu0039zsvw2isa2vtb"},{"post_id":"clvuli6b30033zsvw4le22e2f","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli6cu003czsvw0ea113c9"},{"post_id":"clvuli6ct0035zsvwf3oah1x5","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli6cv003fzsvw6yyf5p1j"},{"post_id":"clvuli6cu0038zsvw3bm36hfq","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli6cw003izsvw9bev2ojs"},{"post_id":"clvuli6cu003bzsvw3dzw5x4x","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli6cw003lzsvwags72uvs"},{"post_id":"clvuli6cv003ezsvwdbt65e74","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli6cx003ozsvwbkrq40nx"},{"post_id":"clvuli6cv003hzsvw95m997nn","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli6cx003qzsvw4j5b2ncc"},{"post_id":"clvuli6cw003kzsvw3xxbcso9","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli6cx003szsvw40hj255k"},{"post_id":"clvuli6cw003nzsvwgu6p1uru","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli6cx003uzsvwau9d3ikj"},{"post_id":"clvuli6da0044zsvw8qet92i3","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli6dn004azsvw6taj8df1"},{"post_id":"clvuli6dk0045zsvw3km77rmq","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli6do004dzsvw5qeke44g"},{"post_id":"clvuli6dm0047zsvwgzxv3z0n","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli6dp004gzsvwgpaa49s6"},{"post_id":"clvuli6dm0049zsvwgcy026hz","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli6dq004jzsvw0s5mfyoq"},{"post_id":"clvuli6dn004czsvwerumem2c","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli6dr004mzsvwfn9e4lvu"},{"post_id":"clvuli6dp004fzsvw3ra045jb","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli6ds004ozsvwbvh954k5"},{"post_id":"clvuli6dq004izsvwgu034mfn","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli6dt004szsvw3op0dch6"},{"post_id":"clvuli6dq004lzsvwafq0ey9i","category_id":"clvuli6ds004pzsvw3l15a6zm","_id":"clvuli6dt004xzsvwgcb7g9vk"},{"post_id":"clvuli6e80051zsvwd8go50hm","category_id":"clvuli6ds004pzsvw3l15a6zm","_id":"clvuli6ea0053zsvwhjv4g1s0"},{"post_id":"clvuli6hj0055zsvweonu523m","category_id":"clvulgd5n000g5ovw3dvd4zg9","_id":"clvuli6hm005czsvw1t0lca4x"},{"post_id":"clvuli6hm005ezsvw5v1a9105","category_id":"clvulgd5n000g5ovw3dvd4zg9","_id":"clvuli6hp005lzsvw5pzo67pp"},{"post_id":"clvuli6hj0056zsvwceyv70az","category_id":"clvulgd5h00075ovw26ovfpxr","_id":"clvuli6hq005ozsvwhd8nahpa"},{"post_id":"clvuli6hj0056zsvwceyv70az","category_id":"clvuli6hm005bzsvwah283iiy","_id":"clvuli6hq005rzsvwhehf8k4y"},{"post_id":"clvuli6hn005fzsvwba8z83va","category_id":"clvulgd5n000g5ovw3dvd4zg9","_id":"clvuli6hr005uzsvwhlwodkjl"},{"post_id":"clvuli6ho005izsvweu4q4cli","category_id":"clvulgd5n000g5ovw3dvd4zg9","_id":"clvuli6hr005wzsvw1c9s33kw"},{"post_id":"clvuli6hk0058zsvw5rac1qsn","category_id":"clvulgd5h00075ovw26ovfpxr","_id":"clvuli6hr005yzsvw0xg6aeol"},{"post_id":"clvuli6hk0058zsvw5rac1qsn","category_id":"clvuli6ho005gzsvw0p8k8gdi","_id":"clvuli6hr005zzsvw7foh5ltr"},{"post_id":"clvuli6hp005kzsvw7nxu3q95","category_id":"clvulgd5n000g5ovw3dvd4zg9","_id":"clvuli6hr0060zsvwcjdvctmc"},{"post_id":"clvuli6hp005nzsvwdulj0ibg","category_id":"clvulgd5n000g5ovw3dvd4zg9","_id":"clvuli6hr0061zsvwcsxwb37c"},{"post_id":"clvuli6hq005qzsvw0ix2hp5m","category_id":"clvulgd5n000g5ovw3dvd4zg9","_id":"clvuli6hr0062zsvw3wwrdo3t"},{"post_id":"clvuli6hq005tzsvwhk4x6227","category_id":"clvulgd5n000g5ovw3dvd4zg9","_id":"clvuli6hr0063zsvw55u52hmr"},{"post_id":"clvuli6ib0064zsvw0fbz1qk5","category_id":"clvuli6ds004pzsvw3l15a6zm","_id":"clvuli6id0067zsvw9fez3vhn"},{"post_id":"clvuli6ic0065zsvw92lk0opi","category_id":"clvuli6ds004pzsvw3l15a6zm","_id":"clvuli6id0068zsvwdkfgbr50"},{"post_id":"clvuli6ig006czsvw2idth514","category_id":"clvuli6ih006dzsvwbvt7aghx","_id":"clvuli6ih006gzsvw36ahdq40"},{"post_id":"clvuli6il006mzsvweutmfr3q","category_id":"clvuli6il006jzsvw6qrx5mo3","_id":"clvuli6in006szsvw13qgaoba"},{"post_id":"clvuli6ik006hzsvw889j45gw","category_id":"clvuli6il006jzsvw6qrx5mo3","_id":"clvuli6io006wzsvwh6s80wfa"},{"post_id":"clvuli6ik006izsvwez3uhi0g","category_id":"clvuli6il006jzsvw6qrx5mo3","_id":"clvuli6io006zzsvw0w3k15ha"},{"post_id":"clvuli6il006lzsvwdl9z084l","category_id":"clvuli6il006jzsvw6qrx5mo3","_id":"clvuli6io0071zsvwa9cg5hbu"},{"post_id":"clvuli6im006nzsvwhzsg7dcd","category_id":"clvuli6io006yzsvw7ciihr6b","_id":"clvuli6io0072zsvwbogn8av9"},{"post_id":"clvuli6iu0073zsvwarxqbwod","category_id":"clvuli6iv0075zsvwf73o8mtd","_id":"clvuli6m9007azsvwfpow2t8i"},{"post_id":"clvuli6iv0074zsvwhefn30gp","category_id":"clvuli6iv0075zsvwf73o8mtd","_id":"clvuli6m9007czsvw584c5hd7"},{"post_id":"clvuli6ru007izsvw6q5o4b9a","category_id":"clvuli6rt007fzsvw7avxgv78","_id":"clvuli6rw007ozsvwan7i6s1z"},{"post_id":"clvuli6rs007dzsvwbgaehcc0","category_id":"clvuli6rt007fzsvw7avxgv78","_id":"clvuli6rx007rzsvw6is19tvw"},{"post_id":"clvuli6ru007jzsvwb5sz6mmf","category_id":"clvuli6rt007fzsvw7avxgv78","_id":"clvuli6rx007szsvw8gyb6xde"},{"post_id":"clvuli6rv007mzsvw3u0sbtnt","category_id":"clvuli6rt007fzsvw7avxgv78","_id":"clvuli6rx007uzsvwczv76w74"},{"post_id":"clvuli6rt007ezsvwfmmi0kz3","category_id":"clvuli6rt007fzsvw7avxgv78","_id":"clvuli6ry007wzsvwdc9g1jzc"},{"post_id":"clvuli6rw007nzsvwc330fg0q","category_id":"clvuli6rt007fzsvw7avxgv78","_id":"clvuli6ry007zzsvw360d0oqd"},{"post_id":"clvuli6ru007hzsvw20nje1j2","category_id":"clvuli6rt007fzsvw7avxgv78","_id":"clvuli6ry0081zsvw27i9f7yn"},{"post_id":"clvuli6s4008uzsvw6l2y84hk","category_id":"clvuli6rt007fzsvw7avxgv78","_id":"clvuli6s5008xzsvwdg8t1sc7"},{"post_id":"clvuli6ux008zzsvw6ky64vxy","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli6uz0095zsvw82n3heb5"},{"post_id":"clvuli6uy0090zsvw8g1j37jp","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli6v00098zsvw1hgv3t8y"},{"post_id":"clvuli6uy0092zsvwa8jk06o1","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli6v0009bzsvw31rz7fec"},{"post_id":"clvuli6uz0094zsvw8gwm4y2j","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli6v1009ezsvwdu0z9lm5"},{"post_id":"clvuli6uz0097zsvw4rv2acyw","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli6v1009gzsvw1btyhmb7"},{"post_id":"clvuli6v0009azsvwbt70gpu3","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli6v1009izsvw4ymhdd9q"},{"post_id":"clvuli6v0009dzsvwe9fq72tk","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli6v1009kzsvwb16y12tk"},{"post_id":"clvulgd6c003f5ovwfukoh6k0","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli9bi009rzsvw594ab3wp"},{"post_id":"clvulgd6d003h5ovw7ohf656t","category_id":"clvuli5qz000bzsvw3gat9qtb","_id":"clvuli9bl009szsvwgptr9gr0"},{"post_id":"clvulgd68002t5ovw2a7590xu","category_id":"clvuli6ds004pzsvw3l15a6zm","_id":"clvuli9bo009tzsvw1pt32fbd"},{"post_id":"clvulgd67002q5ovw9c4e8eo6","category_id":"clvuli6ds004pzsvw3l15a6zm","_id":"clvuli9bp009vzsvw6w9i3fxd"},{"post_id":"clvulgd67002m5ovwdvly80ss","category_id":"clvuli6ds004pzsvw3l15a6zm","_id":"clvuli9bp009wzsvwd6k02qyb"}],"PostTag":[{"post_id":"clvulgd5f00055ovw7lud76kl","tag_id":"clvulgd5i00085ovw15enhknj","_id":"clvulgd5o000k5ovw0ano9nn9"},{"post_id":"clvulgd5f00055ovw7lud76kl","tag_id":"clvulgd5l000d5ovw1uvueim9","_id":"clvulgd5p000n5ovwaskl0j0g"},{"post_id":"clvulgd5i00095ovw1otn5y7u","tag_id":"clvulgd5i00085ovw15enhknj","_id":"clvulgd5r000w5ovw6laaemlv"},{"post_id":"clvulgd5i00095ovw1otn5y7u","tag_id":"clvulgd5l000d5ovw1uvueim9","_id":"clvulgd5s000y5ovw8af874k0"},{"post_id":"clvulgd5j000a5ovw5s6w9cbx","tag_id":"clvulgd5r000t5ovw0xyn6dxb","_id":"clvulgd5t00135ovw1gpw7mnn"},{"post_id":"clvulgd5k000b5ovw5zpo42gm","tag_id":"clvulgd5s00115ovw3k4n8xa0","_id":"clvulgd5v00195ovweetwe15f"},{"post_id":"clvulgd5m000e5ovwh89e8uhh","tag_id":"clvulgd5r000t5ovw0xyn6dxb","_id":"clvulgd5w001f5ovwgrfq96kj"},{"post_id":"clvulgd5m000f5ovw92jebf5u","tag_id":"clvulgd5r000t5ovw0xyn6dxb","_id":"clvulgd5y001m5ovw60boexeu"},{"post_id":"clvulgd5o000j5ovw9s5o0sam","tag_id":"clvulgd5x001j5ovw4ct00bav","_id":"clvulgd6300275ovwbjh44iu6"},{"post_id":"clvulgd5o000j5ovw9s5o0sam","tag_id":"clvulgd5z001s5ovwg1vfete2","_id":"clvulgd64002b5ovw7gxgc4m4"},{"post_id":"clvulgd5o000j5ovw9s5o0sam","tag_id":"clvulgd61001y5ovw35h9fuga","_id":"clvulgd65002f5ovw61z46fjr"},{"post_id":"clvulgd5p000l5ovwdki71pig","tag_id":"clvulgd5x001j5ovw4ct00bav","_id":"clvulgd68002s5ovw8hut5saa"},{"post_id":"clvulgd5p000l5ovwdki71pig","tag_id":"clvulgd5z001s5ovwg1vfete2","_id":"clvulgd68002v5ovw6i471zqx"},{"post_id":"clvulgd5p000l5ovwdki71pig","tag_id":"clvulgd66002j5ovw3nbrhze2","_id":"clvulgd6900305ovw5hk26ea0"},{"post_id":"clvulgd5q000q5ovw711kdnsv","tag_id":"clvulgd5x001j5ovw4ct00bav","_id":"clvulgd6b00375ovw5ubi60gd"},{"post_id":"clvulgd5q000q5ovw711kdnsv","tag_id":"clvulgd5z001s5ovwg1vfete2","_id":"clvulgd6b003a5ovwhm0xg4py"},{"post_id":"clvulgd5q000r5ovw922r30zr","tag_id":"clvulgd5x001j5ovw4ct00bav","_id":"clvulgd6d003l5ovwflkh9xqm"},{"post_id":"clvulgd5q000r5ovw922r30zr","tag_id":"clvulgd6b003b5ovw76hrhj0n","_id":"clvulgd6d003n5ovw6obxdf83"},{"post_id":"clvulgd5r000v5ovwell8gto0","tag_id":"clvulgd5x001j5ovw4ct00bav","_id":"clvulgd6f003w5ovwayct6gtx"},{"post_id":"clvulgd5r000v5ovwell8gto0","tag_id":"clvulgd6e003p5ovwhx6qcrwp","_id":"clvulgd6f003x5ovwgakc70xm"},{"post_id":"clvulgd5s000x5ovw2vjt9c6j","tag_id":"clvulgd5x001j5ovw4ct00bav","_id":"clvulgd6f00415ovw3cuw7xp2"},{"post_id":"clvulgd5t00125ovwd7qf9rm1","tag_id":"clvulgd5x001j5ovw4ct00bav","_id":"clvulgd6g00485ovwd57h7ih2"},{"post_id":"clvulgd5t00125ovwd7qf9rm1","tag_id":"clvulgd6f00425ovwbpeu5zrb","_id":"clvulgd6g00495ovwdopietkw"},{"post_id":"clvulgd5t00145ovw4k4d9z5y","tag_id":"clvulgd5x001j5ovw4ct00bav","_id":"clvulgd6h004h5ovw08qq6pbi"},{"post_id":"clvulgd5t00145ovw4k4d9z5y","tag_id":"clvulgd6e003p5ovwhx6qcrwp","_id":"clvulgd6h004i5ovw0rtmfo81"},{"post_id":"clvulgd5t00145ovw4k4d9z5y","tag_id":"clvulgd6f00425ovwbpeu5zrb","_id":"clvulgd6i004k5ovw290hg1nu"},{"post_id":"clvulgd5u00185ovwdgnf79h0","tag_id":"clvulgd5x001j5ovw4ct00bav","_id":"clvulgd6i004m5ovw5imm3ql6"},{"post_id":"clvulgd5u00185ovwdgnf79h0","tag_id":"clvulgd6h004j5ovw6croadtv","_id":"clvulgd6i004n5ovw2r9g0qpz"},{"post_id":"clvulgd5v001a5ovw8qtq7tw0","tag_id":"clvulgd5x001j5ovw4ct00bav","_id":"clvulgd6j004r5ovw50dybcj3"},{"post_id":"clvulgd5v001a5ovw8qtq7tw0","tag_id":"clvulgd6h004j5ovw6croadtv","_id":"clvulgd6j004s5ovw8s0308ij"},{"post_id":"clvulgd5v001a5ovw8qtq7tw0","tag_id":"clvulgd6i004p5ovw9ti77dhz","_id":"clvulgd6j004u5ovw6ne53mrc"},{"post_id":"clvulgd5v001e5ovw4y6h7sly","tag_id":"clvulgd5x001j5ovw4ct00bav","_id":"clvulgd6j004v5ovw4z9l0vu3"},{"post_id":"clvulgd5w001g5ovwfi7zd0sc","tag_id":"clvulgd5x001j5ovw4ct00bav","_id":"clvulgd6k004z5ovw3b9fgt5s"},{"post_id":"clvulgd5w001g5ovwfi7zd0sc","tag_id":"clvulgd6h004j5ovw6croadtv","_id":"clvulgd6k00505ovw81maang5"},{"post_id":"clvulgd5w001g5ovwfi7zd0sc","tag_id":"clvulgd6j004x5ovwbr1g45up","_id":"clvulgd6k00525ovw2budcijm"},{"post_id":"clvulgd5x001k5ovwd7192w0p","tag_id":"clvulgd5x001j5ovw4ct00bav","_id":"clvulgd6k00535ovw69vs8jjs"},{"post_id":"clvulgd5y001n5ovw0o8lcnb5","tag_id":"clvulgd5x001j5ovw4ct00bav","_id":"clvulgd6k00565ovwckqq8nyh"},{"post_id":"clvulgd5y001n5ovw0o8lcnb5","tag_id":"clvulgd6h004j5ovw6croadtv","_id":"clvulgd6k00575ovwhilg7sjs"},{"post_id":"clvulgd5y001n5ovw0o8lcnb5","tag_id":"clvulgd6k00545ovw6uu0h7v2","_id":"clvulgd6l00595ovwaeel4b0t"},{"post_id":"clvulgd5z001q5ovwbsozai6g","tag_id":"clvulgd5x001j5ovw4ct00bav","_id":"clvulgd6l005a5ovwgzk676ic"},{"post_id":"clvulgd5z001q5ovwbsozai6g","tag_id":"clvulgd6k00555ovwgsvq5qln","_id":"clvulgd6l005c5ovw3suhgck3"},{"post_id":"clvulgd5z001t5ovwasrobibp","tag_id":"clvulgd5x001j5ovw4ct00bav","_id":"clvulgd6l005d5ovw2r13baxk"},{"post_id":"clvulgd5z001t5ovwasrobibp","tag_id":"clvulgd6k00555ovwgsvq5qln","_id":"clvulgd6l005f5ovw6ll99noz"},{"post_id":"clvulgd60001w5ovw9psv8bnw","tag_id":"clvulgd5x001j5ovw4ct00bav","_id":"clvulgd6l005h5ovw3843etej"},{"post_id":"clvulgd60001w5ovw9psv8bnw","tag_id":"clvulgd6k00555ovwgsvq5qln","_id":"clvulgd6m005i5ovw0svtclew"},{"post_id":"clvulgd60001w5ovw9psv8bnw","tag_id":"clvulgd6l005e5ovwguehb1n6","_id":"clvulgd6m005k5ovwbaut5od0"},{"post_id":"clvulgd61001z5ovwf8v0bsfj","tag_id":"clvulgd5x001j5ovw4ct00bav","_id":"clvulgd6m005l5ovwa7qy13qu"},{"post_id":"clvulgd61001z5ovwf8v0bsfj","tag_id":"clvulgd6l005g5ovw5xav7pct","_id":"clvulgd6m005n5ovweuwhax1x"},{"post_id":"clvulgd6200225ovwg1jbc0l5","tag_id":"clvulgd5x001j5ovw4ct00bav","_id":"clvulgd6m005p5ovwbv7i8mtp"},{"post_id":"clvulgd6200225ovwg1jbc0l5","tag_id":"clvulgd6k00555ovwgsvq5qln","_id":"clvulgd6n005q5ovw1pmy7llc"},{"post_id":"clvulgd6200225ovwg1jbc0l5","tag_id":"clvulgd6m005m5ovw5ft06mgv","_id":"clvulgd6n005s5ovw6og81xk8"},{"post_id":"clvulgd6300255ovw2lu89hru","tag_id":"clvulgd5x001j5ovw4ct00bav","_id":"clvulgd6n005u5ovwdpi69kf8"},{"post_id":"clvulgd6300255ovw2lu89hru","tag_id":"clvulgd6m005o5ovw38re8h0d","_id":"clvulgd6n005v5ovw8m52evow"},{"post_id":"clvulgd6300255ovw2lu89hru","tag_id":"clvulgd6n005r5ovw05qih89n","_id":"clvulgd6n005x5ovw763zhgip"},{"post_id":"clvulgd6400295ovwf5ym0vcu","tag_id":"clvulgd5x001j5ovw4ct00bav","_id":"clvulgd6o005z5ovw68gfe7b6"},{"post_id":"clvulgd6400295ovwf5ym0vcu","tag_id":"clvulgd6m005o5ovw38re8h0d","_id":"clvulgd6o00605ovw04rff7l0"},{"post_id":"clvulgd6400295ovwf5ym0vcu","tag_id":"clvulgd6n005w5ovw81u484aq","_id":"clvulgd6o00625ovw9tu67u1d"},{"post_id":"clvulgd65002d5ovwg99l48gu","tag_id":"clvulgd5x001j5ovw4ct00bav","_id":"clvulgd6o00635ovw2zvp9nqj"},{"post_id":"clvulgd65002d5ovwg99l48gu","tag_id":"clvulgd6m005o5ovw38re8h0d","_id":"clvulgd6o00655ovwf4j94fpu"},{"post_id":"clvulgd65002h5ovw3zwq9mls","tag_id":"clvulgd5x001j5ovw4ct00bav","_id":"clvulgd6o00665ovwdp4x2yp7"},{"post_id":"clvulgd65002h5ovw3zwq9mls","tag_id":"clvulgd6m005o5ovw38re8h0d","_id":"clvulgd6o00685ovw98f5a8cm"},{"post_id":"clvulgd66002k5ovw3oiq0s1j","tag_id":"clvulgd6o00645ovw0wbv050c","_id":"clvulgd6p00695ovw1g74gz5z"},{"post_id":"clvulgd67002m5ovwdvly80ss","tag_id":"clvulgd6o00675ovw7ryjehw0","_id":"clvulgd6p006b5ovw1z6p8jpq"},{"post_id":"clvulgd68002t5ovw2a7590xu","tag_id":"clvulgd6p006e5ovwdsynb17q","_id":"clvulgd6q006h5ovw277d8vk2"},{"post_id":"clvulgd69002x5ovwad858886","tag_id":"clvulgd6p006f5ovwa97c0ldv","_id":"clvulgd6q006l5ovwchglb2ph"},{"post_id":"clvulgd69002x5ovwad858886","tag_id":"clvulgd6q006i5ovwheli4zcv","_id":"clvulgd6q006m5ovw4ovd563f"},{"post_id":"clvulgd69002x5ovwad858886","tag_id":"clvulgd6q006j5ovw9ky88rmx","_id":"clvulgd6r006o5ovw2v5dflqu"},{"post_id":"clvulgd6a00315ovw6j3r5f1e","tag_id":"clvulgd6p006f5ovwa97c0ldv","_id":"clvulgd6r006r5ovwf9czajne"},{"post_id":"clvulgd6a00315ovw6j3r5f1e","tag_id":"clvulgd6q006i5ovwheli4zcv","_id":"clvulgd6r006s5ovwctwe7mkn"},{"post_id":"clvulgd6a00315ovw6j3r5f1e","tag_id":"clvulgd6q006j5ovw9ky88rmx","_id":"clvulgd6r006u5ovw2lj5c4jc"},{"post_id":"clvulgd6a00345ovwh29708fk","tag_id":"clvulgd6p006f5ovwa97c0ldv","_id":"clvulgd6s006x5ovw54nf189a"},{"post_id":"clvulgd6a00345ovwh29708fk","tag_id":"clvulgd6q006i5ovwheli4zcv","_id":"clvulgd6s006y5ovw68pudfif"},{"post_id":"clvulgd6a00345ovwh29708fk","tag_id":"clvulgd6q006j5ovw9ky88rmx","_id":"clvulgd6s00705ovw9v63do2d"},{"post_id":"clvulgd6b00385ovwfka40h8y","tag_id":"clvulgd6p006f5ovwa97c0ldv","_id":"clvulgd6t00735ovw4igo62xd"},{"post_id":"clvulgd6b00385ovwfka40h8y","tag_id":"clvulgd6q006i5ovwheli4zcv","_id":"clvulgd6t00745ovwaxzm09kr"},{"post_id":"clvulgd6b00385ovwfka40h8y","tag_id":"clvulgd6q006j5ovw9ky88rmx","_id":"clvulgd6t00765ovw6vx68rmy"},{"post_id":"clvulgd6b003c5ovw4qm6gvjw","tag_id":"clvulgd6p006f5ovwa97c0ldv","_id":"clvulgd6u00795ovw9l8g02jc"},{"post_id":"clvulgd6b003c5ovw4qm6gvjw","tag_id":"clvulgd6q006i5ovwheli4zcv","_id":"clvulgd6u007a5ovw1sap6my4"},{"post_id":"clvulgd6b003c5ovw4qm6gvjw","tag_id":"clvulgd6q006j5ovw9ky88rmx","_id":"clvulgd6u007c5ovwcuyq3s1p"},{"post_id":"clvulgd6c003f5ovwfukoh6k0","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvulgd6u007g5ovwbv431jdm"},{"post_id":"clvulgd6c003f5ovwfukoh6k0","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvulgd6v007i5ovw12ba2opd"},{"post_id":"clvulgd6d003h5ovw7ohf656t","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvulgd6v007m5ovw2fa7fcws"},{"post_id":"clvulgd6d003h5ovw7ohf656t","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvulgd6v007n5ovw6y0tgn3o"},{"post_id":"clvuli5p50000zsvwaswv0eae","tag_id":"clvulgd6o00645ovw0wbv050c","_id":"clvuli5p70001zsvw3vbw6xky"},{"post_id":"clvuli5qi0003zsvwfe0a1deh","tag_id":"clvulgd6o00645ovw0wbv050c","_id":"clvuli5qk0005zsvw6p2rfga7"},{"post_id":"clvuli5qj0004zsvw90t2hkf5","tag_id":"clvulgd6o00645ovw0wbv050c","_id":"clvuli5qk0006zsvw9wojg6ba"},{"post_id":"clvuli5qy0009zsvw4zcaeia8","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli5qz000czsvw519hh3ar"},{"post_id":"clvuli5qy0009zsvw4zcaeia8","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli5r0000ezsvwbwe25lir"},{"post_id":"clvuli5qz000azsvw34t1cfb7","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli5r1000gzsvw7e0ycnry"},{"post_id":"clvuli5qz000azsvw34t1cfb7","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli5r2000jzsvw7lewc6qt"},{"post_id":"clvuli5r0000dzsvwf76e8bno","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli5r2000kzsvw0fr53f2b"},{"post_id":"clvuli5r0000dzsvwf76e8bno","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli5r3000nzsvw5mvse1h0"},{"post_id":"clvuli5r0000fzsvwbe0hgd80","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli5r3000pzsvw54q4d5ks"},{"post_id":"clvuli5r0000fzsvwbe0hgd80","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli5r3000rzsvw5dbq8vo1"},{"post_id":"clvuli5r1000hzsvw6a1wex7x","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli5r3000tzsvw1fyjaqfq"},{"post_id":"clvuli5r1000hzsvw6a1wex7x","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli5r3000vzsvwfl142izg"},{"post_id":"clvuli5sv000wzsvwa9xncfth","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli5sx000xzsvw6q8m90qi"},{"post_id":"clvuli5sv000wzsvwa9xncfth","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli5t5000yzsvw0gzt4a9e"},{"post_id":"clvuli5vy0010zsvw6s0yhr6g","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli5vz0011zsvw8q2t6xi2"},{"post_id":"clvuli5vy0010zsvw6s0yhr6g","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli5w30012zsvw36m12rw2"},{"post_id":"clvuli5zc0014zsvwb31setbq","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli5ze0016zsvw8sjv3vea"},{"post_id":"clvuli5zc0014zsvwb31setbq","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli5ze0018zsvw2mtfghvp"},{"post_id":"clvuli5zd0015zsvw74e5co72","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli5zf001azsvwe25d3n3e"},{"post_id":"clvuli5zd0015zsvw74e5co72","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli5zg001czsvwhpnm2nbt"},{"post_id":"clvuli5ze0017zsvwevac35jc","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli5zg001ezsvwdtz64c1s"},{"post_id":"clvuli5ze0017zsvwevac35jc","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli5zh001fzsvw0r5kgyhr"},{"post_id":"clvuli60d001gzsvw8h9dgxao","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli60f001izsvwf13i5p8s"},{"post_id":"clvuli60d001gzsvw8h9dgxao","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli60f001jzsvw5pnxb0gw"},{"post_id":"clvuli60e001hzsvw5ydaf8j7","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli60f001lzsvw1qfr2bb7"},{"post_id":"clvuli60e001hzsvw5ydaf8j7","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli60f001nzsvwflhq1841"},{"post_id":"clvuli66n001ozsvwbw6kanwy","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli66o001qzsvw40uk4jml"},{"post_id":"clvuli66n001ozsvwbw6kanwy","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli66p001szsvw432of8mc"},{"post_id":"clvuli66o001pzsvw2hg1azdc","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli66q001vzsvwbtz918is"},{"post_id":"clvuli66o001pzsvw2hg1azdc","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli66r001yzsvweb7dbsrw"},{"post_id":"clvuli66o001rzsvwgqpsf2my","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli66r0021zsvwaoj0as6z"},{"post_id":"clvuli66o001rzsvwgqpsf2my","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli66r0023zsvwhpg2c2sl"},{"post_id":"clvuli66p001tzsvwdh4s5r7i","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli66s0025zsvwg0nqf610"},{"post_id":"clvuli66p001tzsvwdh4s5r7i","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli66s0027zsvw538tfchl"},{"post_id":"clvuli66q001wzsvw62c4dag9","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli66s0028zsvwglgzf3mh"},{"post_id":"clvuli66q001wzsvw62c4dag9","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli66s0029zsvwh9k7glcp"},{"post_id":"clvuli66r001zzsvweltc9e4r","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli66s002azsvwbpta3miu"},{"post_id":"clvuli66r001zzsvweltc9e4r","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli66s002bzsvw5rffhkr3"},{"post_id":"clvuli68r002czsvw4ro12gs7","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli68s002ezsvwgqz738fo"},{"post_id":"clvuli68r002czsvw4ro12gs7","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli68s002gzsvwed6p1i1z"},{"post_id":"clvuli68r002dzsvw5mwa9j07","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli68t002jzsvw1p2792qd"},{"post_id":"clvuli68r002dzsvw5mwa9j07","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli68u002mzsvw0z5bd6vo"},{"post_id":"clvuli68s002fzsvwc6fyauki","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli68u002pzsvw2g4s1tdm"},{"post_id":"clvuli68s002fzsvwc6fyauki","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli68u002rzsvwd6by330m"},{"post_id":"clvuli68s002hzsvw1fwxg3ha","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli68v002tzsvwbletgzco"},{"post_id":"clvuli68s002hzsvw1fwxg3ha","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli68v002vzsvwcgn4cqz7"},{"post_id":"clvuli68t002kzsvw1iqd4yri","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli68v002wzsvwbhoy0enz"},{"post_id":"clvuli68t002kzsvw1iqd4yri","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli68v002xzsvw1xup5do7"},{"post_id":"clvuli68u002nzsvwcyccalod","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli68v002yzsvwgrk7fski"},{"post_id":"clvuli68u002nzsvwcyccalod","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli68v002zzsvwerc63q6m"},{"post_id":"clvuli6b10030zsvw3ikwbcd9","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli6b20032zsvw79m44t00"},{"post_id":"clvuli6b10030zsvw3ikwbcd9","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli6ct0034zsvwepop2qjd"},{"post_id":"clvuli6b20031zsvw0eny14nb","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli6ct0037zsvw7ysye0ez"},{"post_id":"clvuli6b20031zsvw0eny14nb","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli6cu003azsvwb7g4evt7"},{"post_id":"clvuli6b30033zsvw4le22e2f","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli6cv003dzsvwbm16fk0k"},{"post_id":"clvuli6b30033zsvw4le22e2f","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli6cv003gzsvw0ois01on"},{"post_id":"clvuli6ct0035zsvwf3oah1x5","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli6cw003jzsvwcqn466li"},{"post_id":"clvuli6ct0035zsvwf3oah1x5","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli6cw003mzsvwat5xbmbg"},{"post_id":"clvuli6cu0038zsvw3bm36hfq","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli6cx003pzsvw8og2dz4b"},{"post_id":"clvuli6cu0038zsvw3bm36hfq","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli6cx003rzsvwf0hb0p5w"},{"post_id":"clvuli6cu003bzsvw3dzw5x4x","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli6cx003tzsvwgfox1khw"},{"post_id":"clvuli6cu003bzsvw3dzw5x4x","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli6cx003vzsvw4oxp13ik"},{"post_id":"clvuli6cv003ezsvwdbt65e74","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli6cx003wzsvw7f1f4vp8"},{"post_id":"clvuli6cv003ezsvwdbt65e74","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli6cx003xzsvwdzdu39ac"},{"post_id":"clvuli6cv003hzsvw95m997nn","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli6cx003yzsvw4g5mg6c1"},{"post_id":"clvuli6cv003hzsvw95m997nn","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli6cx003zzsvw1vpc243c"},{"post_id":"clvuli6cw003kzsvw3xxbcso9","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli6cx0040zsvwf4z0arhn"},{"post_id":"clvuli6cw003kzsvw3xxbcso9","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli6cx0041zsvwgqpy6tsf"},{"post_id":"clvuli6cw003nzsvwgu6p1uru","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli6cx0042zsvwdnhb3qqe"},{"post_id":"clvuli6cw003nzsvwgu6p1uru","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli6cx0043zsvwf8mw1fw7"},{"post_id":"clvuli6da0044zsvw8qet92i3","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli6dl0046zsvwhmg15wnn"},{"post_id":"clvuli6da0044zsvw8qet92i3","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli6dm0048zsvwb4bie1wi"},{"post_id":"clvuli6dk0045zsvw3km77rmq","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli6dn004bzsvw3b0haxmf"},{"post_id":"clvuli6dk0045zsvw3km77rmq","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli6do004ezsvw3ibi448q"},{"post_id":"clvuli6dm0047zsvwgzxv3z0n","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli6dp004hzsvw72pa07vx"},{"post_id":"clvuli6dm0047zsvwgzxv3z0n","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli6dq004kzsvw7i7h02a7"},{"post_id":"clvuli6dm0049zsvwgcy026hz","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli6dr004nzsvwc304ffnf"},{"post_id":"clvuli6dm0049zsvwgcy026hz","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli6ds004rzsvwgtri3m31"},{"post_id":"clvuli6dn004czsvwerumem2c","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli6dt004tzsvw7dx75623"},{"post_id":"clvuli6dn004czsvwerumem2c","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli6dt004uzsvw4ia08uge"},{"post_id":"clvuli6dp004fzsvw3ra045jb","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli6dt004vzsvw6sw4gx1w"},{"post_id":"clvuli6dp004fzsvw3ra045jb","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli6dt004wzsvw2oygfew6"},{"post_id":"clvuli6dq004izsvwgu034mfn","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli6dt004yzsvwbmzx1z9f"},{"post_id":"clvuli6dq004izsvwgu034mfn","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli6dt004zzsvw7i1oh3gz"},{"post_id":"clvuli6dq004lzsvwafq0ey9i","tag_id":"clvuli6ds004qzsvwazqi4sds","_id":"clvuli6dt0050zsvwg7m999wp"},{"post_id":"clvuli6e80051zsvwd8go50hm","tag_id":"clvuli6ea0052zsvwcsmw6vy9","_id":"clvuli6ea0054zsvwgz4w5a6k"},{"post_id":"clvuli6hj0055zsvweonu523m","tag_id":"clvulgd5r000t5ovw0xyn6dxb","_id":"clvuli6hk0057zsvw7jkee66a"},{"post_id":"clvuli6hj0056zsvwceyv70az","tag_id":"clvulgd5i00085ovw15enhknj","_id":"clvuli6hl0059zsvwc90k2nyz"},{"post_id":"clvuli6hk0058zsvw5rac1qsn","tag_id":"clvulgd5i00085ovw15enhknj","_id":"clvuli6hm005dzsvwdlow6fzt"},{"post_id":"clvuli6hm005ezsvw5v1a9105","tag_id":"clvulgd5r000t5ovw0xyn6dxb","_id":"clvuli6ho005hzsvwc5glgxvb"},{"post_id":"clvuli6hn005fzsvwba8z83va","tag_id":"clvulgd5r000t5ovw0xyn6dxb","_id":"clvuli6ho005jzsvw303c4syz"},{"post_id":"clvuli6ho005izsvweu4q4cli","tag_id":"clvulgd5r000t5ovw0xyn6dxb","_id":"clvuli6hp005mzsvwglgp988w"},{"post_id":"clvuli6hp005kzsvw7nxu3q95","tag_id":"clvulgd5r000t5ovw0xyn6dxb","_id":"clvuli6hq005pzsvw2ylnhbdc"},{"post_id":"clvuli6hp005nzsvwdulj0ibg","tag_id":"clvulgd5r000t5ovw0xyn6dxb","_id":"clvuli6hq005szsvw444y90lu"},{"post_id":"clvuli6hq005qzsvw0ix2hp5m","tag_id":"clvulgd5r000t5ovw0xyn6dxb","_id":"clvuli6hr005vzsvw1eoa3k4f"},{"post_id":"clvuli6hq005tzsvwhk4x6227","tag_id":"clvulgd5r000t5ovw0xyn6dxb","_id":"clvuli6hr005xzsvw48h05emn"},{"post_id":"clvuli6ib0064zsvw0fbz1qk5","tag_id":"clvuli6id0066zsvw5t974w0d","_id":"clvuli6id006azsvw6add7hbw"},{"post_id":"clvuli6ic0065zsvw92lk0opi","tag_id":"clvuli6id0069zsvw78o284kn","_id":"clvuli6id006bzsvwfwrr19dt"},{"post_id":"clvuli6ig006czsvw2idth514","tag_id":"clvuli6ih006ezsvw82y7axzm","_id":"clvuli6ih006fzsvw5z8403e8"},{"post_id":"clvuli6il006mzsvweutmfr3q","tag_id":"clvuli6il006kzsvwfzo4a817","_id":"clvuli6in006qzsvw6qqdhydt"},{"post_id":"clvuli6ik006hzsvw889j45gw","tag_id":"clvuli6il006kzsvwfzo4a817","_id":"clvuli6in006rzsvw8526ei09"},{"post_id":"clvuli6im006nzsvwhzsg7dcd","tag_id":"clvuli6il006kzsvwfzo4a817","_id":"clvuli6io006vzsvwcj46fj6j"},{"post_id":"clvuli6ik006izsvwez3uhi0g","tag_id":"clvuli6il006kzsvwfzo4a817","_id":"clvuli6io006xzsvw8zqyfqio"},{"post_id":"clvuli6il006lzsvwdl9z084l","tag_id":"clvuli6il006kzsvwfzo4a817","_id":"clvuli6io0070zsvwdsmsa553"},{"post_id":"clvuli6iu0073zsvwarxqbwod","tag_id":"clvuli6iv0076zsvw9wwn9ilb","_id":"clvuli6m90079zsvwazln1dzr"},{"post_id":"clvuli6iv0074zsvwhefn30gp","tag_id":"clvuli6iv0076zsvw9wwn9ilb","_id":"clvuli6m9007bzsvw5ehnfqiv"},{"post_id":"clvuli6rs007dzsvwbgaehcc0","tag_id":"clvuli6rt007gzsvweuhxe2wc","_id":"clvuli6rx007vzsvwdzj1gwjx"},{"post_id":"clvuli6rs007dzsvwbgaehcc0","tag_id":"clvuli6rv007lzsvw7az5egpx","_id":"clvuli6ry007xzsvwfvrf8ksy"},{"post_id":"clvuli6rs007dzsvwbgaehcc0","tag_id":"clvuli6rw007qzsvwb731augi","_id":"clvuli6ry0080zsvwfk9af9x7"},{"post_id":"clvuli6rt007ezsvwfmmi0kz3","tag_id":"clvuli6rt007gzsvweuhxe2wc","_id":"clvuli6ry0084zsvwcnfvaj28"},{"post_id":"clvuli6rt007ezsvwfmmi0kz3","tag_id":"clvuli6rv007lzsvw7az5egpx","_id":"clvuli6ry0085zsvwc8nl2auf"},{"post_id":"clvuli6rt007ezsvwfmmi0kz3","tag_id":"clvuli6rw007qzsvwb731augi","_id":"clvuli6rz0087zsvw773p03jk"},{"post_id":"clvuli6ru007hzsvw20nje1j2","tag_id":"clvuli6rt007gzsvweuhxe2wc","_id":"clvuli6rz008azsvw1ku53o5n"},{"post_id":"clvuli6ru007hzsvw20nje1j2","tag_id":"clvuli6rv007lzsvw7az5egpx","_id":"clvuli6rz008bzsvw0rvlacr6"},{"post_id":"clvuli6ru007hzsvw20nje1j2","tag_id":"clvuli6rw007qzsvwb731augi","_id":"clvuli6s0008dzsvwgsvxg7bo"},{"post_id":"clvuli6ru007izsvw6q5o4b9a","tag_id":"clvuli6rt007gzsvweuhxe2wc","_id":"clvuli6s0008fzsvwgaxkd1p5"},{"post_id":"clvuli6ru007izsvw6q5o4b9a","tag_id":"clvuli6rv007lzsvw7az5egpx","_id":"clvuli6s0008gzsvwbid21h2m"},{"post_id":"clvuli6ru007izsvw6q5o4b9a","tag_id":"clvuli6rw007qzsvwb731augi","_id":"clvuli6s0008izsvw98j2h3eg"},{"post_id":"clvuli6ru007jzsvwb5sz6mmf","tag_id":"clvuli6rt007gzsvweuhxe2wc","_id":"clvuli6s1008kzsvw8lq1dcgy"},{"post_id":"clvuli6ru007jzsvwb5sz6mmf","tag_id":"clvuli6rv007lzsvw7az5egpx","_id":"clvuli6s1008lzsvwafqfeegt"},{"post_id":"clvuli6ru007jzsvwb5sz6mmf","tag_id":"clvuli6rw007qzsvwb731augi","_id":"clvuli6s1008nzsvw3m9d7dof"},{"post_id":"clvuli6rv007mzsvw3u0sbtnt","tag_id":"clvuli6rt007gzsvweuhxe2wc","_id":"clvuli6s1008ozsvwghgqcr00"},{"post_id":"clvuli6rv007mzsvw3u0sbtnt","tag_id":"clvuli6rv007lzsvw7az5egpx","_id":"clvuli6s1008pzsvw5qkdeaw0"},{"post_id":"clvuli6rv007mzsvw3u0sbtnt","tag_id":"clvuli6rw007qzsvwb731augi","_id":"clvuli6s1008qzsvw8w4x0jte"},{"post_id":"clvuli6rw007nzsvwc330fg0q","tag_id":"clvuli6rt007gzsvweuhxe2wc","_id":"clvuli6s1008rzsvw9jit65nh"},{"post_id":"clvuli6rw007nzsvwc330fg0q","tag_id":"clvuli6rv007lzsvw7az5egpx","_id":"clvuli6s1008szsvwhh8bh1om"},{"post_id":"clvuli6rw007nzsvwc330fg0q","tag_id":"clvuli6rw007qzsvwb731augi","_id":"clvuli6s1008tzsvwasv45akk"},{"post_id":"clvuli6s4008uzsvw6l2y84hk","tag_id":"clvuli6rt007gzsvweuhxe2wc","_id":"clvuli6s5008vzsvw5r5i9g8y"},{"post_id":"clvuli6s4008uzsvw6l2y84hk","tag_id":"clvuli6rv007lzsvw7az5egpx","_id":"clvuli6s5008wzsvw4e5ncoqk"},{"post_id":"clvuli6s4008uzsvw6l2y84hk","tag_id":"clvuli6rw007qzsvwb731augi","_id":"clvuli6s5008yzsvw69j42nbm"},{"post_id":"clvuli6ux008zzsvw6ky64vxy","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli6uy0091zsvw2bgca4sz"},{"post_id":"clvuli6ux008zzsvw6ky64vxy","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli6uz0093zsvwdk2g7sdl"},{"post_id":"clvuli6uy0090zsvw8g1j37jp","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli6uz0096zsvwb677dgg1"},{"post_id":"clvuli6uy0090zsvw8g1j37jp","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli6v00099zsvwdez0dpfy"},{"post_id":"clvuli6uy0092zsvwa8jk06o1","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli6v0009czsvwadppfqfg"},{"post_id":"clvuli6uy0092zsvwa8jk06o1","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli6v1009fzsvwg3ip1rvf"},{"post_id":"clvuli6uz0094zsvw8gwm4y2j","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli6v1009hzsvw3ajiezg3"},{"post_id":"clvuli6uz0094zsvw8gwm4y2j","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli6v1009jzsvw0mhd0zg5"},{"post_id":"clvuli6uz0097zsvw4rv2acyw","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli6v1009lzsvw2ezygxkg"},{"post_id":"clvuli6uz0097zsvw4rv2acyw","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli6v1009mzsvwhjeqd343"},{"post_id":"clvuli6v0009azsvwbt70gpu3","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli6v1009nzsvw1ykq6i8q"},{"post_id":"clvuli6v0009azsvwbt70gpu3","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli6v1009ozsvw0xoodqiw"},{"post_id":"clvuli6v0009dzsvwe9fq72tk","tag_id":"clvulgd6u007b5ovw63at4v8q","_id":"clvuli6v1009pzsvwc17s2wu2"},{"post_id":"clvuli6v0009dzsvwe9fq72tk","tag_id":"clvulgd6u007d5ovw1uqr3avw","_id":"clvuli6v2009qzsvw0o3d7l22"},{"post_id":"clvulgd67002q5ovw9c4e8eo6","tag_id":"clvuli9bo009uzsvw3hz28tqz","_id":"clvuli9bp009xzsvw3c90e99o"}],"Tag":[{"name":"博客","_id":"clvulgd5i00085ovw15enhknj"},{"name":"hexo","_id":"clvulgd5l000d5ovw1uvueim9"},{"name":"Linux","_id":"clvulgd5r000t5ovw0xyn6dxb"},{"name":"微信小程序","_id":"clvulgd5s00115ovw3k4n8xa0"},{"name":"数据结构","_id":"clvulgd5x001j5ovw4ct00bav"},{"name":"线性表","_id":"clvulgd5z001s5ovwg1vfete2"},{"name":"双链表","_id":"clvulgd61001y5ovw35h9fuga"},{"name":"单链表","_id":"clvulgd66002j5ovw3nbrhze2"},{"name":"串","_id":"clvulgd6b003b5ovw76hrhj0n"},{"name":"栈","_id":"clvulgd6e003p5ovwhx6qcrwp"},{"name":"队列","_id":"clvulgd6f00425ovwbpeu5zrb"},{"name":"查找","_id":"clvulgd6h004j5ovw6croadtv"},{"name":"二叉排序树","_id":"clvulgd6i004p5ovw9ti77dhz"},{"name":"平衡二叉树","_id":"clvulgd6j004x5ovwbr1g45up"},{"name":"B树","_id":"clvulgd6k00545ovw6uu0h7v2"},{"name":"树","_id":"clvulgd6k00555ovwgsvq5qln"},{"name":"哈夫曼树","_id":"clvulgd6l005e5ovwguehb1n6"},{"name":"并查集","_id":"clvulgd6l005g5ovw5xav7pct"},{"name":"二叉树","_id":"clvulgd6m005m5ovw5ft06mgv"},{"name":"图","_id":"clvulgd6m005o5ovw38re8h0d"},{"name":"最小生成树","_id":"clvulgd6n005r5ovw05qih89n"},{"name":"最短路径","_id":"clvulgd6n005w5ovw81u484aq"},{"name":"算法，排序","_id":"clvulgd6o00645ovw0wbv050c"},{"name":"vscode","_id":"clvulgd6o00675ovw7ryjehw0"},{"name":"其他","_id":"clvulgd6p006a5ovw41pg7d4r"},{"name":"pycharm","_id":"clvulgd6p006e5ovwdsynb17q"},{"name":"深度学习","_id":"clvulgd6p006f5ovwa97c0ldv"},{"name":"论文","_id":"clvulgd6q006i5ovwheli4zcv"},{"name":"超分","_id":"clvulgd6q006j5ovw9ky88rmx"},{"name":"机器学习","_id":"clvulgd6u007b5ovw63at4v8q"},{"name":"pytorch","_id":"clvulgd6u007d5ovw1uqr3avw"},{"name":"数据预处理","_id":"clvulgd6v007k5ovwgv5m3gko"},{"name":"鼠标","_id":"clvuli6ds004qzsvwazqi4sds"},{"name":"经验","_id":"clvuli6ea0052zsvwcsmw6vy9"},{"name":"kaggle","_id":"clvuli6id0066zsvw5t974w0d"},{"name":"键盘","_id":"clvuli6id0069zsvw78o284kn"},{"name":"安装","_id":"clvuli6ih006ezsvw82y7axzm"},{"name":"python","_id":"clvuli6il006kzsvwfzo4a817"},{"name":"windows","_id":"clvuli6iv0076zsvw9wwn9ilb"},{"name":"生产实习","_id":"clvuli6rt007gzsvweuhxe2wc"},{"name":"java","_id":"clvuli6rv007lzsvw7az5egpx"},{"name":"springboot","_id":"clvuli6rw007qzsvwb731augi"},{"name":"Windows","_id":"clvuli9bo009uzsvw3hz28tqz"}]}}