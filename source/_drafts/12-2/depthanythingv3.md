- 单个普通变压器（例如，vanilla DINO 编码器）足以作为主干，无需架构专门化；单个深度射线预测目标消除了复杂的多任务学习的需要。通过我们的师生培训范例，该模型达到了与 Depth Anything 2 (DA2) 相当的细节和概括水平。
- 虽然最近的努力[91, 97]已经探索了统一模型来同时处理多个任务，但它们通常受到关键限制：它们通常依赖于复杂的定制架构，通过从头开始对任务进行联合优化进行训练，因此无法有效利用大规模预训练模型。 
- 从任意视觉输入（无论是单个图像、场景的多个视图还是视频流）恢复 3D 结构。
- 我们放弃了复杂的建筑工程，追求以两个核心问题为指导的最小建模策略。首先，是否存在最小的预测目标集，或者是否需要跨众多 3D 任务进行联合建模？二、单个普通变压器可以吗
- 通过专门选择的光线表示进行联合任意视图深度和姿态估计的训练。我们证明，这种最小的方法足以从任意数量的图像重建视觉空间，无论是否有已知的相机姿势。
- Depth Anything 3 将上述几何重建目标制定为密集预测任务。对于给定的 N 个输入图像集，模型被训练为输出 N 个相应的深度图和光线图，每个像素与其各自的输入对齐。实现这一目标的架构始于标准的预训练视觉转换器（例如，Oquab 等人 61）作为其骨干，利用其强大的特征提取功能。
	- 为了处理任意视图计数，我们引入了一个关键修改：输入自适应跨视图自注意力机制。该模块在选定层的前向传递过程中动态地重新排列令牌，从而实现跨所有视图的高效信息交换。
	- 对于最终预测，我们提出了一种新的双 DPT 头，旨在通过处理具有不同融合参数的同一组特征来联合输出深度和光线值。为了增强灵活性，该模型可以选择通过简单的相机编码器合并已知的相机姿势，从而使其能够适应各种实际设置。这种整体设计产生了一个干净且可扩展的架构，该架构直接继承了其预训练主干的可扩展属性。
	- 通过师生范式训练 Depth Anything 3，以统一不同的训练数据，这对于通才模型来说是必要的。我们的数据源包括各种格式，例如真实世界深度相机捕获（例如 Baruch 等人 5）、3D 重建（例如 Reizenstein 等人 68）和合成数据，其中真实世界深度可能质量较差（图 4）。为了解决这个问题，我们采用了受先前作品启发的伪标签策略 [112, 113]。具体来说，我们在合成数据上训练一个强大的教师单目深度模型，为所有现实世界数据生成密集的、高质量的伪深度。**至关重要的是，为了保持几何完整性，我们将这些密集的伪深度图与原始的稀疏或噪声深度对齐**。事实证明，这种方法非常有效，在不牺牲几何精度的情况下显着增强了标签细节和完整性。
- 为了进一步展示 Depth Anything 3 在推进其他 3D 视觉任务方面的基本能力，我们引入了一个具有挑战性的前馈新颖视图合成 (FF-NVS) 基准，其中包含 160 多个场景。我们坚持最小建模策略，并使用额外的 DPT 头微调我们的模型，以预测像素对齐的 3D 高斯参数。广泛的实验产生了两个关键发现：1）微调 NVS 的几何基础模型大大优于高度专业化的特定任务模型 [108]； 2) 增强的几何重建能力与改进的 FF-NVS 性能直接相关，将 Depth Anything 3 确立为该任务的最佳骨干。





- 输入：一张或多张图，不需要相机参数，通过模型