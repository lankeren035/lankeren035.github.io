用于同步相机位姿和 3DGS 重建的增量联合优化方法，减少局部最小值并确保全局一致性。 

强大的姿态估计模块，利用学习的 3D 先验知识进行准确的相机姿态估计。 

自适应八叉树锚形成策略，可显着减少内存使用，同时保持重建质量





假设我输入的是一个100帧的长视频，本文方法

1）先用最开始的3帧，输入MAST3R得到这3帧的：相机位姿、相机内参（如果没提供），深度图，稠密点云。

2）然后开始八叉书锚点生成过程：

- 自己先设定一个分辨率，根据这个分辨率可以知道体素大小，将稠密点云进行体素化（划分网格，将高斯分到各个格子），对每个体素内的点云数量进行统计，得到点云密度。
- 如果点云密度大于某个预定义阈值，就将这个体素均分成八个，体素边长减半。最多迭代L层，层数越深说明几何约复杂
- 如果点云密度小于某个预定义阈值，则将这个体素内的点云删除
- 整个过程中对于新生成的体素会检测与已有体素的重叠程度，如果重叠太多则删掉新的
- 最终得到的体素中心就是anchor，每个anchor里面高斯表示形式用Scaffold-GS

3）进行global optimization：

- 根据当前3dgs渲染出的图片与最开始的3张GT图算损失，反向传播更新：高斯参数，相机参数。
- 这里用的损失是：$L_ {total} = L_ {photo} + \lambda_ {depth} L_ {depth} + \lambda_{reproj} L_ {reproj}$
	- 第一个L是渲染图与GT图的L2损失
	- 第二个L是渲染深度图与MAST3R预测的深度图计算L2损失（初始的global optimization需要用这一项吗？感觉初始的时候是恒等，优化一会儿之后高斯位置可能变了，所以需要这个损失？）
	- 第三个L是约束相机参数预测的，他把 **MASt3R 的 3D 特征点云** 用当前相机参数投影到 2D，让这些投影点的位置，跟 **GT 图里 MASt3R 给出的 2D 特征点位置一样**

4）增量训练

- 纳入新的帧（一次纳入一帧）
- 对新的一帧t，将t-1帧和t帧输入mast3r进行特征点匹配，得到：匹配对（xi, x'i），第t-1帧的深度图Dt-1
- 将xt-1帧投影到3D（只能投影有匹配对的点）
- 对于新帧t，现在已经有了2D图和3D高斯（匹配点），用这些2D->3D对应关系，可以根据PnP+RANSAC得到粗略的相机位姿Tt
- 上面计算的Tt还是不太精确，用当前3dgs和Tt渲染出图然后和GT算l2损失，梯度更新Tt和当前帧的可见高斯（如果这一步计算Tt失败或者质量太差，则对之前所有帧和 3DGS 再跑一次全局 joint optimization。  这里怎么判断质量较差？)
- 对于长序列使用mast3r预测深度随着长度增加会有误差累计，导致scale drift（是啥？），对此使用一个scale进行修正，但是这里的scale修正这个Dt align是为了干嘛？这个Dt align不知道是在哪里使用的啊，当前帧的深度图不是也可以根据对应关系从2D得到吗？

- 对于新帧中的新区域也需要创建3d高斯点，对此，将上一帧的深度图Dt-1，根据特征点匹配情况做warp得到当前帧深度图Dt（这个步骤只有匹配对的点才能正确预测深度图），拿这个深度图跟mast3r预测的当前帧深度图逐像素计算误差，如果某个像素上深度差异较大，则认为该像素为新区域，由此可以得到一个mask，然后对新区域内的像素点用mast3r预测的深度图映射到3D（一个像素映射出一个3D点）
- 将新区域的点云也进行八叉树处理，这样就得到了新的3dgs模型

5）local优化

- 对于当前帧，v(t)表示当前帧的视锥可见高斯，取其前面k帧，对每一帧也计算其可见高斯，如果v(t)与v(t')的IoU低于阈值，则将t'帧剔除，这样就从k帧中赛选出符合要求的帧，然后对这些帧对应的可见高斯（以及当前帧），根据总损失只优化高斯参数，不优化pose
- 只对 window 中被选到的那些帧累积 loss；只对「当前帧视锥内可见的 Gaussians」回传梯度。

6）global优化

- 周期性的将优化范围扩大到所有高斯以及已处理帧的pose，使用总损失优化

- 总损失包括：渲染图损失，深度图损失，第三项怎么理解？















### 4.4 scale drift 是啥，以及 D_t^align 用在哪？

你写：

> 对于长序列…mast3r 深度会有误差累计，导致 scale drift（是啥？）…scale 修正这个 D_t align 是为了干嘛？…当前帧深度图不是也可以根据对应关系从2D得到吗？

先说“scale drift”：

- MASt3R 会给每帧一个「metric depth」，但在长序列、不规则运动里，**不保证所有帧的深度共同处在一个完美一致的 global scale 上**；
- 另外，随着 3DGS 不断 densify、prune、高斯挪位置，模型内部实际的「尺度」也在变；
- 所以长期跑下来，**MASt3R 的 depth 和 3DGS 的 depth 会逐渐不在同一个 scale 上**，这就是 scale drift。[arXiv+1](https://arxiv.org/html/2508.14041v1)

论文做法是：

1. 用当前 3DGS + 当前 pose $T_t$ 渲染这一帧的深度 $D^\text{render}_t$。

2. 拿 MASt3R 当前帧的深度 $D^\text{MASt3R}_t$，算一个 **最小二乘意义上的尺度 s_t**，Eq.(10)：
	$$
	s_t = \arg\min_s \|D^\text{render}_t - s \cdot D^\text{MASt3R}_t\|^2
	$$

3. 得到「尺度对齐」后的深度
	$$
	D^{\text{align}}_t = s_t \cdot D^\text{MASt3R}_t
	$$

这个 D_t^align 在两个地方用：[arXiv+1](https://arxiv.org/html/2508.14041v1)

1. **深度损失 $L_\text{depth}$**：
	$$
	L_\text{depth} = \|D^\text{render}_t - D^{\text{align}}_t\|^2
	$$
	强制 3DGS 的几何贴近 MASt3R 的深度先验，但要在同一尺度下比。

2. **遮挡 / 新区域检测**：

	- 用上一帧的渲染深度 $D^\text{render}_{t-1}$ 前向 warping 到当前帧，得到一个预测的深度图 $\tilde D_t$。
	- 把 $\tilde D_t$ 和 $D^{\text{align}}_t$ 做比较：差异大的地方就是「以前没看到、现在新暴露的区域」 → 得到 $M_\text{occ}$。