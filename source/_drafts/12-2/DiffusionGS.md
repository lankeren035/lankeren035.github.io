现有的前馈图像到 3D 方法主要依赖于 2D 多视图扩散模型，无法保证 3D 一致性。

这些方法在改变提示视图方向时很容易崩溃，并且主要处理以对象为中心的情况。

单阶段 3D 扩散模型 DiffusionGS

DiffusionGS 在每个时间步直接输出 3D 高斯点云，以强制视图一致性

  现有的前馈图像转 3D 方法主要是两阶段的 [30,36,68,71]。他们首先采用 2D 扩散模型来生成分块多视图图像，然后将多视图图像输入 3D 重建模型。如果扩散中没有 3D 模型，这些方法无法强制视图一致性，并且在提示视图方向改变时很容易崩溃。

另一条研究较少的技术路线[2,64,74]是训练具有2D渲染损失的3D扩散模型。然而，这些方法主要依赖于三平面神经辐射场（NeRF）[44]。 NeRF 的体积渲染非常耗时，并且三平面分辨率有限，导致模型无法扩展到更大的场景。

我们提出了一种基于单阶段 3D 高斯泼溅 (3DGS) [25] 的新型扩散模型 DiffusionGS，用于从单个视图生成 3D 对象和场景重建。

与之前仅从给定视图预测 3D 表示的单视图场景重建方法 [66,69,79] 不同，DiffusionGS 沿着相机轨迹生成其他视图，以预测更精细和结构化的高斯点云。

第一类[18,23,49,50,59,85,89]使用对点云或网格等3D模型的直接监督，这在实践中很难获得。

- diffGS

第二种方法 [56,67,70,77,78] 使用 SDS 损失 [54] 从 2D 扩散中提取 3D 模型。然而，这些方法需要耗时的每个资产优化。

- 先初始化一个场景A，然后根据输入视角渲染出来一个图片x0.   然后进行前向加噪：对x0加噪 epsilon得到xt。  然后我把xt输入diffusion的噪声估计网络预测出来一个epsilon'，然后拿epsilon'跟epsilon来计算损失，反向传播更新3d模型参数

第三类 [6,17,37,40,41,47,58] 添加相机姿势作为输入条件来微调 2D 扩散模型以渲染固定的新颖视图。然而，这些方法不能保证 3D 一致性，并且当视图方向发生即时变化时很容易崩溃。

- 还是 2D diffusion，只是学会“带相机条件生成多视图”，用3d渲染出来的多视图做GT监督，训练的是diffusion，一致性差。

最后一类 [2,3,64,74] 训练具有 2D 渲染损失的 3D 扩散模型。然而，这些主要基于三平面-NeRF的方法存在三平面分辨率有限、体绘制速度慢的问题。

- 扩散的对象是 3D 场本身，只是通过 2D 渲染来给它监督







下面我来做一个总结，你看看对不对，首先将普朗克坐标与本文坐标归为A类，传统坐标为B类，这两类的区别在于，对于远点和近点（由于真实世界物体表面不规则，比较复杂，如果两条射线摄像同一个表面的同一个点，那么通常这两条射线是比较相似比价平行的的那种，因为如果旋转的角度打了一点的话，他跟表面的交点就变了）以远点和近点为例，A类坐标可以根据垂足距离体现出远点近点的区别；而B类坐标无法提现。     再看普朗克坐标和本文坐标的区别，由于前面说的3d物体表面比较复杂，同一个表面点被同两条射线观察到的情况下，这两条射线一般是比较接近的，比较平行的那种情况（围绕表面焦点旋转射线，旋转的角度是比较小的），因此对于近点，他们的垂足在空间距离上本身就离得近，通过Pref直接一眼就能看出来，而普朗克距离的m是通过m的方向变化类似体现出来的，但是这样类似没那么直观，不容易聚类，这种对3d向量方向类似的聚类没有3d空间距离近的点的这种聚类来的直接，没有这种聚类好学习。  是这个意思吗？