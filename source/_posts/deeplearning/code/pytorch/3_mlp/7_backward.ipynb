{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 3.7 前向传播、反向传播和计算图\n",
    "date: 2024-2-7 14:00:00\n",
    "tags: [机器学习,pytorch]\n",
    "categories: [机器学习]\n",
    "comment: true\n",
    "toc: true\n",
    "---\n",
    "#  \n",
    "<!--more-->\n",
    "# 7 前向传播、反向传播和计算图\n",
    "- 以带dropout的单层隐藏层mlp为例\n",
    "## 7.1 前向传播\n",
    "- 按顺序（从输入层到输出层）计算和存储神经网络中每层的结果s。\n",
    "    - 1）输入是$ /mathbf{x} /in /mathbb{R}^ d $，隐藏层不含偏置,得到中间变量：\n",
    "    $$/mathbf{z}=/mathbf{W}^{(1)}/mathbf{x}$$\n",
    "    - 2）激活：\n",
    "    $$/mathbf{h}=/phi(/mathbf{z})$$\n",
    "    - 3）输出：\n",
    "    $$/mathbf{o}=/mathbf{W}^{(2)}/mathbf{h}$$\n",
    "    - 4）损失：\n",
    "    $$L=l(/mathbf{o},y)$$\n",
    "    - 5）正则化项（权重衰减）：\n",
    "        $$s=/frac{/lambda}{2}(||/mathbf{W}^{(1)}||^2_F+||/mathbf{W}^{(2)}||^2_F)$$\n",
    "        - 其中矩阵的Frobenius范数是将矩阵展平成向量后应用$L_2$范数。\n",
    "    - 6）目标函数：\n",
    "    $$J=L+s$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 前向传播计算图\n",
    "- 7.1的计算图：\n",
    "![](D:/blog/themes/yilia/source/img/deeplearning/code/pytorch/3_mlp/7/1.png)\n",
    "![](img/deeplearning/code/pytorch/3_mlp/7/1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 反向传播\n",
    "- 根据微积分中的链式规则，按相反的顺序从输出层到输入层遍历网络。\n",
    "- 链式法则：假设有$Y=f(X)$和$Z=g(Y)$，有:\n",
    "    $$\\frac{\\partial{Z}}{\\partial{X}}=prod(\\frac{\\partial{Z}}{\\partial{Y}},\\frac{\\partial{Y}}{\\partial{X}})$$\n",
    "    - prod表示一定的操作\n",
    "- 针对前向传播计算图计算反向梯度：\n",
    "    $$ \\frac{ \\partial J}{ \\partial L} = 1 \\mathrm{ ~ and ~ } \\frac{ \\partial J}{ \\partial s} = 1 $$\n",
    "\n",
    "    $$\\frac{\\partial J}{\\partial\\mathbf{o}}=\\text{prod}\\left(\\frac{\\partial J}{\\partial L},\\frac{\\partial L}{\\partial\\mathbf{o}}\\right)=\\frac{\\partial L}{\\partial\\mathbf{o}}\\in\\mathbb{R}^q$$\n",
    "\n",
    "    $$ \\frac{ \\partial s}{ \\partial \\mathbf{W}^ {(1)}}= \\lambda \\mathbf{W}^ {(1)} \\mathrm{~ and ~} \\frac{ \\partial s}{ \\partial \\mathbf{W}^ {(2)}}= \\lambda \\mathbf{W}^ {(2)}$$\n",
    "\n",
    "    $$\\frac{\\partial J}{\\partial\\mathbf{W}^{(2)}}=\\mathrm{prod}\\left(\\frac{\\partial J}{\\partial\\mathbf{o}},\\frac{\\partial\\mathbf{o}}{\\partial\\mathbf{W}^{(2)}}\\right)+\\mathrm{prod}\\left(\\frac{\\partial J}{\\partial s},\\frac{\\partial s}{\\partial\\mathbf{W}^{(2)}}\\right)=\\frac{\\partial J}{\\partial\\mathbf{o}}\\mathbf{h}^\\top+\\lambda\\mathbf{W}^{(2)}$$\n",
    "\n",
    "    $$\\frac{\\partial J}{\\partial\\mathbf{h}}=\\text{prod}\\left(\\frac{\\partial J}{\\partial\\mathbf{o}},\\frac{\\partial\\mathbf{o}}{\\partial\\mathbf{h}}\\right)=\\mathbf{W}^{(2)^\\top}\\frac{\\partial J}{\\partial\\mathbf{o}}$$\n",
    "    由于激活函数$\\phi$是按元素计算的，因此计算$\\frac{\\partial J}{\\partial \\mathbf{z}}$需要使用按元素乘法运算符：\n",
    "    $$\\frac{\\partial J}{\\partial\\mathbf{z}}=\\mathrm{prod}\\left(\\frac{\\partial J}{\\partial\\mathbf{h}},\\frac{\\partial\\mathbf{h}}{\\partial\\mathbf{z}}\\right)=\\frac{\\partial J}{\\partial\\mathbf{h}}\\odot\\phi^{\\prime}\\left(\\mathbf{z}\\right)$$\n",
    "    \n",
    "    $$\\begin{aligned}\\frac{\\partial J}{\\partial\\mathbf{W}^{(1)}}&=\\text{prod}\\left(\\frac{\\partial J}{\\partial\\mathbf{z}},\\frac{\\partial\\mathbf{z}}{\\partial\\mathbf{W}^{(1)}}\\right)+\\text{prod}\\left(\\frac{\\partial J}{\\partial s},\\frac{\\partial s}{\\partial\\mathbf{W}^{(1)}}\\right)=\\frac{\\partial J}{\\partial\\mathbf{z}}\\mathbf{x}^\\top+\\lambda\\mathbf{W}^{(1)}\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 训练神经网络\n",
    "- 训练神经网络时，对于前向传播，沿着依赖的方向遍历计算图并计算路径上的所有变量。然后将这些用于反向传播，其中计算顺序与计算图的相反。\n",
    "- 一方面，在前向传播期间计算正则项取决于模型参数W(1)和 W(2)的当前值。它们是由优化算法根据最近迭代的反向传播给出的。\n",
    "- 另一方面，反向传播期间参数$\\frac{\\partial J}{\\partial W^{(2)}}$的梯度计算，取决于由前向传播给出的隐藏变量h的当前值。\n",
    "- 因此，在训练神经网络时，在初始化模型参数后，我们交替使用前向传播和反向传播，利用反向传播给出的梯度来更新模型参数。\n",
    "- 反向传播重复利用前向传播中存储的中间值，以避免重复计算。带来的影响之一是我们需要保留中间值，直到反向传播完成。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
