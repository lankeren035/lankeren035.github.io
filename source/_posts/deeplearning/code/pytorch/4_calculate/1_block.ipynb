{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 4.1 层和块\n",
    "date: 2024-4-18 14:00:00\n",
    "tags: [机器学习,pytorch]\n",
    "categories: [机器学习]\n",
    "comment: true\n",
    "toc: true\n",
    "---\n",
    "#  \n",
    "<!--more-->\n",
    "# 1 层和块\n",
    "- 单个神经网络：\n",
    "    - 一个输入\n",
    "    - 标量输出\n",
    "    - 一组相关参数，这些参数可以通过学习而优化\n",
    "- 层：\n",
    "    - 一组输入\n",
    "    - 一组输出\n",
    "    - 一组可调参数\n",
    "- 从编程的角度看，块由类来表示。通常需要定义一个将输入转换成输出的forward函数，并且必须存储任何必须的参数。\n",
    "- 定义一个网络：256个单元和ReLU的全连接隐藏层，10个隐藏单元且不带激活函数的全连接输出层\n",
    "    - nn.Sequential是一种特殊的module，表示一个块，维护了一个由module组成的有序列表\n",
    "    - net(x)相当于net.__call__(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0302, -0.1207, -0.0915,  0.0522, -0.3685,  0.0474,  0.0665, -0.0055,\n",
      "         -0.1445, -0.1954],\n",
      "        [ 0.0275, -0.1345,  0.0009,  0.0987, -0.4689,  0.0405,  0.0012, -0.0243,\n",
      "         -0.2954, -0.1414]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "net= nn.Sequential( #一种特殊的module，表示一个块，维护了一个由module组成的有序列表\n",
    "    nn.Linear(20,256),\n",
    "    nn.ReLU(), \n",
    "    nn.Linear(256,10))\n",
    "\n",
    "X = torch.rand(2,20)\n",
    "print(net(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 自定义块\n",
    "- 每个块必须提供的功能：\n",
    "    - 数据输入forward函数得到输出\n",
    "    - 计算输出关于输入的梯度，通过backward函数\n",
    "    - 存储和访问前向传播计算所需要的参数\n",
    "    - 初始化模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2163,  0.0644, -0.1128, -0.2947,  0.0708,  0.0907, -0.0680, -0.0381,\n",
      "         -0.0843,  0.0921],\n",
      "        [ 0.2196,  0.0087,  0.0257, -0.1403, -0.0191,  0.0435, -0.1980,  0.0350,\n",
      "          0.0158,  0.0848]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    # 用模型参数声明层。声明两个全连接层\n",
    "    def __init__(self):\n",
    "        super().__init__() #Module的构造函数进行必要的初始化\n",
    "        self.hidden = nn.Linear(20,256) #隐藏层\n",
    "        self.out = nn.Linear(256,10) #输出层\n",
    "\n",
    "    # 前行传播，如何根据输入x返回所需的模型输出\n",
    "    def forward(self,X):\n",
    "        return self.out(F.relu(self.hidden(X)))\n",
    "    \n",
    "net = MLP()\n",
    "print(net(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 顺序块\n",
    "- 构建自己简化的Sequential类需要\n",
    "    - 将block逐个追加到列表中\n",
    "    - forward函数中，将输入按顺序传递"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1585, -0.0420,  0.3813, -0.3592, -0.0889,  0.0362, -0.0543, -0.0557,\n",
      "         -0.2022,  0.0183],\n",
      "        [-0.0438, -0.1248,  0.5774, -0.3087, -0.0576, -0.0479,  0.0954, -0.2362,\n",
      "         -0.2333, -0.1394]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args) -> None:\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            # module是Module子类的一个实例\n",
    "            # _modules中，_module的类型是OrderedDict\n",
    "            #为啥每个Module都有一个_modules属性，为啥不用python列表？\n",
    "            # _modules优点：在模块的参数初始化过程中，系统知道在_modules字典中查找需要初始化参数的子块。\n",
    "            self._modules[str(idx)] = module\n",
    "    def forward(self, X):\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X\n",
    "    \n",
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256,10))\n",
    "print(net(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 在前向传播函数中执行代码\n",
    "- 有时我们希望合并既不是上一层的结果也不是可更新参数的项，成为常数参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.2985, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # 不计算梯度的随机权重参数，因此其在训练期间保持不变\n",
    "        self.rand_weight = torch.rand((20,20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20,20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        # 使用创建的常量参数以及relu和mm函数\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        # 复用全连接层。相当于两个全连接成共享参数\n",
    "        X = self.linear(X)\n",
    "        # 控制流：\n",
    "        while X.abs().sum() > 1: #l1范数\n",
    "            X/=2\n",
    "        return X.sum()\n",
    "net = FixedHiddenMLP()\n",
    "print(net(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 效率\n",
    "我们在一个高性能的深度学习库中进行了大量的字典查找、代码执行和许多其他的Python代码。Python的问题全局解释器锁是众所周知的。在深度学习环境中，我们担心速度极快的GPU可能要等到CPU运行Python代码后才能运行另一个作业。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
