{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 1.1 pytorch数据操作\n",
    "date: 2023-11-11 14:00:00\n",
    "tags: [机器学习,pytorch]\n",
    "categories: [机器学习]\n",
    "comment: false\n",
    "toc: true\n",
    "---\n",
    "# \n",
    "<!--more-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. pytorch数据操作\n",
    "\n",
    "- pytorch与tensorflow中的Tensor类似于numpy的ndarray\n",
    "- pytorch中的Tensor可以在GPU上运行\n",
    "- pytorch中的Tensor可以用于自动求导\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 创建张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])\n",
      "tensor([[[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]]])\n",
      "tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n",
      "tensor([[-0.0847, -0.2406,  0.1735, -1.5543],\n",
      "        [-0.2820, -0.6689,  0.0565,  0.4746],\n",
      "        [ 0.9841, -1.6116, -0.1587, -1.5121]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "# 1. 导入torch，不是pytorch\n",
    "import torch\n",
    "\n",
    "# 2. 创建范围张量\n",
    "x=torch.arange(12) # 0-11,默认为int64，可以指定dtype，默认存储在CPU上\n",
    "y=torch.arange(12,dtype=torch.float32) # 指定dtype\n",
    "\n",
    "# 3. 创建全0张量\n",
    "zeros=torch.zeros(2,3,4) # 2*3*4的全0张量\n",
    "\n",
    "# 4. 创建全1张量\n",
    "ones=torch.ones(2,3,4) # 2*3*4的全1张量\n",
    "\n",
    "# 5. 创建采样张量\n",
    "sample=torch.randn(3,4) #从标准高斯分布中采样\n",
    "\n",
    "# 6. 列表张量\n",
    "lists=torch.tensor([[1,2,3],[4,5,6]])\n",
    "\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "print(zeros)\n",
    "print(ones)\n",
    "print(sample)\n",
    "print(lists)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 张量的属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x的规模： torch.Size([12])\n",
      "x中元素个数： 12\n",
      "改变x的形状： tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "自动计算的形状： tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "# 3. 查看张量的形状\n",
    "print('x的规模：',x.shape)\n",
    "\n",
    "# 4. 查看张量的元素总数\n",
    "print('x中元素个数：',x.numel())\n",
    "\n",
    "# 5. 改变张量的形状\n",
    "x=x.reshape(3,4)\n",
    "print('改变x的形状：',x)\n",
    "print('自动计算的形状：',x.reshape(-1,6)) # -1表示自动计算这个维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 运算符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "按元素加法： tensor([0, 2, 4, 6])\n",
      "按元素减法： tensor([0, 0, 0, 0])\n",
      "按元素乘法： tensor([0, 1, 4, 9])\n",
      "按元素除法： tensor([nan, 1., 1., 1.])\n",
      "按元素幂运算： tensor([ 1,  1,  4, 27])\n",
      "按元素开方： tensor([0.0000, 1.0000, 1.4142, 1.7321])\n",
      "求幂运算： tensor([ 1.0000,  2.7183,  7.3891, 20.0855])\n",
      "沿行合并： tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "沿列合并： tensor([[0., 0., 0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1., 1., 1.]])\n",
      "tensor([[False, False, False, False],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False, False]])\n",
      "求和： tensor(12.)\n"
     ]
    }
   ],
   "source": [
    "x=torch.arange(4)\n",
    "\n",
    "# 1. 按元素运算\n",
    "print('按元素加法：',x+x)\n",
    "print('按元素减法：',x-x)\n",
    "print('按元素乘法：',x*x)\n",
    "print('按元素除法：',x/x)\n",
    "print('按元素幂运算：',x**x)\n",
    "print('按元素开方：',x.sqrt())\n",
    "print('求幂运算：',torch.exp(x))\n",
    "\n",
    "# 2. 矩阵运算\n",
    "\n",
    "# 3. 合并张量(记得加括号)\n",
    "x=torch.zeros(12).reshape(3,4)\n",
    "y=torch.ones(12).reshape(3,4)\n",
    "print('沿行合并：',torch.cat((x,y),dim=0)) # 沿行合并\n",
    "print('沿列合并：',torch.cat((x,y),dim=1)) # 沿列合并\n",
    "\n",
    "# 4. 逻辑运算\n",
    "print(x==y)\n",
    "\n",
    "# 5. 求和\n",
    "print('求和：',y.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 广播机制\n",
    "- 对两个形状不同的张量按元素运算时，可能会触发广播机制：先适当复制元素使这两个张量形状相同后再按元素运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([[0],\n",
      "        [1],\n",
      "        [2]])\n",
      "b: tensor([[0, 1]])\n",
      "a+b: tensor([[0, 1],\n",
      "        [1, 2],\n",
      "        [2, 3]])\n"
     ]
    }
   ],
   "source": [
    "a=torch.arange(3).reshape(3,1)\n",
    "b=torch.arange(2).reshape(1,2)\n",
    "print('a:',a)\n",
    "print('b:',b)\n",
    "print('a+b:',a+b) #通常沿着长度为1的维度进行广播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 索引和切片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "x[-1]: tensor([0., 0., 0., 0.])\n",
      "x[1:3]: tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "x[1,2]=9后 x: tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 9., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "x[0:2,:]=12后 x: tensor([[12., 12., 12., 12.],\n",
      "        [12., 12., 12., 12.],\n",
      "        [ 0.,  0.,  0.,  0.]])\n"
     ]
    }
   ],
   "source": [
    "print('x:',x)\n",
    "print('x[-1]:',x[-1])\n",
    "print('x[1:3]:',x[1:3]) #选择第2和第3个元素\n",
    "x[1,2]=9 # 修改元素\n",
    "print('x[1,2]=9后 x:',x)\n",
    "x[0:2,:]=12 # 修改一行\n",
    "print('x[0:2,:]=12后 x:',x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 节省内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z: tensor([[12., 12., 12., 12.],\n",
      "        [12., 12., 12., 12.],\n",
      "        [ 0.,  0.,  0.,  0.]])\n",
      "id(z)==before: False\n",
      "id(z)==before: True\n"
     ]
    }
   ],
   "source": [
    "z=torch.zeros(1)\n",
    "before=id(z)\n",
    "z=z+x #对于这种类似列表的数据结构，加号会导致新的内存分配\n",
    "print('z:',z)\n",
    "print('id(z)==before:',id(z)==before) # False，说明z指向了新的地址\n",
    "\n",
    "z=torch.zeros(3,4) #使用如下操作可以避免新的内存分配，但是无法广播，所以这里要求x和z的形状一致\n",
    "before=id(z)\n",
    "z+=x # +=不会导致新的内存分配\n",
    "z[:]=x+z # 也不会导致新的内存分配\n",
    "print('id(z)==before:',id(z)==before) # True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 转换为其他python对象\n",
    "- tensor与numpy数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12. 12. 12. 12.]\n",
      " [12.  3. 12. 12.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "tensor([[12., 12., 12., 12.],\n",
      "        [12.,  3., 12., 12.],\n",
      "        [ 0.,  0.,  0.,  0.]])\n",
      "[[12. 12. 12. 12.]\n",
      " [12.  3. 12. 12.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "tensor([[12., 12., 12., 12.],\n",
      "        [12.,  3., 12., 12.],\n",
      "        [ 0.,  0.,  0.,  0.]])\n",
      "tensor([[12., 12., 12., 12.],\n",
      "        [12.,  3., 12., 12.],\n",
      "        [ 0.,  0.,  0.,  0.]])\n"
     ]
    }
   ],
   "source": [
    "A=x.numpy() # 将张量转换为numpy数组\n",
    "B=torch.tensor(A) # 将numpy数组转换为张量\n",
    "print(A,B,sep='\\n')\n",
    "\n",
    "A[1,1]=3\n",
    "print(A,B,x,sep='\\n') #A与X共享内存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 张量转为python标量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.3000])\n",
      "1.2999999523162842\n",
      "1.2999999523162842\n"
     ]
    }
   ],
   "source": [
    "a=torch.tensor([1.3])\n",
    "print(a)\n",
    "\n",
    "#由于pytorch中小数用32位，python中用64位；尝试输出.20f就能发现精确度问题\n",
    "#由于舍入（四舍五入到最接近的偶数），1.3在pytorch中表示是1.299多，而在python中是1.300多\n",
    "print(a.item()) #使用item函数\n",
    "print(float(a)) #python内置函数"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
