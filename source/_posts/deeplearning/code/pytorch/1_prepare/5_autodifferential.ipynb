{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 5. 自动微分\n",
    "date: 2024-2-1 14:00:00\n",
    "tags: [深度学习,机器学习,pytorch]\n",
    "categories: [深度学习]\n",
    "comment: true\n",
    "toc: true\n",
    "---\n",
    "#\n",
    "<!--more-->\n",
    "# 5. 自动微分\n",
    "- 深度学习框架通过自动计算导数，即自动微分（automatic differentiation）来加快求导。根据设计好的模型，系统会构建一个计算图（computational graph），来跟踪计算是哪些数据通过哪些操作组合起来产生输出。自动微分使系统能够随后反向传播梯度。这里，反向传播（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。\n",
    "## 5.1 \n",
    "- 对函数$y=2\\mathbf{x}^{\\top}\\mathbf{x}$关于列向量$\\mathbf{x}$求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.])\n",
      "tensor([0., 1., 2., 3.], requires_grad=True)\n",
      "tensor(28., grad_fn=<MulBackward0>)\n",
      "None\n",
      "tensor([ 0.,  4.,  8., 12.])\n",
      "tensor([ 1.,  5.,  9., 13.])\n",
      "tensor([1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#1 创建变量x\n",
    "x=torch.arange(4.0)\n",
    "y=2*torch.dot(x,x)\n",
    "print(x)\n",
    "\n",
    "#2 存储梯度 （一个标量函数关于向量x的梯度是向量，并且与x具有相同的形状。）\n",
    "x.requires_grad_(True) #等价于x=torch.arange(4.0,requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "#3 计算y\n",
    "y=2*torch.dot(x,x) #点积\n",
    "print(y)\n",
    "\n",
    "#4 反向传播计算梯度\n",
    "print(x.grad) #默认为None\n",
    "y.backward() #反向传播\n",
    "print(x.grad) #梯度为4x\n",
    "\n",
    "#5 阻止跟踪\n",
    "y=x.sum() #y=6\n",
    "y.backward()\n",
    "print(x.grad) #x的梯度会累加\n",
    "x.grad.zero_() #清除梯度后就不会累加了\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 非标量变量的反向传播\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。\n",
    "# 本例只想求偏导数的和，所以传递一个1的梯度是合适的\n",
    "x.grad.zero_()\n",
    "y=x*x #这里y是向量\n",
    "y.sum().backward() #这里y.sum()是标量,所以可以调用backward，相当于y.backward(torch.ones(len(x)))\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 分离计算\n",
    "- 将某些计算移动到记录的计算图之外。（作为常数处理）\n",
    "    - 假设$y=f(x), z=g(x,y)$。我们想计算z关于x的梯度，但由于某种原因，希望将y视为一个常数，并且只考虑到x在y被计算后发挥的作用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 4., 9.])\n",
      "tensor([ 0.,  3., 12., 27.])\n"
     ]
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y=x*x\n",
    "u=y.detach() #分u相当于一个常数，不需要求梯度\n",
    "z=u*x\n",
    "z.sum().backward()\n",
    "print(x.grad) #u\n",
    "\n",
    "x.grad.zero_()\n",
    "z=y*x\n",
    "z.sum().backward()\n",
    "print(x.grad) #3x^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 python控制流的梯度计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(a):\n",
    "    b=a*2\n",
    "    while b.norm()<1000: #经过while后b=ka\n",
    "        b=b*2\n",
    "    if b.sum()>0:\n",
    "        c=b #c=kb=ka\n",
    "    else:\n",
    "        c=100*b #c=100kb=100ka\n",
    "    return c #ka\n",
    "a=torch.randn(size=(),requires_grad=True)\n",
    "D=f(a) #\n",
    "D.backward()\n",
    "a.grad==D/a #梯度就是k=D/a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
