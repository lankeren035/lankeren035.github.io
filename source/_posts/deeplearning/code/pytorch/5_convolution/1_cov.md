---
title: 5.1 卷积神经网络
date: 2024-4-20 18:00:00
tags: [机器学习,pytorch]
categories: [机器学习]
comment: true
toc: true
---
#  
<!--more-->
# 1 卷积神经网络
- 一种强大的、为图像数据而设计的神经网络
- mlp十分适合表格数据，但是对于高维数据，mlp不实用了
## 1.1 不变性
- 平移不变性：不管检测对象出现在图像的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应。
- 局部性：神经网络的前几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系
## 1.2 多层感知机的限制
- 假设输入图像：$\mathbf{X}$，隐藏表示：$\mathbf{H}$。$\mathbf{X}$与$\mathbf{H}$具有相同的形状。权重矩阵：$W$，偏置：$\mathbf{U}$。全连接层可以表示成：$V$是对$W$的一个偏移。

$$ \begin{aligned} \left[ \mathbf{H} \right]_ {i,j} & = [ \mathbf{U}]_ {i,j}+ \sum_k \sum_l[ \mathbf{W}]_ {i,j,k,l}[ \mathbf{X}]_ {k,l} \\ 
&= [ \mathbf{U}]_ {i,j}+ \sum_a \sum_b[ \mathbf{V}]_ {i,j,a,b}[ \mathbf{X}]_ {i+a,j+b} \end{aligned}$$

- 索引a和b通过在正偏移和负偏移之间移动覆盖了整个图像。对于像素$\mathbf{H}_ {i,j}$，可以通过在$\mathbf{X}$中以(i,j)为中心对像素进行加权求和得到，加权使用的权重为$V_ {i,j,a,b}$。
### 1.2.1 平移不变性
- 检测对象在输入$\mathbf{X}$中的平移，应该仅导致隐藏表示$\mathbf{H}$中的平移。也就是说，v和$\mathbf{U}$实际上不依赖于(i,j)的值，即$V_ {i,j,a,b}=\mathbf{V}_ {a,b}$。且 $\mathbf{U}$是一个常数。因此， $\mathbf{H}$的计算可以简化为：
$$\mathbf{H}_ {i,j} = u+\sum_a \sum_b \mathbf{V}_ {a,b} \mathbf{X}_ {i+a,j+b}$$
- 这就是卷积（convolution）。使用系数$\mathbf{V}_ {a,b}$对位置(i,j)附近的像素(i+a,j+b)进行加权求和，得到$\mathbf{H}_ {i,j}$。注意，$\mathbf{V}_ {a,b}$的系数比$V_{i,j,a,b}$少得多，因为前者不再依赖于图像中的位置。
### 1.2.2 局部性
- 为了收集用来训练参数$\mathbf{H}_ {i,j}$的相关信息，我们不应该偏离到距离(i,j)很远的地方。这意味着在$|a|>\Delta$或$|b|>\Delta$的范围之外，我们可以设置$\mathbf{V}_ {a,b}=0$。这样，我们可以将$\mathbf{H}_ {i,j}$的计算简化为：
$$\mathbf{H}_ {i,j}=u+ \sum_ {a=- \Delta}^ {\Delta} \sum_ {b=- \Delta}^ {\Delta} \mathbf{V}_ {a,b} \mathbf{X}_ {i+a,j+b}$$
- 上述公式就是卷积层的计算公式。
- 卷积核（滤波器）：$\mathbf{V}$，卷积核大小：$2\Delta+1$。
- 参数大幅减少的代价是，确定每个隐藏活性值时，每一层只包含局部的信息。
- 以上所有的权重学习都将依赖于归纳偏置。当这种偏置与现实相符时，就能得到样本有效的模型，模型泛化较好；如果这种偏置与现实不符，比如当图像不满足平移不变性时，模型难以拟合训练数据。

## 1.3 卷积
- 数学上，两个函数之间的卷积定义为：
    - 卷积相当于把一个函数翻转并位移x时，测量f和g之间的重叠。
$$(f*g)(\mathbf{x})=\int f(\mathbf{z})g(\mathbf{x-z})d\mathbf{z}$$




- 在离散情况下，卷积定义为：
$$(f*g)(i)=\sum_a\sum_bf(a)g(i-a)$$

- 对于二维张量，则为f的索引(a,b)和g的索引(i-a,j-b)上的对应加和：
    - 卷积核：g，输入：f，输出：f*g

$$ (f*g)(i,j)=\sum_a \sum_b f(a,b) g(i-a,j-b) $$

## 1.4 通道
- 由于输入图像是三维的，隐藏表示$\mathbf{H}$也最好采用三维张量。因此，隐藏表示可以想象成一系列具有二维张量的通道。这些通道也成为特征映射（feature maps）。
- 因为每个通道都向后续层提供一组空间化的特征学习。在靠近输入的底层，一些通道专门识别边缘，一些通道专门识别纹理。

    
